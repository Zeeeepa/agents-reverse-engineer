{
  "runId": "2026-02-09T21:27:20.854Z",
  "startTime": "2026-02-09T21:27:20.854Z",
  "endTime": "2026-02-09T21:39:57.996Z",
  "entries": [
    {
      "timestamp": "2026-02-09T21:27:20.905Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/custom.ts\n\n```typescript\n/**\n * Custom pattern filter for file discovery.\n *\n * Allows users to specify additional exclusion patterns in their configuration.\n * Uses the `ignore` library for gitignore-style pattern matching.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a custom filter that excludes files matching user-provided patterns.\n *\n * Patterns use gitignore syntax and are checked against relative paths from\n * the root directory. If no patterns are provided, the filter passes all files.\n *\n * @param patterns - Array of gitignore-style patterns to exclude\n * @param root - Root directory for converting absolute paths to relative\n * @returns A FileFilter that checks paths against custom patterns\n *\n * @example\n * ```typescript\n * const filter = createCustomFilter(['*.log', 'tmp/**', 'secret.txt'], '/project');\n * filter.shouldExclude('/project/debug.log'); // true\n * filter.shouldExclude('/project/src/app.ts'); // false\n * ```\n */\nexport function createCustomFilter(patterns: string[], root: string): FileFilter {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Add all patterns to the ignore instance\n  if (patterns.length > 0) {\n    ig.add(patterns);\n  }\n\n  return {\n    name: 'custom',\n\n    shouldExclude(absolutePath: string): boolean {\n      // If no patterns, nothing to exclude\n      if (patterns.length === 0) {\n        return false;\n      }\n\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createCustomFilter() implements gitignore-style pattern matching for user-defined exclusion rules during file discovery.**\n\n## Exported Interface\n\n- **`createCustomFilter(patterns: string[], root: string): FileFilter`** — Constructs FileFilter that excludes paths matching gitignore-style patterns relative to root directory. Returns filter with `name: 'custom'` and `shouldExclude()` method. Empty patterns array bypasses all exclusion logic (returns false for all paths).\n\n## Pattern Matching Implementation\n\nUses `ignore` library to parse gitignore syntax. Creates `Ignore` instance via `ignore()`, adds patterns via `ig.add(patterns)`, evaluates paths via `ig.ignores(relativePath)`. Normalizes root via `path.resolve(root)` to ensure consistent absolute path comparison.\n\n## Path Transformation Logic\n\nConverts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)` before pattern matching (ignore library requires relative paths). Bypasses exclusion for paths outside root hierarchy (relative path starts with `..`) or empty relative paths. Pattern evaluation only runs if `patterns.length > 0`.\n\n## Filter Contract Compliance\n\nImplements `FileFilter` interface from `../types.js` with required `name: 'custom'` property and `shouldExclude(absolutePath: string): boolean` method. Integrates with filter chain orchestrated by discovery walker.\n\n## Example Behavior\n\nPatterns `['*.log', 'tmp/**', 'secret.txt']` with root `/project`:\n- `/project/debug.log` → relative `debug.log` → matches `*.log` → returns `true`\n- `/project/src/app.ts` → relative `src/app.ts` → no match → returns `false`\n- `/other/file.log` → relative `../other/file.log` → outside root → returns `false`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 431,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10738,
      "latencyMs": 20486,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/custom.ts",
          "sizeBytes": 1847
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.908Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/gitignore.ts\n\n```typescript\n/**\n * Gitignore pattern filter for file discovery.\n *\n * Uses the `ignore` library to parse and match .gitignore patterns.\n * This filter loads the root .gitignore file and checks paths against\n * the patterns to determine exclusion.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a gitignore filter that excludes files matching .gitignore patterns.\n *\n * @param root - The root directory containing the .gitignore file\n * @returns A FileFilter that checks paths against gitignore patterns\n *\n * @example\n * ```typescript\n * const filter = await createGitignoreFilter('/path/to/project');\n * if (filter.shouldExclude('/path/to/project/dist/bundle.js')) {\n *   console.log('File is gitignored');\n * }\n * ```\n */\nexport async function createGitignoreFilter(root: string): Promise<FileFilter> {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Load .gitignore from root if it exists\n  const gitignorePath = path.join(normalizedRoot, '.gitignore');\n  try {\n    const content = await fs.readFile(gitignorePath, 'utf-8');\n    ig.add(content);\n  } catch {\n    // No .gitignore file, filter will pass everything through\n  }\n\n  return {\n    name: 'gitignore',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      // CRITICAL: The ignore library treats paths differently based on trailing slash\n      // We check the path as-is (for files) - directory handling would need trailing slash\n      // Since our walker returns files only, we don't append slash here\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createGitignoreFilter() builds a FileFilter that excludes paths matching .gitignore patterns using the `ignore` library.**\n\n## Exported Interface\n\n```typescript\nasync function createGitignoreFilter(root: string): Promise<FileFilter>\n```\n\nReturns a `FileFilter` object with `name: 'gitignore'` and `shouldExclude(absolutePath: string): boolean` method.\n\n## Implementation Details\n\ncreateGitignoreFilter() normalizes `root` via `path.resolve()`, attempts to read `.gitignore` from `path.join(normalizedRoot, '.gitignore')`, passes content to `ignore.add()` if file exists, silently skips if ENOENT occurs (try-catch with empty catch block).\n\nshouldExclude() converts `absolutePath` to relative via `path.relative(normalizedRoot, absolutePath)`, returns `false` immediately if relativePath is empty or starts with `'..'` (path outside root), calls `ig.ignores(relativePath)` otherwise.\n\n## Critical Constraints\n\nThe `ignore` library requires relative paths without leading slashes. shouldExclude() does NOT append trailing slash to paths, assuming walker returns files only (not directories). Directory exclusion would require appending `/` to directory paths before calling `ig.ignores()`.\n\nReturns `false` for paths outside root tree to prevent exclusion of unrelated filesystem locations.\n\n## Dependencies\n\n- `ignore` library (`import ignore, { type Ignore } from 'ignore'`) for .gitignore pattern matching\n- `fs.promises.readFile()` for async .gitignore content loading\n- `path.resolve()`, `path.join()`, `path.relative()` for path normalization\n\n## Integration\n\nUsed in composable filter chain via `src/discovery/filters/index.ts` alongside binary, vendor, and custom pattern filters. Returned `FileFilter` object matches `FileFilter` interface from `src/discovery/types.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 449,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10802,
      "latencyMs": 32119,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/gitignore.ts",
          "sizeBytes": 1990
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.936Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/index.ts\n\n```typescript\nexport type {\n  PromptContext,\n} from './types.js';\nexport { SUMMARY_GUIDELINES } from './types.js';\nexport {\n  buildFilePrompt,\n  buildDirectoryPrompt,\n  buildRootPrompt,\n  detectLanguage,\n} from './builder.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export module re-exporting prompt construction functions (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage`) and shared prompt types (`PromptContext`, `SUMMARY_GUIDELINES`) from internal `builder.ts` and `types.ts` modules.**\n\n## Exported Symbols\n\n- **`PromptContext`** (type): Re-exported from `./types.js` — context object passed to prompt builders containing file metadata, import maps, and configuration\n- **`SUMMARY_GUIDELINES`** (constant): Re-exported from `./types.js` — string constant defining documentation density rules, identifier preservation requirements, and behavioral contract specifications\n- **`buildFilePrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 1 prompt for AI-driven file analysis, embedding source content and import relationships\n- **`buildDirectoryPrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 2 prompt for `AGENTS.md` generation from aggregated `.sum` files and child directory docs\n- **`buildRootPrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 3 prompt for root integration document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`)\n- **`detectLanguage(filePath: string): string`**: Re-exported from `./builder.js` — infers programming language from file extension for syntax-aware prompt customization\n\n## Module Role\n\nCentralizes public API for prompt construction subsystem. Internal implementation split across:\n- `builder.ts`: Implements prompt assembly logic with template interpolation\n- `types.ts`: Defines `PromptContext` interface and `SUMMARY_GUIDELINES` constant (likely contains reproduction-critical prompt text per annex pattern)\n- `templates.ts`: Likely contains multi-line prompt templates (annex candidate based on project structure showing `.annex.md` siblings)\n\nConsumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` to construct AI service call prompts for three-phase pipeline.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 514,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9275,
      "latencyMs": 22787,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/index.ts",
          "sizeBytes": 213
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.892Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/cleanup.ts\n\n```typescript\n/**\n * Telemetry log cleanup utility.\n *\n * Removes old run log files from `.agents-reverse-engineer/logs/`,\n * keeping only the N most recent. Files are sorted lexicographically\n * by name, which works correctly because filenames contain ISO timestamps.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Remove old telemetry log files, keeping only the most recent ones.\n *\n * Reads the logs directory, filters for files matching `run-*.json`,\n * sorts newest-first (lexicographic sort on ISO timestamp filenames),\n * and deletes everything beyond `keepCount`.\n *\n * If the logs directory does not exist, returns 0 without error.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent log files to retain\n * @returns Number of files deleted\n *\n * @example\n * ```typescript\n * const deleted = await cleanupOldLogs('/home/user/project', 10);\n * console.log(`Cleaned up ${deleted} old log files`);\n * ```\n */\nexport async function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await fs.readdir(logsDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('run-') && name.endsWith('.json'),\n    );\n  } catch (error) {\n    // Directory doesn't exist -- nothing to clean up\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort lexicographically (newest first) and find files to delete\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await fs.unlink(path.join(logsDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`cleanup.ts` removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent `run-*.json` files sorted lexicographically by ISO timestamp filenames.**\n\n## Exported Functions\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` reads the logs directory at `path.join(projectRoot, LOGS_DIR)`, filters entries matching the pattern `run-*.json`, sorts lexicographically (newest first via `sort()` then `reverse()`), slices beyond `keepCount`, and unlinks excess files. Returns the number of deleted files. Returns `0` without error if the logs directory does not exist (catches `ENOENT` error code). Throws on other filesystem errors.\n\n## Constants\n\n`LOGS_DIR` equals `'.agents-reverse-engineer/logs'`, defining the relative path from project root to the telemetry log directory.\n\n## Integration Points\n\nCalled by `src/ai/telemetry/logger.ts` after writing a new run log to enforce retention policy specified in `config.ai.telemetry.keepRuns` (default 50). Works in conjunction with `src/ai/telemetry/run-log.ts` which defines the `RunLog` structure and `src/orchestration/runner.ts` which orchestrates telemetry logging during command execution.\n\n## Sorting Behavior\n\nLexicographic sorting works correctly for ISO 8601 timestamps embedded in filenames (e.g., `run-2026-02-09T12-34-56-789Z.json`) because the timestamp format naturally orders chronologically when sorted as strings. The `reverse()` call after `sort()` produces newest-first ordering, so `slice(keepCount)` targets oldest files for deletion.\n\n## Error Handling\n\nSuppresses `ENOENT` errors during `fs.readdir()` to handle missing logs directory gracefully (fresh projects or post-`are clean` state). Propagates all other filesystem errors to the caller without wrapping.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 469,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9792,
      "latencyMs": 23273,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/cleanup.ts",
          "sizeBytes": 1937
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.887Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/gemini.ts\n\n```typescript\n/**\n * Gemini CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the Gemini CLI (`gemini`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the Gemini CLI JSON output is stable (see RESEARCH.md Open Question 2).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * Gemini CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the Gemini adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new GeminiBackend();\n * console.log(await backend.isAvailable()); // true if `gemini` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class GeminiBackend implements AIBackend {\n  readonly name = 'gemini';\n  readonly cliCommand = 'gemini';\n\n  /**\n   * Check if the `gemini` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Gemini invocation.\n   *\n   * Based on documented Gemini CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the Gemini CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['-p', '--output-format', 'json'];\n  }\n\n  /**\n   * Parse Gemini CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- Gemini backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'Gemini backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the Gemini CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Gemini CLI (experimental):',\n      '  npm install -g @anthropic-ai/gemini-cli',\n      '  https://github.com/google-gemini/gemini-cli',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GeminiBackend implements AIBackend interface as stub demonstrating extension pattern for Gemini CLI integration, currently throwing \"not implemented\" errors pending stable JSON output format.**\n\n## Exports\n\n**GeminiBackend** class implementing AIBackend interface with properties:\n- `name: 'gemini'` — Backend identifier string\n- `cliCommand: 'gemini'` — CLI executable name\n\n## Public Interface\n\n**isAvailable(): Promise<boolean>** checks if `gemini` CLI exists on PATH via `isCommandOnPath(this.cliCommand)` imported from `claude.ts`.\n\n**buildArgs(_options: AICallOptions): string[]** returns CLI argument array `['-p', '--output-format', 'json']` based on documented Gemini CLI flags from RESEARCH.md. Prompt delivered via stdin through subprocess wrapper.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'` Always fails—parser not implemented.\n\n**getInstallInstructions(): string** returns multiline installation guidance:\n```\nGemini CLI (experimental):\n  npm install -g @anthropic-ai/gemini-cli\n  https://github.com/google-gemini/gemini-cli\n```\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` from `../types.js`. Imports `isCommandOnPath` utility from `./claude.js` for CLI detection.\n\n## Implementation Status\n\nStub backend demonstrating AIBackend extension contract. Full implementation deferred until Gemini CLI JSON output stabilizes (RESEARCH.md Open Question 2). Detection and argument building functional, response parsing unimplemented.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 427,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9900,
      "latencyMs": 23343,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/gemini.ts",
          "sizeBytes": 2281
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.898Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/run-log.ts\n\n```typescript\n/**\n * Run log writer for AI telemetry.\n *\n * Writes a completed {@link RunLog} to disk as pretty-printed JSON.\n * Each run produces a single file in `.agents-reverse-engineer/logs/`\n * with a filename derived from the run's start timestamp.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { RunLog } from '../types.js';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Write a completed run log to disk as pretty-printed JSON.\n *\n * Creates the logs directory if it does not exist. The filename is derived\n * from the run's `startTime` field with `:` and `.` replaced by `-` so\n * that it forms a valid filename on all platforms.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param runLog - The completed run log to write\n * @returns Absolute path to the written file\n *\n * @example\n * ```typescript\n * const logPath = await writeRunLog('/home/user/project', runLog);\n * // logPath: /home/user/project/.agents-reverse-engineer/logs/run-2026-02-07T12-00-00-000Z.json\n * ```\n */\nexport async function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  // Create the logs directory if it does not exist\n  await fs.mkdir(logsDir, { recursive: true });\n\n  // Build filename: replace : and . with - so ISO timestamps become valid filenames\n  const safeTimestamp = runLog.startTime.replace(/[:.]/g, '-');\n  const filename = `run-${safeTimestamp}.json`;\n  const filePath = path.join(logsDir, filename);\n\n  // Write pretty-printed JSON\n  await fs.writeFile(filePath, JSON.stringify(runLog, null, 2), 'utf-8');\n\n  return filePath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeRunLog() serializes completed RunLog objects to pretty-printed JSON files in `.agents-reverse-engineer/logs/` with ISO timestamp-derived filenames.**\n\n## Exported Functions\n\n**writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>**\n\nWrites a completed RunLog to disk as pretty-printed JSON (2-space indentation). Creates `.agents-reverse-engineer/logs/` directory via `fs.mkdir()` with `recursive: true` if absent. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to ensure cross-platform filesystem compatibility. Returns absolute path to written file. Pattern: `run-${safeTimestamp}.json` where `safeTimestamp` transforms `2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z`.\n\n## Constants\n\n**LOGS_DIR**: `'.agents-reverse-engineer/logs'`\n\nRelative path from project root to telemetry log directory.\n\n## Dependencies\n\n- `node:fs/promises`: Async `mkdir()` and `writeFile()` operations\n- `node:path`: Path joining via `path.join()`\n- `RunLog` type from `../types.js`: Telemetry run log schema with `startTime` (ISO 8601 string), token counts, costs, errors, filesRead metadata\n\n## Integration Points\n\nCalled by `TelemetryLogger.endRun()` in `src/ai/telemetry/logger.ts` after aggregating all AIServiceCall results from a generation/update run. Paired with `cleanupOldLogs()` from `src/ai/telemetry/cleanup.ts` which enforces retention limits by deleting oldest run-*.json files beyond configured threshold.\n\n## Behavioral Contracts\n\nFilename transformation: `runLog.startTime.replace(/[:.]/g, '-')` converts ISO 8601 timestamps with colons and periods into filesystem-safe format. Example: `2026-02-07T12:00:00.000Z` → `run-2026-02-07T12-00-00-000Z.json`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 508,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9750,
      "latencyMs": 24739,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/run-log.ts",
          "sizeBytes": 1766
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.890Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/opencode.ts\n\n```typescript\n/**\n * OpenCode CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the OpenCode CLI (`opencode`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the OpenCode JSONL output parsing is built (see RESEARCH.md Open Question 3).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * OpenCode CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the OpenCode adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new OpenCodeBackend();\n * console.log(await backend.isAvailable()); // true if `opencode` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class OpenCodeBackend implements AIBackend {\n  readonly name = 'opencode';\n  readonly cliCommand = 'opencode';\n\n  /**\n   * Check if the `opencode` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for an OpenCode invocation.\n   *\n   * Based on documented OpenCode CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the OpenCode CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['run', '--format', 'json'];\n  }\n\n  /**\n   * Parse OpenCode CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- OpenCode backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'OpenCode backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the OpenCode CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'OpenCode (experimental):',\n      '  curl -fsSL https://opencode.ai/install | bash',\n      '  https://opencode.ai',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCodeBackend implements the AIBackend interface as a stub demonstrating backend extension patterns, detecting CLI availability and building argument arrays but throwing AIServiceError on parseResponse() until JSONL output parsing is implemented.**\n\n## Public Interface\n\n**OpenCodeBackend** class implements `AIBackend` with properties:\n- `name: 'opencode'` — Backend identifier constant\n- `cliCommand: 'opencode'` — Executable name for PATH detection\n\n**isAvailable(): Promise<boolean>** — Delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.js` to check if `opencode` binary exists on PATH.\n\n**buildArgs(_options: AICallOptions): string[]** — Returns `['run', '--format', 'json']` argument array for OpenCode CLI invocation. Prompt input delivered via stdin by subprocess wrapper in `src/ai/subprocess.ts`.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** — Unconditionally throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'OpenCode backend is not yet implemented. Use Claude backend.'` to prevent runtime execution until JSONL parsing implemented.\n\n**getInstallInstructions(): string** — Returns multiline installation guide:\n```\nOpenCode (experimental):\n  curl -fsSL https://opencode.ai/install | bash\n  https://opencode.ai\n```\n\n## Integration Points\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js` for interface compliance and `AIServiceError` for error construction. Imports `isCommandOnPath` from `./claude.js` for PATH detection reuse.\n\nBackend registered in `src/ai/registry.ts` alongside ClaudeBackend and GeminiBackend for auto-detection via `detectFirstAvailableBackend()`.\n\n## Implementation Status\n\nStub implementation deferred per RESEARCH.md Open Question 3 pending JSONL output format specification. Current `parseResponse()` behavior blocks production use while allowing compilation and registration scaffolding to remain in place.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 487,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9875,
      "latencyMs": 25542,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/opencode.ts",
          "sizeBytes": 2298
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.895Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/logger.ts\n\n```typescript\n/**\n * In-memory telemetry logger for AI service calls.\n *\n * Accumulates {@link TelemetryEntry} instances during a run and computes\n * aggregate summaries. The logger is created once per CLI invocation and\n * finalized when the run completes.\n *\n * @module\n */\n\nimport type { TelemetryEntry, RunLog, FileRead } from '../types.js';\n\n/**\n * Accumulates per-call telemetry entries in memory and produces a\n * complete {@link RunLog} when the run finishes.\n *\n * @example\n * ```typescript\n * const logger = new TelemetryLogger('2026-02-07T12:00:00.000Z');\n * logger.addEntry(entry);\n * const summary = logger.getSummary();\n * const runLog = logger.toRunLog();\n * ```\n */\nexport class TelemetryLogger {\n  /** Unique identifier for this run (ISO timestamp-based) */\n  readonly runId: string;\n\n  /** ISO 8601 timestamp when the run started */\n  readonly startTime: string;\n\n  /** Accumulated telemetry entries */\n  private readonly entries: TelemetryEntry[] = [];\n\n  /**\n   * Create a new telemetry logger for a run.\n   *\n   * @param runId - Unique run identifier (typically an ISO timestamp)\n   */\n  constructor(runId: string) {\n    this.runId = runId;\n    this.startTime = new Date().toISOString();\n  }\n\n  /**\n   * Record a telemetry entry for a completed AI call.\n   *\n   * @param entry - The telemetry entry to record\n   */\n  addEntry(entry: TelemetryEntry): void {\n    this.entries.push(entry);\n  }\n\n  /**\n   * Get all recorded entries as a read-only array.\n   *\n   * @returns Immutable view of the accumulated entries\n   */\n  getEntries(): readonly TelemetryEntry[] {\n    return this.entries;\n  }\n\n  /**\n   * Update the most recent entry's filesRead array.\n   *\n   * Called by the AI service after the command runner attaches file\n   * metadata to the last call.\n   *\n   * @param filesRead - Array of file-read records to attach\n   */\n  setFilesReadOnLastEntry(filesRead: FileRead[]): void {\n    if (this.entries.length === 0) return;\n    this.entries[this.entries.length - 1]!.filesRead = filesRead;\n  }\n\n  /**\n   * Compute aggregate summary statistics from all recorded entries.\n   *\n   * Totals are computed on every call (not cached) so the summary\n   * always reflects the current state of the entries array.\n   *\n   * @returns Summary with totals for calls, tokens, duration, and errors\n   */\n  getSummary(): RunLog['summary'] {\n    let totalInputTokens = 0;\n    let totalOutputTokens = 0;\n    let totalCacheReadTokens = 0;\n    let totalCacheCreationTokens = 0;\n    let totalDurationMs = 0;\n    let errorCount = 0;\n    let totalFilesRead = 0;\n    const uniqueFilePaths = new Set<string>();\n\n    for (const entry of this.entries) {\n      totalInputTokens += entry.inputTokens;\n      totalOutputTokens += entry.outputTokens;\n      totalCacheReadTokens += entry.cacheReadTokens;\n      totalCacheCreationTokens += entry.cacheCreationTokens;\n      totalDurationMs += entry.latencyMs;\n      if (entry.error !== undefined) {\n        errorCount++;\n      }\n      totalFilesRead += entry.filesRead.length;\n      for (const file of entry.filesRead) {\n        uniqueFilePaths.add(file.path);\n      }\n    }\n\n    return {\n      totalCalls: this.entries.length,\n      totalInputTokens,\n      totalOutputTokens,\n      totalCacheReadTokens,\n      totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount,\n      totalFilesRead,\n      uniqueFilesRead: uniqueFilePaths.size,\n    };\n  }\n\n  /**\n   * Assemble the complete {@link RunLog} for this run.\n   *\n   * Sets `endTime` to the current time, includes all entries, and\n   * computes the summary. Call this once when the run is finished.\n   *\n   * @returns Complete run log ready for serialization\n   */\n  toRunLog(): RunLog {\n    return {\n      runId: this.runId,\n      startTime: this.startTime,\n      endTime: new Date().toISOString(),\n      entries: [...this.entries],\n      summary: this.getSummary(),\n    };\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**TelemetryLogger accumulates per-call AI service telemetry entries in memory and computes aggregate RunLog summaries with token counts, latency metrics, file-read statistics, and error tallies.**\n\n## Exported Class\n\n**TelemetryLogger** — In-memory accumulator for TelemetryEntry instances created once per CLI invocation. Constructor accepts `runId: string` (ISO timestamp-based identifier). Exposes read-only `runId: string` and `startTime: string` (ISO 8601 timestamp set at construction).\n\n## Public Methods\n\n**addEntry(entry: TelemetryEntry): void** — Records telemetry entry for a completed AI subprocess call. Appends to internal `entries: TelemetryEntry[]` array.\n\n**getEntries(): readonly TelemetryEntry[]** — Returns immutable view of accumulated entries array.\n\n**setFilesReadOnLastEntry(filesRead: FileRead[]): void** — Updates `filesRead` property of most recent entry. Called by AIService after command runner attaches file metadata post-execution. No-op if entries array is empty.\n\n**getSummary(): RunLog['summary']** — Computes aggregate statistics over all entries on every invocation (no caching). Returns object with:\n- `totalCalls: number` — Length of entries array\n- `totalInputTokens: number` — Sum of `entry.inputTokens` across all entries\n- `totalOutputTokens: number` — Sum of `entry.outputTokens`\n- `totalCacheReadTokens: number` — Sum of `entry.cacheReadTokens`\n- `totalCacheCreationTokens: number` — Sum of `entry.cacheCreationTokens`\n- `totalDurationMs: number` — Sum of `entry.latencyMs`\n- `errorCount: number` — Count of entries where `entry.error !== undefined`\n- `totalFilesRead: number` — Sum of `entry.filesRead.length` across all entries\n- `uniqueFilesRead: number` — Distinct file paths via `Set<string>` deduplication of `file.path` values\n\n**toRunLog(): RunLog** — Assembles final RunLog for serialization. Sets `endTime` to `new Date().toISOString()`, copies entries array, invokes `getSummary()`. Call once when run completes. Returns object with `runId`, `startTime`, `endTime`, `entries`, and `summary` properties.\n\n## Integration Points\n\nConsumes `TelemetryEntry`, `RunLog`, and `FileRead` types from `../types.js`. Used by AIService (`src/ai/service.ts`) to track subprocess call metrics. Output written to `.agents-reverse-engineer/logs/run-<timestamp>.json` via run-log persistence layer (`src/ai/telemetry/run-log.ts`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 663,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10404,
      "latencyMs": 26087,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/logger.ts",
          "sizeBytes": 3866
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.923Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/vendor.ts\n\n```typescript\n/**\n * Vendor directory filter for file discovery.\n *\n * Excludes files within common vendor/dependency directories that typically\n * contain third-party code not relevant for documentation purposes.\n */\n\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Default vendor directories to exclude.\n * These are common directories containing third-party code, build output,\n * or generated files that should not be analyzed.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n] as const;\n\n/**\n * Creates a vendor filter that excludes files within specified directories.\n *\n * Supports two patterns:\n * - Single directory names (e.g., 'node_modules') - matches anywhere in path\n * - Path patterns (e.g., 'apps/vendor' or '.agents/skills') - matches path containing this sequence\n *\n * @param vendorDirs - Array of directory names or path patterns to exclude.\n * @returns A FileFilter that checks if a path is within a vendor directory\n *\n * @example\n * ```typescript\n * const filter = createVendorFilter(['node_modules', '.agents/skills']);\n * filter.shouldExclude('/project/node_modules/lodash/index.js'); // true\n * filter.shouldExclude('/project/apps/foo/.agents/skills/bar.md'); // true\n * filter.shouldExclude('/project/src/utils.js'); // false\n * ```\n */\nexport function createVendorFilter(vendorDirs: string[]): FileFilter {\n  // Separate single segments from path patterns\n  const singleSegments = new Set<string>();\n  const pathPatterns: string[] = [];\n\n  for (const dir of vendorDirs) {\n    // Normalize path separators to current OS\n    const normalized = dir.replace(/[\\\\/]/g, path.sep);\n    if (normalized.includes(path.sep)) {\n      pathPatterns.push(normalized);\n    } else {\n      singleSegments.add(dir);\n    }\n  }\n\n  return {\n    name: 'vendor',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Check single segment matches\n      const segments = absolutePath.split(path.sep);\n      for (const segment of segments) {\n        if (singleSegments.has(segment)) {\n          return true;\n        }\n      }\n\n      // Check path pattern matches\n      for (const pattern of pathPatterns) {\n        if (absolutePath.includes(pattern)) {\n          return true;\n        }\n      }\n\n      return false;\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createVendorFilter() excludes files within third-party dependency directories via single-segment matching and path-pattern substring containment, supporting both flat directory names (e.g., `node_modules`) and hierarchical patterns (e.g., `.agents/skills`).**\n\n## Exported Symbols\n\n**DEFAULT_VENDOR_DIRS**: `readonly string[]` — Array of 10 default vendor directory names to exclude: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**createVendorFilter(vendorDirs: string[]): FileFilter** — Constructs a `FileFilter` instance with `shouldExclude()` method implementing dual-pattern matching logic.\n\n## Matching Algorithms\n\n**createVendorFilter()** partitions input `vendorDirs` into two collections:\n\n1. **Single segments** (no path separators): Stored in `Set<string>` for O(1) lookup. Path split via `path.sep` yields segment array; filter returns `true` if any segment exists in set.\n2. **Path patterns** (containing `path.sep`): Stored in `string[]`. Filter returns `true` if `absolutePath.includes(pattern)` for any pattern.\n\nPath normalization via `dir.replace(/[\\\\/]/g, path.sep)` ensures cross-platform separator handling before classification.\n\n## Filter Interface\n\nReturned `FileFilter` object contains:\n- **name**: `'vendor'` — Filter identifier for debugging/logging.\n- **shouldExclude(absolutePath: string): boolean** — Returns `true` if path matches either single-segment or path-pattern rules.\n\n## Integration Points\n\nImports `FileFilter` from `../types.js`. Consumed by filter chain in `src/discovery/filters/index.ts` alongside `createBinaryFilter()`, `createGitignoreFilter()`, `createCustomFilter()`. Applied during `walk()` in `src/discovery/walker.ts` to exclude vendor directories from file discovery results.\n\n## Pattern Examples\n\nSingle-segment match: `/project/node_modules/lodash/index.js` → splits to `['', 'project', 'node_modules', 'lodash', 'index.js']` → `'node_modules'` in `singleSegments` → returns `true`.\n\nPath-pattern match: `/project/apps/foo/.agents/skills/bar.md` → `includes('.agents/skills')` → returns `true` (assuming `.agents/skills` in `pathPatterns` after normalization).\n\nNo match: `/project/src/utils.js` → no segment in `singleSegments`, no pattern in `pathPatterns` → returns `false`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 638,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9898,
      "latencyMs": 26965,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/vendor.ts",
          "sizeBytes": 2375
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.916Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/index.ts\n\n```typescript\n/**\n * Filter chain orchestration for file discovery.\n *\n * This module exports all filter creators and provides the applyFilters\n * function that runs files through the filter chain, recording which\n * filter excluded each file.\n */\n\nimport pc from 'picocolors';\nimport type { FileFilter, FilterResult, ExcludedFile } from '../types.js';\nimport type { ITraceWriter } from '../../orchestration/trace.js';\n\n// Re-export all filter creators\nexport { createGitignoreFilter } from './gitignore.js';\nexport { createVendorFilter, DEFAULT_VENDOR_DIRS } from './vendor.js';\nexport { createBinaryFilter, BINARY_EXTENSIONS, type BinaryFilterOptions } from './binary.js';\nexport { createCustomFilter } from './custom.js';\n\n/**\n * Applies a chain of filters to a list of files.\n *\n * Each file is run through filters in order until one excludes it\n * (short-circuit evaluation). Files are processed with bounded concurrency\n * to avoid opening too many file handles simultaneously (important for\n * binary content detection which performs file I/O).\n *\n * @param files - Array of absolute file paths to filter\n * @param filters - Array of filters to apply in order\n * @param options - Optional tracing and debug options\n * @returns Promise resolving to FilterResult with included and excluded lists\n *\n * @example\n * ```typescript\n * const filters = [\n *   createVendorFilter(['node_modules']),\n *   createBinaryFilter({}),\n * ];\n * const result = await applyFilters(['/a/b.js', '/a/node_modules/c.js'], filters);\n * // result.included: ['/a/b.js']\n * // result.excluded: [{ path: '/a/node_modules/c.js', filter: 'vendor', reason: '...' }]\n * ```\n */\nexport async function applyFilters(\n  files: string[],\n  filters: FileFilter[],\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<FilterResult> {\n  const included: string[] = [];\n  const excluded: ExcludedFile[] = [];\n\n  // Track exclusions per filter for trace events\n  const filterStats = new Map<string, { matched: number; rejected: number }>();\n  for (const filter of filters) {\n    filterStats.set(filter.name, { matched: 0, rejected: 0 });\n  }\n\n  // Process files with bounded concurrency to avoid exhausting file descriptors.\n  // Binary filter calls isBinaryFile() which does file I/O.\n  const CONCURRENCY = 30;\n  const iterator = files.entries();\n\n  async function worker(\n    iter: IterableIterator<[number, string]>,\n  ): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>> {\n    const results: Array<{ index: number; file: string; excluded?: ExcludedFile }> = [];\n    for (const [index, file] of iter) {\n      let wasExcluded = false;\n\n      // Run through filters in order, stop at first exclusion\n      for (const filter of filters) {\n        const shouldExclude = await filter.shouldExclude(file);\n\n        if (shouldExclude) {\n          results.push({\n            index,\n            file,\n            excluded: {\n              path: file,\n              reason: `Excluded by ${filter.name} filter`,\n              filter: filter.name,\n            },\n          });\n          wasExcluded = true;\n          break; // Short-circuit: stop checking other filters\n        }\n      }\n\n      if (!wasExcluded) {\n        results.push({ index, file });\n      }\n    }\n    return results;\n  }\n\n  const effectiveConcurrency = Math.min(CONCURRENCY, files.length);\n  const workers = Array.from({ length: effectiveConcurrency }, () =>\n    worker(iterator),\n  );\n\n  const allResults = (await Promise.all(workers)).flat();\n\n  // Sort by original index to preserve order\n  allResults.sort((a, b) => a.index - b.index);\n\n  // Collect results and update filter stats\n  for (const result of allResults) {\n    if (result.excluded) {\n      excluded.push(result.excluded);\n      const stats = filterStats.get(result.excluded.filter);\n      if (stats) {\n        stats.rejected++;\n      }\n    } else {\n      included.push(result.file);\n      // All filters \"matched\" (passed through) this file\n      for (const filter of filters) {\n        const stats = filterStats.get(filter.name);\n        if (stats) {\n          stats.matched++;\n        }\n      }\n    }\n  }\n\n  // Emit trace events for each filter\n  for (const filter of filters) {\n    const stats = filterStats.get(filter.name);\n    if (stats) {\n      options?.tracer?.emit({\n        type: 'filter:applied',\n        filterName: filter.name,\n        filesMatched: stats.matched,\n        filesRejected: stats.rejected,\n      });\n\n      // Debug output\n      if (options?.debug && stats.rejected > 0) {\n        console.error(pc.dim(`[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`));\n      }\n    }\n  }\n\n  return { included, excluded };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, applying multiple FileFilter predicates sequentially with bounded concurrency and tracking exclusion reasons per filter.**\n\n## Exported Functions\n\n**applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>** processes files through filter chain with short-circuit evaluation. Returns FilterResult containing `included: string[]` and `excluded: ExcludedFile[]` arrays. Each file runs through filters sequentially until one returns true from `shouldExclude()`, at which point remaining filters are skipped. Uses bounded concurrency of 30 workers sharing single iterator to prevent file descriptor exhaustion during I/O-heavy binary content detection.\n\n## Re-Exported Filter Creators\n\nAggregates filter factory functions from sibling modules:\n- **createGitignoreFilter** from `./gitignore.js`\n- **createVendorFilter** and **DEFAULT_VENDOR_DIRS** from `./vendor.js`\n- **createBinaryFilter**, **BINARY_EXTENSIONS**, **BinaryFilterOptions** from `./binary.js`\n- **createCustomFilter** from `./custom.js`\n\n## Concurrency Strategy\n\nUses iterator-based worker pool pattern with `CONCURRENCY = 30` to process files in parallel without exhausting file handles. Workers share single `files.entries()` iterator, each consuming items until exhausted. BinaryFilter calls `isBinaryFile()` which performs synchronous file reads, making concurrency bounds critical to prevent system resource limits (ulimit -n).\n\n## Filter Execution Algorithm\n\n1. Initialize `filterStats` map tracking `matched` and `rejected` counts per filter name\n2. Spawn 30 workers (or `files.length` if fewer) each iterating over shared file entries\n3. For each file, iterate through `filters[]` array sequentially\n4. On first `shouldExclude()` returning true, push `ExcludedFile` with `{ path, reason, filter: filter.name }` and break inner loop (short-circuit)\n5. If no filter excludes file, push path to `included[]` and increment `matched` count for all filters\n6. Flatten worker results, sort by original index to preserve order\n7. Emit `filter:applied` trace events with `filesMatched` and `filesRejected` counts per filter\n\n## Trace Events\n\nEmits `filter:applied` events via `options?.tracer?.emit()` containing:\n- `type: 'filter:applied'`\n- `filterName: string` — filter.name from FileFilter\n- `filesMatched: number` — count of files passing through this filter\n- `filesRejected: number` — count of files excluded by this filter\n\n## Debug Output\n\nWhen `options?.debug` is true and `stats.rejected > 0`, writes `pc.dim()` colored message to stderr: `[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`.\n\n## Return Type\n\nFilterResult contains:\n- `included: string[]` — absolute paths passing all filters\n- `excluded: ExcludedFile[]` — objects with `{ path: string, reason: string, filter: string }` describing which filter excluded each file",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 755,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10548,
      "latencyMs": 27157,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/index.ts",
          "sizeBytes": 4669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.901Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/binary.ts\n\n```typescript\n/**\n * Binary file filter for file discovery.\n *\n * Uses extension-based detection as a fast path, falling back to content\n * analysis via `isbinaryfile` for unknown extensions. Also handles large\n * files by size threshold.\n */\n\nimport { isBinaryFile } from 'isbinaryfile';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Set of file extensions known to be binary.\n * These are excluded without content analysis for performance.\n */\nexport const BINARY_EXTENSIONS = new Set([\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  '.svg',\n  '.tiff',\n  '.tif',\n  '.psd',\n  '.raw',\n  '.heif',\n  '.heic',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  '.bz2',\n  '.xz',\n  '.tgz',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  '.bin',\n  '.msi',\n  '.app',\n  '.dmg',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  '.avi',\n  '.mov',\n  '.mkv',\n  '.flac',\n  '.ogg',\n  '.webm',\n  '.m4a',\n  '.aac',\n  '.wma',\n  '.wmv',\n  '.flv',\n  // Documents (binary formats)\n  '.pdf',\n  '.doc',\n  '.docx',\n  '.xls',\n  '.xlsx',\n  '.ppt',\n  '.pptx',\n  '.odt',\n  '.ods',\n  '.odp',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  '.otf',\n  // Compiled/bytecode\n  '.class',\n  '.pyc',\n  '.pyo',\n  '.o',\n  '.obj',\n  '.a',\n  '.lib',\n  '.wasm',\n  // Database\n  '.db',\n  '.sqlite',\n  '.sqlite3',\n  '.mdb',\n  // Other\n  '.ico',\n  '.icns',\n  '.cur',\n  '.deb',\n  '.rpm',\n  '.jar',\n  '.war',\n  '.ear',\n]);\n\n/**\n * Options for the binary filter.\n */\nexport interface BinaryFilterOptions {\n  /**\n   * Maximum file size in bytes. Files larger than this are excluded.\n   * Default: 1MB (1048576 bytes)\n   */\n  maxFileSize?: number;\n\n  /**\n   * Additional binary extensions to recognize beyond the defaults.\n   */\n  additionalExtensions?: string[];\n}\n\n/** Default maximum file size: 1MB */\nconst DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Creates a binary filter that excludes binary files and files exceeding size limit.\n *\n * The filter uses a two-phase detection approach:\n * 1. Fast path: Check extension against known binary extensions\n * 2. Slow path: For unknown extensions, analyze file content with isbinaryfile\n *\n * @param options - Filter configuration options\n * @returns A FileFilter that identifies binary files\n *\n * @example\n * ```typescript\n * const filter = createBinaryFilter({ maxFileSize: 500000 });\n * await filter.shouldExclude('/path/to/image.png'); // true (extension)\n * await filter.shouldExclude('/path/to/unknown.xyz'); // checks content\n * ```\n */\nexport function createBinaryFilter(options: BinaryFilterOptions = {}): FileFilter {\n  const { maxFileSize = DEFAULT_MAX_FILE_SIZE, additionalExtensions = [] } = options;\n\n  // Create a combined set of all binary extensions\n  const binaryExtensions = new Set(BINARY_EXTENSIONS);\n  for (const ext of additionalExtensions) {\n    // Ensure extension has leading dot\n    binaryExtensions.add(ext.startsWith('.') ? ext : `.${ext}`);\n  }\n\n  return {\n    name: 'binary',\n\n    async shouldExclude(absolutePath: string): Promise<boolean> {\n      // Fast path: check extension\n      const ext = path.extname(absolutePath).toLowerCase();\n      if (binaryExtensions.has(ext)) {\n        return true;\n      }\n\n      // Check file size and content\n      try {\n        const stats = await fs.stat(absolutePath);\n\n        // Exclude files exceeding size limit\n        if (stats.size > maxFileSize) {\n          return true;\n        }\n\n        // Slow path: content analysis for unknown extensions\n        return await isBinaryFile(absolutePath);\n      } catch {\n        // If we can't read it, skip it\n        return true;\n      }\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**binary.ts implements extension-first binary file detection with fallback content analysis and size-based exclusion for the file discovery pipeline.**\n\n## Exported Functions\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` constructs a `FileFilter` that returns `true` from `shouldExclude()` for binary files using a two-phase detection strategy: fast-path extension lookup in `BINARY_EXTENSIONS` set, then slow-path content analysis via `isBinaryFile()` from the `isbinaryfile` library for unknown extensions. Files exceeding `maxFileSize` (default 1MB) are excluded regardless of content type. Returns `true` on `fs.stat()` errors to skip inaccessible files.\n\n## Exported Constants\n\n`BINARY_EXTENSIONS: Set<string>` contains 96 lowercase file extensions with leading dots categorized as images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`), executables (`.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`), media (`.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`), documents (`.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`), compiled bytecode (`.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`), databases (`.db`, `.sqlite`, `.sqlite3`, `.mdb`), and other binary formats (`.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`).\n\n## Configuration Interface\n\n`BinaryFilterOptions` accepts optional `maxFileSize: number` (bytes, default 1048576 via `DEFAULT_MAX_FILE_SIZE`) and `additionalExtensions: string[]` merged into the detection set with automatic leading-dot normalization via `ext.startsWith('.') ? ext : \\`.${ext}\\``.\n\n## Detection Algorithm\n\n`shouldExclude()` executes:\n1. Extract extension via `path.extname(absolutePath).toLowerCase()`\n2. Check membership in merged `binaryExtensions` set (returns `true` immediately)\n3. Call `fs.stat()` to retrieve file size\n4. Compare `stats.size > maxFileSize` (returns `true` if exceeded)\n5. Invoke `isBinaryFile(absolutePath)` for content-based binary detection\n6. Catch all exceptions and return `true` to exclude unreadable files\n\n## Integration Point\n\nReturns `FileFilter` interface with `name: 'binary'` property for pipeline composition via `src/discovery/filters/index.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 762,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10469,
      "latencyMs": 28617,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/binary.ts",
          "sizeBytes": 3701
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:41.396Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/types.ts\n\n```typescript\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines the PromptContext interface for prompt construction and exports SUMMARY_GUIDELINES constants specifying documentation generation rules.**\n\n## Exported Types\n\n**PromptContext** — Interface for prompt builder input with fields:\n- `filePath: string` — Absolute path to the file being analyzed\n- `content: string` — File content to analyze\n- `contextFiles?: Array<{path: string; content: string}>` — Related files for additional context\n- `projectPlan?: string` — Project structure listing for bird's-eye context\n- `existingSSum?: string` — Existing .sum summary text for incremental updates\n\n## Exported Constants\n\n**SUMMARY_GUIDELINES** — Const object defining documentation generation constraints:\n- `targetLength: {min: 300, max: 500}` — Target word count range\n- `include: string[]` — Required content categories (8 items):\n  - Purpose and responsibility\n  - Public interface (exports, key functions)\n  - Key patterns and notable algorithms\n  - Dependencies with usage context\n  - Key function signatures as code snippets\n  - Tightly coupled sibling files\n  - Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables\n  - Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section\n- `exclude: string[]` — Forbidden content categories (3 items):\n  - Control flow minutiae (loop structures, variable naming, temporary state)\n  - Generic TODOs/FIXMEs (keep only security/breaking)\n  - Broad architectural relationships (handled by AGENTS.md)\n\n## Usage Context\n\nPromptContext consumed by `buildPrompt()` in `src/generation/prompts/builder.ts` to construct AI prompts for file analysis. SUMMARY_GUIDELINES referenced in prompt templates (`FILE_SYSTEM_PROMPT` in `src/generation/prompts/templates.ts`) to enforce documentation standards across Phase 1 file analysis and Phase 2 directory aggregation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 487,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26969,
      "latencyMs": 11615,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/types.ts",
          "sizeBytes": 1544
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.881Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/claude.ts\n\n```typescript\n/**\n * Claude CLI backend adapter.\n *\n * Full implementation of the {@link AIBackend} interface for the Claude Code\n * CLI (`claude`). Builds CLI arguments, parses structured JSON responses\n * with Zod validation, detects CLI availability on PATH, and provides\n * install instructions.\n *\n * The prompt is NOT included in the args array -- it goes to stdin via\n * the subprocess wrapper ({@link runSubprocess}).\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { z } from 'zod';\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\n\n// ---------------------------------------------------------------------------\n// Zod schema for Claude CLI JSON output\n// ---------------------------------------------------------------------------\n\n/**\n * Schema validated against Claude CLI v2.1.31 JSON output.\n *\n * When invoked with `claude -p --output-format json`, the CLI produces a\n * single JSON object on stdout matching this shape. See RESEARCH.md for\n * the live verification details.\n */\nconst ClaudeResponseSchema = z.object({\n  type: z.literal('result'),\n  subtype: z.enum(['success', 'error']),\n  is_error: z.boolean(),\n  duration_ms: z.number(),\n  duration_api_ms: z.number(),\n  num_turns: z.number(),\n  result: z.string(),\n  session_id: z.string(),\n  total_cost_usd: z.number(),\n  usage: z.object({\n    input_tokens: z.number(),\n    cache_creation_input_tokens: z.number(),\n    cache_read_input_tokens: z.number(),\n    output_tokens: z.number(),\n  }),\n  modelUsage: z.record(z.object({\n    inputTokens: z.number(),\n    outputTokens: z.number(),\n    cacheReadInputTokens: z.number(),\n    cacheCreationInputTokens: z.number(),\n    costUSD: z.number(),\n  })),\n});\n\n// ---------------------------------------------------------------------------\n// PATH detection utility\n// ---------------------------------------------------------------------------\n\n/**\n * Check whether a command is available on the system PATH.\n *\n * Splits `process.env.PATH` by the platform delimiter and checks each\n * directory for a file matching the command name. On Windows, also checks\n * each extension from `process.env.PATHEXT` (e.g., `.exe`, `.cmd`, `.bat`).\n *\n * Uses `fs.stat` (not `fs.access` with execute bit) for cross-platform\n * compatibility -- Windows does not have Unix execute permissions.\n *\n * @param command - The bare command name to look for (e.g., \"claude\")\n * @returns `true` if the command exists as a file in any PATH directory\n *\n * @example\n * ```typescript\n * if (await isCommandOnPath('claude')) {\n *   console.log('Claude CLI is available');\n * }\n * ```\n */\nexport async function isCommandOnPath(command: string): Promise<boolean> {\n  const envPath = process.env.PATH ?? '';\n  const pathDirs = envPath\n    .replace(/[\"]+/g, '')\n    .split(path.delimiter)\n    .filter(Boolean);\n\n  // On Windows, PATHEXT lists executable extensions (e.g., \".EXE;.CMD;.BAT\").\n  // On other platforms, PATHEXT is unset; check the bare command name only.\n  const envExt = process.env.PATHEXT ?? '';\n  const extensions = envExt ? envExt.split(';') : [''];\n\n  for (const dir of pathDirs) {\n    for (const ext of extensions) {\n      try {\n        const candidate = path.join(dir, command + ext);\n        const stat = await fs.stat(candidate);\n        if (stat.isFile()) {\n          return true;\n        }\n      } catch {\n        // Not found in this dir/ext combination, continue\n      }\n    }\n  }\n\n  return false;\n}\n\n// ---------------------------------------------------------------------------\n// Claude backend\n// ---------------------------------------------------------------------------\n\n/**\n * Claude Code CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the `claude` CLI.\n * This is the primary (and currently only fully implemented) backend.\n *\n * @example\n * ```typescript\n * const backend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Summarize this file' });\n *   const result = await runSubprocess('claude', args, {\n *     timeoutMs: 120_000,\n *     input: 'Summarize this file',\n *   });\n *   const response = backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n * }\n * ```\n */\nexport class ClaudeBackend implements AIBackend {\n  readonly name = 'claude';\n  readonly cliCommand = 'claude';\n\n  /**\n   * Check if the `claude` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Claude invocation.\n   *\n   * Returns the argument array for `claude -p --output-format json`. The\n   * prompt itself is NOT included -- it goes to stdin via the subprocess\n   * wrapper.\n   *\n   * @param options - Call options (model, systemPrompt, maxTurns)\n   * @returns Argument array suitable for {@link runSubprocess}\n   */\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      '-p',                        // Non-interactive print mode\n      '--output-format', 'json',   // Structured JSON output\n      '--no-session-persistence',  // Don't save session to disk\n      '--permission-mode', 'bypassPermissions',  // Non-interactive: skip permission prompts (PITFALLS.md §8)\n    ];\n\n    if (options.model) {\n      args.push('--model', options.model);\n    }\n\n    if (options.systemPrompt) {\n      args.push('--system-prompt', options.systemPrompt);\n    }\n\n    if (options.maxTurns !== undefined) {\n      args.push('--max-turns', String(options.maxTurns));\n    }\n\n    return args;\n  }\n\n  /**\n   * Parse Claude CLI JSON output into a normalized {@link AIResponse}.\n   *\n   * Handles non-JSON prefix text by finding the first `{` character\n   * (defensive parsing per RESEARCH.md Pitfall 4). Validates the response\n   * against the Zod schema and extracts the model name from `modelUsage`.\n   *\n   * @param stdout - Raw stdout from the Claude CLI process\n   * @param durationMs - Wall-clock duration of the subprocess\n   * @param exitCode - Process exit code\n   * @returns Normalized AI response\n   * @throws {AIServiceError} With code `PARSE_ERROR` if JSON is missing or schema validation fails\n   */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    // Find JSON object in stdout (handle any prefix text like upgrade notices)\n    const jsonStart = stdout.indexOf('{');\n    if (jsonStart === -1) {\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `No JSON object found in Claude CLI output. Raw output (first 200 chars): ${stdout.slice(0, 200)}`,\n      );\n    }\n\n    let parsed: z.infer<typeof ClaudeResponseSchema>;\n    try {\n      parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)));\n    } catch (error) {\n      const message = error instanceof Error ? error.message : String(error);\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `Failed to parse Claude CLI JSON response: ${message}`,\n      );\n    }\n\n    // Extract model name from modelUsage keys (first key is the model used)\n    const modelName = Object.keys(parsed.modelUsage)[0] ?? 'unknown';\n\n    return {\n      text: parsed.result,\n      model: modelName,\n      inputTokens: parsed.usage.input_tokens,\n      outputTokens: parsed.usage.output_tokens,\n      cacheReadTokens: parsed.usage.cache_read_input_tokens,\n      cacheCreationTokens: parsed.usage.cache_creation_input_tokens,\n      durationMs,\n      exitCode,\n      raw: parsed,\n    };\n  }\n\n  /**\n   * Get user-facing install instructions for the Claude CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Claude Code (recommended):',\n      '  npm install -g @anthropic-ai/claude-code',\n      '  https://code.claude.com',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), argument construction with bypass permissions, and JSON response parsing with Zod schema validation.**\n\n## Exported Symbols\n\n**isCommandOnPath(command: string): Promise<boolean>** — Splits `process.env.PATH` by platform delimiter, checks each directory for file matching command name, on Windows iterates `process.env.PATHEXT` extensions (`.exe`, `.cmd`, `.bat`), uses `fs.stat()` instead of `fs.access()` for cross-platform compatibility without Unix execute bits.\n\n**ClaudeBackend** — Implements `AIBackend` interface with properties `name: 'claude'`, `cliCommand: 'claude'`, methods `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()`.\n\n## ClaudeBackend Methods\n\n**isAvailable(): Promise<boolean>** — Delegates to `isCommandOnPath(this.cliCommand)` to verify `claude` binary exists on system PATH.\n\n**buildArgs(options: AICallOptions): string[]** — Constructs argument array `['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']`, appends `--model` if `options.model` present, appends `--system-prompt` if `options.systemPrompt` present, appends `--max-turns` converted to string if `options.maxTurns` defined, does NOT include prompt text (goes to stdin via `runSubprocess()`).\n\n**parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse** — Finds first `{` character via `stdout.indexOf('{')` to handle non-JSON prefix text (upgrade notices per RESEARCH.md Pitfall 4), slices stdout from `jsonStart`, parses JSON and validates against `ClaudeResponseSchema`, extracts model name from first key of `parsed.modelUsage` object (fallback `'unknown'`), returns normalized `AIResponse` with `text: parsed.result`, `inputTokens: parsed.usage.input_tokens`, `outputTokens: parsed.usage.output_tokens`, `cacheReadTokens: parsed.usage.cache_read_input_tokens`, `cacheCreationTokens: parsed.usage.cache_creation_input_tokens`, `durationMs`, `exitCode`, `raw: parsed`, throws `AIServiceError` with code `PARSE_ERROR` if JSON object missing or schema validation fails.\n\n**getInstallInstructions(): string** — Returns multiline string with npm install command `npm install -g @anthropic-ai/claude-code` and URL `https://code.claude.com`.\n\n## JSON Schema Validation\n\n**ClaudeResponseSchema** — Zod schema validated against Claude CLI v2.1.31 JSON output format, defines object with `type: z.literal('result')`, `subtype: z.enum(['success', 'error'])`, `is_error: z.boolean()`, `duration_ms: z.number()`, `duration_api_ms: z.number()`, `num_turns: z.number()`, `result: z.string()`, `session_id: z.string()`, `total_cost_usd: z.number()`, nested `usage` object containing `input_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens`, `output_tokens` as numbers, `modelUsage: z.record(z.object({...}))` mapping model names to usage statistics with `inputTokens`, `outputTokens`, `cacheReadInputTokens`, `cacheCreationInputTokens`, `costUSD` fields.\n\n## CLI Argument Patterns\n\n**Permission Mode** — `--permission-mode bypassPermissions` flag enables non-interactive execution by skipping permission prompts (documented in PITFALLS.md §8), required for subprocess operation without TTY.\n\n**Session Persistence** — `--no-session-persistence` flag prevents writing session state to disk, appropriate for stateless documentation generation tasks.\n\n**Output Format** — `--output-format json` flag triggers structured JSON output instead of conversational text, enabling machine parsing with schema validation.\n\n## Integration Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js`, imports `AIServiceError` from `../types.js`, uses `runSubprocess()` from subprocess module (referenced in JSDoc but not imported here), follows backend adapter pattern where prompt goes to stdin and CLI arguments configure behavior.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1067,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12482,
      "latencyMs": 35567,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/claude.ts",
          "sizeBytes": 7804
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:44.167Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/index.ts\n\n```typescript\nexport {\n  writeSumFile,\n  readSumFile,\n  getSumPath,\n  sumFileExists,\n  type SumFileContent,\n} from './sum.js';\n\nexport {\n  writeAgentsMd,\n} from './agents-md.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting file-level documentation writers for `.sum` files and directory-level `AGENTS.md` files.**\n\n## Exported Symbols\n\n### From `sum.js`\n\n- **`writeSumFile`** — writes `.sum` file with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files` fields plus markdown summary content\n- **`readSumFile`** — parses existing `.sum` file, extracts YAML frontmatter and markdown content, returns `SumFileContent` object\n- **`getSumPath`** — computes `.sum` file path from source file path by appending `.sum` extension\n- **`sumFileExists`** — checks existence of `.sum` file for given source path via `existsSync()`\n- **`SumFileContent`** (type) — structured representation of `.sum` file with parsed frontmatter fields and summary text\n\n### From `agents-md.js`\n\n- **`writeAgentsMd`** — generates directory-level `AGENTS.md` by aggregating child `.sum` files and subdirectory `AGENTS.md` content, preserves user-authored `AGENTS.local.md` content above generated sections, writes output with `<!-- Generated by agents-reverse-engineer -->` marker\n\n## Integration Role\n\nProvides unified import point for Phase 1 (file analysis → `.sum` output) and Phase 2 (directory aggregation → `AGENTS.md` output) writers consumed by `src/generation/executor.ts` during three-phase pipeline execution. Separates low-level file I/O and YAML serialization logic from orchestration concerns.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 387,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9266,
      "latencyMs": 14769,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/index.ts",
          "sizeBytes": 165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:48.077Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/index.ts\n\n```typescript\nexport { checkPhantomPaths } from './validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Re-exports `checkPhantomPaths` validator function from `./validator.js` as the public interface for phantom path detection.**\n\n## Exported Interface\n\nExports `checkPhantomPaths` function from `./validator.js` module, providing access to the phantom path validation subsystem that detects unresolvable file references in generated `AGENTS.md` documentation.\n\n## Module Role\n\nServes as the barrel export for the `src/quality/phantom-paths/` subsystem within the larger quality validation pipeline (`src/quality/index.ts`). This pattern centralizes the phantom path detection capability behind a single import point while keeping implementation details isolated in `validator.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 153,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26585,
      "latencyMs": 11508,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/index.ts",
          "sizeBytes": 52
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:53.030Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/types.ts.annex.md\n\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: types.ts\n\nReproduction-critical source content from `types.ts`.\nReferenced by `types.ts.sum`.\n\n```\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Annex file containing reproduction-critical source excerpts from `types.ts` defining `PromptContext` interface and `SUMMARY_GUIDELINES` constant.**\n\n## Annex Purpose\n\nThis `.annex.md` file preserves verbatim TypeScript definitions extracted from `src/generation/prompts/types.ts` that specify behavioral contracts for prompt construction and summary generation. Consumed by AI coding assistants requiring exact interface shapes and guideline constraints.\n\n## Reproduced Definitions\n\n### PromptContext Interface\n\nDefines shape for prompt construction input via `buildPrompt()` in `builder.ts`:\n\n```typescript\nexport interface PromptContext {\n  filePath: string;\n  content: string;\n  contextFiles?: Array<{ path: string; content: string; }>;\n  projectPlan?: string;\n  existingSum?: string;\n}\n```\n\nFive properties:\n- `filePath` — absolute path to file under analysis\n- `content` — file content string to analyze\n- `contextFiles` — optional related file pairs (path + content)\n- `projectPlan` — optional project structure listing (from `GENERATION-PLAN.md`)\n- `existingSum` — optional existing `.sum` summary text for incremental updates\n\n### SUMMARY_GUIDELINES Constant\n\nFrozen object defining documentation constraints consumed by prompt templates:\n\n```typescript\nexport const SUMMARY_GUIDELINES = {\n  targetLength: { min: 300, max: 500 },\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n```\n\nTarget length: 300-500 words. Include list specifies 8 mandatory topics (purpose, public interface, patterns, dependencies, signatures, coupling, behavioral contracts, annex references). Exclude list specifies 3 prohibited topics (control flow, generic todos, broad architecture).\n\n## Integration Point\n\n`FILE_SYSTEM_PROMPT` template in `templates.ts` interpolates `SUMMARY_GUIDELINES.include.join('\\n- ')` and `SUMMARY_GUIDELINES.exclude.join('\\n- ')` into constraint sections. `buildPrompt()` constructs `PromptContext` from `PromptInput` and passes to template rendering.\n\n## Maintenance Contract\n\nWhen modifying `PromptContext` interface shape, update corresponding `PromptInput` type in `types.ts` and `buildPrompt()` mapping logic in `builder.ts`. When modifying `SUMMARY_GUIDELINES`, regenerate dependent prompts via `are update` to refresh interpolated constraint text.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 761,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9653,
      "latencyMs": 6998,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/types.ts.annex.md",
          "sizeBytes": 1703
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:45.640Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/density/validator.ts\n\n```typescript\n/**\n * Heuristic findability validation for AGENTS.md content.\n *\n * validateFindability checks that key exported symbols from .sum files\n * appear in the parent AGENTS.md content.\n * No LLM calls -- purely string-based symbol matching.\n *\n * NOTE: Previously relied on metadata.publicInterface which has been removed.\n * This module is retained for future structured extraction support.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\n\n/**\n * Result of findability validation for a single .sum file.\n *\n * score ranges from 0 (no symbols found) to 1 (all symbols found).\n */\nexport interface FindabilityResult {\n  /** Path to the .sum file that was checked */\n  filePath: string;\n  /** Symbol names that were tested for presence */\n  symbolsTested: string[];\n  /** Symbol names found in AGENTS.md content */\n  symbolsFound: string[];\n  /** Symbol names missing from AGENTS.md content */\n  symbolsMissing: string[];\n  /** Ratio of found to tested (0-1) */\n  score: number;\n}\n\n/**\n * Check that key symbols from .sum files appear in AGENTS.md content.\n *\n * Currently returns an empty array since structured metadata extraction\n * (publicInterface) has been removed. The function signature is preserved\n * for future re-implementation via post-processing passes.\n *\n * @param _agentsMdContent - Full text content of the AGENTS.md file\n * @param _sumFiles - Map of file path to parsed SumFileContent\n * @returns Empty array (no structured symbols to validate)\n */\nexport function validateFindability(\n  _agentsMdContent: string,\n  _sumFiles: Map<string, SumFileContent>,\n): FindabilityResult[] {\n  return [];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts implements stub findability validation for AGENTS.md symbol coverage, currently disabled after removal of structured `publicInterface` metadata from SumFileContent schema.**\n\n## Exported Interfaces\n\n`FindabilityResult` represents validation outcome for a single `.sum` file with fields:\n- `filePath: string` — path to validated `.sum` file\n- `symbolsTested: string[]` — symbol names checked for presence\n- `symbolsFound: string[]` — symbols found in AGENTS.md content\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md content\n- `score: number` — ratio of found to tested symbols (0.0 to 1.0)\n\n## Exported Functions\n\n`validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>): FindabilityResult[]` returns empty array `[]` unconditionally. Function signature preserved for future re-implementation via post-processing passes that extract structured symbols from `.sum` files. Parameters prefixed with underscore indicate unused status.\n\n## Design Context\n\nModule originally validated that exported symbols from `.sum` files appeared in parent `AGENTS.md` content via string-based substring matching (no LLM calls). Validation logic removed after `SumFileContent.metadata.publicInterface` field was deleted from schema (referenced in comment: \"Previously relied on metadata.publicInterface which has been removed\"). Retention rationale stated in comment: \"This module is retained for future structured extraction support.\"\n\n## Integration Points\n\nImported by `src/quality/index.ts` which aggregates quality validators. `SumFileContent` type imported from `src/generation/writers/sum.ts`. Return type `FindabilityResult[]` consumed by quality reporting pipeline but produces no findings in current implementation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 405,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27024,
      "latencyMs": 14665,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/density/validator.ts",
          "sizeBytes": 1637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:46.989Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-doc.ts\n\n```typescript\n/**\n * Heuristic code-vs-doc inconsistency detection.\n *\n * Compares exported symbols in TypeScript/JavaScript source against the\n * content of the corresponding .sum file to flag documentation drift.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\nimport type { CodeDocInconsistency } from '../types.js';\n\n/**\n * Extract named and default export identifiers from TypeScript/JavaScript source.\n *\n * Matches declarations like `export function foo`, `export const BAR`,\n * `export default class App`, etc. Ignores re-exports, commented-out lines,\n * and internal (non-exported) declarations.\n *\n * @param sourceContent - Raw source file content\n * @returns Array of exported identifier names\n */\nexport function extractExports(sourceContent: string): string[] {\n  const exports: string[] = [];\n  const exportRegex =\n    /^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm;\n  let match;\n  while ((match = exportRegex.exec(sourceContent)) !== null) {\n    exports.push(match[1]);\n  }\n  return exports;\n}\n\n/**\n * Compare source exports against .sum documentation content.\n *\n * Detects two kinds of inconsistency:\n * - **missingFromDoc**: symbols exported in source but not mentioned in .sum text\n * - **missingFromCode**: items listed in `publicInterface` with no matching export\n *\n * Uses case-sensitive matching. Returns `null` when documentation is consistent.\n *\n * @param sourceContent - Raw source file content\n * @param sumContent - Parsed .sum file content\n * @param filePath - Path to the source file (used in report)\n * @returns Inconsistency descriptor, or null if consistent\n */\nexport function checkCodeVsDoc(\n  sourceContent: string,\n  sumContent: SumFileContent,\n  filePath: string,\n): CodeDocInconsistency | null {\n  const exports = extractExports(sourceContent);\n  const sumText = sumContent.summary;\n\n  // Exports present in source but not mentioned anywhere in .sum text\n  const missingFromDoc = exports.filter((e) => !sumText.includes(e));\n\n  if (missingFromDoc.length === 0) {\n    return null;\n  }\n\n  return {\n    type: 'code-vs-doc',\n    severity: 'warning',\n    filePath,\n    sumPath: `${filePath}.sum`,\n    description: `Documentation out of sync: ${missingFromDoc.length} exports undocumented`,\n    details: { missingFromDoc, missingFromCode: [] },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**code-vs-doc.ts detects documentation drift by comparing exported identifiers from TypeScript/JavaScript source files against their corresponding .sum file content, flagging symbols that exist in code but are undocumented.**\n\n## Exported Functions\n\n**extractExports(sourceContent: string): string[]** — Extracts exported identifier names from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Captures identifiers from export declarations including `export function`, `export const`, `export default class`, `export type`, `export interface`, and `export enum`. Returns array of identifier strings, ignoring re-exports, comments, and non-exported declarations.\n\n**checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null** — Compares exports extracted via `extractExports()` against `sumContent.summary` text using case-sensitive substring matching. Returns `CodeDocInconsistency` with `type: 'code-vs-doc'` and `severity: 'warning'` when exports are missing from documentation. Populates `details.missingFromDoc` with undocumented export names and `details.missingFromCode` as empty array (legacy field). Returns `null` when all exports appear in `.sum` text.\n\n## Type Dependencies\n\nConsumes `SumFileContent` from `../../generation/writers/sum.js` providing `summary: string` field for substring search. Returns `CodeDocInconsistency` from `../types.js` with discriminated union fields: `type`, `severity`, `filePath`, `sumPath`, `description`, and `details: { missingFromDoc: string[], missingFromCode: string[] }`.\n\n## Detection Strategy\n\nUses heuristic substring presence check: `sumText.includes(e)` determines whether export identifier `e` appears anywhere in `.sum` markdown content. False negatives occur when identifier appears in prose unrelated to API surface (e.g., \"the function exports data\"). No AST analysis or semantic validation performed.\n\n## Regex Limitations\n\nPattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` misses:\n- Destructured exports: `export const { foo, bar } = obj`\n- Namespace exports: `export * as ns from './mod'`\n- Re-exported identifiers: `export { foo } from './other'`\n- Dynamic exports: `module.exports = ...`\n- Multi-line declarations where export keyword and identifier span separate lines",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 632,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9897,
      "latencyMs": 18415,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-doc.ts",
          "sizeBytes": 2350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:46.439Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-code.ts\n\n```typescript\n/**\n * Heuristic cross-file inconsistency detection.\n *\n * Detects duplicate exports: the same symbol name exported from multiple\n * files within a group. The caller is responsible for scoping the file\n * list (typically per-directory) to avoid false positives across\n * unrelated modules.\n */\n\nimport { extractExports } from './code-vs-doc.js';\nimport type { CodeCodeInconsistency } from '../types.js';\n\n/**\n * Detect duplicate exports across a group of files.\n *\n * For each file, extracts exported symbol names via {@link extractExports}\n * and flags any symbol that appears in more than one file.\n *\n * This is a heuristic-only check (no AI calls). The caller should scope\n * the input to per-directory file groups to avoid false positives.\n *\n * @param files - Array of source files with path and content\n * @returns Array of detected cross-file inconsistencies\n */\nexport function checkCodeVsCode(\n  files: Array<{ path: string; content: string }>,\n): CodeCodeInconsistency[] {\n  // Map: export name -> list of file paths that export it\n  const exportMap = new Map<string, string[]>();\n\n  for (const file of files) {\n    const exports = extractExports(file.content);\n    for (const name of exports) {\n      const paths = exportMap.get(name);\n      if (paths) {\n        paths.push(file.path);\n      } else {\n        exportMap.set(name, [file.path]);\n      }\n    }\n  }\n\n  const inconsistencies: CodeCodeInconsistency[] = [];\n\n  for (const [name, paths] of exportMap) {\n    if (paths.length > 1) {\n      inconsistencies.push({\n        type: 'code-vs-code',\n        severity: 'warning',\n        files: paths,\n        description: `Symbol \"${name}\" exported from ${paths.length} files`,\n        pattern: 'duplicate-export',\n      });\n    }\n  }\n\n  return inconsistencies;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**checkCodeVsCode() detects duplicate exports across a scoped file group by aggregating symbol names via extractExports() and flagging any symbol exported from multiple files within the group.**\n\n## Exported Function\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\n\nAccepts array of file objects with `path` and `content` properties, extracts exported symbols from each file via `extractExports()` (imported from `./code-vs-doc.js`), builds aggregation map `Map<string, string[]>` where key is symbol name and value is array of file paths exporting that symbol, returns `CodeCodeInconsistency[]` array containing entries for symbols appearing in more than one file.\n\n## Algorithm\n\nInitializes `exportMap` as `Map<string, string[]>`, iterates `files` array calling `extractExports(file.content)` for each entry, populates map by appending `file.path` to existing array or creating new entry with single-element array. Second pass iterates map entries via `for (const [name, paths] of exportMap)`, pushes `CodeCodeInconsistency` object to output array when `paths.length > 1`.\n\n## Inconsistency Shape\n\nEach returned `CodeCodeInconsistency` object contains:\n- `type: 'code-vs-code'` discriminant\n- `severity: 'warning'` static level\n- `files: paths` array of all file paths exporting the duplicate symbol\n- `description: \\`Symbol \"${name}\" exported from ${paths.length} files\\`` template\n- `pattern: 'duplicate-export'` static sentinel\n\n## Scoping Constraint\n\nCaller must scope input to per-directory file groups to prevent false positives across unrelated modules (e.g., intentional re-exports, monorepo package boundaries, namespace collisions across domains). No directory validation enforced within function—relies on caller to partition files appropriately before invocation.\n\n## Dependencies\n\nImports `extractExports()` from `./code-vs-doc.js` for regex-based export symbol extraction via pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Imports `CodeCodeInconsistency` type from `../types.js`.\n\n## Design Pattern\n\nPure heuristic analysis with no AI service calls, operates entirely on static regex matching of export statements. Detection granularity limited to symbol names—no AST analysis to distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 617,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9724,
      "latencyMs": 19291,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-code.ts",
          "sizeBytes": 1774
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:43.727Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/agents-md.ts\n\n```typescript\nimport { writeFile, readFile, mkdir, rename } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Marker comment to identify generated AGENTS.md files */\nexport const GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->';\n\n/**\n * Check if an AGENTS.md file was generated by us (contains marker).\n */\nexport async function isGeneratedAgentsMd(filePath: string): Promise<boolean> {\n  try {\n    const content = await readFile(filePath, 'utf-8');\n    return content.includes(GENERATED_MARKER);\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write AGENTS.md for a directory with LLM-generated content.\n *\n * If a user-defined AGENTS.md exists (not generated by us), it will be\n * preserved as AGENTS.local.md. The user content is then prepended\n * verbatim above the generated content so AI agents see it first.\n *\n * @param dirPath - Directory to write AGENTS.md for\n * @param projectRoot - Project root for relative paths\n * @param content - LLM-generated AGENTS.md content\n * @returns Path to written AGENTS.md\n */\nexport async function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n): Promise<string> {\n  const agentsPath = path.join(dirPath, 'AGENTS.md');\n  const localPath = path.join(dirPath, 'AGENTS.local.md');\n\n  // Step 1: Preserve user-authored AGENTS.md (rename to AGENTS.local.md)\n  let userContent: string | null = null;\n  try {\n    const existingContent = await readFile(agentsPath, 'utf-8');\n    if (!existingContent.includes(GENERATED_MARKER)) {\n      userContent = existingContent;\n      await rename(agentsPath, localPath);\n    }\n  } catch {\n    // No existing AGENTS.md\n  }\n\n  // Step 2: Check for already-renamed AGENTS.local.md (from a previous run)\n  if (!userContent) {\n    try {\n      userContent = await readFile(localPath, 'utf-8');\n    } catch {\n      // No AGENTS.local.md either\n    }\n  }\n\n  // Step 3: Strip marker from LLM content (we add it ourselves)\n  let llmContent = content;\n  if (llmContent.startsWith(GENERATED_MARKER)) {\n    llmContent = llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '');\n  }\n\n  // Step 4: Build final content with marker + optional user content + LLM content\n  const parts: string[] = [GENERATED_MARKER, ''];\n\n  if (userContent?.trim()) {\n    parts.push(\n      '<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->',\n      '',\n      userContent.trim(),\n      '',\n      '---',\n      '',\n    );\n  }\n\n  parts.push(llmContent);\n\n  const finalContent = parts.join('\\n');\n\n  await mkdir(path.dirname(agentsPath), { recursive: true });\n  await writeFile(agentsPath, finalContent, 'utf-8');\n\n  return agentsPath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeAgentsMd orchestrates preservation of user-authored AGENTS.md files by renaming them to AGENTS.local.md and prepending their content above LLM-generated documentation with marker-based identification.**\n\n## Exported Functions\n\n**writeAgentsMd(dirPath: string, _projectRoot: string, content: string): Promise<string>**\nWrites AGENTS.md to dirPath with four-step preservation workflow: (1) detects existing non-generated AGENTS.md via absence of GENERATED_MARKER and renames to AGENTS.local.md, (2) reads AGENTS.local.md from previous runs if no rename occurred, (3) strips GENERATED_MARKER prefix from incoming LLM content, (4) assembles final content as `GENERATED_MARKER` + optional user content block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter + horizontal rule + LLM content. Returns written file path.\n\n**isGeneratedAgentsMd(filePath: string): Promise<boolean>**\nReturns true if file at filePath contains GENERATED_MARKER substring via readFile, false on read errors (treats missing files as non-generated).\n\n## Marker Constant\n\n**GENERATED_MARKER**\nString constant `'<!-- Generated by agents-reverse-engineer -->'` used for detection of tool-generated AGENTS.md files versus user-authored ones. Referenced by isGeneratedAgentsMd for substring matching and writeAgentsMd for marker injection/stripping.\n\n## User Content Preservation Strategy\n\nwriteAgentsMd implements two-path detection: (1) rename existing AGENTS.md to AGENTS.local.md if non-generated, (2) read AGENTS.local.md if already exists from prior runs. User content prepended with comment header `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` followed by content, horizontal rule separator `---`, then LLM content. Ensures AI assistants see user-defined context before generated summaries.\n\n## Content Assembly Logic\n\nLLM content undergoes marker stripping: if content.startsWith(GENERATED_MARKER), slices marker length and removes leading newlines via `/^\\n+/` regex. Final assembly uses parts array joined with `\\n`: marker, empty line, optional user block (comment + content + rule), LLM content. Directory creation via mkdir with `recursive: true` ensures parent paths exist before writeFile.\n\n## Error Handling\n\nBoth readFile operations (existing AGENTS.md, AGENTS.local.md) wrapped in try-catch blocks that silently continue on errors, treating absence as null userContent. No explicit error propagation for missing files—only writeFile failures bubble up to caller.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 618,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10038,
      "latencyMs": 22513,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/agents-md.ts",
          "sizeBytes": 2636
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:53.014Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/.github/workflows/publish.yml\n\n```yaml\nname: Publish to npm\n\non:\n  release:\n    types: [published]\n  workflow_dispatch:\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          registry-url: 'https://registry.npmjs.org'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npm run build\n\n      - name: Publish to npm\n        run: npm publish --provenance --access public\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GitHub Actions workflow automating npm package publication with Sigstore-signed provenance attestation on release events.**\n\n## Workflow Triggers\n\nExecutes on `release[published]` GitHub events and manual `workflow_dispatch` invocation.\n\n## Job Configuration\n\nSingle `publish` job runs on `ubuntu-latest` with permissions `contents: read` and `id-token: write` (enables OIDC authentication for provenance signing).\n\n## Build and Publish Steps\n\nExecutes `actions/checkout@v4` to clone repository, then `actions/setup-node@v4` with `node-version: '20'` and `registry-url: 'https://registry.npmjs.org'` to configure npm registry authentication.\n\nRuns `npm ci` for dependency installation (clean install from lockfile), then `npm run build` invoking TypeScript compilation via `tsc` and hook copying via `build:hooks` script (triggered through `prepublishOnly` lifecycle hook).\n\nExecutes `npm publish --provenance --access public` with `NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}` environment variable for registry authentication. The `--provenance` flag generates Sigstore-signed attestation linking published tarball to source commit SHA via GitHub's OIDC provider, creating cryptographic proof of build integrity recorded in Sigstore's public transparency log. The `--access public` flag ensures package visibility on npm registry.\n\n## Integration with Project Build System\n\nDepends on `prepublishOnly` script in `package.json` executing `npm run build && npm run build:hooks`, which compiles `src/` to `dist/` via TypeScript compiler and copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion via `scripts/build-hooks.js`.\n\n## Security Model\n\nUses GitHub Actions secrets (`NPM_TOKEN`) for npm registry write access and GitHub's native OIDC identity provider (`id-token: write` permission) for provenance attestation without requiring long-lived signing keys.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 456,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9397,
      "latencyMs": 14897,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".github/workflows/publish.yml",
          "sizeBytes": 668
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:47.894Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/reporter.ts\n\n```typescript\n/**\n * Structured report builder and CLI formatter for inconsistency results.\n *\n * Aggregates individual inconsistency issues into a typed report with\n * summary counts, and formats the report as human-readable plain text\n * for CLI output. No color dependencies -- the reporter stays pure\n * so it can be tested easily.\n */\n\nimport type {\n  Inconsistency,\n  InconsistencyReport,\n} from '../types.js';\n\n/**\n * Build a structured inconsistency report from a list of issues.\n *\n * Computes summary counts by type (code-vs-doc, code-vs-code) and\n * severity (error, warning, info). Attaches run metadata including\n * timestamp, project root, files checked, and duration.\n *\n * @param issues - All detected inconsistencies\n * @param metadata - Run context (projectRoot, filesChecked, durationMs)\n * @returns Structured report with issues and summary counts\n */\nexport function buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number },\n): InconsistencyReport {\n  let codeVsDoc = 0;\n  let codeVsCode = 0;\n  let phantomPaths = 0;\n  let errors = 0;\n  let warnings = 0;\n  let info = 0;\n\n  for (const issue of issues) {\n    if (issue.type === 'code-vs-doc') codeVsDoc++;\n    if (issue.type === 'code-vs-code') codeVsCode++;\n    if (issue.type === 'phantom-path') phantomPaths++;\n    if (issue.severity === 'error') errors++;\n    if (issue.severity === 'warning') warnings++;\n    if (issue.severity === 'info') info++;\n  }\n\n  return {\n    metadata: {\n      timestamp: new Date().toISOString(),\n      projectRoot: metadata.projectRoot,\n      filesChecked: metadata.filesChecked,\n      durationMs: metadata.durationMs,\n    },\n    issues,\n    summary: {\n      total: issues.length,\n      codeVsDoc,\n      codeVsCode,\n      phantomPaths,\n      errors,\n      warnings,\n      info,\n    },\n  };\n}\n\n/**\n * Format an inconsistency report as human-readable plain text.\n *\n * Output format:\n * ```\n * === Inconsistency Report ===\n * Checked 42 files in 150ms\n * Found 3 issue(s)\n *\n * [ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n *   File: src/foo/bar.ts\n *\n * [WARN] Symbol \"Config\" exported from 2 files\n *   Files: src/config/a.ts, src/config/b.ts\n * ```\n *\n * Uses plain text only (no picocolors). Color can be added by the\n * CLI layer if needed.\n *\n * @param report - The structured inconsistency report\n * @returns Formatted string for stderr output\n */\nexport function formatReportForCli(report: InconsistencyReport): string {\n  const lines: string[] = [];\n\n  lines.push('=== Inconsistency Report ===');\n  lines.push(`Checked ${report.metadata.filesChecked} files in ${report.metadata.durationMs}ms`);\n  lines.push(`Found ${report.summary.total} issue(s)`);\n  lines.push('');\n\n  for (const issue of report.issues) {\n    const severityTag =\n      issue.severity === 'error' ? '[ERROR]' :\n      issue.severity === 'warning' ? '[WARN]' :\n      '[INFO]';\n\n    lines.push(`${severityTag} ${issue.description}`);\n\n    if (issue.type === 'code-vs-doc') {\n      lines.push(`  File: ${issue.filePath}`);\n    } else if (issue.type === 'phantom-path') {\n      lines.push(`  Doc: ${issue.agentsMdPath}`);\n      lines.push(`  Path: ${issue.details.referencedPath}`);\n    } else {\n      lines.push(`  Files: ${issue.files.join(', ')}`);\n    }\n\n    lines.push('');\n  }\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**reporter.ts aggregates inconsistency issues into structured InconsistencyReport objects with summary counts and formats them as plain-text CLI output without color dependencies.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nConstructs InconsistencyReport by counting issues across three dimensions: type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). Returns object with `metadata` (timestamp ISO 8601, projectRoot, filesChecked, durationMs), `issues[]` array, and `summary` object containing `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info` integer counts. Iterates issues once using discriminated union pattern on `type` and `severity` fields.\n\n### formatReportForCli\n\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms InconsistencyReport into newline-delimited plain text with no ANSI color codes (caller applies picocolors if needed). Output format:\n\n```\n=== Inconsistency Report ===\nChecked {filesChecked} files in {durationMs}ms\nFound {total} issue(s)\n\n[ERROR|WARN|INFO] {description}\n  File: {filePath}              (for code-vs-doc)\n  Doc: {agentsMdPath}           (for phantom-path)\n  Path: {referencedPath}        (for phantom-path)\n  Files: {files.join(', ')}     (for code-vs-code)\n```\n\nMaps severity to tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`. Appends blank line after each issue block. Uses ternary operator chain for tag selection.\n\n## Type Dependencies\n\nImports `Inconsistency` and `InconsistencyReport` from `../types.js`. Inconsistency is discriminated union handling three issue types with distinct `filePath`, `files[]`, or `agentsMdPath` + `details.referencedPath` properties depending on type field.\n\n## Design Rationale\n\nModule designed for testability by avoiding picocolors dependency—pure string concatenation only. CLI layer applies color formatting to returned string. Summary aggregation uses simple counter variables instead of reduce/groupBy to minimize allocations during validation phase when thousands of inconsistencies may exist.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 640,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10312,
      "latencyMs": 20759,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/reporter.ts",
          "sizeBytes": 3366
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:49.521Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/validator.ts\n\n```typescript\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { PhantomPathInconsistency } from '../types.js';\n\n/**\n * Regex patterns matching path-like references in AGENTS.md content.\n *\n * Matches patterns like:\n * - `src/foo/bar.ts` (source file paths in backticks)\n * - `../foo/bar.js` (relative import paths in backticks)\n * - [text](./path) (markdown links)\n */\nconst PATH_PATTERNS: RegExp[] = [\n  // Markdown links: [text](./path) or [text](path)\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,\n  // Backtick-quoted paths: `src/foo/bar.ts` or `../foo/bar.js`\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,\n  // Prose paths: \"from src/foo/\" or \"in src/foo/bar.ts\"\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi,\n];\n\n/**\n * Paths/patterns to skip during validation (not actual file references).\n */\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,          // template placeholders\n  /\\$\\{/,          // template literals\n  /\\*/,            // glob patterns\n  /\\{[^}]*,[^}]*\\}/,  // brace expansion: {a,b,c}\n];\n\n/**\n * Check an AGENTS.md file for phantom path references.\n *\n * Extracts all path-like strings from the document, resolves them\n * relative to the AGENTS.md file location, and verifies they exist.\n *\n * @param agentsMdPath - Absolute path to the AGENTS.md file\n * @param content - Content of the AGENTS.md file\n * @param projectRoot - Project root for resolving src/ paths\n * @returns Array of phantom path inconsistencies\n */\nexport function checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[] {\n  const issues: PhantomPathInconsistency[] = [];\n  const agentsMdDir = path.dirname(agentsMdPath);\n  const seen = new Set<string>();\n\n  for (const pattern of PATH_PATTERNS) {\n    pattern.lastIndex = 0;\n    let match;\n\n    while ((match = pattern.exec(content)) !== null) {\n      const rawPath = match[1];\n      if (!rawPath || seen.has(rawPath)) continue;\n      if (SKIP_PATTERNS.some((p) => p.test(rawPath))) continue;\n\n      seen.add(rawPath);\n\n      // Try resolving relative to AGENTS.md location\n      const fromAgentsMd = path.resolve(agentsMdDir, rawPath);\n      // Try resolving relative to project root (for src/ paths)\n      const fromRoot = path.resolve(projectRoot, rawPath);\n\n      // Strip .js extension and try .ts (TypeScript import convention)\n      const tryPaths = [fromAgentsMd, fromRoot];\n      if (rawPath.endsWith('.js')) {\n        tryPaths.push(fromAgentsMd.replace(/\\.js$/, '.ts'));\n        tryPaths.push(fromRoot.replace(/\\.js$/, '.ts'));\n      }\n\n      const exists = tryPaths.some((p) => existsSync(p));\n\n      if (!exists) {\n        // Find the line containing this reference for context\n        const lines = content.split('\\n');\n        const contextLine = lines.find((l) => l.includes(rawPath)) ?? '';\n\n        issues.push({\n          type: 'phantom-path',\n          severity: 'warning',\n          agentsMdPath: path.relative(projectRoot, agentsMdPath),\n          description: `Phantom path reference: \"${rawPath}\" does not exist`,\n          details: {\n            referencedPath: rawPath,\n            resolvedTo: path.relative(projectRoot, fromAgentsMd),\n            context: contextLine.trim().slice(0, 120),\n          },\n        });\n      }\n    }\n  }\n\n  return issues;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts extracts path-like strings from AGENTS.md content via three regex patterns, resolves them relative to both the AGENTS.md directory and project root with .ts/.js extension fallback, and reports unresolved references as PhantomPathInconsistency issues.**\n\n## Exported Function\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` scans AGENTS.md content for path references, attempts multi-strategy resolution, and returns phantom path issues with severity='warning', type='phantom-path', including referencedPath, resolvedTo, and contextLine details.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` array contains three RegExp patterns for extracting path-like references:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets starting with `.`\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths starting with `src/`, `./`, or `../` ending in 1-4 letter extension\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded `src/` paths following keywords \"from\", \"in\", \"by\", \"via\", or \"see\"\n\n## Skip Filters\n\n`SKIP_PATTERNS` array excludes non-file references:\n- `/node_modules/` — Third-party dependencies\n- `/\\.git\\//` — Git internal paths\n- `/^https?:/` — HTTP/HTTPS URLs\n- `/\\{\\{/` — Template placeholder delimiters\n- `/\\$\\{/` — Template literal expressions\n- `/\\*/` — Glob wildcard patterns\n- `/\\{[^}]*,[^}]*\\}/` — Brace expansion syntax (e.g., `{a,b,c}`)\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths` attempts resolution in four locations:\n1. `path.resolve(agentsMdDir, rawPath)` — Relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — Relative to project root (handles `src/` paths)\n3. `.ts` extension fallback for `.js` paths resolved from agentsMdDir\n4. `.ts` extension fallback for `.js` paths resolved from projectRoot\n\n`existsSync(p)` validates each candidate path via filesystem check.\n\n## Issue Context Extraction\n\nWhen unresolved, `checkPhantomPaths` searches content line-by-line via `lines.find((l) => l.includes(rawPath))` to extract contextLine containing the reference, trims it, slices to 120 characters maximum, and includes in `PhantomPathInconsistency.details.context` for debugging.\n\n## Deduplication\n\n`seen` Set tracks already-processed rawPath strings to prevent duplicate issues for the same path reference appearing multiple times in content.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 728,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10322,
      "latencyMs": 20383,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/validator.ts",
          "sizeBytes": 3338
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.973Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/templates.ts\n\n```typescript\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`typescript\n{{CONTENT}}\n\\`\\`\\`\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exports six string constants defining system and user prompts for ARE's three-phase documentation pipeline (file analysis, directory aggregation, root synthesis) with density/anchor-preservation rules, incremental update strategies, and behavioral contract reproduction requirements.**\n\n## Exported Constants\n\n- `FILE_SYSTEM_PROMPT` — System prompt for file-level `.sum` generation instructing AI to produce dense, identifier-rich summaries with mandatory behavioral contract preservation (regex patterns verbatim in backticks, format strings, magic constants, environment variables). Defines density rules prohibiting filler phrases (\"this file\", \"provides\", \"responsible for\"), requires every sentence reference specific identifiers, mandates all exported function/class/type/const names appear with exact casing. Specifies annex overflow mechanism for large string constants: write concise summary listing constant names, append `## Annex References` section identifying reproduction-critical constants by name and line count.\n\n- `FILE_USER_PROMPT` — User prompt template for file analysis containing `{{FILE_PATH}}` and `{{CONTENT}}` placeholders. Embeds full project structure tree in `<project-structure>` tags, instructs AI to lead with bold purpose statement `**[FileName] does X.**`, requires minimum sections (purpose + exported symbols with signatures), allows adaptive section selection based on file content.\n\n- `DIRECTORY_SYSTEM_PROMPT` — System prompt for directory-level `AGENTS.md` generation. Mandates first line `<!-- Generated by agents-reverse-engineer -->`, requires `#` heading with directory name, one-paragraph purpose statement. Defines adaptive section strategy (Contents, Subdirectories, Architecture/Data Flow, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants). Enforces path accuracy constraints: use only Import Map paths, exact directory names from Project Directory Structure, no invented/renamed module paths. Requires Behavioral Contracts section when file summaries contain regex/format specs/magic constants (preserve verbatim, do NOT paraphrase). Links to `.annex.md` files for reproduction-critical constants without inlining content. Defines AGENTS.md as \"NAVIGATIONAL INDEX\" focusing on what files do, how files relate, directory patterns.\n\n- `FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates. Instructs AI to preserve structure/headings/phrasing of existing summary where code unchanged, modify only content directly affected by changes, keep unchanged sections VERBATIM without rephrasing. Mandates behavioral contract preservation: regex patterns/format strings/magic constants from existing summary preserved verbatim unless source changed them. Enforces same density/anchor-preservation rules as `FILE_SYSTEM_PROMPT`.\n\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental `AGENTS.md` updates. Instructs AI to preserve structure/descriptions for unaffected content, modify only entries for changed file summaries/subdirectories, add entries for new files, remove entries for deleted files, avoid reorganization unless required by additions/deletions. Mandates first line `<!-- Generated by agents-reverse-engineer -->`, preserves existing purpose statement unless directory role fundamentally changed. Enforces same path accuracy/consistency/density rules as `DIRECTORY_SYSTEM_PROMPT`.\n\n- `ROOT_SYSTEM_PROMPT` — System prompt for root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`). Mandates raw markdown output without conversational text/preamble/meta-commentary. Enforces critical constraint: synthesize ONLY from provided `AGENTS.md` content, do NOT invent/extrapolate/hallucinate features/hooks/APIs/patterns/dependencies not explicitly mentioned, omit missing sections rather than guessing, every claim must be traceable to specific `AGENTS.md` file.\n\n## Behavioral Contracts\n\n**YAML Frontmatter Format:**\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Placeholder Substitution Pattern:**\n- `{{FILE_PATH}}` — Replaced with source file path in `FILE_USER_PROMPT`\n- `{{CONTENT}}` — Replaced with source file content in `FILE_USER_PROMPT`\n\n**Prohibited Filler Phrases:**\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**Output Formatting Constraints:**\n- Start with bold purpose statement: `**Purpose statement here.**`\n- No preamble, thinking, meta-commentary before purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- First line of directory docs must be exactly: `<!-- Generated by agents-reverse-engineer -->`\n\n**Annex Reference Format:**\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n## Integration Points\n\nUsed by `buildFilePrompt()` in `src/generation/prompts/builder.ts` for `.sum` generation (consumes `FILE_SYSTEM_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`). Used by `buildDirectoryPrompt()` for `AGENTS.md` generation (consumes `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`). Used by `buildRootPrompt()` for root document synthesis (consumes `ROOT_SYSTEM_PROMPT`). Template placeholders `{{FILE_PATH}}`, `{{CONTENT}}` replaced by prompt builder string interpolation.\n\n## Annex References\n\n- `FILE_SYSTEM_PROMPT` — System prompt for file-level `.sum` generation (72 lines)\n- `FILE_USER_PROMPT` — User prompt template for file analysis with project structure embedding (109 lines)\n- `DIRECTORY_SYSTEM_PROMPT` — System prompt for directory-level `AGENTS.md` generation (94 lines)\n- `FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates (45 lines)\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental `AGENTS.md` updates (70 lines)\n- `ROOT_SYSTEM_PROMPT` — System prompt for root document synthesis (12 lines)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1530,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14162,
      "latencyMs": 50747,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/templates.ts",
          "sizeBytes": 17806
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.931Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/builder.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport type { PromptContext } from './types.js';\nimport { FILE_SYSTEM_PROMPT, FILE_USER_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT, ROOT_SYSTEM_PROMPT } from './templates.js';\nimport { readSumFile, getSumPath } from '../writers/sum.js';\nimport { GENERATED_MARKER } from '../writers/agents-md.js';\nimport { extractDirectoryImports, formatImportMap } from '../../imports/index.js';\nimport { collectAgentsDocs } from '../collector.js';\n\nfunction logTemplate(debug: boolean, action: string, filePath: string, extra?: string): void {\n  if (!debug) return;\n  const rel = path.relative(process.cwd(), filePath);\n  const msg = `${pc.dim('[prompt]')} ${pc.cyan(action)} ${pc.dim('→')} ${rel}`;\n  console.error(extra ? `${msg} ${pc.dim(extra)}` : msg);\n}\n\n/**\n * Detect language from file extension for syntax highlighting.\n */\nexport function detectLanguage(filePath: string): string {\n  const ext = path.extname(filePath).toLowerCase();\n  const langMap: Record<string, string> = {\n    '.ts': 'typescript',\n    '.tsx': 'tsx',\n    '.js': 'javascript',\n    '.jsx': 'jsx',\n    '.py': 'python',\n    '.rb': 'ruby',\n    '.go': 'go',\n    '.rs': 'rust',\n    '.java': 'java',\n    '.kt': 'kotlin',\n    '.swift': 'swift',\n    '.cs': 'csharp',\n    '.php': 'php',\n    '.vue': 'vue',\n    '.svelte': 'svelte',\n    '.json': 'json',\n    '.yaml': 'yaml',\n    '.yml': 'yaml',\n    '.md': 'markdown',\n    '.css': 'css',\n    '.scss': 'scss',\n    '.html': 'html',\n  };\n  return langMap[ext] ?? 'text';\n}\n\n/**\n * Build a complete prompt for file analysis.\n */\nexport function buildFilePrompt(context: PromptContext, debug = false): {\n  system: string;\n  user: string;\n} {\n  const lang = detectLanguage(context.filePath);\n  logTemplate(debug, 'buildFilePrompt', context.filePath, `lang=${lang}`);\n\n  const planSection = context.projectPlan\n    ? `\\n\\n## Project Structure\\n\\nFull project file listing for context:\\n\\n<project-structure>\\n${context.projectPlan}\\n</project-structure>`\n    : '';\n\n  let userPrompt = FILE_USER_PROMPT\n    .replace(/\\{\\{FILE_PATH\\}\\}/g, context.filePath)\n    .replace(/\\{\\{CONTENT\\}\\}/g, context.content)\n    .replace(/\\{\\{LANG\\}\\}/g, lang)\n    .replace(/\\{\\{PROJECT_PLAN_SECTION\\}\\}/g, planSection);\n\n  // Add context files if provided\n  if (context.contextFiles && context.contextFiles.length > 0) {\n    const contextSection = context.contextFiles\n      .map(\n        (f) =>\n          `\\n### ${f.path}\\n\\`\\`\\`${detectLanguage(f.path)}\\n${f.content}\\n\\`\\`\\``\n      )\n      .join('\\n');\n    userPrompt += `\\n\\n## Related Files\\n${contextSection}`;\n  }\n\n  // For incremental updates: include existing summary and use update-specific system prompt\n  if (context.existingSum) {\n    userPrompt += `\\n\\n## Existing Summary (update this — preserve stable content, modify only what changed)\\n\\n${context.existingSum}`;\n    return {\n      system: FILE_UPDATE_SYSTEM_PROMPT,\n      user: userPrompt,\n    };\n  }\n\n  return {\n    system: FILE_SYSTEM_PROMPT,\n    user: userPrompt,\n  };\n}\n\n/**\n * Build a prompt for generating a directory-level AGENTS.md.\n *\n * Reads all .sum files in the directory, child AGENTS.md files,\n * and AGENTS.local.md to provide full context to the LLM.\n */\nexport async function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }> {\n  const relativePath = path.relative(projectRoot, dirPath) || '.';\n  const dirName = path.basename(dirPath) || 'root';\n\n  // Collect .sum file summaries and subdirectory sections in parallel\n  const entries = await readdir(dirPath, { withFileTypes: true });\n\n  const fileEntries = entries.filter(\n    (e) => e.isFile() && !e.name.endsWith('.sum') && !e.name.startsWith('.'),\n  );\n  const dirEntries = entries.filter((e) => {\n    if (!e.isDirectory()) return false;\n    if (!knownDirs) return true;\n    const relDir = path.relative(projectRoot, path.join(dirPath, e.name));\n    return knownDirs.has(relDir);\n  });\n\n  // Read all .sum files in parallel\n  const fileResults = await Promise.all(\n    fileEntries.map(async (entry) => {\n      const entryPath = path.join(dirPath, entry.name);\n      const sumPath = getSumPath(entryPath);\n      const sumContent = await readSumFile(sumPath);\n      if (sumContent) {\n        return `### ${entry.name}\\n**Purpose:** ${sumContent.metadata.purpose}\\n\\n${sumContent.summary}`;\n      }\n      return null;\n    }),\n  );\n  const fileSummaries = fileResults.filter((r): r is string => r !== null);\n\n  // Read all child AGENTS.md in parallel\n  const subdirResults = await Promise.all(\n    dirEntries.map(async (entry) => {\n      const childAgentsPath = path.join(dirPath, entry.name, 'AGENTS.md');\n      try {\n        const childContent = await readFile(childAgentsPath, 'utf-8');\n        return `### ${entry.name}/\\n${childContent}`;\n      } catch {\n        if (debug) {\n          console.error(pc.dim(`[prompt] Skipping missing ${childAgentsPath}`));\n        }\n        return null;\n      }\n    }),\n  );\n  const subdirSections = subdirResults.filter((r): r is string => r !== null);\n\n  // Check for user-defined documentation: AGENTS.local.md or non-ARE AGENTS.md\n  let localSection = '';\n  try {\n    const localContent = await readFile(path.join(dirPath, 'AGENTS.local.md'), 'utf-8');\n    localSection = `\\n## User Notes (AGENTS.local.md)\\n\\n${localContent}\\n\\nNote: Reference [AGENTS.local.md](./AGENTS.local.md) for additional documentation.`;\n  } catch {\n    // No AGENTS.local.md — check if current AGENTS.md is user-authored (first run)\n    try {\n      const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n      if (!agentsContent.includes(GENERATED_MARKER)) {\n        localSection = `\\n## User Notes (existing AGENTS.md)\\n\\n${agentsContent}\\n\\nNote: This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).`;\n      }\n    } catch {\n      // No AGENTS.md either\n    }\n  }\n\n  // Detect manifest files to hint at package root\n  const manifestNames = ['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile'];\n  const foundManifests = fileEntries\n    .filter((e) => manifestNames.includes(e.name))\n    .map((e) => e.name);\n\n  // Extract actual import statements for cross-reference accuracy\n  const sourceExtensions = /\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/;\n  const sourceFileNames = fileEntries\n    .filter((e) => sourceExtensions.test(e.name))\n    .map((e) => e.name);\n\n  const fileImports = await extractDirectoryImports(dirPath, sourceFileNames);\n  const importMapText = formatImportMap(fileImports);\n\n  logTemplate(debug, 'buildDirectoryPrompt', dirPath, `files=${fileSummaries.length} subdirs=${subdirSections.length} imports=${fileImports.length}`);\n\n  const userSections: string[] = [\n    `Generate AGENTS.md for directory: \"${relativePath}\" (${dirName})`,\n    '',\n    `## File Summaries (${fileSummaries.length} files)`,\n    '',\n    ...fileSummaries,\n  ];\n\n  if (importMapText) {\n    userSections.push(\n      '',\n      '## Import Map (verified — use these exact paths)',\n      '',\n      importMapText,\n    );\n  }\n\n  if (projectStructure) {\n    userSections.push(\n      '',\n      '## Project Directory Structure',\n      '',\n      '<project-structure>',\n      projectStructure,\n      '</project-structure>',\n    );\n  }\n\n  // Scan for annex files in the directory\n  const annexFiles = entries\n    .filter((e) => e.isFile() && e.name.endsWith('.annex.md'))\n    .map((e) => e.name);\n  if (annexFiles.length > 0) {\n    userSections.push(\n      '',\n      '## Annex Files (reproduction-critical constants)',\n      '',\n      ...annexFiles.map((f) => `- ${f}`),\n    );\n  }\n\n  if (subdirSections.length > 0) {\n    userSections.push('', '## Subdirectories', '', ...subdirSections);\n  }\n\n  if (foundManifests.length > 0) {\n    userSections.push('', '## Directory Hints', '', `Contains manifest file(s): ${foundManifests.join(', ')} — likely a package or project root.`);\n  }\n\n  if (localSection) {\n    userSections.push(localSection);\n  }\n\n  // For incremental updates: include existing AGENTS.md and use update-specific system prompt\n  if (existingAgentsMd) {\n    userSections.push(\n      '',\n      '## Existing AGENTS.md (update this — preserve stable content, modify only what changed)',\n      '',\n      existingAgentsMd,\n    );\n    return {\n      system: DIRECTORY_UPDATE_SYSTEM_PROMPT,\n      user: userSections.join('\\n'),\n    };\n  }\n\n  return {\n    system: DIRECTORY_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n/**\n * Build a prompt for generating the root CLAUDE.md document.\n *\n * Collects all generated AGENTS.md files and optional package.json,\n * embedding them directly in the prompt so the LLM does not need\n * tool access to read files.\n */\nexport async function buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }> {\n  // 1. Collect all AGENTS.md files via shared collector\n  const agentsDocs = await collectAgentsDocs(projectRoot);\n  const agentsSections: string[] = agentsDocs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  // 3. Read root package.json for project metadata\n  let packageSection = '';\n  try {\n    const pkgRaw = await readFile(path.join(projectRoot, 'package.json'), 'utf-8');\n    const pkg = JSON.parse(pkgRaw) as Record<string, unknown>;\n    const parts: string[] = [];\n    if (pkg.name) parts.push(`- **Name**: ${pkg.name}`);\n    if (pkg.version) parts.push(`- **Version**: ${pkg.version}`);\n    if (pkg.description) parts.push(`- **Description**: ${pkg.description}`);\n    if (pkg.packageManager) parts.push(`- **Package Manager**: ${pkg.packageManager}`);\n    if (pkg.scripts && typeof pkg.scripts === 'object') {\n      const scripts = Object.entries(pkg.scripts as Record<string, string>)\n        .map(([k, v]) => `  - \\`${k}\\`: \\`${v}\\``)\n        .join('\\n');\n      parts.push(`- **Scripts**:\\n${scripts}`);\n    }\n    if (parts.length > 0) {\n      packageSection = `\\n## Package Metadata (package.json)\\n\\n${parts.join('\\n')}`;\n    }\n  } catch {\n    // No package.json or parse error\n  }\n\n  logTemplate(debug, 'buildRootPrompt', projectRoot, `agents=${agentsSections.length}`);\n\n  const userSections: string[] = [\n    'Generate CLAUDE.md for the project root.',\n    '',\n    'Synthesize the following AGENTS.md files into a single comprehensive project overview document.',\n    'Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.',\n    '',\n    `## AGENTS.md Files (${agentsSections.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (packageSection) {\n    userSections.push(packageSection);\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The document MUST include:',\n    '- Project purpose and description',\n    '- Architecture overview with directory structure',\n    '- Key directories table',\n    '- Getting started (install, build, run commands)',\n    '- Key technologies and dependencies',\n    '',\n    'This document is the COMPREHENSIVE reference for the entire project.',\n    'It should contain architecture, configuration, build instructions, and project-wide patterns.',\n    'Individual file details belong in directory-level AGENTS.md files — reference them, don\\'t duplicate them.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: ROOT_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**builder.ts constructs AI prompt pairs (system/user) for the three-phase documentation pipeline: file analysis with import maps and context files, directory-level AGENTS.md aggregation with child summaries and manifests, and root CLAUDE.md synthesis from all AGENTS.md corpus.**\n\n## Exported Functions\n\n### buildFilePrompt\n\n```typescript\nbuildFilePrompt(context: PromptContext, debug = false): { system: string; user: string }\n```\n\nConstructs file-level analysis prompts by injecting `context.filePath`, `context.content`, detected language (via `detectLanguage()`), and optional `context.projectPlan` into `FILE_USER_PROMPT` template. Appends `context.contextFiles[]` as fenced code blocks if provided. For incremental updates, appends `context.existingSum` and returns `FILE_UPDATE_SYSTEM_PROMPT` instead of `FILE_SYSTEM_PROMPT`.\n\n### buildDirectoryPrompt\n\n```typescript\nasync buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }>\n```\n\nAggregates directory context for AGENTS.md generation via parallel reads:\n- Reads all `.sum` files in `dirPath` via `getSumPath()` + `readSumFile()`, formats as `### filename\\n**Purpose:** ...\\n\\nsummary`\n- Reads all child `AGENTS.md` files from subdirectories (filtered by `knownDirs` set if provided)\n- Detects user-authored documentation: reads `AGENTS.local.md` or non-generated `AGENTS.md` (checked via `GENERATED_MARKER` absence)\n- Detects manifest files via hardcoded array: `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n- Extracts actual import statements via `extractDirectoryImports()` for source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/`, formats via `formatImportMap()`\n- Scans for `.annex.md` files and lists them in \"Annex Files\" section\n- Injects optional `projectStructure` (full file tree) and `existingAgentsMd` for incremental updates\n- Returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided, else `DIRECTORY_SYSTEM_PROMPT`\n\n### buildRootPrompt\n\n```typescript\nasync buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }>\n```\n\nSynthesizes project-wide CLAUDE.md by:\n- Collecting all `AGENTS.md` files via `collectAgentsDocs(projectRoot)`, formatting as `### relativePath\\n\\ncontent`\n- Reading root `package.json` and extracting metadata: `name`, `version`, `description`, `packageManager`, `scripts` (formatted as bullet list with inline code)\n- Embedding synthesis constraints: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs...\"\n- Specifying output requirements: architecture overview, key directories table, getting started, key technologies\n- Returns `ROOT_SYSTEM_PROMPT` with no update variant (root always regenerated fully)\n\n### detectLanguage\n\n```typescript\ndetectLanguage(filePath: string): string\n```\n\nMaps file extensions to syntax highlighting language identifiers via lookup table with 22 entries (`.ts` → `\"typescript\"`, `.py` → `\"python\"`, `.md` → `\"markdown\"`, etc.), defaulting to `\"text\"` for unknown extensions.\n\n### logTemplate\n\n```typescript\nlogTemplate(debug: boolean, action: string, filePath: string, extra?: string): void\n```\n\nEmits debug logs with picocolors formatting: `[prompt] action → relativePath extra` when `debug` is true. Used by all builder functions to trace prompt construction.\n\n## Template Dependencies\n\nImports six constant templates from `./templates.js`:\n- `FILE_SYSTEM_PROMPT` — system prompt for new file analysis\n- `FILE_USER_PROMPT` — user prompt template with placeholders: `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}`\n- `FILE_UPDATE_SYSTEM_PROMPT` — system prompt for incremental file updates\n- `DIRECTORY_SYSTEM_PROMPT` — system prompt for new directory aggregation\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — system prompt for incremental directory updates\n- `ROOT_SYSTEM_PROMPT` — system prompt for root synthesis (no update variant)\n\n## Context Enrichment Strategies\n\n**File-level:**\n- Language detection for fenced code block syntax\n- Full project structure tree via `context.projectPlan` (optional)\n- Related files via `context.contextFiles[]` with auto-detected language per file\n- Existing summary for diff-based updates\n\n**Directory-level:**\n- Child `.sum` file summaries with purpose frontmatter\n- Subdirectory `AGENTS.md` recursion (one level deep)\n- Import maps with verified path constraints via static analysis\n- Manifest detection for package root hints\n- Annex file listing for reproduction-critical constants\n- User-authored documentation preservation (`AGENTS.local.md` or pre-existing `AGENTS.md`)\n- Full project structure tree (optional)\n\n**Root-level:**\n- Complete `AGENTS.md` corpus via `collectAgentsDocs()`\n- Root `package.json` metadata with script listings\n- Explicit anti-invention constraints in prompt text\n\n## Integration Points\n\n- `readSumFile()` from `../writers/sum.js` — parses YAML frontmatter + summary content\n- `getSumPath()` from `../writers/sum.js` — computes `.sum` file path from source path\n- `GENERATED_MARKER` from `../writers/agents-md.js` — detects ARE-generated vs. user-authored AGENTS.md\n- `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` — static import analysis\n- `collectAgentsDocs()` from `../collector.js` — recursive AGENTS.md tree traversal\n\n## Placeholder Syntax\n\nUser prompts use mustache-style placeholders replaced via `String.replace()` with regex flags `/g`:\n- `{{FILE_PATH}}` — source file relative path\n- `{{CONTENT}}` — file content (unescaped)\n- `{{LANG}}` — detected language identifier\n- `{{PROJECT_PLAN_SECTION}}` — project structure tree or empty string\n\n## Debug Logging Pattern\n\nAll builder functions accept optional `debug` boolean (default `false`). When enabled, calls `logTemplate()` with:\n- `buildFilePrompt`: logs `lang=<detected>`\n- `buildDirectoryPrompt`: logs `files=<count> subdirs=<count> imports=<count>`\n- `buildRootPrompt`: logs `agents=<count>`\n\nLogs written to stderr via `console.error()` with picocolors formatting: `pc.dim('[prompt]')`, `pc.cyan(action)`, `pc.dim('→')`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1793,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12997,
      "latencyMs": 52885,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/builder.ts",
          "sizeBytes": 11768
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:20.982Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/templates.ts.annex.md\n\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: templates.ts\n\nReproduction-critical source content from `templates.ts`.\nReferenced by `templates.ts.sum`.\n\n```\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`markdown\n{{CONTENT}}\n\\`\\`\\`\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**templates.ts.annex.md stores reproduction-critical prompt template constants extracted from templates.ts for verbatim reference by AI reconstruction workflows.**\n\n## File Role\n\nThis annex file preserves the complete, unmodified text of five system/user prompt templates exported by `templates.ts`: `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, and `ROOT_SYSTEM_PROMPT`. The `.annex.md` suffix signals to the ARE pipeline that this file contains overflow content too large to embed inline in `.sum` summaries, enabling exact reproduction of prompt engineering artifacts without summary truncation.\n\n## Stored Prompt Constants\n\n### FILE_SYSTEM_PROMPT\n\nMulti-paragraph system prompt for Phase 1 file analysis tasks. Defines density rules (every sentence must reference specific identifiers), anchor term preservation (exact export name casing required), behavioral contract requirements (verbatim regex/format strings), and annex overflow protocol. Specifies output format constraint: start directly with bold purpose statement, no preamble. Approximately 80 lines.\n\n### FILE_USER_PROMPT\n\nUser prompt template for file analysis containing placeholders `{{FILE_PATH}}` and `{{CONTENT}}`. Embeds full project structure tree in `<project-structure>` XML tags with 90+ file paths. Enforces \"Lead with a single bold purpose statement\" instruction. Used by `buildFilePrompt()` in `builder.ts`.\n\n### DIRECTORY_SYSTEM_PROMPT\n\nSystem prompt for Phase 2 AGENTS.md generation. Mandates first line `<!-- Generated by agents-reverse-engineer -->` marker, adaptive section selection (Contents/Subdirectories/Architecture/Stack/Patterns/Behavioral Contracts/Reproduction-Critical Constants), path accuracy constraints (use Import Map paths only, no invented paths), density rules identical to FILE_SYSTEM_PROMPT, and user notes non-repetition protocol. Approximately 95 lines.\n\n### FILE_UPDATE_SYSTEM_PROMPT\n\nIncremental update variant of FILE_SYSTEM_PROMPT. Adds critical rules: preserve existing summary structure/phrasing for unchanged code, modify only sections affected by diffs, keep behavioral contracts verbatim unless source changed, add/remove sections only when code introduces/deletes concepts. Inherits density/anchor/output format rules from FILE_SYSTEM_PROMPT. Approximately 50 lines.\n\n### DIRECTORY_UPDATE_SYSTEM_PROMPT\n\nIncremental update variant of DIRECTORY_SYSTEM_PROMPT. Adds critical rules: preserve unaffected entries/sections/ordering, modify only changed file/subdirectory entries, add/remove entries for new/deleted files, preserve Behavioral Contracts section verbatim unless source file summaries show changes, maintain annex links. Inherits path accuracy/density/consistency rules from DIRECTORY_SYSTEM_PROMPT. Approximately 80 lines.\n\n### ROOT_SYSTEM_PROMPT\n\nSystem prompt for Phase 3 root document synthesis (CLAUDE.md/GEMINI.md/OPENCODE.md). Enforces synthesis-only constraint: no invention/extrapolation beyond provided AGENTS.md content, omit missing sections rather than guessing, every claim traceable to specific AGENTS.md file. Output format: raw markdown only, no conversational preamble. Approximately 10 lines.\n\n## Embedded Project Structure\n\n`FILE_USER_PROMPT` contains hardcoded `<project-structure>` listing with 90+ file paths across 20+ directories, including:\n- Root config files: `LANGUAGES-MANIFEST.md`, `LICENSE`, `README.md`, `package.json`, `tsconfig.json`\n- CI/CD: `.github/workflows/publish.yml`\n- Documentation: `docs/INPUT.md`\n- Session hooks: `hooks/are-check-update.js`, `hooks/are-session-end.js`, `hooks/opencode-are-check-update.js`, `hooks/opencode-are-session-end.js`\n- Build scripts: `scripts/build-hooks.js`\n- Source tree: `src/` with 15 subdirectories (`ai/`, `change-detection/`, `cli/`, `config/`, `discovery/`, `generation/`, `imports/`, `installer/`, `integration/`, `orchestration/`, `output/`, `quality/`, `rebuild/`, `specify/`, `types/`, `update/`)\n- Nested subdirectories: `src/ai/backends/`, `src/ai/telemetry/`, `src/discovery/filters/`, `src/generation/prompts/`, `src/generation/writers/`, `src/quality/density/`, `src/quality/inconsistency/`, `src/quality/phantom-paths/`\n\nThis structure snapshot provides context for relative path resolution during file analysis.\n\n## Format Specifications\n\n### YAML Frontmatter Schema (Implicit)\n\nThough not defined here, prompts reference YAML frontmatter structure with fields: `generated_at` (ISO 8601 timestamp), `content_hash` (SHA-256 hex), `purpose` (one-line string), `critical_todos` (array), `related_files` (array). Example shown in DIRECTORY_SYSTEM_PROMPT documentation.\n\n### Markdown Comment Marker\n\n`DIRECTORY_SYSTEM_PROMPT` mandates exact first line: `<!-- Generated by agents-reverse-engineer -->` for all AGENTS.md files. This marker enables user-authored vs. generated file discrimination during incremental updates.\n\n## Behavioral Contract Patterns\n\n### Placeholder Syntax\n\nTemplates use Mustache-style placeholders:\n- `{{FILE_PATH}}` — replaced with relative source file path\n- `{{CONTENT}}` — replaced with source file content\n\n### XML Tag Conventions\n\n`<project-structure>` wrapper encloses hierarchical directory listings with indentation-based nesting. `</project-structure>` closing tag terminates the listing.\n\n### Instruction Repetition\n\nDensity rules appear verbatim in FILE_SYSTEM_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT with identical phrasing: \"Every sentence must reference at least one specific identifier\", \"Never use filler phrases: 'this file', 'this module', 'provides', 'responsible for', 'is used to'\", etc. This repetition enforces consistency across Phase 1/Phase 2 outputs.\n\n### Output Format Constraint\n\nFour prompts (FILE_SYSTEM_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT) enforce identical output format rules:\n- \"Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\"\n- \"Do NOT say 'Here is...', 'Now I'll...', 'Based on my analysis...', 'Let me create...', 'Perfect.'\"\n- \"Your response IS the documentation — not a message about the documentation\"\n\nROOT_SYSTEM_PROMPT uses variant phrasing: \"Do NOT say 'Here is...' or 'I've generated...' — just output the document itself.\"\n\n## Usage Context\n\nThe annex extraction workflow (triggered when a `.sum` file's \"## Annex References\" section lists this file) copies source code constant definitions into standalone `.annex.md` files, preserving:\n- Triple-backtick code fences with language tags\n- Export keyword and const declarations\n- Template literal syntax with embedded newlines\n- Exact whitespace and indentation\n\nThis file was generated by that workflow from `src/generation/prompts/templates.ts`, ensuring AI reconstruction agents can reproduce prompt engineering behavior without accessing the original TypeScript source.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1800,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14213,
      "latencyMs": 53456,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/templates.ts.annex.md",
          "sizeBytes": 17977
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:44.232Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/sum.ts\n\n```typescript\nimport { writeFile, readFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { SummaryMetadata } from '../types.js';\nimport { GENERATED_MARKER } from './agents-md.js';\n\n/**\n * Content structure for a .sum file.\n */\nexport interface SumFileContent {\n  /** Main summary text (detailed description) */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n  /** Generation timestamp */\n  generatedAt: string;\n  /** SHA-256 hash of source file content (for change detection) */\n  contentHash: string;\n}\n\n/**\n * Parse a .sum file back into structured content.\n * Returns null if file doesn't exist or is invalid.\n */\nexport async function readSumFile(sumPath: string): Promise<SumFileContent | null> {\n  try {\n    const content = await readFile(sumPath, 'utf-8');\n    return parseSumFile(content);\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Parse a YAML-style array from frontmatter.\n * Supports both inline [a, b, c] and multi-line formats.\n */\nfunction parseYamlArray(frontmatter: string, key: string): string[] {\n  // Try inline format first: key: [a, b, c]\n  const inlineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`));\n  if (inlineMatch) {\n    return inlineMatch[1]\n      .split(',')\n      .map(s => s.trim().replace(/^[\"']|[\"']$/g, ''))\n      .filter(s => s.length > 0);\n  }\n\n  // Try multi-line format:\n  // key:\n  //   - item1\n  //   - item2\n  const multiLineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm'));\n  if (multiLineMatch) {\n    return multiLineMatch[1]\n      .split('\\n')\n      .map(line => line.replace(/^\\s*-\\s*/, '').trim())\n      .filter(s => s.length > 0);\n  }\n\n  return [];\n}\n\n/**\n * Parse .sum file content into structured data.\n */\nfunction parseSumFile(content: string): SumFileContent | null {\n  try {\n    // Extract frontmatter\n    const frontmatterMatch = content.match(/^---\\n([\\s\\S]*?)\\n---\\n/);\n    if (!frontmatterMatch) return null;\n\n    const frontmatter = frontmatterMatch[1];\n    const summary = content.slice(frontmatterMatch[0].length).trim();\n\n    // Parse frontmatter (simple YAML-like parsing)\n    const generatedAt = frontmatter.match(/generated_at:\\s*(.+)/)?.[1]?.trim() ?? '';\n    const contentHash = frontmatter.match(/content_hash:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    // Parse purpose (single line value)\n    const purpose = frontmatter.match(/purpose:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    const metadata: SummaryMetadata = {\n      purpose,\n    };\n\n    // Parse optional fields\n    const criticalTodos = parseYamlArray(frontmatter, 'critical_todos');\n    if (criticalTodos.length > 0) {\n      metadata.criticalTodos = criticalTodos;\n    }\n\n    const relatedFiles = parseYamlArray(frontmatter, 'related_files');\n    if (relatedFiles.length > 0) {\n      metadata.relatedFiles = relatedFiles;\n    }\n\n    return {\n      summary,\n      metadata,\n      generatedAt,\n      contentHash,\n    };\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Format a YAML array for frontmatter.\n * Uses inline format for short arrays, multi-line for longer ones.\n */\nfunction formatYamlArray(key: string, values: string[]): string {\n  if (values.length === 0) {\n    return `${key}: []`;\n  }\n  if (values.length <= 3 && values.every(v => v.length < 40)) {\n    // Inline format for short arrays\n    return `${key}: [${values.join(', ')}]`;\n  }\n  // Multi-line format for longer arrays\n  return `${key}:\\n${values.map(v => `  - ${v}`).join('\\n')}`;\n}\n\n/**\n * Format .sum file content for writing.\n */\nfunction formatSumFile(content: SumFileContent): string {\n  const lines = [\n    '---',\n    `generated_at: ${content.generatedAt}`,\n    `content_hash: ${content.contentHash}`,\n    `purpose: ${content.metadata.purpose}`,\n  ];\n\n  // Add optional fields if present\n  if (content.metadata.criticalTodos && content.metadata.criticalTodos.length > 0) {\n    lines.push(formatYamlArray('critical_todos', content.metadata.criticalTodos));\n  }\n  if (content.metadata.relatedFiles && content.metadata.relatedFiles.length > 0) {\n    lines.push(formatYamlArray('related_files', content.metadata.relatedFiles));\n  }\n\n  lines.push('---', '');\n\n  return lines.join('\\n') + content.summary;\n}\n\n/**\n * Write a .sum file alongside a source file.\n * Creates: foo.ts -> foo.ts.sum\n *\n * @param sourcePath - Path to the source file\n * @param content - Summary content to write\n * @returns Path to the written .sum file\n */\nexport async function writeSumFile(\n  sourcePath: string,\n  content: SumFileContent\n): Promise<string> {\n  const sumPath = `${sourcePath}.sum`;\n  const dir = path.dirname(sumPath);\n\n  // Ensure directory exists\n  await mkdir(dir, { recursive: true });\n\n  // Write file\n  const formatted = formatSumFile(content);\n  await writeFile(sumPath, formatted, 'utf-8');\n\n  return sumPath;\n}\n\n/**\n * Get the .sum path for a source file.\n */\nexport function getSumPath(sourcePath: string): string {\n  return `${sourcePath}.sum`;\n}\n\n/**\n * Check if a .sum file exists for a source file.\n */\nexport async function sumFileExists(sourcePath: string): Promise<boolean> {\n  const sumPath = getSumPath(sourcePath);\n  const content = await readSumFile(sumPath);\n  return content !== null;\n}\n\n/**\n * Write an annex file alongside a source file.\n * Contains the full source content for reproduction-critical files\n * whose verbatim constants cannot fit within .sum word limits.\n *\n * Creates: foo.ts -> foo.ts.annex.md\n *\n * @param sourcePath - Absolute path to the source file\n * @param sourceContent - Full source file content\n * @returns Path to the written annex file\n */\nexport async function writeAnnexFile(\n  sourcePath: string,\n  sourceContent: string,\n): Promise<string> {\n  const annexPath = getAnnexPath(sourcePath);\n  const fileName = path.basename(sourcePath);\n  const content = [\n    GENERATED_MARKER,\n    `# Annex: ${fileName}`,\n    '',\n    `Reproduction-critical source content from \\`${fileName}\\`.`,\n    `Referenced by \\`${fileName}.sum\\`.`,\n    '',\n    '```',\n    sourceContent,\n    '```',\n    '',\n  ].join('\\n');\n  await writeFile(annexPath, content, 'utf-8');\n  return annexPath;\n}\n\n/**\n * Get the .annex.md path for a source file.\n */\nexport function getAnnexPath(sourcePath: string): string {\n  return `${sourcePath}.annex.md`;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**sum.ts implements YAML frontmatter-based `.sum` file I/O for storing AI-generated file summaries with SHA-256 content hashes and structured metadata.**\n\n## Exported Types\n\n**SumFileContent** — structured content for a `.sum` file with fields:\n- `summary: string` — main summary text (detailed description)\n- `metadata: SummaryMetadata` — extracted metadata (imported from `../types.js`)\n- `generatedAt: string` — generation timestamp (ISO 8601 format)\n- `contentHash: string` — SHA-256 hash of source file content for change detection\n\n## Core File Operations\n\n**writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>** — writes `.sum` file alongside source file (e.g., `foo.ts` → `foo.ts.sum`), creates parent directory via `mkdir({ recursive: true })`, formats content via `formatSumFile()`, returns written `.sum` path.\n\n**readSumFile(sumPath: string): Promise<SumFileContent | null>** — parses `.sum` file into structured content via `parseSumFile()`, returns `null` if file doesn't exist or parsing fails.\n\n**getSumPath(sourcePath: string): string** — returns `.sum` path by appending `.sum` to source path.\n\n**sumFileExists(sourcePath: string): Promise<boolean>** — checks if `.sum` file exists by calling `readSumFile()` and testing for non-null result.\n\n## Annex File Operations\n\n**writeAnnexFile(sourcePath: string, sourceContent: string): Promise<string>** — writes `.annex.md` file alongside source file (e.g., `foo.ts` → `foo.ts.annex.md`) containing full source content for reproduction-critical files whose verbatim constants cannot fit within `.sum` word limits, wraps source in triple-backtick code fence with markdown header using `GENERATED_MARKER` from `agents-md.js`, returns written annex path.\n\n**getAnnexPath(sourcePath: string): string** — returns `.annex.md` path by appending `.annex.md` to source path.\n\n## YAML Frontmatter Format\n\n**formatSumFile(content: SumFileContent): string** — serializes `SumFileContent` to YAML frontmatter format:\n```\n---\ngenerated_at: <ISO 8601 timestamp>\ncontent_hash: <SHA-256 hex>\npurpose: <single line string>\ncritical_todos: [<array>] or multi-line YAML\nrelated_files: [<array>] or multi-line YAML\n---\n\n<summary text>\n```\n\n**parseSumFile(content: string): SumFileContent | null** — extracts frontmatter via regex `/^---\\n([\\s\\S]*?)\\n---\\n/`, parses fields using line-based regex patterns (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), delegates array parsing to `parseYamlArray()`, returns `null` on parse failure.\n\n## YAML Array Parsing\n\n**parseYamlArray(frontmatter: string, key: string): string[]** — parses YAML array fields from frontmatter supporting two formats:\n1. Inline: `key: [a, b, c]` via regex `/key:\\s*\\[([^\\]]*)\\]/`\n2. Multi-line: `key:\\n  - item1\\n  - item2` via regex `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\nStrips quotes from inline values via `.replace(/^[\"']|[\"']$/g, '')`, trims whitespace, filters empty strings.\n\n**formatYamlArray(key: string, values: string[]): string** — serializes array to YAML frontmatter using inline format for arrays with ≤3 items where all items are <40 chars, otherwise uses multi-line format with `  - ` prefix per item.\n\n## Integration Points\n\nImports `SummaryMetadata` from `../types.js` (contains `purpose`, optional `criticalTodos`, optional `relatedFiles`).\n\nImports `GENERATED_MARKER` from `./agents-md.js` for consistent generated file headers in annex files.\n\nUses Node.js `fs/promises` for async file operations (`writeFile`, `readFile`, `mkdir`).\n\n## Behavioral Contracts\n\n**Frontmatter regex patterns:**\n- Frontmatter block: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar field: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**File naming conventions:**\n- Summary file: `<sourcePath>.sum`\n- Annex file: `<sourcePath>.annex.md`\n\n**Annex file template:**\n```\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<sourceContent>\n```\n\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1285,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28608,
      "latencyMs": 32382,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/sum.ts",
          "sizeBytes": 6247
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:58.938Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/registry.ts\n\n```typescript\n/**\n * Backend registry, factory, and auto-detection.\n *\n * Manages the set of registered AI CLI backends, selects the appropriate\n * backend at runtime via auto-detection or explicit request, and provides\n * actionable error messages when no CLI is found.\n *\n * @module\n */\n\nimport type { AIBackend } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { ClaudeBackend } from './backends/claude.js';\nimport { GeminiBackend } from './backends/gemini.js';\nimport { OpenCodeBackend } from './backends/opencode.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\n/**\n * Registry of available AI CLI backends.\n *\n * Stores backends in insertion order, which determines the priority for\n * auto-detection. Use {@link createBackendRegistry} to get a pre-populated\n * registry with all supported backends.\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const claude = registry.get('claude');\n * const all = registry.getAll(); // [ClaudeBackend, GeminiBackend, OpenCodeBackend]\n * ```\n */\nexport class BackendRegistry {\n  private readonly backends = new Map<string, AIBackend>();\n\n  /**\n   * Register a backend adapter.\n   *\n   * @param backend - The backend to register (keyed by its `name` property)\n   */\n  register(backend: AIBackend): void {\n    this.backends.set(backend.name, backend);\n  }\n\n  /**\n   * Get a specific backend by name.\n   *\n   * @param name - The backend name (e.g., \"claude\", \"gemini\", \"opencode\")\n   * @returns The backend, or `undefined` if not registered\n   */\n  get(name: string): AIBackend | undefined {\n    return this.backends.get(name);\n  }\n\n  /**\n   * Get all registered backends in priority order.\n   *\n   * @returns Array of all registered backends\n   */\n  getAll(): AIBackend[] {\n    return Array.from(this.backends.values());\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a new backend registry pre-populated with all supported backends.\n *\n * Registration order determines auto-detection priority:\n * 1. Claude (recommended, fully implemented)\n * 2. Gemini (experimental, stub)\n * 3. OpenCode (experimental, stub)\n *\n * @returns A populated {@link BackendRegistry}\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * ```\n */\nexport function createBackendRegistry(): BackendRegistry {\n  const registry = new BackendRegistry();\n  registry.register(new ClaudeBackend());\n  registry.register(new GeminiBackend());\n  registry.register(new OpenCodeBackend());\n  return registry;\n}\n\n// ---------------------------------------------------------------------------\n// Auto-detection\n// ---------------------------------------------------------------------------\n\n/**\n * Detect the first available backend on PATH in priority order.\n *\n * Iterates all registered backends and calls `isAvailable()` on each.\n * Returns the first backend whose CLI is found, or `null` if none are\n * available.\n *\n * Priority order is determined by registration order in\n * {@link createBackendRegistry}: Claude > Gemini > OpenCode.\n *\n * @param registry - The backend registry to search\n * @returns The first available backend, or `null` if none found\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * if (backend) {\n *   console.log(`Using ${backend.name} backend`);\n * }\n * ```\n */\nexport async function detectBackend(registry: BackendRegistry): Promise<AIBackend | null> {\n  for (const backend of registry.getAll()) {\n    if (await backend.isAvailable()) {\n      return backend;\n    }\n  }\n  return null;\n}\n\n// ---------------------------------------------------------------------------\n// Install instructions\n// ---------------------------------------------------------------------------\n\n/**\n * Get formatted install instructions for all registered backends.\n *\n * Returns a multi-line string suitable for error messages when no CLI\n * is found. Matches the error message template from RESEARCH.md.\n *\n * @param registry - The backend registry\n * @returns Formatted install instructions string\n *\n * @example\n * ```typescript\n * const instructions = getInstallInstructions(registry);\n * console.error(`No AI CLI found.\\n\\nInstall one of the following:\\n\\n${instructions}`);\n * ```\n */\nexport function getInstallInstructions(registry: BackendRegistry): string {\n  return registry\n    .getAll()\n    .map((backend) => backend.getInstallInstructions())\n    .join('\\n\\n');\n}\n\n// ---------------------------------------------------------------------------\n// Resolution\n// ---------------------------------------------------------------------------\n\n/**\n * Resolve a backend by name or auto-detect the best available one.\n *\n * - If `requested` is `'auto'`: runs {@link detectBackend} and throws with\n *   install instructions if nothing is found.\n * - If `requested` is a specific name: looks it up in the registry, checks\n *   availability, and throws if not found or not available.\n *\n * @param registry - The backend registry\n * @param requested - Backend name or `'auto'` for auto-detection\n * @returns The resolved backend adapter\n * @throws {AIServiceError} With code `CLI_NOT_FOUND` if no backend is available\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n *\n * // Auto-detect\n * const backend = await resolveBackend(registry, 'auto');\n *\n * // Explicit selection\n * const claude = await resolveBackend(registry, 'claude');\n * ```\n */\nexport async function resolveBackend(\n  registry: BackendRegistry,\n  requested: string | 'auto',\n): Promise<AIBackend> {\n  if (requested === 'auto') {\n    const detected = await detectBackend(registry);\n    if (detected) {\n      return detected;\n    }\n\n    const instructions = getInstallInstructions(registry);\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.`,\n    );\n  }\n\n  // Explicit backend requested\n  const backend = registry.get(requested);\n  if (!backend) {\n    const known = registry\n      .getAll()\n      .map((b) => b.name)\n      .join(', ');\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Unknown backend \"${requested}\". Available backends: ${known}`,\n    );\n  }\n\n  if (!(await backend.isAvailable())) {\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Backend \"${requested}\" is not available. The \"${backend.cliCommand}\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}`,\n    );\n  }\n\n  return backend;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**BackendRegistry manages AI CLI backend registration, auto-detection via PATH availability, and resolution with actionable install instructions when no CLI is found.**\n\n## Exported Classes\n\n**BackendRegistry** stores AIBackend instances keyed by `name` with insertion-order priority for auto-detection. `register(backend: AIBackend): void` adds backends, `get(name: string): AIBackend | undefined` retrieves by name, `getAll(): AIBackend[]` returns all registered backends in priority order.\n\n## Factory Function\n\n**createBackendRegistry(): BackendRegistry** pre-populates registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order (Claude first, OpenCode last). Registration order determines auto-detection sequence.\n\n## Auto-Detection\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `backend.isAvailable()` on each, returning first available backend or `null` if none found. Priority follows registration order: Claude > Gemini > OpenCode.\n\n## Install Instructions\n\n**getInstallInstructions(registry: BackendRegistry): string** aggregates `backend.getInstallInstructions()` from all registered backends with `\\n\\n` separators for error message formatting.\n\n## Backend Resolution\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** handles two modes:\n\n- `requested === 'auto'`: calls `detectBackend()`, throws `AIServiceError` with code `CLI_NOT_FOUND` and formatted install instructions via `getInstallInstructions()` if no backend available\n- Explicit name: calls `registry.get(requested)`, validates availability via `backend.isAvailable()`, throws `CLI_NOT_FOUND` with backend-specific install instructions if not found or unavailable\n\nError messages include `backend.cliCommand` and `backend.getInstallInstructions()` for actionable guidance.\n\n## Dependencies\n\nImports `AIBackend` and `AIServiceError` from `./types.js`, backend implementations from `./backends/claude.js`, `./backends/gemini.js`, `./backends/opencode.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 511,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28386,
      "latencyMs": 17702,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/registry.ts",
          "sizeBytes": 6848
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:59.588Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/retry.ts\n\n```typescript\n/**\n * Retry utility with exponential backoff for transient AI service failures.\n *\n * Wraps any async operation with configurable retry logic. Uses exponential\n * delays with jitter to prevent thundering herd when multiple callers hit\n * the same rate limit.\n *\n * @module\n */\n\nimport type { RetryOptions } from './types.js';\n\n/**\n * Default retry configuration values.\n *\n * Does NOT include `isRetryable` or `onRetry` since those are\n * caller-specific. Spread these defaults and provide your own predicates:\n *\n * @example\n * ```typescript\n * import { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n * import { AIServiceError } from './types.js';\n *\n * const result = await withRetry(() => callAI(prompt), {\n *   ...DEFAULT_RETRY_OPTIONS,\n *   isRetryable: (err) => err instanceof AIServiceError && err.code === 'RATE_LIMIT',\n *   onRetry: (attempt, err) => console.warn(`Retry ${attempt}:`, err),\n * });\n * ```\n */\nexport const DEFAULT_RETRY_OPTIONS = {\n  /** Maximum number of retries (3 retries = 4 total attempts) */\n  maxRetries: 3,\n  /** Base delay before first retry: 1 second */\n  baseDelayMs: 1_000,\n  /** Maximum delay cap: 8 seconds */\n  maxDelayMs: 8_000,\n  /** Exponential multiplier: delay doubles each attempt */\n  multiplier: 2,\n} as const satisfies Omit<RetryOptions, 'isRetryable' | 'onRetry'>;\n\n/**\n * Execute an async function with exponential backoff retry on failure.\n *\n * - On success: returns the result immediately.\n * - On transient failure (isRetryable returns true): waits with exponential\n *   backoff + jitter, then retries up to `maxRetries` times.\n * - On permanent failure (isRetryable returns false): throws immediately\n *   without retrying.\n * - After exhausting all retries: throws the last error.\n *\n * Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`\n * where jitter is a random value in [0, 500ms].\n *\n * @typeParam T - The return type of the wrapped function\n * @param fn - Async function to execute (and potentially retry)\n * @param options - Retry configuration including backoff timing and predicates\n * @returns The result of a successful `fn()` invocation\n * @throws The last error if all retry attempts are exhausted or if the error is not retryable\n *\n * @example\n * ```typescript\n * import { withRetry } from './retry.js';\n *\n * // Retry up to 3 times on rate limit errors\n * const response = await withRetry(\n *   () => runAICall(prompt),\n *   {\n *     maxRetries: 3,\n *     baseDelayMs: 1000,\n *     maxDelayMs: 8000,\n *     multiplier: 2,\n *     isRetryable: (err) => isRateLimitError(err),\n *     onRetry: (attempt, err) => logger.warn(`Attempt ${attempt} failed, retrying...`),\n *   },\n * );\n * ```\n */\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions,\n): Promise<T> {\n  for (let attempt = 0; attempt <= options.maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // If we've exhausted retries or the error is permanent, throw immediately.\n      if (attempt === options.maxRetries || !options.isRetryable(error)) {\n        throw error;\n      }\n\n      // Compute exponential delay with cap.\n      const exponentialDelay = options.baseDelayMs * Math.pow(options.multiplier, attempt);\n      const cappedDelay = Math.min(exponentialDelay, options.maxDelayMs);\n\n      // Add jitter (0-500ms) to prevent thundering herd.\n      const jitter = Math.random() * 500;\n      const delay = cappedDelay + jitter;\n\n      // Notify caller before waiting.\n      options.onRetry?.(attempt + 1, error);\n\n      await new Promise((resolve) => setTimeout(resolve, delay));\n    }\n  }\n\n  // This is unreachable -- the loop either returns or throws.\n  // TypeScript needs it for exhaustiveness.\n  throw new Error('withRetry: unreachable');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**retry.ts implements exponential backoff retry logic for transient AI service failures with jitter-based delay to prevent thundering herd scenarios.**\n\n## Exported Functions\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` executes an async function with exponential backoff retry on failure. Returns result immediately on success. On transient failure where `options.isRetryable(error)` returns true, waits with exponential backoff plus jitter (0-500ms random), then retries up to `options.maxRetries` times. On permanent failure where `isRetryable` returns false, throws immediately without retrying. After exhausting all retries, throws the last error. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`.\n\n## Exported Constants\n\n`DEFAULT_RETRY_OPTIONS` provides default retry configuration with `maxRetries: 3`, `baseDelayMs: 1_000`, `maxDelayMs: 8_000`, `multiplier: 2`. Does NOT include `isRetryable` or `onRetry` predicates since those are caller-specific. Type constraint: `Omit<RetryOptions, 'isRetryable' | 'onRetry'>`. Callers must spread these defaults and provide their own predicates.\n\n## Retry Behavior\n\n`withRetry` loops from `attempt = 0` to `options.maxRetries` inclusive (3 retries = 4 total attempts). On each iteration: attempts `fn()`, catches errors, checks `attempt === options.maxRetries` or `!options.isRetryable(error)` to determine if error is terminal (throws immediately), computes exponential delay as `options.baseDelayMs * Math.pow(options.multiplier, attempt)`, caps delay via `Math.min(exponentialDelay, options.maxDelayMs)`, adds jitter via `Math.random() * 500`, invokes `options.onRetry?.(attempt + 1, error)` notification callback, then awaits `setTimeout` promise before next iteration.\n\n## Integration Points\n\nImports `RetryOptions` from `./types.js`. Used by `AIService` in `src/ai/service.ts` to wrap `runSubprocess()` calls with rate limit handling. Expects caller to provide `isRetryable` predicate matching stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\") and `onRetry` callback for telemetry/logging.\n\n## Behavioral Contracts\n\nJitter range: `Math.random() * 500` adds 0-500ms random delay to computed exponential backoff. Delay cap enforced via `Math.min(exponentialDelay, options.maxDelayMs)` before jitter addition. Default base delay: `1_000`ms (1 second). Default max delay: `8_000`ms (8 seconds). Default multiplier: `2` (delay doubles each attempt).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 696,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10407,
      "latencyMs": 18370,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/retry.ts",
          "sizeBytes": 3781
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:27:56.453Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/index.ts\n\n```typescript\n/**\n * Public API for the AI service layer.\n *\n * This barrel export is the ONLY import point for the AI service layer.\n * No other module should reach into `src/ai/backends/` or `src/ai/telemetry/`\n * directly.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   AIService,\n *   createBackendRegistry,\n *   resolveBackend,\n * } from './ai/index.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Hello' });\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types and errors\n// ---------------------------------------------------------------------------\n\nexport type {\n  AIBackend,\n  AIResponse,\n  AICallOptions,\n  SubprocessResult,\n  RetryOptions,\n  TelemetryEntry,\n  RunLog,\n  FileRead,\n} from './types.js';\n\nexport { AIServiceError } from './types.js';\n\n// ---------------------------------------------------------------------------\n// Service orchestrator\n// ---------------------------------------------------------------------------\n\nexport { AIService } from './service.js';\nexport type { AIServiceOptions } from './service.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\nexport {\n  BackendRegistry,\n  createBackendRegistry,\n  resolveBackend,\n  detectBackend,\n  getInstallInstructions,\n} from './registry.js';\n\n// ---------------------------------------------------------------------------\n// Retry utility\n// ---------------------------------------------------------------------------\n\nexport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n\n// ---------------------------------------------------------------------------\n// Subprocess wrapper\n// ---------------------------------------------------------------------------\n\nexport { runSubprocess } from './subprocess.js';\n\n// ---------------------------------------------------------------------------\n// Backend utilities\n// ---------------------------------------------------------------------------\n\nexport { isCommandOnPath } from './backends/claude.js';\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Public API barrel export consolidating type definitions, service orchestration, backend registry, retry utilities, subprocess execution, and backend detection functions for the AI service layer.**\n\n## Exported Types\n\n- `AIBackend` — Interface defining backend implementation contract (name, model, pricing, call/health methods)\n- `AIResponse` — Response envelope containing `content`, `usage` (tokens/costs), and optional `cached` flag\n- `AICallOptions` — Request options with `prompt`, optional `context`, `model`, `disallowedTools`, `maxTokens`\n- `SubprocessResult` — Raw subprocess output with `stdout`, `stderr`, `exitCode`, `signal`, `timedOut`, `duration`, `error`\n- `RetryOptions` — Configuration for exponential backoff: `maxRetries`, `initialDelayMs`, `maxDelayMs`, `backoffMultiplier`, `retryableErrors`\n- `TelemetryEntry` — Single AI call record with `callId`, `timestamp`, `backend`, `model`, `prompt`, `usage`, `duration`, `error`\n- `RunLog` — Aggregated run summary with `runId`, `startedAt`, `totalInputTokens`, `totalCost`, `uniqueFilesRead`, `errorCount`\n- `FileRead` — File metadata structure with `path`, `sizeBytes`, `linesRead` for telemetry tracking\n- `AIServiceError` — Error class with `code` discriminator (\"BACKEND_ERROR\", \"SUBPROCESS_ERROR\", \"RETRY_EXHAUSTED\", etc.)\n\n## Service Orchestration\n\n- `AIService` — Primary orchestrator class managing backend invocation, retry logic, telemetry logging, and trace emission\n- `AIServiceOptions` — Constructor config with `timeoutMs`, `maxRetries`, `telemetry` (enabled/keepRuns/costThresholdUsd), `tracer`, `debug`\n\n## Backend Registry\n\n- `BackendRegistry` — Class managing available backend implementations with `register()`, `get()`, `getAvailable()`, `list()` methods\n- `createBackendRegistry()` — Factory function returning pre-populated registry with Claude, Gemini, OpenCode backends\n- `resolveBackend()` — Async function taking registry and backend name ('auto' | 'claude' | 'gemini' | 'opencode'), returns first healthy backend or throws\n- `detectBackend()` — Async function returning name of first available backend by health check order (Claude → Gemini → OpenCode)\n- `getInstallInstructions()` — Returns Map of backend names to installation command strings\n\n## Retry Utilities\n\n- `withRetry()` — Generic async retry wrapper with exponential backoff, accepts operation function and `RetryOptions`\n- `DEFAULT_RETRY_OPTIONS` — Constant defining baseline retry config: `maxRetries: 3`, `initialDelayMs: 1000`, `maxDelayMs: 32000`, `backoffMultiplier: 2`\n\n## Subprocess Execution\n\n- `runSubprocess()` — Low-level wrapper around `execFile` with resource limits (NODE_OPTIONS heap constraint, UV_THREADPOOL_SIZE=4, CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1), timeout enforcement (SIGTERM → SIGKILL escalation), process group killing (`kill(-pid)`), returns `SubprocessResult`\n\n## Backend Utilities\n\n- `isCommandOnPath()` — Async predicate checking if executable exists in PATH via `which` command, returns boolean\n\n## Module Architecture\n\nEnforces encapsulation boundary: no external module should import from `src/ai/backends/` or `src/ai/telemetry/` directly. All AI service layer dependencies must route through this barrel export. Telemetry implementation (`src/ai/telemetry/logger.ts`, `src/ai/telemetry/run-log.ts`, `src/ai/telemetry/cleanup.ts`) remains internal to service layer, accessed only via `AIService` constructor options.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 925,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9715,
      "latencyMs": 24886,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/index.ts",
          "sizeBytes": 2362
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:06.242Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/index.ts\n\n```typescript\n/**\n * Change detection module\n *\n * Provides git-based change detection for incremental updates.\n * Detects added, modified, deleted, and renamed files.\n */\nexport {\n  isGitRepo,\n  getCurrentCommit,\n  getChangedFiles,\n  computeContentHash,\n  computeContentHashFromString,\n} from './detector.js';\n\nexport type {\n  ChangeType,\n  FileChange,\n  ChangeDetectionResult,\n  ChangeDetectionOptions,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Re-exports git-based change detection functions and types from `detector.ts` and `types.ts` for incremental update workflows.**\n\n## Exported Functions\n\n- `isGitRepo(): Promise<boolean>` — checks if current working directory is a git repository\n- `getCurrentCommit(): Promise<string>` — retrieves current HEAD commit SHA\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — detects added/modified/deleted/renamed files via git diff, returns `ChangeDetectionResult` with `changes: FileChange[]` array\n- `computeContentHash(filePath: string): Promise<string>` — computes SHA-256 hash of file content (reads file from disk)\n- `computeContentHashFromString(content: string): string` — computes SHA-256 hash of string content (synchronous, no I/O)\n\n## Exported Types\n\n- `ChangeType` — discriminated union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — object with `path: string`, `status: ChangeType`, optional `oldPath?: string` (for renames)\n- `ChangeDetectionResult` — object with `changes: FileChange[]`, `baseCommit?: string`, `includesUncommitted: boolean`\n- `ChangeDetectionOptions` — configuration object controlling git diff scope (base commit, uncommitted flag, path filters)\n\n## Module Purpose\n\nServes as public API surface for `src/change-detection/` module, consumed by `src/update/orchestrator.ts` and `src/cli/update.ts` to compute file delta sets for incremental `.sum` regeneration. Hash comparison workflow: read `.sum` frontmatter `content_hash`, call `computeContentHash()` on current file, compare SHA-256 hex strings to detect modifications.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 438,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9331,
      "latencyMs": 16849,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/index.ts",
          "sizeBytes": 412
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:05.407Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/types.ts\n\n```typescript\n/**\n * Shared types for the AI service layer.\n *\n * Defines the contract for backends, responses, subprocess results,\n * retry configuration, and telemetry logging. Every AI service module\n * imports from this file.\n */\n\n// ---------------------------------------------------------------------------\n// Subprocess\n// ---------------------------------------------------------------------------\n\n/**\n * Result returned by the subprocess wrapper after a CLI process completes.\n *\n * Always populated -- even on error or timeout, the fields are filled\n * with whatever information was available.\n */\nexport interface SubprocessResult {\n  /** Standard output captured from the child process */\n  stdout: string;\n  /** Standard error captured from the child process */\n  stderr: string;\n  /** Numeric exit code (0 = success, non-zero = failure) */\n  exitCode: number;\n  /** Signal that terminated the process, or `null` if it exited normally */\n  signal: string | null;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Whether the process was killed because it exceeded its timeout */\n  timedOut: boolean;\n  /** OS PID of the child process (undefined if spawn failed) */\n  childPid?: number;\n}\n\n// ---------------------------------------------------------------------------\n// AI Call\n// ---------------------------------------------------------------------------\n\n/**\n * Input options for an AI call.\n *\n * Only `prompt` is required; all other fields have backend-specific defaults.\n */\nexport interface AICallOptions {\n  /** The prompt to send to the AI model */\n  prompt: string;\n  /** Optional system prompt to set context/behavior */\n  systemPrompt?: string;\n  /** Model identifier (e.g., \"sonnet\", \"opus\") -- backend interprets this */\n  model?: string;\n  /** Subprocess timeout in milliseconds (overrides config default) */\n  timeoutMs?: number;\n  /** Maximum number of agentic turns (backend-specific) */\n  maxTurns?: number;\n  /** Label for tracing (e.g., file path being processed) */\n  taskLabel?: string;\n}\n\n/**\n * Normalized response from any AI CLI backend.\n *\n * Every backend adapter must parse its CLI's raw output into this shape\n * so that callers never need to know which backend was used.\n */\nexport interface AIResponse {\n  /** The AI model's text response */\n  text: string;\n  /** Model identifier as reported by the backend */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Process exit code from the CLI */\n  exitCode: number;\n  /** Original CLI JSON output for debugging */\n  raw: unknown;\n}\n\n// ---------------------------------------------------------------------------\n// Backend\n// ---------------------------------------------------------------------------\n\n/**\n * Contract for an AI CLI backend adapter.\n *\n * Each supported CLI (Claude, Gemini, OpenCode) implements this interface.\n * The registry selects the appropriate backend at runtime.\n *\n * @example\n * ```typescript\n * const backend: AIBackend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Hello' });\n *   // spawn the process with backend.cliCommand and args\n * }\n * ```\n */\nexport interface AIBackend {\n  /** Human-readable backend name (e.g., \"Claude\", \"Gemini\") */\n  readonly name: string;\n  /** CLI executable name on PATH (e.g., \"claude\", \"gemini\") */\n  readonly cliCommand: string;\n\n  /** Check whether this backend's CLI is available on PATH */\n  isAvailable(): Promise<boolean>;\n\n  /** Build the CLI argument array for a given call */\n  buildArgs(options: AICallOptions): string[];\n\n  /** Parse the CLI's stdout into a normalized {@link AIResponse} */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse;\n\n  /** Get user-facing install instructions when the CLI is not found */\n  getInstallInstructions(): string;\n}\n\n// ---------------------------------------------------------------------------\n// Retry\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration for the retry utility.\n *\n * Controls exponential backoff timing and which errors are retryable.\n */\nexport interface RetryOptions {\n  /** Maximum number of retries (e.g., 3 means up to 4 total attempts) */\n  maxRetries: number;\n  /** Base delay in milliseconds before first retry */\n  baseDelayMs: number;\n  /** Maximum delay cap in milliseconds */\n  maxDelayMs: number;\n  /** Exponential multiplier applied to the base delay */\n  multiplier: number;\n  /** Predicate that returns `true` if the error is transient and retryable */\n  isRetryable: (error: unknown) => boolean;\n  /** Optional callback invoked before each retry attempt */\n  onRetry?: (attempt: number, error: unknown) => void;\n}\n\n// ---------------------------------------------------------------------------\n// Telemetry\n// ---------------------------------------------------------------------------\n\n/**\n * Record of a single file read that was sent as context to an AI call.\n */\nexport interface FileRead {\n  /** File path relative to project root */\n  path: string;\n  /** File size in bytes at time of read */\n  sizeBytes: number;\n}\n\n/**\n * Per-call telemetry log entry.\n *\n * Captures everything needed to replay or debug a single AI call\n * without re-running it.\n */\nexport interface TelemetryEntry {\n  /** ISO 8601 timestamp when the call was initiated */\n  timestamp: string;\n  /** The prompt that was sent */\n  prompt: string;\n  /** The system prompt, if one was used */\n  systemPrompt?: string;\n  /** The AI model's text response */\n  response: string;\n  /** Model identifier */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock latency in milliseconds */\n  latencyMs: number;\n  /** Process exit code */\n  exitCode: number;\n  /** Error message, if the call failed */\n  error?: string;\n  /** Number of retries that occurred before this result */\n  retryCount: number;\n  /** AI thinking/reasoning content. \"not supported\" when backend doesn't provide it */\n  thinking: string;\n  /** Files sent as context for this call */\n  filesRead: FileRead[];\n}\n\n/**\n * Per-run log file structure.\n *\n * Aggregates all {@link TelemetryEntry} instances for a single CLI run,\n * plus a computed summary for quick performance review.\n */\nexport interface RunLog {\n  /** Unique run identifier (ISO timestamp-based) */\n  runId: string;\n  /** ISO 8601 timestamp when the run started */\n  startTime: string;\n  /** ISO 8601 timestamp when the run finished */\n  endTime: string;\n  /** All individual call entries */\n  entries: TelemetryEntry[];\n  /** Aggregated summary across all entries */\n  summary: {\n    /** Total number of AI calls made */\n    totalCalls: number;\n    /** Sum of input tokens across all calls */\n    totalInputTokens: number;\n    /** Sum of output tokens across all calls */\n    totalOutputTokens: number;\n    /** Total wall-clock duration in milliseconds */\n    totalDurationMs: number;\n    /** Number of calls that resulted in an error */\n    errorCount: number;\n    /** Sum of cache read tokens across all calls */\n    totalCacheReadTokens: number;\n    /** Sum of cache creation tokens across all calls */\n    totalCacheCreationTokens: number;\n    /** Total file reads across all calls (including duplicates) */\n    totalFilesRead: number;\n    /** Unique files read (deduped by path) */\n    uniqueFilesRead: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// Errors\n// ---------------------------------------------------------------------------\n\n/** Error codes for typed error handling in the AI service layer */\nexport type AIServiceErrorCode =\n  | 'CLI_NOT_FOUND'\n  | 'TIMEOUT'\n  | 'PARSE_ERROR'\n  | 'SUBPROCESS_ERROR'\n  | 'RATE_LIMIT';\n\n/**\n * Typed error for AI service failures.\n *\n * Carries a machine-readable {@link AIServiceErrorCode} so callers can\n * branch on the error type without parsing message strings.\n *\n * @example\n * ```typescript\n * try {\n *   await callAI(options);\n * } catch (error) {\n *   if (error instanceof AIServiceError && error.code === 'RATE_LIMIT') {\n *     // handle rate limiting\n *   }\n * }\n * ```\n */\nexport class AIServiceError extends Error {\n  /** Machine-readable error code */\n  readonly code: AIServiceErrorCode;\n\n  constructor(code: AIServiceErrorCode, message: string) {\n    super(message);\n    this.name = 'AIServiceError';\n    this.code = code;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines the contract for the AI service layer through interfaces for subprocess execution, AI backend adapters, retry configuration, telemetry logging, and typed error handling.**\n\n## Core Interfaces\n\n### SubprocessResult\nCaptures the outcome of a CLI subprocess execution returned by the subprocess wrapper. Contains `stdout: string`, `stderr: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`, and optional `childPid?: number`. Always populated even on error or timeout.\n\n### AICallOptions\nInput specification for an AI call requiring only `prompt: string`. Optional fields: `systemPrompt?: string`, `model?: string` (backend-interpreted identifier like \"sonnet\" or \"opus\"), `timeoutMs?: number` (overrides config default), `maxTurns?: number` (agentic turn limit), `taskLabel?: string` (for tracing).\n\n### AIResponse\nNormalized response structure that all backend adapters must produce. Fields: `text: string` (model response), `model: string` (backend-reported identifier), `inputTokens: number`, `outputTokens: number`, `cacheReadTokens: number`, `cacheCreationTokens: number`, `durationMs: number`, `exitCode: number`, `raw: unknown` (original CLI JSON for debugging).\n\n## Backend Contract\n\n### AIBackend\nInterface implemented by Claude, Gemini, and OpenCode adapters. Readonly properties: `name: string` (human-readable like \"Claude\"), `cliCommand: string` (executable name on PATH like \"claude\"). Methods: `isAvailable(): Promise<boolean>` checks PATH availability, `buildArgs(options: AICallOptions): string[]` constructs CLI argument array, `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` normalizes CLI output, `getInstallInstructions(): string` returns user-facing installation guidance when CLI not found.\n\n## Retry Configuration\n\n### RetryOptions\nControls exponential backoff behavior. Fields: `maxRetries: number` (3 means 4 total attempts), `baseDelayMs: number` (initial wait), `maxDelayMs: number` (cap on exponential growth), `multiplier: number` (exponential factor), `isRetryable: (error: unknown) => boolean` (transient error predicate), `onRetry?: (attempt: number, error: unknown) => void` (optional callback before each retry).\n\n## Telemetry Types\n\n### FileRead\nRecords a file sent as context to an AI call. Fields: `path: string` (relative to project root), `sizeBytes: number` (file size at read time).\n\n### TelemetryEntry\nPer-call log entry capturing everything needed to replay or debug a single AI invocation. Fields: `timestamp: string` (ISO 8601), `prompt: string`, `systemPrompt?: string`, `response: string`, `model: string`, `inputTokens: number`, `outputTokens: number`, `cacheReadTokens: number`, `cacheCreationTokens: number`, `latencyMs: number`, `exitCode: number`, `error?: string`, `retryCount: number`, `thinking: string` (AI reasoning content, \"not supported\" when backend lacks this feature), `filesRead: FileRead[]`.\n\n### RunLog\nAggregates all TelemetryEntry instances for a single CLI run plus computed summary. Top-level fields: `runId: string` (ISO timestamp-based), `startTime: string`, `endTime: string`, `entries: TelemetryEntry[]`, `summary` object containing `totalCalls: number`, `totalInputTokens: number`, `totalOutputTokens: number`, `totalDurationMs: number`, `errorCount: number`, `totalCacheReadTokens: number`, `totalCacheCreationTokens: number`, `totalFilesRead: number` (including duplicates), `uniqueFilesRead: number` (deduped by path).\n\n## Error Handling\n\n### AIServiceErrorCode\nTyped error codes: `'CLI_NOT_FOUND'` (executable missing from PATH), `'TIMEOUT'` (subprocess exceeded timeoutMs), `'PARSE_ERROR'` (malformed CLI output), `'SUBPROCESS_ERROR'` (process failure), `'RATE_LIMIT'` (backend rate limiting detected via stderr patterns like \"rate limit\", \"429\", \"too many requests\", \"overloaded\").\n\n### AIServiceError\nExtends Error with machine-readable error codes. Readonly property: `code: AIServiceErrorCode`. Constructor signature: `constructor(code: AIServiceErrorCode, message: string)`. Sets `name` to `'AIServiceError'`. Enables typed catch blocks branching on `error.code` without string parsing.\n\n## Integration Notes\n\nEvery AI service module imports from this file to ensure consistent contracts across the service layer. Backend adapters in `src/ai/backends/` implement AIBackend, subprocess wrapper in `src/ai/subprocess.ts` returns SubprocessResult, retry logic in `src/ai/retry.ts` consumes RetryOptions, telemetry logger in `src/ai/telemetry/logger.ts` writes TelemetryEntry and RunLog structures, and orchestration components thread AICallOptions through worker pools.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1212,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11516,
      "latencyMs": 23225,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/types.ts",
          "sizeBytes": 8930
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:07.921Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/types.ts\n\n```typescript\n/**\n * Types for git change detection\n */\n\n/**\n * Type of change detected for a file\n */\nexport type ChangeType = 'added' | 'modified' | 'deleted' | 'renamed';\n\n/**\n * A file change detected from git diff\n */\nexport interface FileChange {\n  /** Relative path to the file (new path for renames) */\n  path: string;\n  /** Type of change */\n  status: ChangeType;\n  /** Original path for renamed files */\n  oldPath?: string;\n}\n\n/**\n * Result of change detection\n */\nexport interface ChangeDetectionResult {\n  /** Current commit hash */\n  currentCommit: string;\n  /** Commit hash we're comparing from */\n  baseCommit: string;\n  /** List of changed files */\n  changes: FileChange[];\n  /** Whether uncommitted changes were included */\n  includesUncommitted: boolean;\n}\n\n/**\n * Options for change detection\n */\nexport interface ChangeDetectionOptions {\n  /** Include uncommitted (staged and working directory) changes */\n  includeUncommitted?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces and type aliases for git-based change detection workflows, enabling hash-based incremental updates via status discrimination (`added`/`modified`/`deleted`/`renamed`) and uncommitted change inclusion.**\n\n## Exported Types\n\n**`ChangeType`** — String literal union discriminating file change status: `'added' | 'modified' | 'deleted' | 'renamed'`.\n\n**`FileChange`** — Represents single file delta with `path: string` (new path for renames), `status: ChangeType`, and optional `oldPath?: string` (populated only when `status: 'renamed'`).\n\n**`ChangeDetectionResult`** — Aggregates diff output with `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, and `includesUncommitted: boolean` flag indicating working tree inclusion.\n\n**`ChangeDetectionOptions`** — Configuration object with `includeUncommitted?: boolean` controlling whether `git status --porcelain` changes merge with `git diff` output.\n\n## Integration Context\n\nUsed by `src/update/orchestrator.ts` to compute `filesToAnalyze` (changes requiring `.sum` regeneration) vs `filesToSkip` (hash matches). The `oldPath` field enables orphan cleanup via `cleanupOrphans()` when files renamed via `git diff -M` (50% similarity threshold). The `includesUncommitted` flag determines whether session-end hooks (`are-session-end.js`) trigger auto-updates.\n\n## Discriminated Union Pattern\n\n`FileChange` uses `status` as discriminant where `oldPath` presence correlates with `status: 'renamed'`. Consumers check `change.status === 'renamed'` before accessing `change.oldPath` to avoid undefined errors.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 416,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9470,
      "latencyMs": 21278,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/types.ts",
          "sizeBytes": 946
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:05.733Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/detector.ts\n\n```typescript\n/**\n * Git change detection and content hashing\n *\n * Uses simple-git for git operations and Node.js crypto for hashing.\n */\nimport { simpleGit } from 'simple-git';\nimport { createHash } from 'node:crypto';\nimport { readFile } from 'node:fs/promises';\nimport type { FileChange, ChangeDetectionResult, ChangeDetectionOptions } from './types.js';\n\n/**\n * Check if a path is inside a git repository.\n */\nexport async function isGitRepo(projectRoot: string): Promise<boolean> {\n  const git = simpleGit(projectRoot);\n  return git.checkIsRepo();\n}\n\n/**\n * Get the current HEAD commit hash.\n */\nexport async function getCurrentCommit(projectRoot: string): Promise<string> {\n  const git = simpleGit(projectRoot);\n  const hash = await git.revparse(['HEAD']);\n  return hash.trim();\n}\n\n/**\n * Detect files changed since a base commit.\n *\n * Uses git diff with --name-status and -M for rename detection.\n * Optionally includes uncommitted changes (staged + working directory).\n */\nexport async function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult> {\n  const git = simpleGit(projectRoot);\n  const currentCommit = await getCurrentCommit(projectRoot);\n  const changes: FileChange[] = [];\n\n  // Get committed changes from baseCommit to HEAD\n  const diff = await git.diff([\n    '--name-status',\n    '-M', // Detect renames (50% similarity threshold)\n    baseCommit,\n    'HEAD',\n  ]);\n\n  // Parse diff output\n  // Format: STATUS\\tFILE (or STATUS\\tOLD\\tNEW for renames)\n  const lines = diff.trim().split('\\n').filter(line => line.length > 0);\n\n  for (const line of lines) {\n    const parts = line.split('\\t');\n    if (parts.length < 2) continue;\n\n    const status = parts[0];\n    const filePath = parts[parts.length - 1]; // Last part is always the (new) path\n\n    if (status === 'A') {\n      changes.push({ path: filePath, status: 'added' });\n    } else if (status === 'M') {\n      changes.push({ path: filePath, status: 'modified' });\n    } else if (status === 'D') {\n      changes.push({ path: filePath, status: 'deleted' });\n    } else if (status.startsWith('R')) {\n      // Rename: R100 old new (R followed by similarity percentage)\n      const oldPath = parts[1];\n      changes.push({\n        path: filePath,\n        status: 'renamed',\n        oldPath,\n      });\n    }\n  }\n\n  // Optionally include uncommitted changes\n  if (options.includeUncommitted) {\n    const status = await git.status();\n\n    // Modified but not staged\n    for (const file of status.modified) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'modified' });\n      }\n    }\n\n    // Staged for deletion\n    for (const file of status.deleted) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'deleted' });\n      }\n    }\n\n    // Untracked files (new files not yet added)\n    for (const file of status.not_added) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n\n    // Staged files (new or modified)\n    for (const file of status.staged) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n  }\n\n  return {\n    currentCommit,\n    baseCommit,\n    changes,\n    includesUncommitted: options.includeUncommitted ?? false,\n  };\n}\n\n/**\n * Compute SHA-256 hash of a file's content.\n *\n * @param filePath - Absolute path to the file\n * @returns Hex-encoded SHA-256 hash\n */\nexport async function computeContentHash(filePath: string): Promise<string> {\n  const content = await readFile(filePath);\n  return createHash('sha256').update(content).digest('hex');\n}\n\n/**\n * Compute SHA-256 hash from an already-loaded string.\n *\n * Use this when the file content is already in memory to avoid\n * a redundant disk read.\n *\n * @param content - The file content as a string\n * @returns Hex-encoded SHA-256 hash\n */\nexport function computeContentHashFromString(content: string): string {\n  return createHash('sha256').update(content).digest('hex');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detector.ts implements Git-based change detection using simple-git and SHA-256 content hashing for incremental documentation updates.**\n\n## Exported Functions\n\n### isGitRepo\n```typescript\nasync function isGitRepo(projectRoot: string): Promise<boolean>\n```\nChecks if `projectRoot` is inside a git repository by calling `simpleGit(projectRoot).checkIsRepo()`.\n\n### getCurrentCommit\n```typescript\nasync function getCurrentCommit(projectRoot: string): Promise<string>\n```\nReturns the current HEAD commit hash via `git.revparse(['HEAD'])` with trimmed whitespace.\n\n### getChangedFiles\n```typescript\nasync function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult>\n```\nDetects files changed between `baseCommit` and `HEAD` using `git diff --name-status -M` for rename detection with 50% similarity threshold. Parses diff output line-by-line splitting on tab characters to extract status codes and file paths. Maps git status codes to `FileChange` objects:\n- `A` → `{ status: 'added' }`\n- `M` → `{ status: 'modified' }`\n- `D` → `{ status: 'deleted' }`\n- `R<percentage>` → `{ status: 'renamed', oldPath }` (parts[1] is old path, parts[parts.length-1] is new path)\n\nWhen `options.includeUncommitted` is true, merges uncommitted changes from `git.status()`:\n- `status.modified` → modified files not staged\n- `status.deleted` → staged deletions\n- `status.not_added` → untracked files\n- `status.staged` → staged files\n\nDeduplicates changes using `changes.some(c => c.path === file)` check before appending uncommitted entries. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]`, and `includesUncommitted` flag.\n\n### computeContentHash\n```typescript\nasync function computeContentHash(filePath: string): Promise<string>\n```\nComputes SHA-256 hash of file content at `filePath` (absolute path) by reading via `readFile()` and returning hex-encoded digest from `createHash('sha256').update(content).digest('hex')`.\n\n### computeContentHashFromString\n```typescript\nfunction computeContentHashFromString(content: string): string\n```\nComputes SHA-256 hash from in-memory `content` string using `createHash('sha256').update(content).digest('hex')`. Avoids redundant disk reads when file content is already loaded.\n\n## Dependencies\n\nUses `simpleGit` from `simple-git` library for all git operations (`checkIsRepo`, `revparse`, `diff`, `status`). Uses `createHash` from `node:crypto` for SHA-256 digest computation. Uses `readFile` from `node:fs/promises` for async file content loading.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` for incremental update workflow: reads `.sum` file `content_hash` from YAML frontmatter, compares against `computeContentHash()` result to determine if file needs regeneration. Supports non-git workflows via SHA-256 fallback (no git calls when `isGitRepo()` returns false).\n\n## Behavioral Contracts\n\n**Diff output parsing format:** `STATUS\\tFILE` for add/modify/delete, `STATUS\\tOLD\\tNEW` for renames where STATUS is `R<percentage>` (e.g., `R100\\told/path.ts\\tnew/path.ts`).\n\n**Rename detection flag:** `-M` uses 50% similarity threshold (git default).\n\n**Deduplication strategy:** Linear `Array.some()` scan prevents duplicate entries when uncommitted changes overlap with committed changes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 937,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10470,
      "latencyMs": 26499,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/detector.ts",
          "sizeBytes": 4102
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:14.449Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/init.ts\n\n```typescript\n/**\n * `are init` command - Create default configuration\n *\n * Creates the `.agents-reverse/config.yaml` file with documented defaults.\n * Warns if configuration already exists.\n */\n\nimport path from 'node:path';\nimport { configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\n\n/**\n * Execute the `are init` command.\n *\n * Creates a default configuration file at `.agents-reverse/config.yaml`.\n * If the file already exists, logs a warning and returns without modification.\n *\n * @param root - Root directory where config will be created\n *\n * @example\n * ```typescript\n * await initCommand('.');\n * // Creates .agents-reverse/config.yaml in current directory\n * ```\n */\nexport async function initCommand(root: string, options?: { force?: boolean }): Promise<void> {\n  const resolvedRoot = path.resolve(root);\n  const configPath = path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE);\n  const force = options?.force ?? false;\n\n  const logger = createLogger({ colors: true });\n\n  try {\n    // Check if config already exists\n    if (!force && await configExists(resolvedRoot)) {\n      logger.warn(`Config already exists at ${configPath}`);\n      logger.info('Edit the file to customize exclusions and options.');\n    } else {\n      // Create default config\n      await writeDefaultConfig(resolvedRoot);\n\n      logger.info(`Created configuration at ${configPath}`);\n      logger.info('');\n      logger.info('Edit the file to customize:');\n      logger.info('  - exclude.patterns: Custom glob patterns to exclude');\n      logger.info('  - ai.concurrency: Parallel AI calls (1-20, default: auto)');\n      logger.info('  - ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)');\n      logger.info('  - ai.backend: AI backend (claude/gemini/opencode/auto)');\n      logger.info('');\n      logger.info('See README.md for full configuration reference.');\n    }\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n\n    // Permission error\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: Cannot create ${configPath}`);\n      logger.info('Check that you have write permissions to this directory.');\n      process.exit(1);\n    }\n\n    // Other error\n    logger.error(`Failed to create configuration: ${error.message}`);\n    process.exit(1);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**initCommand() creates `.agents-reverse-engineer/config.yaml` with documented defaults, warning if configuration already exists unless `--force` overrides.**\n\n## Exported Interface\n\n```typescript\nasync function initCommand(\n  root: string,\n  options?: { force?: boolean }\n): Promise<void>\n```\n\nResolves `root` to absolute path via `path.resolve()`, constructs `configPath` by joining `resolvedRoot`, `CONFIG_DIR`, and `CONFIG_FILE` constants from `../config/loader.js`. Checks existence via `configExists(resolvedRoot)` unless `options?.force` is true. Writes default configuration via `writeDefaultConfig(resolvedRoot)` when config missing or force flag set.\n\n## Logger Integration\n\nCreates logger via `createLogger({ colors: true })` for all output. Emits `logger.warn()` when config exists without force flag, `logger.info()` for creation success with customization instructions, `logger.error()` for failures.\n\n## Error Handling Strategy\n\nCatches `NodeJS.ErrnoException` and matches `error.code`:\n- `'EACCES'` or `'EPERM'` → logs permission denied message, suggests checking write permissions, calls `process.exit(1)`\n- Other codes → logs generic failure with `error.message`, calls `process.exit(1)`\n\n## User Guidance Messages\n\nOn successful creation, emits configuration customization instructions via `logger.info()`:\n- `exclude.patterns` — custom glob patterns to exclude\n- `ai.concurrency` — parallel AI calls (1-20, default: auto)\n- `ai.timeoutMs` — subprocess timeout (default: 300,000ms = 5 min)\n- `ai.backend` — AI backend (claude/gemini/opencode/auto)\n- Final line references `README.md` for full configuration reference\n\n## Dependencies\n\nImports `configExists`, `writeDefaultConfig`, `CONFIG_DIR`, `CONFIG_FILE` from `../config/loader.js` for config file operations. Imports `createLogger` from `../output/logger.js` for terminal output with picocolors formatting.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 495,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9866,
      "latencyMs": 20128,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/init.ts",
          "sizeBytes": 2388
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:00.308Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/subprocess.ts\n\n```typescript\n/**\n * Low-level subprocess wrapper for AI CLI invocations.\n *\n * This is the ONLY place in the codebase that spawns AI CLI processes.\n * Centralizes timeout enforcement, stdin piping, zombie prevention,\n * SIGKILL escalation, and exit code extraction.\n *\n * @module\n */\n\nimport { execFile } from 'node:child_process';\nimport type { ExecFileException } from 'node:child_process';\nimport type { SubprocessResult } from './types.js';\n\n/** Grace period after SIGTERM before escalating to SIGKILL (ms) */\nconst SIGKILL_GRACE_MS = 5_000;\n\n/**\n * Track active subprocesses for debugging concurrency.\n * Maps PID to spawn timestamp and command.\n */\nconst activeSubprocesses = new Map<number, { command: string; spawnedAt: number }>();\n\n/**\n * Get current count of active subprocesses.\n */\nexport function getActiveSubprocessCount(): number {\n  return activeSubprocesses.size;\n}\n\n/**\n * Get details of all active subprocesses.\n */\nexport function getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }> {\n  const now = Date.now();\n  return Array.from(activeSubprocesses.entries()).map(([pid, info]) => ({\n    pid,\n    command: info.command,\n    spawnedAt: info.spawnedAt,\n    runningMs: now - info.spawnedAt,\n  }));\n}\n\n/**\n * Options for subprocess execution.\n */\nexport interface SubprocessOptions {\n  /** Maximum time in milliseconds before the process is killed */\n  timeoutMs: number;\n  /** Optional stdin input to pipe to the process */\n  input?: string;\n  /**\n   * Callback fired synchronously when the child process is spawned.\n   * Use this for trace events that need the actual spawn time.\n   */\n  onSpawn?: (pid: number | undefined) => void;\n}\n\n/**\n * Spawn a CLI subprocess with timeout enforcement and stdin piping.\n *\n * Always resolves -- never rejects. Errors are captured in the returned\n * {@link SubprocessResult} fields (`exitCode`, `timedOut`, `stderr`) so\n * that callers can decide how to handle failures.\n *\n * When the subprocess exceeds its timeout, `execFile` sends SIGTERM.\n * If the process doesn't exit within {@link SIGKILL_GRACE_MS} after\n * SIGTERM, we escalate to SIGKILL to prevent hung processes from\n * lingering indefinitely.\n *\n * @param command - The CLI executable to run (e.g., \"claude\", \"gemini\")\n * @param args - Argument array passed to the executable\n * @param options - Timeout, optional stdin input, and spawn callback\n * @returns Resolved result with stdout, stderr, exit code, timing, and timeout flag\n *\n * @example\n * ```typescript\n * import { runSubprocess } from './subprocess.js';\n *\n * const result = await runSubprocess('claude', ['-p', '--output-format', 'json'], {\n *   timeoutMs: 120_000,\n *   input: 'Summarize this codebase',\n *   onSpawn: (pid) => console.log(`Spawned PID ${pid}`),\n * });\n *\n * if (result.timedOut) {\n *   console.error('CLI timed out after 120s');\n * } else if (result.exitCode !== 0) {\n *   console.error('CLI failed:', result.stderr);\n * } else {\n *   const response = JSON.parse(result.stdout);\n * }\n * ```\n */\nexport function runSubprocess(\n  command: string,\n  args: string[],\n  options: SubprocessOptions,\n): Promise<SubprocessResult> {\n  return new Promise((resolve) => {\n    const startTime = Date.now();\n    let sigkillTimer: ReturnType<typeof setTimeout> | undefined;\n\n    // const activeCount = activeSubprocesses.size;\n    // console.error(`[subprocess] Currently active: ${activeCount} subprocess(es)`);\n    // console.error(`[subprocess] Spawning: ${command} ${args.join(' ')}`);\n\n    const child = execFile(\n      command,\n      args,\n      {\n        timeout: options.timeoutMs,\n        killSignal: 'SIGTERM',\n        maxBuffer: 10 * 1024 * 1024, // 10MB for large AI responses\n        encoding: 'utf-8',\n        env: {\n          ...process.env,\n        },\n      },\n      (error: ExecFileException | null, stdout: string, stderr: string) => {\n        const durationMs = Date.now() - startTime;\n        // console.error(`[subprocess:${child.pid}] Callback fired after ${durationMs}ms`);\n\n        // Clear SIGKILL escalation timer -- process has exited\n        if (sigkillTimer !== undefined) {\n          // console.error(`[subprocess:${child.pid}] Clearing SIGKILL timer`);\n          clearTimeout(sigkillTimer);\n        }\n\n        // Ensure the process is fully terminated (no-op if already dead)\n        // This handles edge cases where child processes might still be running\n        // console.error(`[subprocess:${child.pid}] Attempting explicit SIGKILL cleanup`);\n        try {\n          if (child.pid !== undefined) {\n            // Kill the entire process tree by sending signal to process group\n            // Use negative PID to target the process group\n            try {\n              process.kill(-child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to process group -${child.pid}`);\n            } catch (pgError) {\n              // Process group kill failed, try single process\n              process.kill(child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to single process (pg kill failed)`);\n            }\n          }\n        } catch (killError) {\n          // Process is already dead or doesn't exist -- expected in most cases\n          // console.error(`[subprocess:${child.pid}] SIGKILL failed (process already dead): ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n\n        // Detect timeout: execFile sets `killed = true` when the process\n        // is terminated due to exceeding the timeout option.\n        const timedOut = error !== null && 'killed' in error && error.killed === true;\n\n        // Extract exit code from the error or child process.\n        // execFile puts the exit code in error.code when the process exits\n        // with a non-zero code, but error.code can also be a string like\n        // 'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'. Fall back to child.exitCode,\n        // then default to 1 for unknown failures and 0 for no error.\n        let exitCode: number;\n        if (error === null) {\n          exitCode = 0;\n        } else if (typeof error.code === 'number') {\n          exitCode = error.code;\n        } else if (child.exitCode !== null) {\n          exitCode = child.exitCode;\n        } else {\n          exitCode = 1;\n        }\n\n        // console.error(`[subprocess:${child.pid}] Exited: code=${exitCode}, timedOut=${timedOut}, signal=${error !== null && 'signal' in error ? error.signal : null}`);\n\n        // Remove from active tracking\n        if (child.pid !== undefined) {\n          activeSubprocesses.delete(child.pid);\n          // console.error(`[subprocess:${child.pid}] Removed from active list (remaining: ${activeSubprocesses.size})`);\n        }\n\n        resolve({\n          stdout: stdout ?? '',\n          stderr: stderr ?? '',\n          exitCode,\n          signal: (error !== null && 'signal' in error ? error.signal as string : null) ?? null,\n          durationMs,\n          timedOut,\n          childPid: child.pid,\n        });\n      },\n    );\n\n    // console.error(`[subprocess:${child.pid}] Spawned with PID ${child.pid}`);\n\n    // Track this subprocess\n    if (child.pid !== undefined) {\n      activeSubprocesses.set(child.pid, {\n        command: `${command} ${args.join(' ')}`,\n        spawnedAt: startTime,\n      });\n      // console.error(`[subprocess:${child.pid}] Registered in active list (total: ${activeSubprocesses.size})`);\n    }\n\n    // Notify caller of spawn (for trace events at actual spawn time)\n    options.onSpawn?.(child.pid);\n    // console.error(`[subprocess:${child.pid}] onSpawn callback invoked`);\n\n    // SIGKILL escalation: if the process doesn't exit within\n    // timeout + grace period, force-kill it. This handles cases where\n    // SIGTERM is caught/ignored by the child or its process tree.\n    if (child.pid !== undefined) {\n      const killDelay = options.timeoutMs + SIGKILL_GRACE_MS;\n      // console.error(`[subprocess:${child.pid}] Setting SIGKILL timer for ${killDelay}ms (timeout=${options.timeoutMs}ms + grace=${SIGKILL_GRACE_MS}ms)`);\n\n      sigkillTimer = setTimeout(() => {\n        // console.error(`[subprocess:${child.pid}] SIGKILL timer fired - process exceeded timeout+grace`);\n        try {\n          child.kill('SIGKILL');\n          // console.error(`[subprocess:${child.pid}] Sent escalation SIGKILL`);\n        } catch (killError) {\n          // Process may already be dead -- ignore\n          // console.error(`[subprocess:${child.pid}] Escalation SIGKILL failed: ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n      }, killDelay);\n\n      // Don't let this timer keep the event loop alive\n      sigkillTimer.unref();\n      // console.error(`[subprocess:${child.pid}] SIGKILL timer unref'd`);\n    }\n\n    // Write prompt to stdin if provided, then close the stream.\n    // IMPORTANT: Always call .end() -- the child process blocks waiting\n    // for EOF on stdin otherwise (see RESEARCH.md Pitfall 1).\n    if (options.input !== undefined && child.stdin !== null) {\n      const inputSize = Buffer.byteLength(options.input, 'utf-8');\n      // console.error(`[subprocess:${child.pid}] Writing ${inputSize} bytes to stdin`);\n      child.stdin.write(options.input);\n      child.stdin.end();\n      // console.error(`[subprocess:${child.pid}] Stdin closed`);\n    } else {\n      // console.error(`[subprocess:${child.pid}] No stdin input provided`);\n    }\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**subprocess.ts spawns AI CLI processes via execFile() with timeout enforcement, SIGTERM→SIGKILL escalation, stdin piping, process group termination, and active subprocess tracking.**\n\n## Exported Functions\n\n**runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>**\n\nSpawns a CLI subprocess using Node.js `execFile()` with comprehensive process lifecycle management. Never rejects—always resolves with `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, and `childPid`. Executes the following sequence:\n\n1. Spawns child process with `maxBuffer: 10485760` (10MB), `killSignal: 'SIGTERM'`, `encoding: 'utf-8'`\n2. Tracks spawn in `activeSubprocesses` Map with command string and `spawnedAt` timestamp\n3. Invokes `options.onSpawn?.(child.pid)` synchronously after spawn for trace event emission\n4. Writes `options.input` to `child.stdin` if provided, then calls `.end()` to close stream (prevents EOF blocking)\n5. Sets unref'd SIGKILL escalation timer for `options.timeoutMs + SIGKILL_GRACE_MS` (5000ms)\n6. On callback: clears SIGKILL timer, attempts process group kill via `process.kill(-child.pid, 'SIGKILL')` with single-process fallback, removes PID from `activeSubprocesses`, resolves promise\n7. Detects timeout via `error.killed === true` from execFile timeout mechanism\n8. Extracts exit code from `error.code` (if number), else `child.exitCode`, else defaults to 1 (failure) or 0 (success)\n\n**getActiveSubprocessCount(): number**\n\nReturns current size of `activeSubprocesses` Map for debugging concurrency issues.\n\n**getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>**\n\nReturns array of active subprocess metadata with computed `runningMs` delta from `Date.now() - spawnedAt`.\n\n## Interfaces\n\n**SubprocessOptions**\n\n```typescript\n{\n  timeoutMs: number;        // Maximum execution time before SIGTERM\n  input?: string;           // Optional stdin payload\n  onSpawn?: (pid: number | undefined) => void;  // Synchronous spawn callback\n}\n```\n\n**SubprocessResult** (imported from `./types.js`)\n\nExpected shape: `{ stdout: string, stderr: string, exitCode: number, signal: string | null, durationMs: number, timedOut: boolean, childPid: number | undefined }`\n\n## Process Lifecycle Management\n\n**Timeout Enforcement:**\n\n`execFile()` timeout option sends SIGTERM at `timeoutMs`. Unref'd escalation timer sends SIGKILL at `timeoutMs + 5000ms` via `child.kill('SIGKILL')` to handle hung processes ignoring SIGTERM.\n\n**Process Group Termination:**\n\nOn callback completion, attempts `process.kill(-child.pid, 'SIGKILL')` (negative PID targets process group) to kill entire subprocess tree. Falls back to single-process `process.kill(child.pid, 'SIGKILL')` if process group kill throws. Catch block ignores errors (process already dead).\n\n**Zombie Prevention:**\n\nExplicit SIGKILL cleanup in callback ensures no lingering child processes. Timer unref prevents event loop blocking.\n\n## Active Subprocess Tracking\n\n**activeSubprocesses: Map<number, { command: string; spawnedAt: number }>**\n\nModule-level Map tracking PIDs with command string (`${command} ${args.join(' ')}`) and spawn timestamp. Updated on spawn (`.set()`) and callback (`.delete()`). Enables debugging via `getActiveSubprocessCount()` and `getActiveSubprocesses()`.\n\n## Constants\n\n**SIGKILL_GRACE_MS = 5000**\n\nGrace period (5 seconds) between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nImported by `src/ai/service.ts` (`AIService.call()`) which wraps this with retry logic, telemetry logging, and backend-specific argument construction. The `onSpawn` callback is used by `AIService` to emit `subprocess:spawn` trace events with actual spawn timestamp.\n\n## Environment Passthrough\n\nMerges `process.env` into child environment without modification—caller (backends in `src/ai/backends/`) must set resource constraint variables (`NODE_OPTIONS`, `UV_THREADPOOL_SIZE`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS`) before invocation.\n\n## Error Handling Strategy\n\nNever throws. Captures all failure modes in `SubprocessResult`:\n- Non-zero exit codes → `exitCode` field\n- Timeout expiration → `timedOut: true`\n- Kill signals → `signal` field (e.g., `'SIGTERM'`, `'SIGKILL'`)\n- Execution errors → `stderr` field\n\nCaller decides retry/abort via pattern matching on these fields (see `src/ai/retry.ts`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1234,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11971,
      "latencyMs": 36389,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/subprocess.ts",
          "sizeBytes": 9456
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:09.909Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/discover.ts\n\n```typescript\n/**\n * `are discover` command - Discover files to analyze\n *\n * Walks a directory tree and applies filters (gitignore, vendor, binary, custom)\n * to identify files suitable for analysis.\n */\n\nimport path from 'node:path';\nimport { access, mkdir, writeFile } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createLogger } from '../output/logger.js';\nimport { createOrchestrator } from '../generation/orchestrator.js';\nimport { buildExecutionPlan, formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Options for the discover command.\n */\nexport interface DiscoverOptions {\n  /**\n   * Optional trace writer for emitting discovery events.\n   */\n  tracer?: ITraceWriter;\n\n  /**\n   * Enable debug output.\n   * @default false\n   */\n  debug?: boolean;\n}\n\n/**\n * Execute the `are discover` command.\n *\n * Discovers files in the target directory, applying all configured filters\n * (gitignore, vendor, binary, custom patterns).\n *\n * @param targetPath - Directory to scan (defaults to current working directory)\n * @param options - Command options\n *\n * @example\n * ```typescript\n * await discoverCommand('.', {});\n * ```\n */\nexport async function discoverCommand(\n  targetPath: string,\n  options: DiscoverOptions\n): Promise<void> {\n  // Resolve to absolute path (default to cwd)\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  // Load configuration (uses defaults if no config file)\n  const config = await loadConfig(resolvedPath);\n\n  const logger = createLogger({ colors: config.output.colors });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(resolvedPath);\n  progressLog.write(`=== ARE Discover (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${resolvedPath}`);\n  progressLog.write('');\n\n  logger.info(`Discovering files in ${resolvedPath}...`);\n  logger.info('');\n  progressLog.write(`Discovering files in ${resolvedPath}...`);\n\n  // Emit discovery start trace event\n  const discoveryStartTime = process.hrtime.bigint();\n  options.tracer?.emit({\n    type: 'discovery:start',\n    targetPath: resolvedPath,\n  });\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Discovering files in: ${resolvedPath}`));\n  }\n\n  // Run shared discovery pipeline (walk + filter)\n  const result = await discoverFiles(resolvedPath, config, {\n    tracer: options.tracer,\n    debug: options.debug,\n  });\n\n  // Emit discovery end trace event\n  const discoveryEndTime = process.hrtime.bigint();\n  const discoveryDurationMs = Number(discoveryEndTime - discoveryStartTime) / 1_000_000;\n  options.tracer?.emit({\n    type: 'discovery:end',\n    filesIncluded: result.included.length,\n    filesExcluded: result.excluded.length,\n    durationMs: discoveryDurationMs,\n  });\n\n  if (options.debug) {\n    console.error(\n      pc.dim(\n        `[debug] Discovery complete: ${result.included.length} files included, ${result.excluded.length} excluded`\n      )\n    );\n  }\n\n  // Log results\n  // Make paths relative for cleaner output\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  // Show each included file\n  for (const file of result.included) {\n    const rel = relativePath(file);\n    logger.file(rel);\n    progressLog.write(`  + ${rel}`);\n  }\n\n  // Show each excluded file\n  for (const excluded of result.excluded) {\n    const rel = relativePath(excluded.path);\n    logger.excluded(rel, excluded.reason, excluded.filter);\n    progressLog.write(`  - ${rel} (${excluded.reason}: ${excluded.filter})`);\n  }\n\n  // Summary\n  logger.summary(result.included.length, result.excluded.length);\n  progressLog.write(`\\nDiscovered ${result.included.length} files (${result.excluded.length} excluded)`);\n\n  // Generate GENERATION-PLAN.md\n  {\n    logger.info('');\n    logger.info('Generating execution plan...');\n    progressLog.write('');\n    progressLog.write('Generating execution plan...');\n\n    // Create discovery result for orchestrator\n    const discoveryResult: DiscoveryResult = {\n      files: result.included,\n      excluded: result.excluded.map(e => ({ path: e.path, reason: e.reason })),\n    };\n\n    // Create orchestrator and build generation plan\n    const orchestrator = createOrchestrator(config, resolvedPath);\n    const generationPlan = await orchestrator.createPlan(discoveryResult);\n\n    // Build execution plan with post-order traversal\n    const executionPlan = buildExecutionPlan(generationPlan, resolvedPath);\n\n    // Format as markdown\n    const markdown = formatExecutionPlanAsMarkdown(executionPlan);\n\n    // Write to .agents-reverse-engineer/GENERATION-PLAN.md\n    const configDir = path.join(resolvedPath, '.agents-reverse-engineer');\n    const planPath = path.join(configDir, 'GENERATION-PLAN.md');\n\n    try {\n      await mkdir(configDir, { recursive: true });\n      await writeFile(planPath, markdown, 'utf8');\n      const planRelPath = path.relative(resolvedPath, planPath);\n      logger.info(`Created ${planRelPath}`);\n      progressLog.write(`Created ${planRelPath}`);\n    } catch (err) {\n      const msg = (err as Error).message;\n      logger.error(`Failed to write plan: ${msg}`);\n      progressLog.write(`Error: Failed to write plan: ${msg}`);\n      await progressLog.finalize();\n      process.exit(1);\n    }\n  }\n\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Command entry point for `are discover` — walks directory tree with gitignore/vendor/binary/custom filters, writes discovered file list to console and `.agents-reverse-engineer/GENERATION-PLAN.md` with three-phase execution plan.**\n\n## Exported Interface\n\n**discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>**\nResolves `targetPath` to absolute path (defaults to `process.cwd()`), loads config via `loadConfig()`, verifies path accessibility with `access(R_OK)`, runs `discoverFiles()` discovery pipeline, emits trace events (`discovery:start/end` with `filesIncluded`, `filesExcluded`, `durationMs`), logs included/excluded files to console via `createLogger()` and `ProgressLog.create()`, creates `GenerationPlan` via `createOrchestrator().createPlan()`, builds `ExecutionPlan` via `buildExecutionPlan()` with post-order traversal, formats as markdown via `formatExecutionPlanAsMarkdown()`, writes to `.agents-reverse-engineer/GENERATION-PLAN.md`, exits with code 1 on `ENOENT`/`EACCES`/`EPERM` errors or plan write failure.\n\n**DiscoverOptions**\nInterface with `tracer?: ITraceWriter` for trace emission, `debug?: boolean` for verbose output (defaults false).\n\n## Trace Events\n\nEmits `discovery:start` with `targetPath` at start, `discovery:end` with `filesIncluded`, `filesExcluded`, `durationMs` computed via `process.hrtime.bigint()` delta at completion.\n\n## Progress Logging\n\nCreates `ProgressLog` instance via `ProgressLog.create(resolvedPath)`, writes header `=== ARE Discover (${new Date().toISOString()}) ===`, logs each included file as `+ ${relativePath}`, each excluded file as `- ${relativePath} (${reason}: ${filter})`, summary `Discovered ${count} files (${excluded} excluded)`, plan creation status, finalizes with `await progressLog.finalize()`.\n\n## Output Format\n\nConsole output via `createLogger()`:\n- `logger.file(relativePath)` for included files\n- `logger.excluded(relativePath, reason, filter)` for excluded files with reason/filter context\n- `logger.summary(includedCount, excludedCount)` for aggregate counts\n- `logger.info()` for status messages\n- `logger.error()` for fatal errors before `process.exit(1)`\n\nDebug output via `pc.dim()` when `options.debug` is true, writes `[debug] Discovering files in: ${path}` at start, `[debug] Discovery complete: ${included} included, ${excluded} excluded` at end.\n\n## Plan Generation Flow\n\nConstructs `DiscoveryResult` object with `files: result.included`, `excluded: result.excluded.map(e => ({ path, reason }))`, passes to `createOrchestrator(config, resolvedPath).createPlan()` yielding `GenerationPlan`, transforms via `buildExecutionPlan(generationPlan, resolvedPath)` to `ExecutionPlan`, formats via `formatExecutionPlanAsMarkdown()`, writes to `path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md')` after `mkdir(configDir, { recursive: true })`.\n\n## Error Handling\n\nCatches `access()` errors: `ENOENT` → `logger.error('Directory not found')` + exit 1, `EACCES`/`EPERM` → `logger.error('Permission denied')` + exit 1, re-throws other errors. Catches plan write errors: logs `Failed to write plan: ${msg}`, writes to progress log, finalizes, exits with code 1.\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `discoverFiles` from `../discovery/run.js`, `createLogger` from `../output/logger.js`, `createOrchestrator` from `../generation/orchestrator.js`, `buildExecutionPlan`, `formatExecutionPlanAsMarkdown` from `../generation/executor.js`, `ProgressLog` from `../orchestration/index.js`, `DiscoveryResult` type from `../types/index.js`, `ITraceWriter` type from `../orchestration/trace.js`, `picocolors` as `pc`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1058,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10993,
      "latencyMs": 32175,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/discover.ts",
          "sizeBytes": 6071
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:08.686Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/clean.ts\n\n```typescript\n/**\n * `are clean` command - Delete all generated documentation artifacts\n *\n * Removes .sum files, AGENTS.md files, root documents (CLAUDE.md),\n * and the GENERATION-PLAN.md file.\n */\n\nimport path from 'node:path';\nimport { access, readFile, rename, unlink } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport fg from 'fast-glob';\nimport pc from 'picocolors';\nimport { createLogger } from '../output/logger.js';\nimport { GENERATED_MARKER } from '../generation/writers/agents-md.js';\n\n/**\n * Options for the clean command.\n */\nexport interface CleanOptions {\n  /**\n   * Show files that would be deleted without deleting them.\n   * @default false\n   */\n  dryRun: boolean;\n}\n\n/**\n * Execute the `are clean` command.\n *\n * Finds and deletes all generated documentation artifacts:\n * - `*.sum` files\n * - `AGENTS.md` files\n * - `CLAUDE.md` at project root\n * - `.agents-reverse-engineer/GENERATION-PLAN.md`\n *\n * @param targetPath - Project root directory (defaults to current working directory)\n * @param options - Command options\n */\nexport async function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void> {\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  const logger = createLogger({ colors: true });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Find all artifacts\n  const [sumFiles, annexFiles, agentsFiles, localAgentsFiles] = await Promise.all([\n    fg.glob('**/*.sum', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/*.annex.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.local.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n  ]);\n\n  // Filter AGENTS.md to only those generated by ARE (contain the marker).\n  // User-authored AGENTS.md files (e.g. SDK docs) must not be deleted.\n  const generatedAgentsFiles: string[] = [];\n  const skippedAgentsFiles: string[] = [];\n  for (const file of agentsFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER)) {\n        generatedAgentsFiles.push(file);\n      } else {\n        skippedAgentsFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Check for root docs and plan file\n  const singleFiles: string[] = [];\n  const claudeMd = path.join(resolvedPath, 'CLAUDE.md');\n  const planFile = path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md');\n\n  for (const filePath of [claudeMd, planFile]) {\n    try {\n      await access(filePath, constants.F_OK);\n      singleFiles.push(filePath);\n    } catch {\n      // File doesn't exist - skip\n    }\n  }\n\n  const allFiles = [...sumFiles, ...annexFiles, ...generatedAgentsFiles, ...singleFiles];\n\n  if (allFiles.length === 0 && localAgentsFiles.length === 0) {\n    logger.info('No generated artifacts found.');\n    return;\n  }\n\n  // Display found files\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  if (options.dryRun) {\n    logger.info('Files that would be deleted:');\n  }\n\n  for (const file of allFiles) {\n    logger.info(`  ${relativePath(file)}`);\n  }\n\n  if (skippedAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info('Preserving user-authored AGENTS.md:');\n    for (const file of skippedAgentsFiles) {\n      logger.info(`  ${relativePath(file)}`);\n    }\n  }\n\n  if (localAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info(options.dryRun ? 'Files that would be restored:' : 'Restoring user-defined files:');\n    for (const file of localAgentsFiles) {\n      const target = path.join(path.dirname(file), 'AGENTS.md');\n      logger.info(`  ${relativePath(file)} → ${relativePath(target)}`);\n    }\n  }\n\n  logger.info('');\n  logger.info(\n    `${pc.bold(String(sumFiles.length))} .sum file(s), ` +\n    `${pc.bold(String(annexFiles.length))} .annex.md file(s), ` +\n    `${pc.bold(String(generatedAgentsFiles.length))} AGENTS.md file(s), ` +\n    `${pc.bold(String(singleFiles.length))} root doc(s), ` +\n    `${pc.bold(String(localAgentsFiles.length))} AGENTS.local.md to restore`\n  );\n\n  if (options.dryRun) {\n    logger.info('');\n    logger.info(pc.yellow('Dry run — no files were changed.'));\n    return;\n  }\n\n  // Delete all generated files\n  let deleted = 0;\n  for (const file of allFiles) {\n    try {\n      await unlink(file);\n      deleted++;\n    } catch (err) {\n      logger.error(`Failed to delete ${relativePath(file)}: ${(err as Error).message}`);\n    }\n  }\n\n  // Restore AGENTS.local.md → AGENTS.md (undo the rename from generation)\n  let restored = 0;\n  for (const localFile of localAgentsFiles) {\n    const agentsPath = path.join(path.dirname(localFile), 'AGENTS.md');\n    try {\n      await rename(localFile, agentsPath);\n      restored++;\n    } catch (err) {\n      logger.error(`Failed to restore ${relativePath(localFile)}: ${(err as Error).message}`);\n    }\n  }\n\n  logger.info('');\n  const parts = [`Deleted ${deleted} file(s)`];\n  if (restored > 0) {\n    parts.push(`restored ${restored} AGENTS.local.md file(s)`);\n  }\n  logger.info(pc.green(`${parts.join(', ')}.`));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**cleanCommand removes all generated documentation artifacts (.sum, .annex.md, AGENTS.md, CLAUDE.md, GENERATION-PLAN.md) with marker-based filtering to preserve user-authored files and restoration of AGENTS.local.md backups.**\n\n## Exported Interface\n\n**cleanCommand(targetPath: string, options: CleanOptions): Promise<void>**\nExecutes artifact deletion workflow with dry-run support, selective AGENTS.md filtering via `GENERATED_MARKER` substring search, and AGENTS.local.md → AGENTS.md restoration.\n\n**CleanOptions**\n```typescript\ninterface CleanOptions {\n  dryRun: boolean; // Preview deletions without filesystem modifications\n}\n```\n\n## Artifact Discovery Strategy\n\nUses `fast-glob` with parallel Promise.all execution to discover four artifact classes:\n- `**/*.sum` — file-level summaries with YAML frontmatter\n- `**/*.annex.md` — companion files for reproduction-critical constants\n- `**/AGENTS.md` — directory-level aggregated documentation\n- `**/AGENTS.local.md` — user-authored backups created during generation\n\nAll globs exclude `**/node_modules/**` and `**/.git/**`, enable `dot: true` for hidden directories, use `absolute: true` paths for unlink operations.\n\n## AGENTS.md Filtering Protocol\n\nReads each AGENTS.md file via `readFile(file, 'utf-8')`, performs substring search for `GENERATED_MARKER` (imported from `generation/writers/agents-md.ts`). Files containing marker added to `generatedAgentsFiles[]` for deletion, others added to `skippedAgentsFiles[]` and preserved. Prevents deletion of user-authored AGENTS.md (SDK documentation, project guides). Read errors silently skip file (no throw, no log).\n\n## Single-File Cleanup Targets\n\nChecks existence via `access(filePath, constants.F_OK)` for:\n- `CLAUDE.md` at project root (path.join(resolvedPath, 'CLAUDE.md'))\n- `.agents-reverse-engineer/GENERATION-PLAN.md` (path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md'))\n\nNon-existent files skipped without error. All single files added to `singleFiles[]` array merged with glob results.\n\n## Restoration Workflow\n\nFor each AGENTS.local.md file, computes restoration target via `path.join(path.dirname(localFile), 'AGENTS.md')`. In dry-run mode, logs `would be restored` preview. In execution mode, calls `rename(localFile, agentsPath)` to undo the backup rename performed during generation. Tracks success count in `restored` variable, reports errors individually without aborting.\n\n## Deletion Loop\n\nIterates `allFiles` array containing `.sum`, `.annex.md`, generated `AGENTS.md`, and single-file targets. Calls `unlink(file)` for each, increments `deleted` counter on success, logs error message with relative path on failure. Does not throw or exit on individual file deletion errors.\n\n## Output Formatting\n\nUses `createLogger({ colors: true })` for terminal output with picocolors styling. Relative paths computed via `path.relative(resolvedPath, absPath)` for user-friendly display. Summary line formats counts with `pc.bold()` wrappers: `${pc.bold(String(sumFiles.length))} .sum file(s)`. Dry-run mode prints yellow warning via `pc.yellow('Dry run — no files were changed.')`. Success message uses `pc.green()` wrapper.\n\n## Error Handling\n\nVerifies target directory existence/permissions via `access(resolvedPath, constants.R_OK)`. Catches ENOENT (not found), EACCES/EPERM (permission denied), logs error and exits with code 1. Other errors rethrown. Individual file operation failures logged but do not abort command execution.\n\n## Dependencies\n\n- `fast-glob` — parallel artifact discovery with glob patterns\n- `picocolors` — ANSI color formatting for terminal output\n- `createLogger` from `output/logger.ts` — console logging abstraction\n- `GENERATED_MARKER` from `generation/writers/agents-md.ts` — sentinel string for ARE-generated files\n- `node:fs/promises` — async file operations (access, readFile, rename, unlink)\n- `node:fs` constants — permission flags (R_OK, F_OK)\n- `node:path` — path resolution and relative path computation",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1070,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11186,
      "latencyMs": 36079,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/clean.ts",
          "sizeBytes": 5971
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:13.819Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/index.ts\n\n```typescript\n#!/usr/bin/env node\n/**\n * CLI entry point for agents-reverse\n *\n * Commands:\n *   init              Create default configuration\n *   discover [path]   Discover files to analyze\n *   generate [path]   Generate documentation plan\n *   update [path]     Update docs incrementally\n *   specify [path]    Generate project specification from AGENTS.md docs\n *   clean [path]      Delete all generated artifacts\n */\n\nimport { initCommand } from './init.js';\nimport { discoverCommand } from './discover.js';\nimport { generateCommand, type GenerateOptions } from './generate.js';\nimport { updateCommand, type UpdateCommandOptions } from './update.js';\nimport { cleanCommand, type CleanOptions } from './clean.js';\nimport { specifyCommand, type SpecifyOptions } from './specify.js';\nimport { rebuildCommand, type RebuildOptions } from './rebuild.js';\n\nimport { runInstaller, parseInstallerArgs } from '../installer/index.js';\nimport { getVersion } from '../version.js';\n\nconst VERSION = getVersion();\n\nconst USAGE = `\nagents-reverse-engineer - AI-friendly codebase documentation\n\nCommands:\n  install           Install commands and hooks to AI assistant\n  uninstall         Remove installed commands and hooks\n  init              Create default configuration\n  discover [path]   Discover files to analyze (default: current directory)\n  generate [path]   Generate documentation plan (default: current directory)\n  update [path]     Update docs incrementally (default: current directory)\n  specify [path]    Generate project specification from AGENTS.md docs\n  rebuild [path]    Reconstruct project from specification\n  clean [path]      Delete all generated artifacts (.sum, AGENTS.md, etc.)\n\nInstall/Uninstall Options:\n  --runtime <name>  Runtime to target (claude, opencode, gemini, all)\n  -g, --global      Target global config directory\n  -l, --local       Target current project directory\n  --force           Overwrite existing files (init, install, specify)\n\nGeneral Options:\n  --debug           Show AI prompts and backend details\n  --trace           Enable concurrency tracing (.agents-reverse-engineer/traces/)\n  --dry-run         Show plan without writing files (generate, update, specify, rebuild)\n  --output <path>   Output path (specify: spec file, rebuild: output directory)\n  --multi-file      Split specification into multiple files (specify only)\n  --model <name>    AI model to use (e.g., sonnet, opus, haiku)\n  --concurrency <n> Number of concurrent AI calls (default: auto)\n  --fail-fast       Stop on first file analysis failure\n  --uncommitted     Include uncommitted changes (update only)\n  --help, -h        Show this help\n  --version, -V     Show version number\n\nExamples:\n  are install\n  are install --runtime claude -g\n  are uninstall\n  are uninstall --runtime claude -g\n  are init\n  are discover\n  are generate --dry-run\n  are generate --concurrency 3\n  are generate ./my-project --concurrency 3\n  are update\n  are update --uncommitted\n  are specify --dry-run\n  are specify --output ./docs/spec.md --force\n  are rebuild --dry-run\n  are rebuild --output ./out --force\n`;\n\n/**\n * Parse command-line arguments.\n *\n * Extracts the command, positional arguments, and flags.\n * Handles global flags (--help, -h) that may appear before the command.\n */\nfunction parseArgs(args: string[]): {\n  command: string | undefined;\n  positional: string[];\n  flags: Set<string>;\n  values: Map<string, string>;\n} {\n  let command: string | undefined;\n  const positional: string[] = [];\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n    if (arg.startsWith('--')) {\n      const flagName = arg.slice(2);\n      // Check if next arg is a value (not starting with -)\n      if (i + 1 < args.length && !args[i + 1].startsWith('-')) {\n        values.set(flagName, args[i + 1]);\n        i++; // Skip the value\n      } else {\n        flags.add(flagName);\n      }\n    } else if (arg.startsWith('-')) {\n      // Handle short flags (e.g., -h, -g, -l)\n      for (const char of arg.slice(1)) {\n        switch (char) {\n          case 'h':\n            flags.add('help');\n            break;\n          case 'g':\n            flags.add('global');\n            break;\n          case 'l':\n            flags.add('local');\n            break;\n          case 'V':\n            flags.add('version');\n            break;\n          default:\n            // Unknown short flag - ignore\n            break;\n        }\n      }\n    } else if (!command) {\n      // First non-flag argument is the command\n      command = arg;\n    } else {\n      // Subsequent non-flag arguments are positional\n      positional.push(arg);\n    }\n  }\n\n  return { command, positional, flags, values };\n}\n\n/**\n * Show version and exit.\n */\nfunction showVersion(): void {\n  console.log(`agents-reverse-engineer v${VERSION}`);\n  process.exit(0);\n}\n\n/**\n * Display version banner.\n */\nfunction showVersionBanner(): void {\n  console.log(`agents-reverse-engineer v${VERSION}\\n`);\n}\n\n/**\n * Show usage information and exit.\n */\nfunction showHelp(): void {\n  console.log(USAGE);\n  process.exit(0);\n}\n\n/**\n * Show error for unknown command and exit.\n */\nfunction showUnknownCommand(command: string): void {\n  console.error(`Unknown command: ${command}`);\n  console.error(`Run 'are --help' for usage information.`);\n  process.exit(1);\n}\n\n/**\n * Check if command-line has installer-related flags.\n *\n * Used to detect direct installer invocation without 'install' command.\n */\nfunction hasInstallerFlags(flags: Set<string>, values: Map<string, string>): boolean {\n  return (\n    flags.has('global') ||\n    flags.has('local') ||\n    flags.has('force') ||\n    values.has('runtime')\n  );\n}\n\n/**\n * Main CLI entry point.\n */\nasync function main(): Promise<void> {\n  const args = process.argv.slice(2);\n  const { command, positional, flags, values } = parseArgs(args);\n\n  // Handle version flag\n  if (flags.has('version')) {\n    showVersion();\n  }\n\n  // Handle help flag anywhere (but not if --help is for install command)\n  if (flags.has('help') && !command && !hasInstallerFlags(flags, values)) {\n    showHelp();\n  }\n\n  // No command and no args - launch interactive installer\n  if (args.length === 0) {\n    await runInstaller({\n      global: false,\n      local: false,\n      uninstall: false,\n      force: false,\n      help: false,\n      quiet: false,\n    });\n    return;\n  }\n\n  // Direct installer invocation without 'install' command\n  // Supports: npx agents-reverse-engineer --runtime claude -g\n  if (!command && hasInstallerFlags(flags, values)) {\n    const installerArgs = parseInstallerArgs(args);\n    await runInstaller(installerArgs);\n    return;\n  }\n\n  // Show version banner\n  showVersionBanner();\n\n  // Route to command handlers\n  switch (command) {\n    case 'install': {\n      // Re-parse args for installer-specific flags\n      const installerArgs = parseInstallerArgs(args);\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'uninstall': {\n      // Re-parse args and force uninstall mode\n      const installerArgs = parseInstallerArgs(args);\n      installerArgs.uninstall = true;\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'init': {\n      await initCommand(positional[0] || '.', { force: flags.has('force') });\n      break;\n    }\n\n    case 'clean': {\n      const cleanOpts: CleanOptions = {\n        dryRun: flags.has('dry-run'),\n      };\n      await cleanCommand(positional[0] || '.', cleanOpts);\n      break;\n    }\n\n    case 'discover': {\n      await discoverCommand(positional[0] || '.', {});\n      break;\n    }\n\n    case 'generate': {\n      const options: GenerateOptions = {\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n      };\n      await generateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'update': {\n      const options: UpdateCommandOptions = {\n        uncommitted: flags.has('uncommitted'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n      };\n      await updateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'specify': {\n      const specifyOpts: SpecifyOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        multiFile: flags.has('multi-file'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n      };\n      await specifyCommand(positional[0] || '.', specifyOpts);\n      break;\n    }\n\n    case 'rebuild': {\n      const rebuildOpts: RebuildOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency')\n          ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n      };\n      await rebuildCommand(positional[0] || '.', rebuildOpts);\n      break;\n    }\n\n    default:\n      if (command) {\n        showUnknownCommand(command);\n      }\n      showHelp();\n  }\n}\n\n// Run main and handle any uncaught errors\nmain().catch((err: Error) => {\n  console.error(`Error: ${err.message}`);\n  process.exit(1);\n});\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CLI entry point dispatching install/uninstall/init/discover/generate/update/specify/rebuild/clean commands with flag parsing, version display, and error handling.**\n\n## Command Routing\n\n`main()` parses `process.argv.slice(2)` via `parseArgs()` and routes to command handlers:\n\n- `install` → `runInstaller()` with `parseInstallerArgs()`\n- `uninstall` → `runInstaller()` with `uninstall: true`\n- `init` → `initCommand(positional[0] || '.', { force })`\n- `clean` → `cleanCommand(positional[0] || '.', { dryRun })`\n- `discover` → `discoverCommand(positional[0] || '.', {})`\n- `generate` → `generateCommand(positional[0] || '.', GenerateOptions)`\n- `update` → `updateCommand(positional[0] || '.', UpdateCommandOptions)`\n- `specify` → `specifyCommand(positional[0] || '.', SpecifyOptions)`\n- `rebuild` → `rebuildCommand(positional[0] || '.', RebuildOptions)`\n\nNo command with `args.length === 0` → launches interactive installer via `runInstaller({ global: false, local: false, uninstall: false, force: false, help: false, quiet: false })`.\n\nNo command with installer flags (detected via `hasInstallerFlags()`) → direct installer invocation with `parseInstallerArgs(args)`.\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command, positional, flags, values }`:\n\n- **Long flags**: `--flag` or `--key value` patterns\n- **Short flags**: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- **Command**: first non-flag argument\n- **Positional**: subsequent non-flag arguments after command\n- **Values map**: stores `--key value` pairs (e.g., `--concurrency 3`, `--output ./spec.md`, `--model sonnet`)\n- **Flags set**: boolean flags without values (e.g., `--dry-run`, `--debug`, `--trace`, `--force`)\n\n`hasInstallerFlags(flags, values)` detects installer-related flags: `global`, `local`, `force`, or `runtime` value presence.\n\n## Option Types\n\n**GenerateOptions** (from `./generate.js`):\n```typescript\n{\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**UpdateCommandOptions** (from `./update.js`):\n```typescript\n{\n  uncommitted?: boolean;\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (from `./clean.js`):\n```typescript\n{\n  dryRun?: boolean;\n}\n```\n\n**SpecifyOptions** (from `./specify.js`):\n```typescript\n{\n  output?: string;\n  force?: boolean;\n  dryRun?: boolean;\n  multiFile?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (from `./rebuild.js`):\n```typescript\n{\n  output?: string;\n  force?: boolean;\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n## Global Flags\n\n- `--version`, `-V` → `showVersion()` prints `agents-reverse-engineer v${VERSION}` and exits\n- `--help`, `-h` → `showHelp()` prints `USAGE` constant and exits (suppressed if installer flags present)\n- `--debug` → passed to command handlers for verbose subprocess logging\n- `--trace` → enables NDJSON trace output to `.agents-reverse-engineer/traces/`\n- `--dry-run` → preview mode without file writes (generate, update, specify, rebuild, clean)\n- `--fail-fast` → abort on first file analysis failure (generate, update, rebuild)\n- `--concurrency <n>` → worker pool size override (generate, update, rebuild)\n- `--model <name>` → AI model override (generate, update, specify, rebuild)\n- `--force` → overwrite existing files (init, install, specify, rebuild)\n- `--uncommitted` → include working tree changes (update only)\n- `--output <path>` → output destination (specify: spec file path, rebuild: output directory)\n- `--multi-file` → split specification into per-directory files (specify only)\n\n## Version Management\n\n`getVersion()` imported from `../version.js` returns package version string.\n\n`showVersionBanner()` prints `agents-reverse-engineer v${VERSION}\\n` before command execution.\n\n`showVersion()` prints version and exits with code 0.\n\n## Error Handling\n\n`showUnknownCommand(command)` prints error message referencing `are --help` and exits with code 1.\n\n`main().catch((err: Error) => ...)` catches uncaught errors, prints `Error: ${err.message}`, exits with code 1.\n\n## Usage Template\n\n`USAGE` constant defines help text with command summaries, option descriptions, and example invocations. Covers install/uninstall workflow with `--runtime` targeting (claude, opencode, gemini, all), global/local installation modes, and command-specific flags.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1372,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12032,
      "latencyMs": 33386,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/index.ts",
          "sizeBytes": 9723
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:28.635Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/schema.ts\n\n```typescript\n/**\n * Zod schema for configuration validation\n *\n * This schema defines the structure of `.agents-reverse/config.yaml`\n * and provides sensible defaults for all fields.\n */\n\nimport { z } from 'zod';\nimport {\n  DEFAULT_VENDOR_DIRS,\n  DEFAULT_BINARY_EXTENSIONS,\n  DEFAULT_MAX_FILE_SIZE,\n  DEFAULT_EXCLUDE_PATTERNS,\n  getDefaultConcurrency,\n} from './defaults.js';\n\n/**\n * Schema for exclusion configuration\n */\nconst ExcludeSchema = z.object({\n  /** Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"]) */\n  patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS]),\n  /** Vendor directories to exclude */\n  vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS]),\n  /** Binary file extensions to exclude */\n  binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS]),\n}).default({});\n\n/**\n * Schema for options configuration\n */\nconst OptionsSchema = z.object({\n  /** Whether to follow symbolic links during traversal */\n  followSymlinks: z.boolean().default(false),\n  /** Maximum file size in bytes (files larger than this are skipped) */\n  maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE),\n}).default({});\n\n/**\n * Schema for output configuration\n */\nconst OutputSchema = z.object({\n  /** Whether to use colors in terminal output */\n  colors: z.boolean().default(true),\n}).default({});\n\n/**\n * Schema for AI service configuration.\n *\n * Controls backend selection, model, timeout, retry behavior, and\n * telemetry log retention. All fields have sensible defaults.\n */\nconst AISchema = z.object({\n  /** AI CLI backend to use ('auto' detects from PATH) */\n  backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto'),\n  /** Model identifier (backend-specific, e.g., \"sonnet\", \"opus\") */\n  model: z.string().default('sonnet'),\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: z.number().positive().default(300_000),\n  /** Maximum number of retries for transient errors */\n  maxRetries: z.number().min(0).default(3),\n  /** Default parallelism for concurrent AI calls (1-20). Auto-detected from CPU cores and available memory. */\n  concurrency: z.number().min(1).max(20).default(getDefaultConcurrency),\n  /** Telemetry settings */\n  telemetry: z.object({\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: z.number().min(0).default(50),\n  }).default({}),\n}).default({});\n\n/**\n * Main configuration schema for agents-reverse.\n *\n * All fields have sensible defaults, so an empty object `{}` is valid\n * and will result in a fully populated configuration.\n *\n * @example\n * ```typescript\n * // Parse with defaults\n * const config = ConfigSchema.parse({});\n *\n * // Parse with partial overrides\n * const config = ConfigSchema.parse({\n *   exclude: { patterns: ['*.log'] },\n *   ai: { backend: 'claude', model: 'opus' },\n * });\n * ```\n */\nexport const ConfigSchema = z.object({\n  /** Exclusion rules for files and directories */\n  exclude: ExcludeSchema,\n  /** Discovery options */\n  options: OptionsSchema,\n  /** Output formatting options */\n  output: OutputSchema,\n  /** AI service configuration */\n  ai: AISchema,\n}).default({});\n\n/**\n * Inferred TypeScript type from the schema.\n * Use this type for function parameters and return types.\n */\nexport type Config = z.infer<typeof ConfigSchema>;\n\n/**\n * Type for the exclude section of config\n */\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>;\n\n/**\n * Type for the options section of config\n */\nexport type OptionsConfig = z.infer<typeof OptionsSchema>;\n\n/**\n * Type for the output section of config\n */\nexport type OutputConfig = z.infer<typeof OutputSchema>;\n\n/**\n * Type for the AI service section of config\n */\nexport type AIConfig = z.infer<typeof AISchema>;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ConfigSchema defines Zod validation schemas for `.agents-reverse-engineer/config.yaml` with nested object structures for exclusion rules, discovery options, output formatting, and AI service configuration, exporting ConfigSchema, Config, and section-specific types (ExcludeConfig, OptionsConfig, OutputConfig, AIConfig).**\n\n## Schema Hierarchy\n\nConfigSchema composes four nested schemas via z.object():\n- ExcludeSchema validates `exclude` section with `patterns: string[]`, `vendorDirs: string[]`, `binaryExtensions: string[]`\n- OptionsSchema validates `options` section with `followSymlinks: boolean`, `maxFileSize: number`\n- OutputSchema validates `output` section with `colors: boolean`\n- AISchema validates `ai` section with `backend: 'claude' | 'gemini' | 'opencode' | 'auto'`, `model: string`, `timeoutMs: number`, `maxRetries: number`, `concurrency: number`, nested `telemetry: { keepRuns: number }`\n\nEach schema calls `.default({})` to enable partial overrides while preserving unspecified fields.\n\n## Exported Types\n\n```typescript\nexport const ConfigSchema: z.ZodObject<{\n  exclude: typeof ExcludeSchema,\n  options: typeof OptionsSchema,\n  output: typeof OutputSchema,\n  ai: typeof AISchema\n}>\n\nexport type Config = z.infer<typeof ConfigSchema>\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>\nexport type OptionsConfig = z.infer<typeof OptionsSchema>\nexport type OutputConfig = z.infer<typeof OutputSchema>\nexport type AIConfig = z.infer<typeof AISchema>\n```\n\nConfig serves as the inferred TypeScript type for validated YAML config objects consumed by `src/config/loader.ts`.\n\n## Default Value Integration\n\nExcludeSchema.patterns defaults to spread of `DEFAULT_EXCLUDE_PATTERNS` from `./defaults.js`. ExcludeSchema.vendorDirs defaults to spread of `DEFAULT_VENDOR_DIRS`. ExcludeSchema.binaryExtensions defaults to spread of `DEFAULT_BINARY_EXTENSIONS`. OptionsSchema.maxFileSize defaults to `DEFAULT_MAX_FILE_SIZE`. AISchema.concurrency defaults to `getDefaultConcurrency` function result (dynamic CPU/memory detection).\n\nSpread operators (`[...DEFAULT_VENDOR_DIRS]`) create shallow copies preventing shared array mutation across config instances.\n\n## Validation Constraints\n\nAISchema.backend constrains to z.enum(['claude', 'gemini', 'opencode', 'auto']) rejecting invalid backend strings. AISchema.timeoutMs applies z.number().positive() requiring >0 milliseconds. AISchema.maxRetries applies z.number().min(0) requiring non-negative integers. AISchema.concurrency applies z.number().min(1).max(20) clamping parallelism to 1-20 workers. OptionsSchema.maxFileSize applies z.number().positive() requiring >0 byte limit. AISchema.telemetry.keepRuns applies z.number().min(0) requiring non-negative run retention count.\n\n## Usage Pattern\n\n```typescript\n// Parse with defaults\nconst config = ConfigSchema.parse({});\n\n// Parse with partial overrides\nconst config = ConfigSchema.parse({\n  exclude: { patterns: ['*.log'] },\n  ai: { backend: 'claude', model: 'opus' },\n});\n```\n\nZod parse() throws ZodError on validation failure. Loader applies this schema to YAML-parsed config objects before returning validated Config instances.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 846,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10316,
      "latencyMs": 19490,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/schema.ts",
          "sizeBytes": 3737
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:34.579Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/walker.ts\n\n```typescript\n/**\n * Directory walker for the agents-reverse file discovery system.\n * \n * Uses fast-glob to traverse directories and return all candidate files.\n * Filters are applied separately via the filter chain (not in this module).\n */\n\nimport fg from 'fast-glob';\nimport type { WalkerOptions } from './types.js';\n\n/**\n * Walk a directory tree and return all files.\n * \n * This walker returns ALL files in the directory tree. Filtering happens\n * separately via the filter chain (gitignore, binary, vendor, custom patterns).\n * \n * @param options - Walker configuration\n * @returns Array of absolute file paths\n * \n * @example\n * ```typescript\n * const files = await walkDirectory({ cwd: '/path/to/repo' });\n * console.log(`Found ${files.length} files`);\n * ```\n */\nexport async function walkDirectory(options: WalkerOptions): Promise<string[]> {\n  return fg.glob('**/*', {\n    cwd: options.cwd,\n    absolute: true,\n    onlyFiles: true,\n    dot: options.dot ?? true,\n    followSymbolicLinks: options.followSymlinks ?? false,\n    suppressErrors: true, // Don't throw on permission errors (per RESEARCH.md)\n    // Always exclude .git internals for performance\n    ignore: ['**/.git/**'],\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**walkDirectory() wraps fast-glob to traverse directories, returning all absolute file paths with performance optimizations (`.git` exclusion, symlink control, permission error suppression) but no filtering logic (deferred to filter chain in `src/discovery/filters/`).**\n\n## Exported Function\n\n`walkDirectory(options: WalkerOptions): Promise<string[]>` — Async function returning absolute paths for all files discovered via `fg.glob('**/*', ...)` with configuration from `WalkerOptions`.\n\n## WalkerOptions Interface\n\nExpects `WalkerOptions` type (defined in `src/discovery/types.ts`) with fields:\n- `cwd` — Base directory for traversal (required)\n- `dot` — Include dotfiles (optional, defaults to `true`)\n- `followSymlinks` — Follow symbolic links (optional, defaults to `false`)\n\n## fast-glob Configuration\n\nPasses following parameters to `fg.glob()`:\n- `cwd: options.cwd` — Base directory\n- `absolute: true` — Returns absolute paths instead of relative\n- `onlyFiles: true` — Excludes directories from results\n- `dot: options.dot ?? true` — Includes hidden files by default\n- `followSymbolicLinks: options.followSymlinks ?? false` — Disables symlink traversal by default\n- `suppressErrors: true` — Continues on permission errors (per RESEARCH.md design decision)\n- `ignore: ['**/.git/**']` — Hardcoded exclusion of `.git` internals for performance\n\n## Design Constraint\n\nDoes NOT apply filtering logic for gitignore patterns, binary detection, vendor directories, or custom globs. Filtering happens via separate filter chain modules (`src/discovery/filters/gitignore.ts`, `src/discovery/filters/binary.ts`, `src/discovery/filters/vendor.ts`, `src/discovery/filters/custom.ts`) after `walkDirectory()` returns raw file list.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 449,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9543,
      "latencyMs": 14282,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/walker.ts",
          "sizeBytes": 1187
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:21.349Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/defaults.ts\n\n```typescript\n/**\n * Default configuration values for agents-reverse\n */\n\nimport os from 'node:os';\n\n/** Multiplier applied to CPU core count for default concurrency */\nconst CONCURRENCY_MULTIPLIER = 5;\n\n/** Minimum default concurrency */\nconst MIN_CONCURRENCY = 2;\n\n/** Maximum default concurrency (matches schema .max(20)) */\nconst MAX_CONCURRENCY = 20;\n\n/** Heap budget per subprocess in GB (matches NODE_OPTIONS --max-old-space-size=512) */\nconst SUBPROCESS_HEAP_GB = 0.512;\n\n/** Fraction of total system memory to allocate to subprocesses */\nconst MEMORY_FRACTION = 0.5;\n\n/**\n * Compute the default concurrency based on available CPU cores and system memory.\n *\n * Formula: clamp(cores * 5, MIN, min(memCap, MAX))\n * - cores: os.availableParallelism() or os.cpus().length\n * - memCap: floor(totalMemGB * 0.5 / 0.512) — use at most 50% of RAM for subprocesses\n *\n * @returns Default concurrency value (integer between MIN_CONCURRENCY and MAX_CONCURRENCY)\n */\nexport function getDefaultConcurrency(): number {\n  const cores = typeof os.availableParallelism === 'function'\n    ? os.availableParallelism()\n    : (os.cpus().length || MIN_CONCURRENCY);\n\n  const totalMemGB = os.totalmem() / (1024 ** 3);\n  const memCap = totalMemGB > 1\n    ? Math.floor((totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)\n    : Infinity;\n\n  const computed = cores * CONCURRENCY_MULTIPLIER;\n  return Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY));\n}\n\n/**\n * Default vendor directories to exclude from analysis.\n * These are typically package managers, build outputs, or version control directories.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n  '.cargo',\n  '.gradle',\n  // AI assistant tooling directories\n  '.agents-reverse-engineer',\n  '.agents',\n  '.planning',\n  '.claude',\n  '.opencode',\n  '.gemini',\n] as const;\n\n/**\n * Default file patterns to exclude from analysis.\n * These patterns use gitignore syntax and are matched by the custom filter.\n */\nexport const DEFAULT_EXCLUDE_PATTERNS = [\n  // AI assistant documentation files\n  'AGENTS.md',\n  'CLAUDE.md',\n  'OPENCODE.md',\n  'GEMINI.md',\n  '**/AGENTS.md',\n  '**/CLAUDE.md',\n  '**/OPENCODE.md',\n  '**/GEMINI.md',\n  // Lock files (not useful for documentation, can be very large)\n  '*.lock',\n  'package-lock.json',\n  'yarn.lock',\n  'pnpm-lock.yaml',\n  'bun.lock',\n  'bun.lockb',\n  'Gemfile.lock',\n  'Cargo.lock',\n  'poetry.lock',\n  'composer.lock',\n  'go.sum',\n  // Dotfiles and generated artifacts (path.extname returns '' for dotfiles,\n  // so these must be matched as glob patterns, not binary extensions)\n  '.gitignore',\n  '.gitattributes',\n  '.gitkeep',\n  '.env',\n  '**/.env',\n  '**/.env.*',\n  '*.log',\n  '*.sum',\n  '**/*.sum',\n  '**/SKILL.md',\n] as const;\n\n/**\n * Default binary file extensions to exclude from analysis.\n * These files cannot be meaningfully analyzed as text.\n */\nexport const DEFAULT_BINARY_EXTENSIONS = [\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  // Documents\n  '.pdf',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  // Compiled\n  '.class',\n  '.pyc',\n] as const;\n\n/**\n * Default maximum file size in bytes (1MB).\n * Files larger than this will be skipped with a warning.\n */\nexport const DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Default configuration object matching the schema structure.\n * This is used when no config file is present or for missing fields.\n */\nexport const DEFAULT_CONFIG = {\n  exclude: {\n    patterns: [...DEFAULT_EXCLUDE_PATTERNS],\n    vendorDirs: [...DEFAULT_VENDOR_DIRS],\n    binaryExtensions: [...DEFAULT_BINARY_EXTENSIONS],\n  },\n  options: {\n    followSymlinks: false,\n    maxFileSize: DEFAULT_MAX_FILE_SIZE,\n  },\n  output: {\n    colors: true,\n  },\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**defaults.ts exports configuration constants and dynamic concurrency calculation for agents-reverse-engineer's resource-adaptive worker pool.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** — Computes default worker pool concurrency by multiplying `os.availableParallelism()` (or `os.cpus().length`) by `CONCURRENCY_MULTIPLIER` (5), then clamping the result between `MIN_CONCURRENCY` (2) and the minimum of `MAX_CONCURRENCY` (20) and a memory-derived cap. Memory cap formula: `Math.floor((os.totalmem() * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)` where `MEMORY_FRACTION` is 0.5 and `SUBPROCESS_HEAP_GB` is 0.512, ensuring subprocesses consume at most 50% of total system RAM given 512MB heap per subprocess (matching `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts).\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS: readonly string[]** — Array of 18 directory names excluded from file discovery: `'node_modules'`, `'vendor'`, `'.git'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`, `'.agents-reverse-engineer'`, `'.agents'`, `'.planning'`, `'.claude'`, `'.opencode'`, `'.gemini'`. Last 6 entries are AI assistant tooling directories.\n\n**DEFAULT_EXCLUDE_PATTERNS: readonly string[]** — Gitignore-style glob patterns excluding 26 file types: AI documentation files (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` with `**/` recursive variants), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), dotfiles (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`), and generated artifacts (`*.log`, `*.sum`, `**/*.sum`, `**/SKILL.md`). Comment notes that dotfiles require glob pattern matching since `path.extname()` returns empty string.\n\n**DEFAULT_BINARY_EXTENSIONS: readonly string[]** — Array of 26 binary file extensions categorized as images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled files (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE: number** — File size threshold of `1024 * 1024` bytes (1MB) triggering binary detection warnings.\n\n**DEFAULT_CONFIG: object** — Nested configuration object with structure matching Zod schema in schema.ts: `exclude` property containing `patterns`, `vendorDirs`, `binaryExtensions` arrays (spread from corresponding DEFAULT_ constants); `options` property with `followSymlinks: false` and `maxFileSize: DEFAULT_MAX_FILE_SIZE`; `output` property with `colors: true`.\n\n## Internal Constants\n\n**CONCURRENCY_MULTIPLIER: number** — Value `5` multiplied by CPU core count in getDefaultConcurrency formula.\n\n**MIN_CONCURRENCY: number** — Value `2` defining lower bound for worker pool size.\n\n**MAX_CONCURRENCY: number** — Value `20` defining upper bound matching Zod schema `.max(20)` constraint in schema.ts.\n\n**SUBPROCESS_HEAP_GB: number** — Value `0.512` representing 512MB heap limit per subprocess, matching `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in subprocess.ts.\n\n**MEMORY_FRACTION: number** — Value `0.5` representing 50% memory allocation threshold for subprocess pool in memory cap calculation.\n\n## Resource Adaptive Strategy\n\nThe concurrency formula `clamp(cores * 5, MIN, min(memCap, MAX))` prevents RAM exhaustion in constrained environments (WSL, containers) by computing `memCap = floor(totalMemGB * 0.5 / 0.512)`. On systems with 4GB RAM: `memCap = floor(4 * 0.5 / 0.512) = 3`, limiting concurrency to 3 even if CPU count suggests higher value. On systems with < 1GB RAM, `memCap = Infinity` delegates to MAX_CONCURRENCY cap. The 0.512GB denominator synchronizes with subprocess heap limit enforced in subprocess.ts via `NODE_OPTIONS` environment variable.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1218,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27960,
      "latencyMs": 28744,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/defaults.ts",
          "sizeBytes": 3998
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:32.235Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/types.ts\n\n```typescript\n/**\n * Discovery types for the agents-reverse file discovery system.\n * \n * This module defines the core interfaces and types used by the directory\n * walker and filter chain for file discovery operations.\n */\n\nimport type { Stats } from 'node:fs';\n\n/**\n * Interface for file filters in the discovery pipeline.\n * \n * Filters are applied to each file discovered by the walker to determine\n * whether it should be included or excluded from analysis. Filters can be\n * synchronous or asynchronous.\n * \n * Examples: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter\n */\nexport interface FileFilter {\n  /** Name of the filter for logging which filter excluded a file */\n  readonly name: string;\n  \n  /**\n   * Determine whether a file should be excluded from discovery.\n   * \n   * @param path - Absolute path to the file\n   * @param stats - Optional file stats (for size-based filtering, etc.)\n   * @returns true if the file should be excluded, false to include\n   */\n  shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean;\n}\n\n/**\n * Record of an excluded file with reason and responsible filter.\n */\nexport interface ExcludedFile {\n  /** Absolute path to the excluded file */\n  path: string;\n  /** Human-readable reason for exclusion */\n  reason: string;\n  /** Name of the filter that excluded this file */\n  filter: string;\n}\n\n/**\n * Result of running the discovery and filter chain.\n */\nexport interface FilterResult {\n  /** Files that passed all filters and should be analyzed */\n  included: string[];\n  /** Files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Options for the directory walker.\n */\nexport interface WalkerOptions {\n  /** Root directory to walk (absolute path) */\n  cwd: string;\n  \n  /** \n   * Whether to follow symbolic links.\n   * Default: false (per CONTEXT.md - skip symlinks by default)\n   */\n  followSymlinks?: boolean;\n  \n  /**\n   * Whether to include dotfiles (files starting with .).\n   * Default: true (include dotfiles for analysis)\n   */\n  dot?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`src/discovery/types.ts` defines the core type contracts for the file discovery pipeline's filter chain architecture and walker configuration.**\n\n## Exported Interfaces\n\n**`FileFilter`** — Contract for pluggable filters in the discovery pipeline. Requires `name: string` for logging which filter excluded a file, and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` to determine exclusion synchronously or asynchronously. Concrete implementations include `GitignoreFilter`, `BinaryFilter`, `VendorFilter`, `CustomPatternFilter` (per JSDoc examples).\n\n**`ExcludedFile`** — Record of a file rejected by the filter chain. Contains `path: string` (absolute path), `reason: string` (human-readable explanation), `filter: string` (name of the `FileFilter` that excluded it).\n\n**`FilterResult`** — Output of running the discovery filter chain. Contains `included: string[]` (files passing all filters for analysis) and `excluded: ExcludedFile[]` (rejected files with exclusion metadata).\n\n**`WalkerOptions`** — Configuration for the directory walker. Requires `cwd: string` (absolute root directory). Optional `followSymlinks?: boolean` (default `false` per CONTEXT.md to skip symlinks) and `dot?: boolean` (default `true` to include dotfiles for analysis).\n\n## Filter Chain Pattern\n\nThe `FileFilter` interface enables composable filter chains where each filter independently evaluates files. The `shouldExclude()` method supports both synchronous (direct `boolean` return) and asynchronous (`Promise<boolean>` return) implementations, allowing filters to perform I/O operations like reading file headers for binary detection or parsing `.gitignore` rules.\n\n## Integration Points\n\nThese types are consumed by `src/discovery/walker.ts` (directory traversal), `src/discovery/filters/` implementations (gitignore parsing, binary detection, vendor exclusion, custom glob patterns), and `src/discovery/run.ts` (filter chain orchestration). The `Stats` parameter in `shouldExclude()` enables size-based filtering (e.g., `maxFileSize` threshold from config) without redundant filesystem calls.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 503,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9753,
      "latencyMs": 18582,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/types.ts",
          "sizeBytes": 2043
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:29.214Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/run.ts\n\n```typescript\n/**\n * High-level file discovery pipeline.\n *\n * Combines directory walking and filter application into a single function\n * that all commands can share.\n */\n\nimport { walkDirectory } from './walker.js';\nimport {\n  applyFilters,\n  createGitignoreFilter,\n  createVendorFilter,\n  createBinaryFilter,\n  createCustomFilter,\n} from './filters/index.js';\nimport type { FilterResult } from './types.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Configuration subset needed for file discovery.\n * Structurally compatible with the full Config type from config/schema.ts.\n */\nexport interface DiscoveryConfig {\n  exclude: {\n    vendorDirs: string[];\n    binaryExtensions: string[];\n    patterns: string[];\n  };\n  options: {\n    maxFileSize: number;\n    followSymlinks: boolean;\n  };\n}\n\n/**\n * Options for the discovery pipeline.\n */\nexport interface DiscoverFilesOptions {\n  tracer?: ITraceWriter;\n  debug?: boolean;\n}\n\n/**\n * Discover files in a directory by walking and applying the standard filter chain.\n *\n * Creates the four standard filters (gitignore, vendor, binary, custom),\n * walks the directory tree, and applies the filters. Returns the full\n * FilterResult so callers can access both included and excluded files\n * with filter attribution.\n *\n * @param root - Absolute path to the directory to scan\n * @param config - Discovery-related configuration (exclude rules and options)\n * @param options - Optional tracing and debug settings\n * @returns FilterResult with included and excluded file lists\n */\nexport async function discoverFiles(\n  root: string,\n  config: DiscoveryConfig,\n  options?: DiscoverFilesOptions,\n): Promise<FilterResult> {\n  // Create filters in standard order\n  const gitignoreFilter = await createGitignoreFilter(root);\n  const vendorFilter = createVendorFilter(config.exclude.vendorDirs);\n  const binaryFilter = createBinaryFilter({\n    maxFileSize: config.options.maxFileSize,\n    additionalExtensions: config.exclude.binaryExtensions,\n  });\n  const customFilter = createCustomFilter(config.exclude.patterns, root);\n  const filters = [gitignoreFilter, vendorFilter, binaryFilter, customFilter];\n\n  // Walk directory\n  const files = await walkDirectory({\n    cwd: root,\n    followSymlinks: config.options.followSymlinks,\n  });\n\n  // Apply filters and return\n  return applyFilters(files, filters, {\n    tracer: options?.tracer,\n    debug: options?.debug,\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**High-level file discovery orchestrator that composes directory walking with a four-stage filter chain (gitignore, vendor, binary, custom) into a single reusable pipeline for all commands.**\n\n## Exported Symbols\n\n### `discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>`\nExecutes the standard discovery pipeline: creates four filters via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, walks directory tree via `walkDirectory()`, applies filters via `applyFilters()`, and returns full `FilterResult` with included/excluded file lists and filter attribution. The `root` parameter must be an absolute path. The `config.exclude.vendorDirs` array feeds `createVendorFilter()`, `config.exclude.binaryExtensions` feeds `createBinaryFilter()` additionalExtensions, `config.exclude.patterns` feeds `createCustomFilter()` with gitignore-style globs, `config.options.maxFileSize` sets binary detection threshold, and `config.options.followSymlinks` controls symlink traversal in `walkDirectory()`.\n\n### `DiscoveryConfig` Interface\nStructural subset of full config schema defining discovery-related settings. Contains `exclude` object with `vendorDirs: string[]` (third-party directory names like `node_modules`), `binaryExtensions: string[]` (file extensions like `.png`, `.zip`), `patterns: string[]` (gitignore-style glob patterns), and `options` object with `maxFileSize: number` (binary detection byte threshold), `followSymlinks: boolean` (symlink traversal toggle). Structurally compatible with full `Config` type from `src/config/schema.ts` per JSDoc comment.\n\n### `DiscoverFilesOptions` Interface\nOptional parameters for discovery pipeline execution. Contains `tracer?: ITraceWriter` for NDJSON trace event emission and `debug?: boolean` for verbose logging. Both fields forwarded to `applyFilters()` third parameter.\n\n## Integration Points\n\nDepends on `walkDirectory()` from `./walker.js` for filesystem traversal and five filter factories from `./filters/index.js`: `createGitignoreFilter()` (parses `.gitignore`), `createVendorFilter()` (excludes vendor directories), `createBinaryFilter()` (detects non-text files), `createCustomFilter()` (applies user-defined glob patterns), `applyFilters()` (chains filter execution). Imports `FilterResult` type from `./types.js` and `ITraceWriter` from `../orchestration/trace.js`. Filter creation order (gitignore → vendor → binary → custom) determines priority when multiple filters match a file.\n\n## Design Pattern\n\nImplements the Facade pattern by hiding filter instantiation complexity behind a single function call. Commands in `src/cli/` (discover, generate, update) invoke `discoverFiles()` rather than manually constructing filter chains. The returned `FilterResult` contains both `included: string[]` for files passing all filters and `excluded: Array<{file: string, reason: string, filter: string}>` for audit trails showing why files were rejected.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 749,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9877,
      "latencyMs": 23362,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/run.ts",
          "sizeBytes": 2419
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:11.730Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/generate.ts\n\n```typescript\n/**\n * CLI generate command\n *\n * Creates and executes a documentation generation plan by:\n * 1. Discovering files to analyze\n * 2. Detecting file types and creating analysis tasks\n * 3. Resolving an AI CLI backend\n * 4. Running concurrent AI analysis via CommandRunner\n * 5. Producing .sum files, AGENTS.md, and root documents\n *\n * With --dry-run, shows the plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createOrchestrator, type GenerationPlan } from '../generation/orchestrator.js';\nimport { buildExecutionPlan } from '../generation/executor.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the generate command.\n */\nexport interface GenerateOptions {\n  /** Dry run - show plan without generating */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n  /** Override AI model (e.g., \"sonnet\", \"opus\") */\n  model?: string;\n}\n\n/**\n * Format the generation plan for display.\n */\nfunction formatPlan(plan: GenerationPlan): string {\n  const lines: string[] = [];\n\n  lines.push(`\\n=== Generation Plan ===\\n`);\n\n  // File summary\n  lines.push(`Files to analyze: ${plan.files.length}`);\n  lines.push(`Tasks to execute: ${plan.tasks.length}`);\n  lines.push('');\n\n  // Complexity\n  lines.push('Complexity:');\n  lines.push(`  Files: ${plan.complexity.fileCount}`);\n  lines.push(`  Directory depth: ${plan.complexity.directoryDepth}`);\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n/**\n * Generate command - discovers files, plans analysis, and executes AI-driven\n * documentation generation.\n *\n * Default behavior: resolves an AI CLI backend, builds an execution plan,\n * and runs concurrent AI analysis via the CommandRunner. Produces .sum files,\n * AGENTS.md per directory, and root documents (CLAUDE.md, ARCHITECTURE.md, etc.).\n *\n * @param targetPath - Directory to generate documentation for\n * @param options - Command options (concurrency, failFast, debug, etc.)\n */\nexport async function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Generating documentation plan for: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and discovery)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Discover files\n  logger.info('Discovering files...');\n\n  const filterResult = await discoverFiles(absolutePath, config, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create discovery result for orchestrator\n  const discoveryResult = {\n    files: filterResult.included,\n    excluded: filterResult.excluded.map(e => ({ path: e.path, reason: e.reason })),\n  };\n\n  logger.info(`Found ${discoveryResult.files.length} files to analyze`);\n\n  // Create generation plan\n  logger.info('Creating generation plan...');\n  const orchestrator = createOrchestrator(\n    config,\n    absolutePath,\n    { tracer, debug: options.debug }\n  );\n  const plan = await orchestrator.createPlan(discoveryResult);\n\n  // Display plan\n  console.log(formatPlan(plan));\n\n  // ---------------------------------------------------------------------------\n  // Dry-run: show execution plan summary without making AI calls\n  // ---------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    const executionPlan = buildExecutionPlan(plan, absolutePath);\n    const dirCount = Object.keys(executionPlan.directoryFileMap).length;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Files to analyze:     ${pc.cyan(String(executionPlan.fileTasks.length))}`);\n    console.log(`  Directories:          ${pc.cyan(String(dirCount))}`);\n    console.log(`  Root documents:       ${pc.cyan(String(executionPlan.rootTasks.length))}`);\n    console.log(`  Estimated AI calls:   ${pc.cyan(String(executionPlan.tasks.length))}`);\n    console.log('');\n    console.log(pc.dim('Files:'));\n    for (const task of executionPlan.fileTasks) {\n      console.log(pc.dim(`  ${task.path}`));\n    }\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI analysis\n  // ---------------------------------------------------------------------------\n\n  // Resolve AI CLI backend\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Resolve effective model (CLI flag > config)\n  const effectiveModel = options.model ?? config.ai.model;\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.log(pc.dim(`[debug] Model: ${effectiveModel}`));\n  }\n\n  // Create AI service\n  const aiService = new AIService(backend, {\n    timeoutMs: config.ai.timeoutMs,\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build execution plan\n  const executionPlan = buildExecutionPlan(plan, absolutePath);\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`Files: ${executionPlan.fileTasks.length} | Directories: ${executionPlan.directoryTasks.length}`);\n  progressLog.write('');\n\n  // Create command runner\n  const runner = new CommandRunner(aiService, {\n    concurrency,\n    failFast: options.failFast,\n    debug: options.debug,\n    tracer,\n    progressLog,\n  });\n\n  // Execute the three-phase pipeline\n  const summary = await runner.executeGenerate(executionPlan);\n\n  // Write telemetry run log\n  await aiService.finalize(absolutePath);\n\n  // Finalize trace, progress log, and clean up old trace files\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // Determine exit code from RunSummary\n  //   0: all files succeeded\n  //   1: some files failed (partial failure)\n  //   2: no files succeeded (total failure)\n  if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n    process.exit(2);\n  } else if (summary.filesFailed > 0) {\n    process.exit(1);\n  }\n  // Exit code 0 -- all files succeeded (or no files to process)\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateCommand orchestrates the three-phase ARE documentation pipeline by discovering files, resolving an AI backend, building an execution plan, and running concurrent analysis via CommandRunner to produce .sum files, AGENTS.md per directory, and root integration documents.**\n\n## Exported Interface\n\n**generateCommand(targetPath: string, options: GenerateOptions): Promise<void>**\n- Main CLI entry point resolving `targetPath` to absolute path\n- Loads config via `loadConfig()`, discovers files via `discoverFiles()`, creates generation plan via `createOrchestrator().createPlan()`\n- Dry-run mode displays execution plan summary without AI calls\n- Normal mode resolves backend via `resolveBackend()`, creates `AIService`, builds `executionPlan` via `buildExecutionPlan()`, executes via `CommandRunner.executeGenerate()`\n- Finalizes telemetry via `aiService.finalize()`, progress log via `ProgressLog.finalize()`, trace via `tracer.finalize()`, cleans up old traces via `cleanupOldTraces()`\n- Exit codes: 0 (all succeeded), 1 (partial failure), 2 (total failure)\n\n**GenerateOptions interface**\n- `dryRun?: boolean` — display plan without AI calls\n- `concurrency?: number` — override worker pool size (1-10)\n- `failFast?: boolean` — abort on first task failure\n- `debug?: boolean` — enable subprocess logging with heap/RSS metrics\n- `trace?: boolean` — emit NDJSON events to `.agents-reverse-engineer/traces/`\n- `model?: string` — override AI model (e.g., \"sonnet\", \"opus\")\n\n## Execution Flow\n\n1. **Initialization**: Resolves `targetPath` to absolute, creates `createLogger()`, creates `createTraceWriter()` (early to thread through config/discovery)\n2. **Configuration**: Calls `loadConfig(absolutePath, { tracer, debug })` loading `.agents-reverse-engineer/config.yaml` with Zod validation\n3. **Discovery**: Calls `discoverFiles(absolutePath, config, { tracer, debug })` returning `FilterResult` with `included`/`excluded` arrays\n4. **Planning**: Creates `createOrchestrator()`, calls `orchestrator.createPlan(discoveryResult)` producing `GenerationPlan` with `files`, `tasks`, `complexity`\n5. **Dry-run branch**: If `options.dryRun`, calls `buildExecutionPlan()`, logs summary with `fileTasks.length`, `directoryTasks.length`, `rootTasks.length`, exits without AI calls\n6. **Backend resolution**: Calls `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)` throwing `AIServiceError` with code `'CLI_NOT_FOUND'` on failure\n7. **AI service creation**: Instantiates `AIService(backend, { timeoutMs, maxRetries, model, telemetry })`, calls `aiService.setDebug(true)` if debug enabled, calls `aiService.setSubprocessLogDir()` if trace enabled\n8. **Execution**: Builds `executionPlan` via `buildExecutionPlan(plan, absolutePath)`, determines concurrency from `options.concurrency ?? config.ai.concurrency`, creates `ProgressLog.create(absolutePath)`, creates `CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog })`, executes via `runner.executeGenerate(executionPlan)` returning `RunSummary`\n9. **Finalization**: Calls `aiService.finalize(absolutePath)` writing telemetry run log, `progressLog.finalize()`, `tracer.finalize()`, `cleanupOldTraces(absolutePath)` retaining 500 most recent traces\n\n## Error Handling\n\n- `resolveBackend()` throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` when no AI CLI detected, caught and logged via `getInstallInstructions(registry)`, exits with code 2\n- `CommandRunner.executeGenerate()` returns `RunSummary` with `filesProcessed`, `filesFailed` counts determining exit code without throwing\n\n## Progress Reporting\n\n**formatPlan(plan: GenerationPlan): string**\n- Returns formatted string with `plan.files.length`, `plan.tasks.length`, `plan.complexity.fileCount`, `plan.complexity.directoryDepth`\n\n**ProgressLog integration**\n- Creates via `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`\n- Writes header with ISO 8601 timestamp, project path, file/directory counts\n- Passes to `CommandRunner` constructor for streaming updates during execution\n- Enables real-time monitoring via `tail -f .agents-reverse-engineer/progress.log`\n\n## Trace Integration\n\n- Creates `createTraceWriter(absolutePath, options.trace ?? false)` before config loading to thread through all operations\n- Logs trace file path via `pc.dim()` if `options.trace && tracer.filePath`\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner` constructor\n- Calls `cleanupOldTraces(absolutePath)` after finalization retaining 500 most recent trace files\n\n## Subprocess Logging\n\n- When `options.trace` enabled, calls `aiService.setSubprocessLogDir(logDir)` with timestamp-based path under `.agents-reverse-engineer/subprocess-logs/`\n- Logs subprocess output directory via `pc.dim()` for debugging concurrent AI calls\n\n## Model Resolution\n\n- Effective model determined by `options.model ?? config.ai.model` (CLI flag overrides config)\n- Passed to `AIService` constructor via `model` option\n- Debug mode logs backend name, CLI command, effective model via `pc.dim()`\n\n## Dependencies\n\n- **Node.js path**: `path.resolve()` for absolute path conversion\n- **picocolors**: `pc.red()`, `pc.cyan()`, `pc.bold()`, `pc.dim()` for terminal formatting\n- **Config**: `loadConfig()` from `src/config/loader.ts`\n- **Output**: `createLogger()` from `src/output/logger.ts`\n- **Discovery**: `discoverFiles()` from `src/discovery/run.ts`\n- **Generation**: `createOrchestrator()`, `buildExecutionPlan()` from `src/generation/`\n- **AI**: `AIService`, `AIServiceError`, `createBackendRegistry()`, `resolveBackend()`, `getInstallInstructions()` from `src/ai/`\n- **Orchestration**: `CommandRunner`, `ProgressLog`, `createTraceWriter()`, `cleanupOldTraces()` from `src/orchestration/`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1632,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11589,
      "latencyMs": 54097,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/generate.ts",
          "sizeBytes": 8154
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:16.644Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/specify.ts\n\n```typescript\n/**\n * CLI specify command\n *\n * Generates a project specification from AGENTS.md documentation by:\n * 1. Loading configuration\n * 2. Collecting all AGENTS.md files (auto-generating if none exist)\n * 3. Building a synthesis prompt from the collected docs\n * 4. Resolving an AI CLI backend and calling the AI service\n * 5. Writing the specification to disk (single or multi-file)\n *\n * With --dry-run, shows input statistics without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport { access, readdir } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { collectAgentsDocs, collectAnnexFiles } from '../generation/collector.js';\nimport { buildSpecPrompt, writeSpec, SpecExistsError } from '../specify/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport { generateCommand } from './generate.js';\n\n/**\n * Options for the specify command.\n */\nexport interface SpecifyOptions {\n  /** Custom output path (default: specs/SPEC.md) */\n  output?: string;\n  /** Overwrite existing specs */\n  force?: boolean;\n  /** Show plan without calling AI */\n  dryRun?: boolean;\n  /** Split output into multiple files */\n  multiFile?: boolean;\n  /** Show verbose debug info */\n  debug?: boolean;\n  /** Enable tracing */\n  trace?: boolean;\n  /** Override AI model (defaults to \"opus\" for specify) */\n  model?: string;\n}\n\n/**\n * Specify command - collects AGENTS.md documentation, synthesizes it via AI,\n * and writes a comprehensive project specification.\n *\n * @param targetPath - Directory to generate specification for\n * @param options - Command options (output, force, dryRun, multiFile, debug, trace)\n */\nexport async function specifyCommand(\n  targetPath: string,\n  options: SpecifyOptions,\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const outputPath = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'specs', 'SPEC.md');\n\n  // Early exit if spec file(s) already exist (avoids waiting for AI call)\n  if (!options.force && !options.dryRun) {\n    const conflicts: string[] = [];\n    if (options.multiFile) {\n      const outputDir = path.dirname(outputPath);\n      try {\n        const entries = await readdir(outputDir);\n        for (const entry of entries) {\n          if (entry.endsWith('.md')) {\n            conflicts.push(path.join(outputDir, entry));\n          }\n        }\n      } catch {\n        // Directory doesn't exist — no conflicts\n      }\n    } else {\n      try {\n        await access(outputPath, constants.F_OK);\n        conflicts.push(outputPath);\n      } catch {\n        // File doesn't exist — no conflict\n      }\n    }\n\n    if (conflicts.length > 0) {\n      const list = conflicts.map((p) => `  - ${p}`).join('\\n');\n      console.error(pc.red(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`));\n      process.exit(1);\n    }\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, { debug: options.debug });\n\n  // Collect AGENTS.md files\n  let docs = await collectAgentsDocs(absolutePath);\n\n  // ---------------------------------------------------------------------------\n  // Dry-run mode: show summary without calling AI or generating\n  // ---------------------------------------------------------------------------\n\n  // Collect annex files for reproduction-critical content\n  let annexFiles = await collectAnnexFiles(absolutePath);\n\n  if (options.dryRun) {\n    const totalChars = docs.reduce((sum, d) => sum + d.content.length, 0)\n      + annexFiles.reduce((sum, d) => sum + d.content.length, 0);\n    const estimatedTokensK = Math.ceil(totalChars / 4) / 1000;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  AGENTS.md files:   ${pc.cyan(String(docs.length))}`);\n    console.log(`  Annex files:       ${pc.cyan(String(annexFiles.length))}`);\n    console.log(`  Total input:       ${pc.cyan(`~${estimatedTokensK}K tokens`)}`);\n    console.log(`  Output:            ${pc.cyan(outputPath)}`);\n    console.log(`  Mode:              ${pc.cyan(options.multiFile ? 'multi-file' : 'single-file')}`);\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n\n    if (docs.length === 0) {\n      console.log('');\n      console.log(pc.yellow('Warning: No AGENTS.md files found. Run `are generate` first or omit --dry-run to auto-generate.'));\n    } else if (estimatedTokensK > 150) {\n      console.log('');\n      console.log(pc.yellow('Warning: Input exceeds 150K tokens. Consider using a model with extended context.'));\n    }\n\n    return;\n  }\n\n  // Auto-generate if no AGENTS.md files exist\n  if (docs.length === 0) {\n    console.log(pc.yellow('No AGENTS.md files found. Running generate first...'));\n    await generateCommand(targetPath, {\n      debug: options.debug,\n      trace: options.trace,\n    });\n    docs = await collectAgentsDocs(absolutePath);\n    annexFiles = await collectAnnexFiles(absolutePath);\n    if (docs.length === 0) {\n      console.error(pc.red('Error: No AGENTS.md files found after generation. Cannot proceed.'));\n      process.exit(1);\n    }\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI synthesis\n  // ---------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Resolve effective model: CLI flag > config override > opus default\n  // Specify benefits from the best model; upgrade default sonnet to opus\n  const effectiveModel = options.model\n    ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model);\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n  }\n\n  // Create AI service with extended timeout (spec generation takes longer)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 900_000),\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build prompt from collected docs and annex files\n  const prompt = buildSpecPrompt(docs, annexFiles.length > 0 ? annexFiles : undefined);\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] System prompt: ${prompt.system.length} chars`));\n    console.error(pc.dim(`[debug] User prompt: ${prompt.user.length} chars`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Specify (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`AGENTS.md files: ${docs.length}`);\n  progressLog.write(`Annex files: ${annexFiles.length}`);\n  progressLog.write('');\n\n  console.log(pc.bold('Generating specification...'));\n  console.log(pc.dim('This may take several minutes depending on project size.'));\n  progressLog.write('Generating specification...');\n\n  const response = await aiService.call({\n    prompt: prompt.user,\n    systemPrompt: prompt.system,\n    taskLabel: 'specify',\n  });\n\n  // ---------------------------------------------------------------------------\n  // Write output\n  // ---------------------------------------------------------------------------\n\n  try {\n    const writtenFiles = await writeSpec(response.text, {\n      outputPath,\n      force: options.force ?? false,\n      multiFile: options.multiFile ?? false,\n    });\n\n    console.log('');\n    console.log(pc.green(pc.bold('Specification written successfully:')));\n    for (const file of writtenFiles) {\n      console.log(pc.green(`  ${file}`));\n      progressLog.write(`Written: ${file}`);\n    }\n  } catch (error) {\n    if (error instanceof SpecExistsError) {\n      progressLog.write(`Error: ${error.message}`);\n      await progressLog.finalize();\n      console.error(pc.red(error.message));\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Finalize telemetry\n  // ---------------------------------------------------------------------------\n\n  const { summary } = await aiService.finalize(absolutePath);\n\n  const summaryLine = `Tokens: ${summary.totalInputTokens} in / ${summary.totalOutputTokens} out` +\n    ` | Duration: ${(summary.totalDurationMs / 1000).toFixed(1)}s` +\n    ` | Output: ${outputPath}`;\n  console.log('');\n  console.log(pc.dim(summaryLine));\n  progressLog.write(summaryLine);\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`specify.ts` implements the `specifyCommand()` CLI entry point that synthesizes project specifications from AGENTS.md documentation corpus via AI, with auto-generation fallback, dry-run preview, single/multi-file output modes, and forced overwrite control.**\n\n## Exported Interface\n\n**`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>`** — Main command function that orchestrates specification generation: loads configuration via `loadConfig()`, collects AGENTS.md files via `collectAgentsDocs()` and annex files via `collectAnnexFiles()`, resolves AI backend via `resolveBackend()`, builds synthesis prompt via `buildSpecPrompt()`, calls `AIService.call()` with task label `'specify'`, writes output via `writeSpec()`, and finalizes telemetry via `aiService.finalize()`.\n\n**`interface SpecifyOptions`** — Command options controlling behavior:\n- `output?: string` — Custom output path (defaults to `specs/SPEC.md`)\n- `force?: boolean` — Overwrite existing specification files\n- `dryRun?: boolean` — Preview input statistics without AI calls or generation\n- `multiFile?: boolean` — Split output into multiple per-directory files\n- `debug?: boolean` — Enable verbose subprocess logging\n- `trace?: boolean` — Enable NDJSON trace emission\n- `model?: string` — Override AI model selection (defaults to `'opus'` for specification synthesis)\n\n## Model Resolution Strategy\n\n**Model override chain:** CLI flag `options.model` > config file `config.ai.model` > default `'opus'`. Specification generation benefits from highest-quality models, so default `'sonnet'` is upgraded to `'opus'` via conditional: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)`.\n\n## Pre-Flight Conflict Detection\n\n**Early exit for existing specs:** Before loading configuration or collecting documentation, checks for conflicting output files via `access()` with `constants.F_OK`. In `multiFile` mode, scans output directory via `readdir()` for any `.md` files. In single-file mode, checks specified output path. Exits with status code `1` and red-colored error message listing conflicts unless `--force` flag present. Avoids waiting for expensive AI call before detecting overwrite conflicts.\n\n## Dry-Run Mode\n\n**Token estimation:** Sums character counts from `docs[]` and `annexFiles[]`, estimates tokens via `totalChars / 4 / 1000` for K-token display. Prints summary with cyan-colored statistics: AGENTS.md count, annex file count, estimated token count, output path, output mode (`'multi-file'` | `'single-file'`). Emits yellow warnings if no AGENTS.md found (`'Run \\`are generate\\` first'`) or if input exceeds 150K tokens (`'Consider using a model with extended context'`). Returns early without AI backend resolution or `AIService` instantiation.\n\n## Auto-Generation Fallback\n\n**Zero-documentation handling:** After `collectAgentsDocs()` returns empty array, prints yellow warning `'No AGENTS.md files found. Running generate first...'`, invokes `generateCommand(targetPath, { debug, trace })` to execute full three-phase pipeline, re-collects documentation and annex files, verifies non-empty result or exits with status code `1` and red error `'No AGENTS.md files found after generation. Cannot proceed.'`.\n\n## Backend Resolution\n\n**AI CLI detection:** Creates backend registry via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`. Catches `AIServiceError` with `code === 'CLI_NOT_FOUND'`, prints red error message followed by installation instructions via `getInstallInstructions(registry)`, exits with status code `2` (distinct from file conflict exit code `1`).\n\n## Timeout Configuration\n\n**Extended synthesis duration:** Instantiates `AIService` with timeout `Math.max(config.ai.timeoutMs, 900_000)` (15 minutes minimum) to accommodate specification synthesis workload exceeding standard file analysis timeout (default 120 seconds). Passes `maxRetries: config.ai.maxRetries`, `model: effectiveModel`, and `telemetry: { keepRuns: config.ai.telemetry.keepRuns }`.\n\n## Progress Monitoring\n\n**Tail-friendly logging:** Creates `ProgressLog` via `ProgressLog.create(absolutePath)`, writes ISO 8601 timestamp header `'=== ARE Specify (${new Date().toISOString()}) ==='`, project path, AGENTS.md count, annex file count. Logs `'Generating specification...'` before AI call. Writes `'Written: ${file}'` for each output file from `writeSpec()`. Logs summary line with format `'Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${durationSec}s | Output: ${outputPath}'`. Finalizes log via `await progressLog.finalize()`.\n\n## Error Handling\n\n**`SpecExistsError` recovery:** Catches `SpecExistsError` thrown by `writeSpec()`, logs error message to progress log, awaits `progressLog.finalize()`, prints red-colored error via `console.error()`, exits with status code `1`. Re-throws all other errors for upstream handling.\n\n## Debug Output\n\n**Conditional verbosity:** When `options.debug === true`, logs backend name, CLI command, effective model, system prompt character count, user prompt character count prefixed with dim-styled `'[debug]'` via `console.error()`. Enables debug mode on `AIService` instance via `aiService.setDebug(true)`.\n\n## Dependencies\n\n**Core modules:** `node:path` (path resolution), `node:fs/promises` (`access`, `readdir`), `node:fs` (`constants.F_OK`). **Styling:** `picocolors` for ANSI color output (`pc.red`, `pc.green`, `pc.yellow`, `pc.cyan`, `pc.dim`, `pc.bold`). **Configuration:** `loadConfig` from `../config/loader.js`. **Collection:** `collectAgentsDocs`, `collectAnnexFiles` from `../generation/collector.js`. **Specification:** `buildSpecPrompt`, `writeSpec`, `SpecExistsError` from `../specify/index.js`. **AI orchestration:** `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `../ai/index.js`. **Progress:** `ProgressLog` from `../orchestration/index.js`. **Fallback:** `generateCommand` from `./generate.js`.\n\n## Task Label\n\n**Telemetry categorization:** Passes `taskLabel: 'specify'` to `AIService.call()` for run log aggregation and cost tracking distinct from file analysis (`'file-analysis'`) and directory synthesis (`'directory-synthesis'`) tasks.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1654,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11853,
      "latencyMs": 38471,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/specify.ts",
          "sizeBytes": 9185
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:36.700Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/collector.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\n\n/**\n * A collected AGENTS.md document with its project-relative path and content.\n */\nexport type AgentsDocs = Array<{ relativePath: string; content: string }>;\n\n/** Directories to skip during recursive AGENTS.md collection. */\nconst SKIP_DIRS = new Set([\n  'node_modules', '.git', '.agents-reverse-engineer',\n  'vendor', 'dist', 'build', '__pycache__', '.next',\n  'venv', '.venv', 'target', '.cargo', '.gradle',\n]);\n\n/**\n * Recursively collect all AGENTS.md files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Skips vendor/build/meta directories and gracefully handles\n * unreadable directories or files.\n */\nexport async function collectAgentsDocs(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return; // Permission denied or inaccessible\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.name === 'AGENTS.md') {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n/**\n * Recursively collect all `.annex.md` files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Uses the same skip-list as `collectAgentsDocs()`.\n */\nexport async function collectAnnexFiles(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return;\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.isFile() && entry.name.endsWith('.annex.md')) {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**collector.ts provides recursive directory traversal functions to collect all `AGENTS.md` and `.annex.md` files from a project tree, returning sorted arrays of relative paths and content while skipping vendor/build/meta directories.**\n\n## Exported Functions\n\n`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>` recursively walks the directory tree starting from `projectRoot`, collects all files named exactly `AGENTS.md`, reads their content via `readFile(filePath, 'utf-8')`, and returns an array of `{ relativePath, content }` objects sorted alphabetically by `relativePath` using `localeCompare()`. Skips directories in `SKIP_DIRS` set and gracefully handles permission errors by catching and ignoring exceptions from `readdir()` and `readFile()`.\n\n`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>` uses identical traversal logic to `collectAgentsDocs()` but collects files ending with `.annex.md` instead of exact-match `AGENTS.md`. Tests file names with `entry.name.endsWith('.annex.md')` and returns the same `AgentsDocs` structure sorted alphabetically.\n\n## Type Definitions\n\n`AgentsDocs` is an exported type alias for `Array<{ relativePath: string; content: string }>` representing collected documentation files with their project-relative paths and text content.\n\n## Directory Exclusion\n\n`SKIP_DIRS` is a `Set<string>` constant containing 13 directory names to exclude during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Both collection functions check `SKIP_DIRS.has(entry.name)` before recursing into subdirectories.\n\n## Traversal Pattern\n\nBoth functions use internal async `walk(currentDir: string): Promise<void>` closures that call `readdir(currentDir, { withFileTypes: true })` to obtain `Dirent` objects, iterate over entries, recurse on directories not in `SKIP_DIRS` via `walk(path.join(currentDir, entry.name))`, and collect matching files by pushing `{ relativePath: path.relative(projectRoot, filePath), content }` into the `results` array. Final sorting happens after traversal completes via `results.sort((a, b) => a.relativePath.localeCompare(b.relativePath))`.\n\n## Error Handling\n\nBoth functions wrap `readdir()` calls in try-catch blocks that return early on exception (permission denied or inaccessible directories). File read operations wrap `readFile()` in try-catch blocks that skip unreadable files silently without logging. No error propagation occurs—all failures are suppressed to ensure partial collection succeeds.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 694,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10079,
      "latencyMs": 18734,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/collector.ts",
          "sizeBytes": 2949
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:00.031Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/service.ts\n\n```typescript\n/**\n * AI service orchestrator.\n *\n * The {@link AIService} class is the main entry point for making AI calls.\n * It ties together the subprocess wrapper, retry logic, backend selection,\n * and telemetry logging into a clean `call()` method.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport type { AIBackend, AICallOptions, AIResponse, SubprocessResult, TelemetryEntry, RunLog, FileRead } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { runSubprocess } from './subprocess.js';\nimport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\nimport { TelemetryLogger } from './telemetry/logger.js';\nimport { writeRunLog } from './telemetry/run-log.js';\nimport { cleanupOldLogs } from './telemetry/cleanup.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n// ---------------------------------------------------------------------------\n// Rate-limit detection patterns\n// ---------------------------------------------------------------------------\n\n/** Patterns in stderr that indicate a transient rate-limit error */\nconst RATE_LIMIT_PATTERNS = [\n  'rate limit',\n  '429',\n  'too many requests',\n  'overloaded',\n];\n\n/**\n * Check whether stderr text contains rate-limit indicators.\n *\n * @param stderr - Standard error output from the subprocess\n * @returns `true` if any rate-limit pattern matches (case-insensitive)\n */\nfunction isRateLimitStderr(stderr: string): boolean {\n  const lower = stderr.toLowerCase();\n  return RATE_LIMIT_PATTERNS.some((pattern) => lower.includes(pattern));\n}\n\n/**\n * Format bytes as a human-readable string.\n */\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)}KB`;\n  return `${(bytes / (1024 * 1024)).toFixed(1)}MB`;\n}\n\n// ---------------------------------------------------------------------------\n// AIService options\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration options for the {@link AIService}.\n *\n * These are typically sourced from the config schema's `ai` section.\n */\nexport interface AIServiceOptions {\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: number;\n  /** Maximum number of retries for transient errors */\n  maxRetries: number;\n  /** Default model identifier (e.g., \"sonnet\", \"opus\") applied to all calls unless overridden per-call */\n  model?: string;\n  /** Telemetry settings */\n  telemetry: {\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// AIService\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI CLI calls with retry, timeout, and telemetry.\n *\n * Create one instance per CLI run. Call {@link call} for each AI invocation.\n * Call {@link finalize} at the end to write the run log and clean up old files.\n *\n * @example\n * ```typescript\n * import { AIService } from './service.js';\n * import { resolveBackend, createBackendRegistry } from './registry.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Summarize this codebase' });\n * console.log(response.text);\n *\n * const { logPath, summary } = await service.finalize('/path/to/project');\n * console.log(`Log written to ${logPath}`);\n * ```\n */\nexport class AIService {\n  /** The backend adapter used for CLI invocations */\n  private readonly backend: AIBackend;\n\n  /** Service configuration */\n  private readonly options: AIServiceOptions;\n\n  /** In-memory telemetry logger for this run */\n  private readonly logger: TelemetryLogger;\n\n  /** Running count of calls made (used for entry tracking) */\n  private callCount: number = 0;\n\n  /** Trace writer for concurrency debugging (may be no-op) */\n  private tracer: ITraceWriter | null = null;\n\n  /** Whether debug mode is enabled */\n  private debug: boolean = false;\n\n  /** Number of currently active subprocesses */\n  private activeSubprocesses: number = 0;\n\n  /** Directory for subprocess output logs (null = disabled) */\n  private subprocessLogDir: string | null = null;\n\n  /** Serializes log writes so concurrent workers don't interleave mkdirs */\n  private logWriteQueue: Promise<void> = Promise.resolve();\n\n  /**\n   * Create a new AI service instance.\n   *\n   * @param backend - The resolved backend adapter\n   * @param options - Service configuration (timeout, retries, telemetry)\n   */\n  constructor(backend: AIBackend, options: AIServiceOptions) {\n    this.backend = backend;\n    this.options = options;\n    this.logger = new TelemetryLogger(new Date().toISOString());\n  }\n\n  /**\n   * Set the trace writer for subprocess and retry event tracing.\n   *\n   * @param tracer - The trace writer instance\n   */\n  setTracer(tracer: ITraceWriter): void {\n    this.tracer = tracer;\n  }\n\n  /**\n   * Enable debug mode for verbose subprocess logging to stderr.\n   */\n  setDebug(enabled: boolean): void {\n    this.debug = enabled;\n  }\n\n  /**\n   * Set a directory for writing subprocess stdout/stderr log files.\n   *\n   * When set, each subprocess invocation writes a `.log` file containing\n   * the metadata header, stdout, and stderr. Useful for diagnosing\n   * timed-out or failed subprocesses whose output would otherwise be lost.\n   *\n   * @param dir - Absolute path to the log directory (created on first write)\n   */\n  setSubprocessLogDir(dir: string): void {\n    this.subprocessLogDir = dir;\n  }\n\n  /**\n   * Make an AI call with retry logic and telemetry recording.\n   *\n   * The call flow:\n   * 1. Build CLI args via the backend adapter\n   * 2. Wrap the subprocess invocation in retry logic\n   * 3. On success: parse response via backend, record telemetry entry\n   * 4. On failure: record error telemetry entry, throw the error\n   *\n   * Retries are attempted for `RATE_LIMIT` and `TIMEOUT` errors only.\n   * All other errors are treated as permanent failures.\n   *\n   * @param options - The call options (prompt, model, timeout, etc.)\n   * @returns The normalized AI response\n   * @throws {AIServiceError} On timeout, rate limit exhaustion, parse error, or subprocess failure\n   */\n  async call(options: AICallOptions): Promise<AIResponse> {\n    this.callCount++;\n    const callStart = Date.now();\n    const timestamp = new Date().toISOString();\n    const taskLabel = options.taskLabel ?? 'unknown';\n\n    // Merge service-level model as default (per-call options.model wins)\n    const effectiveOptions: AICallOptions = {\n      ...options,\n      model: options.model ?? this.options.model,\n    };\n\n    const args = this.backend.buildArgs(effectiveOptions);\n    const timeoutMs = options.timeoutMs ?? this.options.timeoutMs;\n\n    let retryCount = 0;\n\n    try {\n      const response = await withRetry(\n        async () => {\n          if (this.debug) {\n            const mem = process.memoryUsage();\n            console.error(\n              `[debug] Spawning subprocess for \"${taskLabel}\" ` +\n              `(active: ${this.activeSubprocesses}, ` +\n              `heapUsed: ${formatBytes(mem.heapUsed)}, ` +\n              `rss: ${formatBytes(mem.rss)}, ` +\n              `timeout: ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n          }\n\n          this.activeSubprocesses++;\n\n          const result = await runSubprocess(this.backend.cliCommand, args, {\n            timeoutMs,\n            input: options.prompt,\n            onSpawn: (pid) => {\n              // Emit subprocess:spawn at actual spawn time (not after completion)\n              this.tracer?.emit({\n                type: 'subprocess:spawn',\n                childPid: pid ?? -1,\n                command: this.backend.cliCommand,\n                taskLabel,\n              });\n            },\n          });\n\n          this.activeSubprocesses--;\n\n          // Emit subprocess:exit after completion\n          if (this.tracer && result.childPid !== undefined) {\n            this.tracer.emit({\n              type: 'subprocess:exit',\n              childPid: result.childPid,\n              command: this.backend.cliCommand,\n              taskLabel,\n              exitCode: result.exitCode,\n              signal: result.signal,\n              durationMs: result.durationMs,\n              timedOut: result.timedOut,\n            });\n          }\n\n          // Write subprocess output log (fire-and-forget, non-critical)\n          this.enqueueSubprocessLog(result, taskLabel);\n\n          if (result.timedOut) {\n            console.error(\n              `[warn] Subprocess timed out after ${(result.durationMs / 1000).toFixed(1)}s ` +\n              `for \"${taskLabel}\" (PID ${result.childPid ?? 'unknown'}, ` +\n              `timeout was ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n            throw new AIServiceError('TIMEOUT', 'Subprocess timed out');\n          }\n\n          if (this.debug) {\n            console.error(\n              `[debug] Subprocess exited for \"${taskLabel}\" ` +\n              `(PID ${result.childPid ?? 'unknown'}, ` +\n              `exitCode: ${result.exitCode}, ` +\n              `duration: ${(result.durationMs / 1000).toFixed(1)}s, ` +\n              `active: ${this.activeSubprocesses})`,\n            );\n          }\n\n          if (result.exitCode !== 0) {\n            if (isRateLimitStderr(result.stderr)) {\n              throw new AIServiceError(\n                'RATE_LIMIT',\n                `Rate limited by ${this.backend.name}: ${result.stderr.slice(0, 200)}`,\n              );\n            }\n            throw new AIServiceError(\n              'SUBPROCESS_ERROR',\n              `${this.backend.name} CLI exited with code ${result.exitCode}: ${result.stderr.slice(0, 500)}`,\n            );\n          }\n\n          // Parse the response -- wrap in try/catch for parse errors\n          try {\n            return this.backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n          } catch (error) {\n            if (error instanceof AIServiceError) {\n              throw error;\n            }\n            const message = error instanceof Error ? error.message : String(error);\n            throw new AIServiceError('PARSE_ERROR', `Failed to parse response: ${message}`);\n          }\n        },\n        {\n          ...DEFAULT_RETRY_OPTIONS,\n          maxRetries: this.options.maxRetries,\n          isRetryable: (error: unknown): boolean => {\n            // Only retry rate limits. Timeouts are NOT retried because\n            // spawning another heavyweight subprocess on a system that's\n            // already struggling (or against an unresponsive API) makes\n            // things worse and can exhaust system resources.\n            return (\n              error instanceof AIServiceError &&\n              error.code === 'RATE_LIMIT'\n            );\n          },\n          onRetry: (attempt: number, error: unknown) => {\n            retryCount++;\n\n            const errorCode = error instanceof AIServiceError ? error.code : 'UNKNOWN';\n\n            // Always warn on retry (not just debug) -- retries are noteworthy\n            console.error(\n              `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`,\n            );\n\n            // Emit retry trace event\n            if (this.tracer) {\n              this.tracer.emit({\n                type: 'retry',\n                attempt,\n                taskLabel,\n                errorCode,\n              });\n            }\n          },\n        },\n      );\n\n      // Record successful call\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: response.text,\n        model: response.model,\n        inputTokens: response.inputTokens,\n        outputTokens: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        latencyMs: response.durationMs,\n        exitCode: response.exitCode,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      return response;\n    } catch (error) {\n      // Record failed call\n      const latencyMs = Date.now() - callStart;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: '',\n        model: options.model ?? 'unknown',\n        inputTokens: 0,\n        outputTokens: 0,\n        cacheReadTokens: 0,\n        cacheCreationTokens: 0,\n        latencyMs,\n        exitCode: 1,\n        error: errorMessage,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      throw error;\n    }\n  }\n\n  /**\n   * Finalize the run: write the run log to disk and clean up old files.\n   *\n   * Call this once at the end of a CLI invocation, after all `call()`\n   * invocations have completed (or failed).\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns The log file path and the run summary\n   */\n  async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }> {\n    const runLog = this.logger.toRunLog();\n    const logPath = await writeRunLog(projectRoot, runLog);\n    await cleanupOldLogs(projectRoot, this.options.telemetry.keepRuns);\n    return { logPath, summary: runLog.summary };\n  }\n\n  /**\n   * Attach file-read metadata to the most recent telemetry entry.\n   *\n   * Called by the command runner after an AI call completes, to record\n   * which source files were sent as context for that call.\n   *\n   * @param filesRead - Array of file-read records (path + size)\n   */\n  addFilesReadToLastEntry(filesRead: FileRead[]): void {\n    this.logger.setFilesReadOnLastEntry(filesRead);\n  }\n\n  /**\n   * Get the current run summary without finalizing.\n   *\n   * Useful for displaying progress during a run.\n   *\n   * @returns Current summary statistics\n   */\n  getSummary(): RunLog['summary'] {\n    return this.logger.getSummary();\n  }\n\n  /**\n   * Enqueue a subprocess output log write.\n   *\n   * Serializes writes via a promise chain to avoid concurrent mkdir races.\n   * Failures are silently swallowed -- log writing is non-critical.\n   */\n  private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void {\n    if (this.subprocessLogDir === null) return;\n\n    const dir = this.subprocessLogDir;\n    const sanitized = taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_');\n    const filename = `${sanitized}_pid${result.childPid ?? 0}.log`;\n    const filePath = path.join(dir, filename);\n\n    const content =\n      `task:      ${taskLabel}\\n` +\n      `pid:       ${result.childPid ?? 'unknown'}\\n` +\n      `command:   ${this.backend.cliCommand}\\n` +\n      `exit:      ${result.exitCode}\\n` +\n      `signal:    ${result.signal ?? 'none'}\\n` +\n      `duration:  ${result.durationMs}ms\\n` +\n      `timed_out: ${result.timedOut}\\n` +\n      `\\n--- stdout ---\\n` +\n      result.stdout +\n      `\\n--- stderr ---\\n` +\n      result.stderr;\n\n    this.logWriteQueue = this.logWriteQueue\n      .then(async () => {\n        await mkdir(dir, { recursive: true });\n        await writeFile(filePath, content, 'utf-8');\n      })\n      .catch(() => { /* non-critical -- log loss is acceptable */ });\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**AIService orchestrates AI CLI invocations with subprocess management, exponential backoff retry, telemetry recording, and trace emission.**\n\n## Exported Class\n\n`AIService` — Main orchestrator for AI calls with retry logic, timeout enforcement, and telemetry aggregation.\n\n**Constructor signature:**\n```typescript\nconstructor(backend: AIBackend, options: AIServiceOptions)\n```\n\n**Public methods:**\n- `async call(options: AICallOptions): Promise<AIResponse>` — Execute AI call with retry and telemetry recording\n- `async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>` — Write run log to disk and cleanup old logs\n- `setTracer(tracer: ITraceWriter): void` — Attach trace writer for subprocess/retry event emission\n- `setDebug(enabled: boolean): void` — Enable verbose subprocess logging to stderr\n- `setSubprocessLogDir(dir: string): void` — Set directory for writing subprocess stdout/stderr log files\n- `addFilesReadToLastEntry(filesRead: FileRead[]): void` — Attach file-read metadata to most recent telemetry entry\n- `getSummary(): RunLog['summary']` — Get current run summary without finalizing\n\n## Configuration Types\n\n`AIServiceOptions` interface with fields:\n- `timeoutMs: number` — Default subprocess timeout in milliseconds\n- `maxRetries: number` — Maximum retry attempts for transient errors\n- `model?: string` — Default model identifier applied to all calls unless overridden per-call\n- `telemetry.keepRuns: number` — Number of most recent run logs to keep on disk\n\n## Call Flow\n\n`call()` method executes:\n1. Merge service-level `model` with per-call `options.model` (per-call wins)\n2. Build CLI args via `backend.buildArgs(effectiveOptions)`\n3. Wrap subprocess invocation in `withRetry()` with `DEFAULT_RETRY_OPTIONS` and `maxRetries` from service options\n4. Increment `activeSubprocesses` counter before spawn, decrement after completion\n5. Invoke `runSubprocess(backend.cliCommand, args, { timeoutMs, input: options.prompt, onSpawn })` with `onSpawn` callback emitting `subprocess:spawn` trace event\n6. Emit `subprocess:exit` trace event after completion with `childPid`, `exitCode`, `signal`, `durationMs`, `timedOut` fields\n7. Enqueue non-critical subprocess log write via `enqueueSubprocessLog()` promise chain\n8. If `result.timedOut`, throw `AIServiceError('TIMEOUT', 'Subprocess timed out')`\n9. If `exitCode !== 0`, check `isRateLimitStderr(result.stderr)` → throw `AIServiceError('RATE_LIMIT', ...)` or `AIServiceError('SUBPROCESS_ERROR', ...)`\n10. Parse response via `backend.parseResponse(result.stdout, result.durationMs, result.exitCode)`, catch errors and wrap in `AIServiceError('PARSE_ERROR', ...)`\n11. On success: record telemetry entry via `logger.addEntry()` with token counts, latency, retry count\n12. On failure: record telemetry entry with error message and latency\n13. Return `AIResponse` or rethrow error\n\n## Retry Strategy\n\n`withRetry()` invoked with custom `isRetryable` predicate:\n- **Retryable:** `AIServiceError` with `code === 'RATE_LIMIT'` only\n- **Non-retryable:** All other errors including `TIMEOUT`, `SUBPROCESS_ERROR`, `PARSE_ERROR`\n\nRationale (from inline comment): \"Timeouts are NOT retried because spawning another heavyweight subprocess on a system that's already struggling (or against an unresponsive API) makes things worse and can exhaust system resources.\"\n\n`onRetry` callback:\n- Increments `retryCount` local variable\n- Logs warning to stderr: `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`\n- Emits `retry` trace event with `attempt`, `taskLabel`, `errorCode` fields\n\n## Rate Limit Detection\n\n`isRateLimitStderr(stderr: string)` checks for patterns in `RATE_LIMIT_PATTERNS`:\n- `\"rate limit\"`\n- `\"429\"`\n- `\"too many requests\"`\n- `\"overloaded\"`\n\nCase-insensitive substring matching via `stderr.toLowerCase().includes(pattern)`.\n\n## Telemetry Recording\n\n`TelemetryLogger` instance created in constructor with `new Date().toISOString()` as run timestamp.\n\n`addEntry()` called on both success and failure paths with `TelemetryEntry` containing:\n- `timestamp`, `prompt`, `systemPrompt`, `response`, `model`\n- `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`\n- `latencyMs`, `exitCode`, `retryCount`, `error` (on failure)\n- `thinking: 'not supported'` (hardcoded constant)\n- `filesRead: []` (populated via `addFilesReadToLastEntry()` by command runner after call completes)\n\n`finalize()` workflow:\n1. Call `logger.toRunLog()` to aggregate entries into `RunLog` structure\n2. Write via `writeRunLog(projectRoot, runLog)` returning log file path\n3. Cleanup via `cleanupOldLogs(projectRoot, options.telemetry.keepRuns)` to enforce retention limit\n4. Return `{ logPath, summary: runLog.summary }`\n\n## Debug Logging\n\nWhen `debug` flag enabled via `setDebug(true)`:\n- Pre-spawn: Log `[debug] Spawning subprocess for \"${taskLabel}\" (active: ${activeSubprocesses}, heapUsed: ${formatBytes(mem.heapUsed)}, rss: ${formatBytes(mem.rss)}, timeout: ${(timeoutMs / 1000).toFixed(0)}s)`\n- Post-completion: Log `[debug] Subprocess exited for \"${taskLabel}\" (PID ${childPid ?? 'unknown'}, exitCode: ${exitCode}, duration: ${(durationMs / 1000).toFixed(1)}s, active: ${activeSubprocesses})`\n\nMemory stats retrieved via `process.memoryUsage()` with `heapUsed` and `rss` fields formatted via `formatBytes()`.\n\n## Subprocess Output Logging\n\n`setSubprocessLogDir(dir)` enables logging of subprocess stdout/stderr to individual `.log` files.\n\n`enqueueSubprocessLog(result, taskLabel)` private method:\n- Sanitizes `taskLabel` via `.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')`\n- Generates filename: `${sanitized}_pid${result.childPid ?? 0}.log`\n- Serializes writes via `logWriteQueue` promise chain to prevent concurrent mkdir races\n- Writes header with `task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out` metadata followed by stdout and stderr sections\n- Failures silently swallowed (fire-and-forget pattern, log loss acceptable)\n\n## State Management\n\nInstance fields:\n- `backend: AIBackend` — Resolved backend adapter from registry\n- `options: AIServiceOptions` — Service configuration\n- `logger: TelemetryLogger` — In-memory telemetry accumulator\n- `callCount: number` — Running count of calls made (initialized to 0)\n- `tracer: ITraceWriter | null` — Trace writer for concurrency debugging (null until `setTracer()` called)\n- `debug: boolean` — Debug mode flag (default false)\n- `activeSubprocesses: number` — Count of currently active subprocesses (incremented before spawn, decremented after completion)\n- `subprocessLogDir: string | null` — Directory for subprocess logs (null = disabled)\n- `logWriteQueue: Promise<void>` — Promise chain for serializing subprocess log writes (initialized to `Promise.resolve()`)\n\n## Helper Functions\n\n`formatBytes(bytes: number): string` — Human-readable byte formatting with thresholds:\n- `< 1024` → `${bytes}B`\n- `< 1024 * 1024` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1024 * 1024` → `${(bytes / (1024 * 1024)).toFixed(1)}MB`\n\n## Dependencies\n\nImports from project modules:\n- `./types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `SubprocessResult`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError`\n- `./subprocess.js` — `runSubprocess`\n- `./retry.js` — `withRetry`, `DEFAULT_RETRY_OPTIONS`\n- `./telemetry/logger.js` — `TelemetryLogger`\n- `./telemetry/run-log.js` — `writeRunLog`\n- `./telemetry/cleanup.js` — `cleanupOldLogs`\n- `../orchestration/trace.js` — `ITraceWriter`\n\nNode.js built-ins:\n- `node:fs/promises` — `writeFile`, `mkdir`\n- `node:path` — Path manipulation for subprocess log filenames",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2277,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13561,
      "latencyMs": 55663,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/service.ts",
          "sizeBytes": 15566
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:42.088Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/complexity.ts\n\n```typescript\nimport * as path from 'node:path';\n\n/**\n * Metrics about codebase complexity.\n */\nexport interface ComplexityMetrics {\n  /** Total number of source files */\n  fileCount: number;\n  /** Maximum directory depth */\n  directoryDepth: number;\n  /** List of source file paths */\n  files: string[];\n  /** Unique directory paths */\n  directories: Set<string>;\n}\n\n/**\n * Calculate maximum directory depth from file paths.\n */\nfunction calculateDirectoryDepth(files: string[], projectRoot: string): number {\n  let maxDepth = 0;\n\n  for (const file of files) {\n    const relativePath = path.relative(projectRoot, file);\n    const depth = relativePath.split(path.sep).length - 1; // -1 for the file itself\n    maxDepth = Math.max(maxDepth, depth);\n  }\n\n  return maxDepth;\n}\n\n/**\n * Extract unique directories from file paths.\n */\nfunction extractDirectories(files: string[]): Set<string> {\n  const directories = new Set<string>();\n\n  for (const file of files) {\n    let dir = path.dirname(file);\n    while (dir && dir !== '.') {\n      directories.add(dir);\n      const parent = path.dirname(dir);\n      if (parent === dir) break; // Reached root\n      dir = parent;\n    }\n  }\n\n  return directories;\n}\n\n/**\n * Analyze codebase complexity from discovered files.\n *\n * @param files - List of source file paths\n * @param projectRoot - Project root directory\n * @returns Complexity metrics\n */\nexport function analyzeComplexity(\n  files: string[],\n  projectRoot: string\n): ComplexityMetrics {\n  return {\n    fileCount: files.length,\n    directoryDepth: calculateDirectoryDepth(files, projectRoot),\n    files,\n    directories: extractDirectories(files),\n  };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`complexity.ts` computes codebase complexity metrics from discovered file lists via directory depth calculation and unique directory extraction.**\n\n## Exported Interface\n\n`ComplexityMetrics` describes structural complexity with:\n- `fileCount: number` — total source files discovered\n- `directoryDepth: number` — maximum depth from project root (file itself excluded from count)\n- `files: string[]` — array of absolute file paths\n- `directories: Set<string>` — unique directory paths extracted via upward traversal\n\n## Exported Function\n\n`analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` orchestrates metric computation by calling `calculateDirectoryDepth()` and `extractDirectories()`, returning populated `ComplexityMetrics` object.\n\n## Internal Functions\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` computes maximum nesting depth via `path.relative()` followed by `split(path.sep).length - 1` (subtracting 1 to exclude filename component). Iterates all files tracking `Math.max()` depth.\n\n`extractDirectories(files: string[]): Set<string>` walks each file path upward via `path.dirname()` loop until reaching root (detected when `parent === dir`), accumulating unique directory paths into `Set<string>`. Stops at `'.'` or root equivalence to prevent infinite loops.\n\n## Usage Context\n\nCalled by Phase 1 orchestration (`src/generation/orchestrator.ts`) after file discovery to compute project structure metrics. Results inform concurrency tuning decisions and progress estimation. The `directories` set drives Phase 2 post-order traversal by identifying all directories requiring `AGENTS.md` aggregation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 395,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9682,
      "latencyMs": 14677,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/complexity.ts",
          "sizeBytes": 1642
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:23.095Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/loader.ts\n\n```typescript\n/**\n * Configuration loader for agents-reverse\n *\n * Loads and validates configuration from `.agents-reverse/config.yaml`.\n * Returns sensible defaults when no config file exists.\n */\n\nimport { readFile, writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport path from 'node:path';\nimport { parse, stringify } from 'yaml';\nimport { ZodError } from 'zod';\nimport pc from 'picocolors';\nimport { ConfigSchema, Config } from './schema.js';\nimport { DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency } from './defaults.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Quote a string value for YAML output if it contains characters that\n * would be misinterpreted (e.g. `*` is the YAML alias indicator).\n */\nfunction yamlScalar(value: string): string {\n  // Characters that require quoting: *, {, }, [, ], ?, :, #, &, !, |, >, etc.\n  if (/[*{}\\[\\]?,:#&!|>'\"%@`]/.test(value)) {\n    return `\"${value.replace(/\\\\/g, '\\\\\\\\').replace(/\"/g, '\\\\\"')}\"`;\n  }\n  return value;\n}\n\n/** Directory name for agents-reverse-engineer configuration */\nexport const CONFIG_DIR = '.agents-reverse-engineer';\n\n/** Configuration file name */\nexport const CONFIG_FILE = 'config.yaml';\n\n/**\n * Error thrown when configuration parsing or validation fails\n */\nexport class ConfigError extends Error {\n  constructor(\n    message: string,\n    public readonly filePath: string,\n    public readonly cause?: Error\n  ) {\n    super(message);\n    this.name = 'ConfigError';\n  }\n}\n\n/**\n * Load configuration from `.agents-reverse/config.yaml`.\n *\n * If the file doesn't exist, returns default configuration.\n * If the file exists but is invalid, throws a ConfigError with details.\n *\n * @param root - Root directory containing `.agents-reverse/` folder\n * @param options - Optional configuration loading options\n * @param options.tracer - Trace writer for emitting config:loaded events\n * @param options.debug - Enable debug output for configuration loading\n * @returns Validated configuration object with all defaults applied\n * @throws ConfigError if the config file exists but is invalid\n *\n * @example\n * ```typescript\n * const config = await loadConfig('/path/to/project');\n * console.log(config.exclude.vendorDirs);\n * ```\n */\nexport async function loadConfig(\n  root: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<Config> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n\n  try {\n    const content = await readFile(configPath, 'utf-8');\n    const raw = parse(content);\n\n    try {\n      const config = ConfigSchema.parse(raw);\n\n      // Emit trace event\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: path.relative(root, configPath),\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config loaded from: ${path.relative(root, configPath)}`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    } catch (err) {\n      if (err instanceof ZodError) {\n        const issues = err.issues\n          .map((issue) => `  - ${issue.path.join('.')}: ${issue.message}`)\n          .join('\\n');\n        throw new ConfigError(\n          `Invalid configuration in ${configPath}:\\n${issues}`,\n          configPath,\n          err\n        );\n      }\n      throw err;\n    }\n  } catch (err) {\n    // File not found - return defaults\n    if ((err as NodeJS.ErrnoException).code === 'ENOENT') {\n      const config = ConfigSchema.parse({});\n\n      // Emit trace event for defaults\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: '(defaults)',\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config file not found, using defaults`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    }\n\n    // Re-throw ConfigError as-is\n    if (err instanceof ConfigError) {\n      throw err;\n    }\n\n    // YAML parse error\n    throw new ConfigError(\n      `Failed to parse ${configPath}: ${(err as Error).message}`,\n      configPath,\n      err as Error\n    );\n  }\n}\n\n/**\n * Check if a configuration file exists.\n *\n * @param root - Root directory to check\n * @returns true if `.agents-reverse/config.yaml` exists\n *\n * @example\n * ```typescript\n * if (!await configExists('.')) {\n *   console.log('Run `are init` to create configuration');\n * }\n * ```\n */\nexport async function configExists(root: string): Promise<boolean> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n  try {\n    await access(configPath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write a default configuration file with helpful comments.\n *\n * Creates the `.agents-reverse/` directory if it doesn't exist.\n * The generated file includes comments explaining each option.\n *\n * @param root - Root directory where `.agents-reverse/` will be created\n *\n * @example\n * ```typescript\n * await writeDefaultConfig('/path/to/project');\n * // Creates /path/to/project/.agents-reverse/config.yaml\n * ```\n */\nexport async function writeDefaultConfig(root: string): Promise<void> {\n  const configDir = path.join(root, CONFIG_DIR);\n  const configPath = path.join(configDir, CONFIG_FILE);\n\n  // Create directory if needed\n  await mkdir(configDir, { recursive: true });\n\n  // Generate config content with comments\n  const configContent = `# agents-reverse-engineer configuration\n# See: https://github.com/your-org/agents-reverse-engineer\n\n# ============================================================================\n# FILE & DIRECTORY EXCLUSIONS\n# ============================================================================\nexclude:\n  # Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"])\n  # Default patterns exclude AI-generated documentation files\n  patterns:\n${DEFAULT_EXCLUDE_PATTERNS.map((pattern) => `    - ${yamlScalar(pattern)}`).join('\\n')}\n\n  # Vendor directories to exclude from analysis\n  # These are typically package managers, build outputs, or version control\n  vendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n\n  # Binary file extensions to exclude from analysis\n  # These files cannot be meaningfully analyzed as text\n  binaryExtensions:\n${DEFAULT_BINARY_EXTENSIONS.map((ext) => `    - ${ext}`).join('\\n')}\n\n# ============================================================================\n# DISCOVERY OPTIONS\n# ============================================================================\noptions:\n  # Whether to follow symbolic links during traversal\n  followSymlinks: false\n\n  # Maximum file size in bytes (files larger than this are skipped)\n  # Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)\n  maxFileSize: ${DEFAULT_MAX_FILE_SIZE}\n\n# ============================================================================\n# OUTPUT FORMATTING\n# ============================================================================\noutput:\n  # Whether to use colors in terminal output\n  colors: true\n\n# ============================================================================\n# AI SERVICE CONFIGURATION\n# ============================================================================\nai:\n  # AI CLI backend to use\n  # Options: 'claude', 'gemini', 'opencode', 'auto' (auto-detect from PATH)\n  backend: auto\n\n  # Model identifier (backend-specific)\n  # Examples: \"sonnet\", \"opus\", \"haiku\" (for Claude)\n  model: sonnet\n\n  # Subprocess timeout in milliseconds\n  # Default: 300,000ms (5 minutes)\n  # Increase for very large files or slow connections\n  timeoutMs: 300000\n\n  # Maximum number of retries for transient errors\n  # Default: 3\n  maxRetries: 3\n\n  # Number of concurrent AI calls (parallelism)\n  # Range: 1-20, Default: auto-detected from CPU cores and available memory\n  # Current machine default: ${getDefaultConcurrency()}\n  # Uncomment to override:\n  # concurrency: ${getDefaultConcurrency()}\n\n  # Telemetry settings\n  telemetry:\n    # Number of most recent run logs to keep on disk\n    # Logs stored in .agents-reverse-engineer/logs/\n    keepRuns: 50\n`;\n\n  await writeFile(configPath, configContent, 'utf-8');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**loadConfig() orchestrates YAML configuration parsing from `.agents-reverse-engineer/config.yaml` with Zod schema validation, ConfigError-wrapped failure reporting, default fallback, and optional trace emission via ITraceWriter.**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nLoads configuration from `{root}/.agents-reverse-engineer/config.yaml` by reading file content, parsing YAML via `parse()`, validating against ConfigSchema via `ConfigSchema.parse(raw)`, and returning validated Config object. On ENOENT error, returns default Config from `ConfigSchema.parse({})`. On ZodError, extracts `issue.path` and `issue.message` fields to construct multi-line error report wrapped in ConfigError. Emits `'config:loaded'` trace event with `configPath`, `model`, `concurrency` fields when tracer provided. Logs debug output with `pc.dim()` formatting when `options.debug` enabled.\n\n**configExists(root: string): Promise<boolean>**\nChecks existence of configuration file at `{root}/.agents-reverse-engineer/config.yaml` via `access(configPath, constants.F_OK)`. Returns true if file accessible, false if ENOENT caught.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `.agents-reverse-engineer/` directory via `mkdir(configDir, { recursive: true })` and writes `config.yaml` with commented YAML template. Content includes sections for `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions`, `options.followSymlinks`, `options.maxFileSize`, `output.colors`, `ai.backend`, `ai.model`, `ai.timeoutMs`, `ai.maxRetries`, `ai.concurrency`, `ai.telemetry.keepRuns`. Applies DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE constants from defaults module. Calls `yamlScalar()` on pattern values to escape YAML special characters.\n\n**yamlScalar(value: string): string**\nQuotes string values containing YAML special characters matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/`. Escapes backslashes (`\\\\`) and double-quotes (`\\\"`) when quoting applied. Returns unmodified value when no special characters detected.\n\n## ConfigError Class\n\n**ConfigError extends Error** with additional properties `filePath: string` and `cause?: Error`. Constructor accepts `message`, `filePath`, optional `cause`. Sets `name` property to `'ConfigError'`.\n\n## Constants\n\n**CONFIG_DIR = '.agents-reverse-engineer'** — Directory name for configuration storage.\n\n**CONFIG_FILE = 'config.yaml'** — Configuration filename.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises` for file I/O. Imports `constants` from `node:fs` for F_OK flag. Imports `parse` and `stringify` from `yaml` library. Imports ZodError from `zod` for validation error handling. Imports `pc` from `picocolors` for terminal formatting. Imports ConfigSchema and Config type from `./schema.js`. Imports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from `./defaults.js`. Imports ITraceWriter from `../orchestration/trace.js`.\n\n## Error Handling Strategy\n\nloadConfig() distinguishes three error cases via instanceof checks and `(err as NodeJS.ErrnoException).code` inspection:\n1. ENOENT errors trigger default config return via `ConfigSchema.parse({})`\n2. ZodError instances map `err.issues[]` to formatted validation report and wrap in ConfigError\n3. Existing ConfigError instances re-thrown without modification\n4. All other errors wrapped in ConfigError with YAML parse failure message\n\n## Trace Event Contract\n\nEmits trace event with type `'config:loaded'` containing fields:\n- `configPath: string` — Relative path from root or literal `'(defaults)'`\n- `model: string` — Value from `config.ai.model`\n- `concurrency: number` — Value from `config.ai.concurrency`\n\n## Template Generation Pattern\n\nwriteDefaultConfig() constructs YAML content via template literals with embedded array mapping:\n```typescript\nvendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n```\nProduces indented YAML list items with 4-space prefix. Uses `yamlScalar()` wrapper for patterns array to escape glob special characters like `*`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1129,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11566,
      "latencyMs": 34670,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/loader.ts",
          "sizeBytes": 8501
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:16.619Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/rebuild.ts\n\n```typescript\n/**\n * CLI rebuild command\n *\n * Reconstructs a project from specification files by:\n * 1. Reading spec files from specs/ directory\n * 2. Partitioning into ordered rebuild units\n * 3. Resolving an AI CLI backend\n * 4. Running the rebuild orchestrator with checkpoint-based session continuity\n * 5. Writing generated source files to an output directory\n *\n * With --dry-run, shows the rebuild plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\nimport {\n  readSpecFiles,\n  partitionSpec,\n  CheckpointManager,\n  executeRebuild,\n} from '../rebuild/index.js';\n\n/**\n * Options for the rebuild command.\n */\nexport interface RebuildOptions {\n  /** Custom output directory (default: rebuild/) */\n  output?: string;\n  /** Wipe output directory and start fresh */\n  force?: boolean;\n  /** Show plan without executing */\n  dryRun?: boolean;\n  /** Override worker pool size */\n  concurrency?: number;\n  /** Stop on first failure */\n  failFast?: boolean;\n  /** Verbose subprocess logging */\n  debug?: boolean;\n  /** Enable NDJSON tracing */\n  trace?: boolean;\n  /** Override AI model (defaults to \"opus\" for rebuild) */\n  model?: string;\n}\n\n/**\n * Rebuild command - reconstructs a project from specification files via\n * AI-driven code generation with checkpoint-based session continuity.\n *\n * @param targetPath - Directory containing specs/ to rebuild from\n * @param options - Command options (output, force, dryRun, concurrency, etc.)\n */\nexport async function rebuildCommand(\n  targetPath: string,\n  options: RebuildOptions,\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const outputDir = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'rebuild');\n\n  // Create trace writer\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Read spec files early (before backend resolution)\n  const specFiles = await readSpecFiles(absolutePath);\n\n  // Partition specs into rebuild units\n  const units = partitionSpec(specFiles);\n\n  // -------------------------------------------------------------------------\n  // Dry-run mode: show rebuild plan without making AI calls\n  // -------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Spec files:        ${pc.cyan(String(specFiles.length))}`);\n    console.log(`  Rebuild units:     ${pc.cyan(String(units.length))}`);\n    console.log(`  Output directory:  ${pc.cyan(outputDir)}`);\n    console.log('');\n    console.log(pc.dim('Rebuild units:'));\n    for (const unit of units) {\n      console.log(pc.dim(`  ${unit.order}. ${unit.name}`));\n    }\n\n    // Check for existing checkpoint\n    const unitNames = units.map((u) => u.name);\n    const { manager: checkpoint, isResume } = await CheckpointManager.load(\n      outputDir,\n      specFiles,\n      unitNames,\n    );\n    if (isResume) {\n      const pending = checkpoint.getPendingUnits();\n      console.log('');\n      console.log(pc.yellow(`Checkpoint found: ${units.length - pending.length} of ${units.length} modules already complete.`));\n      console.log(pc.yellow(`Would resume with ${pending.length} remaining modules.`));\n    }\n\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // -------------------------------------------------------------------------\n  // Handle --force warning\n  // -------------------------------------------------------------------------\n\n  if (options.force) {\n    console.log(pc.yellow(`Forcing fresh rebuild: output directory will be wiped (${outputDir})`));\n  }\n\n  // -------------------------------------------------------------------------\n  // Resolve backend and create AI service\n  // -------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Resolve effective model: CLI flag > config override > opus default\n  // Rebuild benefits from the best model; upgrade default sonnet to opus\n  const effectiveModel = options.model\n    ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model);\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n  }\n\n  // Create AI service with extended timeout (rebuild modules are large)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min minimum\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs -> ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Rebuild (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`Output: ${outputDir}`);\n  progressLog.write(`Rebuild units: ${units.length}`);\n  progressLog.write('');\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // -------------------------------------------------------------------------\n  // Execute rebuild\n  // -------------------------------------------------------------------------\n\n  console.log(pc.bold(`Rebuilding project from ${specFiles.length} spec file(s)...`));\n  console.log(pc.dim(`Output directory: ${outputDir}`));\n  console.log(pc.dim(`Rebuild units: ${units.length}`));\n  console.log('');\n\n  const result = await executeRebuild(aiService, absolutePath, {\n    outputDir,\n    concurrency,\n    failFast: options.failFast,\n    force: options.force,\n    debug: options.debug,\n    tracer,\n    progressLog,\n  });\n\n  // -------------------------------------------------------------------------\n  // Finalize telemetry, trace, and progress log\n  // -------------------------------------------------------------------------\n\n  await aiService.finalize(absolutePath);\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // -------------------------------------------------------------------------\n  // Print summary and set exit code\n  // -------------------------------------------------------------------------\n\n  console.log('');\n  console.log(pc.bold('Rebuild complete:'));\n  console.log(`  Modules processed: ${pc.green(String(result.modulesProcessed))}`);\n  if (result.modulesSkipped > 0) {\n    console.log(`  Modules skipped:   ${pc.yellow(String(result.modulesSkipped))} (already complete)`);\n  }\n  if (result.modulesFailed > 0) {\n    console.log(`  Modules failed:    ${pc.red(String(result.modulesFailed))}`);\n  }\n  console.log(`  Output directory:  ${pc.cyan(outputDir)}`);\n\n  if (result.modulesProcessed === 0 && result.modulesFailed > 0) {\n    process.exit(2);\n  } else if (result.modulesFailed > 0) {\n    process.exit(1);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**rebuildCommand reconstructs a project from specification files via AI-driven code generation with checkpoint-based session continuity, partitioning specs into ordered rebuild units, resolving AI backend, and orchestrating parallel module generation with progress tracking.**\n\n## Exported Interface\n\n### rebuildCommand\n```typescript\nasync function rebuildCommand(\n  targetPath: string,\n  options: RebuildOptions,\n): Promise<void>\n```\nEntry point for `are rebuild` CLI command. Reads spec files from `specs/` directory in `targetPath`, partitions them via `partitionSpec()`, resolves AI backend through `createBackendRegistry()` and `resolveBackend()`, creates `AIService` with extended timeout (15min minimum), executes `executeRebuild()` orchestrator, finalizes telemetry/trace/progress log, and exits with status code (0=success, 1=partial failure, 2=total failure).\n\n### RebuildOptions\n```typescript\ninterface RebuildOptions {\n  output?: string;      // Custom output directory (default: rebuild/)\n  force?: boolean;      // Wipe output directory and start fresh\n  dryRun?: boolean;     // Show plan without executing\n  concurrency?: number; // Override worker pool size\n  failFast?: boolean;   // Stop on first failure\n  debug?: boolean;      // Verbose subprocess logging\n  trace?: boolean;      // Enable NDJSON tracing\n  model?: string;       // Override AI model (defaults to \"opus\" for rebuild)\n}\n```\n\n## Workflow Phases\n\n### Dry-Run Mode\nWhen `options.dryRun` is true, displays rebuild plan without AI calls:\n- Logs spec file count, rebuild unit count, and output directory path\n- Iterates over `units` array printing `unit.order` and `unit.name`\n- Calls `CheckpointManager.load()` to detect existing checkpoint state\n- If checkpoint exists, reports completed vs. pending module counts via `checkpoint.getPendingUnits()`\n- Returns early without backend resolution or AI service initialization\n\n### Backend Resolution\n- Calls `createBackendRegistry()` to enumerate available AI CLIs (Claude, Gemini, OpenCode)\n- Calls `resolveBackend(registry, config.ai.backend)` with config backend preference (`'auto'` | `'claude'` | `'gemini'` | `'opencode'`)\n- Catches `AIServiceError` with `code: 'CLI_NOT_FOUND'`, prints `getInstallInstructions(registry)`, exits with code 2\n- Resolves effective model via priority: CLI `--model` flag > config override > opus default (upgrades sonnet to opus for rebuild)\n\n### AIService Configuration\nCreates `AIService` instance with:\n- `timeoutMs: Math.max(config.ai.timeoutMs, 900_000)` — enforces 15-minute minimum for large rebuild modules\n- `maxRetries: config.ai.maxRetries` — exponential backoff retry count\n- `model: effectiveModel` — resolved from CLI flag or config or \"opus\" default\n- `telemetry.keepRuns: config.ai.telemetry.keepRuns` — run log retention limit\n\nEnables optional features:\n- `aiService.setDebug(true)` when `options.debug` is true\n- `aiService.setSubprocessLogDir(logDir)` when `options.trace` is true, creates timestamped directory at `.agents-reverse-engineer/subprocess-logs/<ISO-timestamp>/`\n\n### Progress Tracking\n- Calls `ProgressLog.create(absolutePath)` to initialize `.agents-reverse-engineer/progress.log` writer\n- Logs rebuild header with ISO 8601 timestamp, project path, output directory, and unit count\n- Writes progress during `executeRebuild()` via injected `progressLog` parameter\n- Calls `progressLog.finalize()` after orchestrator completes\n\n### Rebuild Execution\nCalls `executeRebuild(aiService, absolutePath, options)` with:\n- `outputDir: options.output ?? path.join(absolutePath, 'rebuild')` — target directory for generated source files\n- `concurrency: options.concurrency ?? config.ai.concurrency` — worker pool size\n- `failFast: options.failFast` — abort-on-first-failure flag\n- `force: options.force` — wipe-and-restart flag\n- `debug: options.debug` — subprocess verbose logging flag\n- `tracer` — NDJSON trace writer instance from `createTraceWriter(absolutePath, options.trace)`\n- `progressLog` — stream writer for tail-monitored progress output\n\nReturns `RebuildResult` with `modulesProcessed`, `modulesSkipped`, `modulesFailed` counts.\n\n### Finalization\nSequential cleanup:\n1. `aiService.finalize(absolutePath)` — writes run log to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs()`\n2. `progressLog.finalize()` — closes progress log file stream\n3. `tracer.finalize()` — closes NDJSON trace file stream\n4. `cleanupOldTraces(absolutePath)` if `options.trace` is true — enforces 500-trace retention limit\n\n### Exit Code Strategy\n- `process.exit(2)` if `modulesProcessed === 0 && modulesFailed > 0` (total failure, no progress)\n- `process.exit(1)` if `modulesFailed > 0` (partial failure, some modules succeeded)\n- Implicit exit code 0 if `modulesFailed === 0` (success)\n\n## Dependencies\n\n### External Imports\n- `picocolors` (aliased as `pc`) — terminal color formatting for dry-run output, summaries, and error messages\n- `node:path` — path resolution for `absolutePath`, `outputDir`, and subprocess log directory construction\n\n### Internal Imports\n- `src/config/loader.js` — `loadConfig()` for YAML config parsing with Zod validation\n- `src/ai/index.js` — `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` for backend abstraction\n- `src/orchestration/index.js` — `ProgressLog`, `createTraceWriter`, `cleanupOldTraces` for telemetry infrastructure\n- `src/rebuild/index.js` — `readSpecFiles`, `partitionSpec`, `CheckpointManager`, `executeRebuild` for rebuild orchestration\n\n## Design Patterns\n\nUses **command pattern** with options object for CLI flag injection. Applies **early return pattern** for dry-run mode to avoid unnecessary backend resolution and AIService instantiation. Implements **defensive defaults** by upgrading sonnet model to opus for rebuild quality (rebuild benefits from best model unlike incremental updates).\n\n## Error Handling\n\nCatches `AIServiceError` with `code: 'CLI_NOT_FOUND'` during backend resolution, prints human-readable installation instructions via `getInstallInstructions(registry)`, exits with code 2 to distinguish \"missing dependency\" from \"task failure\". Rethrows all other errors to propagate unexpected failures to CLI error handler.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1699,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11525,
      "latencyMs": 46239,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/rebuild.ts",
          "sizeBytes": 8283
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:17.962Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/update.ts\n\n```typescript\n/**\n * CLI update command\n *\n * Updates documentation incrementally based on git changes since last run.\n * Uses the AI service for real file analysis (not placeholders) and processes\n * changed files concurrently via the CommandRunner orchestration engine.\n * Regenerates AGENTS.md for affected directories after analysis.\n */\nimport * as path from 'node:path';\nimport { readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport {\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from '../update/index.js';\nimport { writeAgentsMd, GENERATED_MARKER } from '../generation/writers/agents-md.js';\nimport { buildDirectoryPrompt } from '../generation/prompts/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the update command.\n */\nexport interface UpdateCommandOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  uncommitted?: boolean;\n  /** Dry run - show plan without making changes */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n  /** Override AI model (e.g., \"sonnet\", \"opus\") */\n  model?: string;\n}\n\n/**\n * Format cleanup results for display.\n */\nfunction formatCleanup(plan: UpdatePlan): string[] {\n  const lines: string[] = [];\n\n  if (plan.cleanup.deletedSumFiles.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted .sum files):'));\n    for (const file of plan.cleanup.deletedSumFiles) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  if (plan.cleanup.deletedAgentsMd.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted AGENTS.md from empty dirs):'));\n    for (const file of plan.cleanup.deletedAgentsMd) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  return lines;\n}\n\n/**\n * Format the update plan for display.\n */\nfunction formatPlan(plan: UpdatePlan): string {\n  const lines: string[] = [];\n\n  // Header\n  lines.push('');\n  lines.push(pc.bold('=== Update Plan ==='));\n  lines.push('');\n\n  // Baseline info\n  if (plan.isFirstRun) {\n    lines.push(pc.yellow('First run detected. Use \"are generate\" for initial documentation.'));\n    lines.push('');\n  } else {\n    lines.push(`Current commit: ${pc.dim(plan.currentCommit.slice(0, 7))}`);\n    lines.push('');\n  }\n\n  // Summary\n  const analyzeCount = plan.filesToAnalyze.length;\n  const skipCount = plan.filesToSkip.length;\n  const cleanupCount = plan.cleanup.deletedSumFiles.length + plan.cleanup.deletedAgentsMd.length;\n\n  if (analyzeCount === 0 && skipCount === 0 && cleanupCount === 0) {\n    lines.push(pc.green('No changes detected since last run.'));\n    lines.push('');\n    return lines.join('\\n');\n  }\n\n  lines.push(`Files to analyze: ${pc.cyan(String(analyzeCount))}`);\n  lines.push(`Files unchanged: ${pc.dim(String(skipCount))}`);\n  if (cleanupCount > 0) {\n    lines.push(`Cleanup actions: ${pc.yellow(String(cleanupCount))}`);\n  }\n  lines.push('');\n\n  // File list with status markers\n  if (plan.filesToAnalyze.length > 0) {\n    lines.push(pc.cyan('Files to analyze:'));\n    for (const file of plan.filesToAnalyze) {\n      const status = file.status === 'added' ? pc.green('+') :\n                    file.status === 'renamed' ? pc.blue('R') :\n                    pc.yellow('M');\n      lines.push(`  ${status} ${file.path}`);\n      if (file.status === 'renamed' && file.oldPath) {\n        lines.push(`    ${pc.dim(`(was: ${file.oldPath})`)}`);\n      }\n    }\n    lines.push('');\n  }\n\n  if (plan.filesToSkip.length > 0) {\n    lines.push(pc.dim('Files unchanged (skipped):'));\n    for (const file of plan.filesToSkip) {\n      lines.push(`  ${pc.dim('=')} ${pc.dim(file)}`);\n    }\n    lines.push('');\n  }\n\n  // Cleanup\n  lines.push(...formatCleanup(plan));\n\n  // Affected directories\n  if (plan.affectedDirs.length > 0) {\n    lines.push('');\n    lines.push(pc.cyan('Directories for AGENTS.md regeneration:'));\n    for (const dir of plan.affectedDirs) {\n      lines.push(`  ${dir}`);\n    }\n  }\n\n  lines.push('');\n  return lines.join('\\n');\n}\n\n/**\n * Update command - incrementally updates documentation based on git changes.\n *\n * This command:\n * 1. Checks git repository status\n * 2. Detects files changed since last run (via content-hash comparison)\n * 3. Cleans up orphaned .sum files\n * 4. Resolves an AI CLI backend and creates the AI service\n * 5. Analyzes changed files concurrently via CommandRunner\n * 6. Regenerates AGENTS.md for affected directories\n * 7. Writes telemetry run log and prints run summary\n *\n * Exit codes: 0 = all success, 1 = partial failure, 2 = total failure / no CLI\n */\nexport async function updateCommand(\n  targetPath: string,\n  options: UpdateCommandOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Checking for updates in: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and orchestrator)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create orchestrator\n  const orchestrator = createUpdateOrchestrator(config, absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  try {\n    // Prepare update plan\n    const plan = await orchestrator.preparePlan({\n      includeUncommitted: options.uncommitted,\n      dryRun: options.dryRun,\n    });\n\n    // Display plan\n    console.log(formatPlan(plan));\n\n    // Handle first run\n    if (plan.isFirstRun) {\n      console.log(pc.yellow('Hint: Run \"are generate\" first to create initial documentation.'));\n      console.log(pc.yellow('Then run \"are update\" after making changes.'));\n      return;\n    }\n\n    // Handle no changes\n    if (plan.filesToAnalyze.length === 0 &&\n        plan.cleanup.deletedSumFiles.length === 0 &&\n        plan.cleanup.deletedAgentsMd.length === 0) {\n      console.log(pc.green('All files are up to date.'));\n      return;\n    }\n\n    if (options.dryRun) {\n      logger.info('Dry run complete. No files written.');\n      return;\n    }\n\n    // -------------------------------------------------------------------------\n    // Backend resolution (same pattern as generate command)\n    // -------------------------------------------------------------------------\n\n    const registry = createBackendRegistry();\n    let backend;\n    try {\n      backend = await resolveBackend(registry, config.ai.backend);\n    } catch (error) {\n      if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n        console.error(pc.red('Error: No AI CLI found.\\n'));\n        console.error(getInstallInstructions(registry));\n        process.exit(2);\n      }\n      throw error;\n    }\n\n    // Resolve effective model (CLI flag > config)\n    const effectiveModel = options.model ?? config.ai.model;\n\n    // Debug: log backend info\n    if (options.debug) {\n      console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n      console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n      console.log(pc.dim(`[debug] Model: ${effectiveModel}`));\n    }\n\n    // -------------------------------------------------------------------------\n    // AI service setup\n    // -------------------------------------------------------------------------\n\n    const aiService = new AIService(backend, {\n      timeoutMs: config.ai.timeoutMs,\n      maxRetries: config.ai.maxRetries,\n      model: effectiveModel,\n      telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n    });\n\n    if (options.debug) {\n      aiService.setDebug(true);\n    }\n\n    // Determine concurrency\n    const concurrency = options.concurrency ?? config.ai.concurrency;\n\n    // Enable subprocess output logging alongside tracing\n    if (options.trace) {\n      const logDir = path.join(\n        absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n        new Date().toISOString().replace(/[:.]/g, '-'),\n      );\n      aiService.setSubprocessLogDir(logDir);\n      console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n    }\n\n    // Create progress log for tail -f monitoring\n    const progressLog = ProgressLog.create(absolutePath);\n    progressLog.write(`=== ARE Update (${new Date().toISOString()}) ===`);\n    progressLog.write(`Project: ${absolutePath}`);\n    progressLog.write(`Files to analyze: ${plan.filesToAnalyze.length} | Directories: ${plan.affectedDirs.length}`);\n    progressLog.write('');\n\n    // Create command runner\n    const runner = new CommandRunner(aiService, {\n      concurrency,\n      failFast: options.failFast,\n      debug: options.debug,\n      tracer,\n      progressLog,\n    });\n\n    // -------------------------------------------------------------------------\n    // Phase 1: File analysis via CommandRunner (concurrent AI calls)\n    // -------------------------------------------------------------------------\n\n    const summary = await runner.executeUpdate(\n      plan.filesToAnalyze,\n      absolutePath,\n      config,\n    );\n\n    // -------------------------------------------------------------------------\n    // Phase 2: AGENTS.md regeneration for affected directories\n    // -------------------------------------------------------------------------\n\n    if (plan.affectedDirs.length > 0) {\n      const knownDirs = new Set(plan.affectedDirs);\n      const phase2Start = Date.now();\n      let dirsCompleted = 0;\n      let dirsFailed = 0;\n\n      // Emit phase start\n      tracer.emit({\n        type: 'phase:start',\n        phase: 'update-phase-dir-regen',\n        taskCount: plan.affectedDirs.length,\n        concurrency: 1,\n      });\n\n      const dirReporter = new ProgressReporter(0, plan.affectedDirs.length, progressLog);\n      for (const dir of plan.affectedDirs) {\n        const taskStart = Date.now();\n        const taskLabel = dir || '.';\n\n        // Emit task:start\n        tracer.emit({\n          type: 'task:start',\n          taskLabel,\n          phase: 'update-phase-dir-regen',\n        });\n\n        const dirPath = dir === '.' ? absolutePath : path.join(absolutePath, dir);\n        dirReporter.onDirectoryStart(dir || '.');\n        try {\n          // Read existing generated AGENTS.md for incremental update context\n          let existingAgentsMd: string | undefined;\n          try {\n            const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n            if (agentsContent.includes(GENERATED_MARKER)) {\n              existingAgentsMd = agentsContent;\n            }\n          } catch {\n            // No existing AGENTS.md — will generate from scratch\n          }\n\n          const prompt = await buildDirectoryPrompt(dirPath, absolutePath, options.debug, knownDirs, undefined, existingAgentsMd);\n          const response = await aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n          });\n          await writeAgentsMd(dirPath, absolutePath, response.text);\n          const dirDurationMs = Date.now() - taskStart;\n          dirReporter.onDirectoryDone(\n            dir || '.',\n            dirDurationMs,\n            response.inputTokens,\n            response.outputTokens,\n            response.model,\n            response.cacheReadTokens,\n            response.cacheCreationTokens,\n          );\n          dirsCompleted++;\n\n          // Emit task:done (success)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: true,\n            activeTasks: 0,\n          });\n        } catch (error) {\n          dirsFailed++;\n          const errorMsg = error instanceof Error ? error.message : String(error);\n          console.log(`${pc.dim('[dir]')} ${pc.yellow('WARN')} ${dir || '.'}: ${errorMsg}`);\n\n          // Emit task:done (failure)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted + dirsFailed - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: false,\n            error: errorMsg,\n            activeTasks: 0,\n          });\n        }\n      }\n\n      // Emit phase end\n      tracer.emit({\n        type: 'phase:end',\n        phase: 'update-phase-dir-regen',\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: dirsCompleted,\n        tasksFailed: dirsFailed,\n      });\n    }\n\n    // -------------------------------------------------------------------------\n    // Telemetry finalization\n    // -------------------------------------------------------------------------\n\n    await aiService.finalize(absolutePath);\n\n    // Finalize progress log, trace, and clean up old trace files\n    await progressLog.finalize();\n    await tracer.finalize();\n    if (options.trace) {\n      await cleanupOldTraces(absolutePath);\n    }\n\n    // -------------------------------------------------------------------------\n    // Record run state (no-op in frontmatter mode, kept for API compatibility)\n    // -------------------------------------------------------------------------\n\n    const filesSkipped = plan.filesToSkip.length;\n    await orchestrator.recordRun(\n      plan.currentCommit,\n      summary.filesProcessed,\n      filesSkipped\n    );\n\n    // -------------------------------------------------------------------------\n    // Exit code: 0 = all success, 1 = partial failure, 2 = total failure\n    // -------------------------------------------------------------------------\n\n    if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n      process.exit(2);\n    } else if (summary.filesFailed > 0) {\n      process.exit(1);\n    }\n    // Exit code 0 -- all files succeeded (or no files to process)\n\n  } finally {\n    orchestrator.close();\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**update.ts implements the incremental documentation update CLI command that detects changed files via hash comparison, cleans up orphaned artifacts, analyzes modified files concurrently via CommandRunner, and regenerates AGENTS.md for affected directories.**\n\n## Exported Interface\n\n**updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>** — Main entry point for `are update` CLI command. Loads config, creates UpdateOrchestrator, prepares UpdatePlan, resolves AI backend, analyzes changed files via CommandRunner (Phase 1), regenerates AGENTS.md for affectedDirs (Phase 2), finalizes telemetry/tracing, and exits with status codes: 0=success, 1=partial failure, 2=total failure/no CLI.\n\n**UpdateCommandOptions** — Interface with optional fields: `uncommitted?: boolean` (include staged+working changes), `dryRun?: boolean` (show plan without writes), `concurrency?: number` (worker pool size), `failFast?: boolean` (abort on first failure), `debug?: boolean` (show prompts/backend info), `trace?: boolean` (emit NDJSON to `.agents-reverse-engineer/traces/`), `model?: string` (override AI model).\n\n## Workflow Steps\n\n1. **Plan Preparation**: Calls `orchestrator.preparePlan({ includeUncommitted, dryRun })` returning UpdatePlan with `filesToAnalyze[]`, `filesToSkip[]`, `cleanup.deletedSumFiles[]`, `cleanup.deletedAgentsMd[]`, `affectedDirs[]`, `currentCommit`, `isFirstRun`.\n\n2. **First Run Detection**: If `plan.isFirstRun === true`, prints hint to run `are generate` first and exits without processing.\n\n3. **No Changes**: If `filesToAnalyze.length === 0` and cleanup arrays empty, prints \"All files are up to date\" and exits.\n\n4. **Backend Resolution**: Creates `BackendRegistry`, calls `resolveBackend(registry, config.ai.backend)`. On `CLI_NOT_FOUND` error, prints `getInstallInstructions(registry)` and exits with code 2.\n\n5. **AI Service Setup**: Instantiates `AIService(backend, { timeoutMs, maxRetries, model, telemetry })`. If `options.trace`, calls `aiService.setSubprocessLogDir(logDir)` to write subprocess stdout/stderr to timestamped directory.\n\n6. **Phase 1 (File Analysis)**: Creates `CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog })`, calls `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` returning `summary` with `filesProcessed`, `filesFailed`.\n\n7. **Phase 2 (Directory Regeneration)**: Iterates `plan.affectedDirs[]` sequentially (concurrency=1). For each dir, calls `buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd)` passing existing AGENTS.md content if it contains `GENERATED_MARKER`. Calls `aiService.call({ prompt, systemPrompt })` and `writeAgentsMd(dirPath, absolutePath, response.text)`. Emits trace events: `phase:start`, `task:start`, `task:done`, `phase:end` with `phase='update-phase-dir-regen'`.\n\n8. **Finalization**: Calls `aiService.finalize(absolutePath)` for telemetry, `progressLog.finalize()`, `tracer.finalize()`, `cleanupOldTraces(absolutePath)` if tracing enabled, `orchestrator.recordRun(commit, filesProcessed, filesSkipped)`.\n\n## Plan Display\n\n**formatPlan(plan: UpdatePlan): string** — Formats UpdatePlan into multi-line string with sections: header, baseline commit (first 7 chars), summary counts (analyze/skip/cleanup), file list with status markers (`+`=added, `M`=modified, `R`=renamed), cleanup actions, affected directories. Uses picocolors: `pc.cyan()` for file counts, `pc.green('+')`, `pc.yellow('M')`, `pc.blue('R')` for status, `pc.dim()` for unchanged files.\n\n**formatCleanup(plan: UpdatePlan): string[]** — Returns array of formatted lines for `cleanup.deletedSumFiles[]` (yellow \"Cleanup (deleted .sum files):\" header, red `-` prefix) and `cleanup.deletedAgentsMd[]` (yellow \"Cleanup (deleted AGENTS.md from empty dirs):\" header).\n\n## Exit Code Strategy\n\n- **0**: All files succeeded or no files to process\n- **1**: Partial failure (`summary.filesProcessed > 0 && summary.filesFailed > 0`)\n- **2**: Total failure (`summary.filesProcessed === 0 && summary.filesFailed > 0`) or no AI CLI found\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `createUpdateOrchestrator` + `UpdatePlan` from `../update/index.js`, `writeAgentsMd` + `GENERATED_MARKER` from `../generation/writers/agents-md.js`, `buildDirectoryPrompt` from `../generation/prompts/index.js`, `AIService` + `AIServiceError` + `createBackendRegistry` + `resolveBackend` + `getInstallInstructions` from `../ai/index.js`, `CommandRunner` + `ProgressReporter` + `ProgressLog` + `createTraceWriter` + `cleanupOldTraces` from `../orchestration/index.js`.\n\n## Progress Logging\n\nCreates `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`. Writes header with ISO timestamp, project path, counts (files to analyze, directories). Passes to `CommandRunner` and `ProgressReporter.onDirectoryStart/Done()` for streaming ETA-enabled output. Calls `progressLog.finalize()` before exit.\n\n## Trace Integration\n\nIf `options.trace === true`, creates `TraceWriter` via `createTraceWriter(absolutePath, true)`, prints trace file path to stderr with `pc.dim()`, threads `tracer` through `loadConfig()`, `createUpdateOrchestrator()`, `CommandRunner()`. Phase 2 manually emits events: `phase:start` with `taskCount=plan.affectedDirs.length`, `task:start/done` for each directory, `phase:end` with `durationMs`, `tasksCompleted`, `tasksFailed`.\n\n## Incremental Update Context\n\nReads existing AGENTS.md via `readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8')`. If content includes `GENERATED_MARKER`, passes as `existingAgentsMd` parameter to `buildDirectoryPrompt()` for incremental update context (LLM receives previous output to preserve user annotations). Skips if file missing (catch block sets `existingAgentsMd = undefined`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1688,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13315,
      "latencyMs": 50426,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/update.ts",
          "sizeBytes": 14357
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:48.128Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/types.ts\n\n```typescript\n/**\n * Types for the documentation generation pipeline\n */\n\n/**\n * Result of file analysis (populated by LLM via host)\n */\nexport interface AnalysisResult {\n  /** Generated summary text */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n}\n\n/**\n * Metadata extracted during analysis\n */\nexport interface SummaryMetadata {\n  /** Primary purpose of the file */\n  purpose: string;\n  /** Only security/breaking issues */\n  criticalTodos?: string[];\n  /** Tightly coupled siblings */\n  relatedFiles?: string[];\n}\n\n/**\n * Options for summary generation\n */\nexport interface SummaryOptions {\n  /** Target length for generated summaries */\n  targetLength: 'short' | 'standard' | 'detailed';\n  /** Whether to include code snippets in output */\n  includeCodeSnippets: boolean;\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for the three-phase documentation generation pipeline: `AnalysisResult` for Phase 1 file analysis outputs, `SummaryMetadata` for extracted file metadata, and `SummaryOptions` for summary generation configuration.**\n\n## Exported Types\n\n**`AnalysisResult`** — Return type for Phase 1 file analysis operations populated by LLM subprocess calls via `AIService.call()`. Contains two fields:\n- `summary: string` — Generated markdown summary text written to `.sum` file body\n- `metadata: SummaryMetadata` — Structured metadata written to YAML frontmatter\n\n**`SummaryMetadata`** — Structured metadata extracted during file analysis and serialized as YAML frontmatter in `.sum` files. Fields:\n- `purpose: string` — Single-line file purpose statement (mandatory)\n- `criticalTodos?: string[]` — Optional array of security/breaking issues only (excludes routine TODOs)\n- `relatedFiles?: string[]` — Optional array of tightly coupled sibling file paths\n\n**`SummaryOptions`** — Configuration for summary generation behavior passed to prompt builders. Fields:\n- `targetLength: 'short' | 'standard' | 'detailed'` — Controls summary verbosity level\n- `includeCodeSnippets: boolean` — Toggles inclusion of code examples in generated summaries\n\n## Integration Points\n\nReferenced by:\n- `src/generation/prompts/builder.ts` — Consumes `SummaryOptions` when constructing file analysis prompts\n- `src/generation/executor.ts` — Produces `AnalysisResult` from AI subprocess responses, validates against schema\n- `src/generation/writers/sum.ts` — Serializes `AnalysisResult.metadata` as YAML frontmatter via `yaml.stringify()`, writes `AnalysisResult.summary` as markdown body\n\n## Schema Constraints\n\nNo runtime validation types exported (Zod schemas located elsewhere). `SummaryMetadata.purpose` is mandatory while `criticalTodos` and `relatedFiles` are optional arrays. Empty arrays should be omitted (undefined) rather than serialized as `[]` in YAML frontmatter.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 509,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9422,
      "latencyMs": 20458,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/types.ts",
          "sizeBytes": 797
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:50.827Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/types.ts\n\n```typescript\n/** A single import statement extracted from source. */\nexport interface ImportEntry {\n  /** The raw import specifier as written in source (e.g., '../ai/index.js') */\n  specifier: string;\n  /** Imported symbols (e.g., ['AIService', 'AIResponse']) */\n  symbols: string[];\n  /** Whether this is a type-only import */\n  typeOnly: boolean;\n}\n\n/** All imports from a single source file. */\nexport interface FileImports {\n  /** Relative file path (e.g., 'runner.ts') */\n  fileName: string;\n  /** External imports (from other modules, not same directory) */\n  externalImports: ImportEntry[];\n  /** Internal imports (from same directory) */\n  internalImports: ImportEntry[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for representing import statement metadata extracted from source files, distinguishing between external module imports and internal same-directory imports.**\n\n## Exported Types\n\n### ImportEntry\nRepresents a single import statement extracted from source code.\n\n**Fields:**\n- `specifier: string` — Raw import specifier as written in source (e.g., `'../ai/index.js'`, `'@anthropic-ai/sdk'`)\n- `symbols: string[]` — Array of imported symbol names (e.g., `['AIService', 'AIResponse']` from `import { AIService, AIResponse } from '...'`)\n- `typeOnly: boolean` — True for `import type` statements, false for value imports\n\n### FileImports\nAggregates all imports from a single source file, partitioned by external vs internal origin.\n\n**Fields:**\n- `fileName: string` — Relative file path (e.g., `'runner.ts'`, `'src/ai/service.ts'`)\n- `externalImports: ImportEntry[]` — Imports from other modules (parent directories, npm packages, absolute paths)\n- `internalImports: ImportEntry[]` — Imports from same directory (e.g., `'./types.js'`, `'./helper.js'`)\n\n## Usage Context\n\nThese types support the import analysis system (`src/imports/extractor.ts`) which feeds dependency information into directory-level `AGENTS.md` prompt construction via `extractDirectoryImports()`. The external/internal partition enables documentation to emphasize cross-module coupling while de-emphasizing sibling file imports within the same directory.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 372,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9386,
      "latencyMs": 18153,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/types.ts",
          "sizeBytes": 669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:50.096Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/index.ts\n\n```typescript\nexport { extractImports, extractDirectoryImports, formatImportMap } from './extractor.js';\nexport type { ImportEntry, FileImports } from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting static import analysis functions and types for dependency graph construction.**\n\n## Exported Symbols\n\n- `extractImports` — Function analyzing TypeScript/JavaScript source text to extract import declarations with source module paths\n- `extractDirectoryImports` — Function aggregating imports across all source files in a directory, returning structured import maps\n- `formatImportMap` — Function converting parsed import data structures into human-readable markdown formatted output\n- `ImportEntry` — Type representing single import statement with source module path and imported symbols\n- `FileImports` — Type representing all imports within a single file, mapping file paths to ImportEntry arrays\n\n## Integration Points\n\nRe-exports from `./extractor.js` and `./types.js` provide public API surface for import analysis consumed by:\n- `src/generation/prompts/builder.ts` — Includes import maps in file analysis prompts via `extractDirectoryImports()` calls\n- `src/generation/prompts/templates.ts` — Directory aggregation prompts reference import relationships for understanding module coupling\n\n## Module Organization\n\nFollows barrel pattern: centralizes exports from implementation module (`extractor.js`) and type definitions (`types.js`) for single import point. Consumers import via `import { extractImports } from './imports/index.js'` rather than deep file references.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 296,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26607,
      "latencyMs": 20015,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/index.ts",
          "sizeBytes": 151
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:48.867Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/extractor.ts\n\n```typescript\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { ImportEntry, FileImports } from './types.js';\n\n/**\n * Regex matching TypeScript/JavaScript import statements.\n *\n * Captures:\n * - Group 1: 'type' keyword if present (type-only import)\n * - Group 2: named symbols (between braces)\n * - Group 3: namespace import (* as name)\n * - Group 4: default import (bare identifier)\n * - Group 5: module specifier (the string after 'from')\n */\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm;\n\n/**\n * Extract import statements from source content.\n *\n * Only processes lines starting with 'import' to avoid matching\n * dynamic imports or imports inside comments/strings.\n */\nexport function extractImports(sourceContent: string): ImportEntry[] {\n  const entries: ImportEntry[] = [];\n  let match;\n  IMPORT_REGEX.lastIndex = 0;\n\n  while ((match = IMPORT_REGEX.exec(sourceContent)) !== null) {\n    const typeOnly = !!match[1];\n    const namedSymbols = match[2]; // { Foo, Bar }\n    const namespaceImport = match[3]; // * as name\n    const defaultImport = match[4]; // name\n    const specifier = match[5];\n\n    let symbols: string[] = [];\n    if (namedSymbols) {\n      symbols = namedSymbols\n        .split(',')\n        .map((s) => s.trim().replace(/\\s+as\\s+\\w+/, ''))\n        .filter(Boolean);\n    } else if (namespaceImport) {\n      symbols = [namespaceImport.replace('* as ', '').trim()];\n    } else if (defaultImport) {\n      symbols = [defaultImport];\n    }\n\n    entries.push({ specifier, symbols, typeOnly });\n  }\n\n  return entries;\n}\n\n/**\n * Extract and classify imports for all source files in a directory.\n *\n * Reads only the first 100 lines of each file (imports are at the top)\n * for performance. Classifies imports as internal (same directory via './')\n * or external (everything else).\n *\n * Skips node: built-ins and bare package specifiers (npm packages).\n */\nexport async function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[],\n): Promise<FileImports[]> {\n  const results: FileImports[] = [];\n\n  for (const fileName of fileNames) {\n    const filePath = path.join(dirPath, fileName);\n    try {\n      const content = await readFile(filePath, 'utf-8');\n      // Only read import region (first 100 lines) for performance\n      const importRegion = content.split('\\n').slice(0, 100).join('\\n');\n      const imports = extractImports(importRegion);\n\n      // Filter out bare specifiers (npm packages) and node: builtins\n      const relativeImports = imports.filter(\n        (i) => i.specifier.startsWith('.') || i.specifier.startsWith('..'),\n      );\n\n      const internal = relativeImports.filter((i) =>\n        i.specifier.startsWith('./'),\n      );\n      const external = relativeImports.filter((i) =>\n        i.specifier.startsWith('../'),\n      );\n\n      if (external.length > 0 || internal.length > 0) {\n        results.push({\n          fileName,\n          externalImports: external,\n          internalImports: internal,\n        });\n      }\n    } catch {\n      // Skip unreadable files\n    }\n  }\n\n  return results;\n}\n\n/**\n * Format import data as a structured text block for LLM prompts.\n *\n * Example output:\n * ```\n * runner.ts:\n *   ../ai/index.js → AIService\n *   ../generation/executor.js → ExecutionPlan, ExecutionTask\n *\n * pool.ts:\n *   ./trace.js → ITraceWriter (type)\n * ```\n */\nexport function formatImportMap(fileImports: FileImports[]): string {\n  const sections: string[] = [];\n\n  for (const fi of fileImports) {\n    const lines: string[] = [`${fi.fileName}:`];\n    for (const imp of fi.externalImports) {\n      const typeTag = imp.typeOnly ? ' (type)' : '';\n      lines.push(`  ${imp.specifier} → ${imp.symbols.join(', ')}${typeTag}`);\n    }\n    if (lines.length > 1) {\n      sections.push(lines.join('\\n'));\n    }\n  }\n\n  return sections.join('\\n\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files via regex parsing, producing structured import maps for LLM prompt integration with internal/external classification and type-only detection.**\n\n## Exported Functions\n\n**`extractImports(sourceContent: string): ImportEntry[]`**\nParses source text via `IMPORT_REGEX` to extract import statements, returning array of `ImportEntry` objects with `specifier` (module path), `symbols` (imported identifiers), and `typeOnly` (boolean flag for `import type` syntax). Handles named imports (`{ Foo, Bar }`), namespace imports (`* as name`), and default imports. Strips alias syntax (`as alias`) from named imports via `.replace(/\\s+as\\s+\\w+/, '')` to retain only source identifier.\n\n**`extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>`**\nReads first 100 lines of each file in `fileNames` array (optimization assuming imports at top), calls `extractImports()` on sliced content, filters for relative imports (starts with `.` or `..`), excludes `node:` builtins and npm bare specifiers, classifies as `internalImports` (same-directory via `./`) or `externalImports` (parent-directory via `../`), returns `FileImports[]` with `fileName`, `externalImports`, `internalImports` fields. Silently skips unreadable files via empty catch block.\n\n**`formatImportMap(fileImports: FileImports[]): string`**\nTransforms `FileImports[]` into structured text format for LLM prompts. Output pattern: `fileName:\\n  specifier → symbol1, symbol2 (type)\\n`. Type-only imports append ` (type)` suffix via `imp.typeOnly` check. Only includes files with non-empty `externalImports` arrays. Sections separated by double newlines.\n\n## Import Regex Pattern\n\n**`IMPORT_REGEX`** constant: `/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`\n\nCapture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nMatches only lines starting with `import` (anchored with `^`) to avoid matching dynamic imports, comments, or string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset.\n\n## Classification Strategy\n\nRelative imports categorized by specifier prefix:\n- **Internal**: Starts with `./` (same directory)\n- **External**: Starts with `../` (parent directory)\n- **Excluded**: Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nFiltering via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before classification. Used in Phase 2 directory aggregation prompts via `src/generation/prompts/builder.ts` to show dependency relationships within `.sum` generation context.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Full file content read via `readFile(filePath, 'utf-8')` but only import region substring passed to `extractImports()`.\n\n## Integration Points\n\nConsumed by `src/generation/prompts/builder.ts` in `buildFilePrompt()` and `buildDirectoryPrompt()` to include import context in AI analysis prompts. Output of `formatImportMap()` embedded in prompt templates to inform LLM about cross-file dependencies when generating `.sum` summaries and `AGENTS.md` aggregations.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1003,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10461,
      "latencyMs": 29604,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/extractor.ts",
          "sizeBytes": 3904
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:44.768Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/executor.ts\n\n```typescript\n/**\n * Plan executor for documentation generation\n *\n * Builds execution plans from generation plans:\n * - File tasks as individual analysis jobs\n * - Directory completion tracking\n * - Markdown plan output for dry-run display\n */\n\nimport * as path from 'node:path';\nimport type { GenerationPlan } from './orchestrator.js';\nimport { sumFileExists } from './writers/sum.js';\n\n/**\n * Execution task ready for AI processing.\n */\nexport interface ExecutionTask {\n  /** Unique task ID */\n  id: string;\n  /** Task type */\n  type: 'file' | 'directory' | 'root-doc';\n  /** File or directory path (relative) */\n  path: string;\n  /** Absolute path */\n  absolutePath: string;\n  /** System prompt for AI */\n  systemPrompt: string;\n  /** User prompt for AI */\n  userPrompt: string;\n  /** Dependencies (task IDs that must complete first) */\n  dependencies: string[];\n  /** Output path for generated content */\n  outputPath: string;\n  /** Metadata for tracking */\n  metadata: {\n    directoryFiles?: string[];\n    /** Directory depth (for post-order traversal) */\n    depth?: number;\n    /** Package root path (for supplementary docs) */\n    packageRoot?: string;\n  };\n}\n\n/**\n * Execution plan with dependency graph.\n */\nexport interface ExecutionPlan {\n  /** Project root */\n  projectRoot: string;\n  /** All tasks in execution order */\n  tasks: ExecutionTask[];\n  /** File tasks (can run in parallel) */\n  fileTasks: ExecutionTask[];\n  /** Directory tasks (depend on file tasks) */\n  directoryTasks: ExecutionTask[];\n  /** Root document tasks (depend on directories) */\n  rootTasks: ExecutionTask[];\n  /** Directory to file mapping */\n  directoryFileMap: Record<string, string[]>;\n  /** Compact project directory listing for directory prompt context */\n  projectStructure?: string;\n}\n\n/**\n * Calculate directory depth (number of path segments).\n * Root \".\" has depth 0, \"src\" has depth 1, \"src/cli\" has depth 2, etc.\n */\nfunction getDirectoryDepth(dir: string): number {\n  if (dir === '.') return 0;\n  return dir.split(path.sep).length;\n}\n\n/**\n * Build execution plan from generation plan.\n *\n * Directory tasks are sorted using post-order traversal (deepest directories first)\n * so child AGENTS.md files are generated before their parents.\n */\nexport function buildExecutionPlan(\n  plan: GenerationPlan,\n  projectRoot: string\n): ExecutionPlan {\n  const fileTasks: ExecutionTask[] = [];\n  const directoryTasks: ExecutionTask[] = [];\n  const rootTasks: ExecutionTask[] = [];\n  const directoryFileMap: Record<string, string[]> = {};\n\n  // Track files by directory\n  for (const file of plan.files) {\n    const dir = path.dirname(file.relativePath);\n    if (!directoryFileMap[dir]) {\n      directoryFileMap[dir] = [];\n    }\n    directoryFileMap[dir].push(file.relativePath);\n  }\n\n  // Create file tasks\n  for (const task of plan.tasks) {\n    if (task.type === 'file') {\n      const absolutePath = path.join(projectRoot, task.filePath);\n      fileTasks.push({\n        id: `file:${task.filePath}`,\n        type: 'file',\n        path: task.filePath,\n        absolutePath,\n        systemPrompt: task.systemPrompt!,\n        userPrompt: task.userPrompt!,\n        dependencies: [],\n        outputPath: `${absolutePath}.sum`,\n        metadata: {},\n      });\n    }\n  }\n\n  // Sort file tasks by directory depth (deepest first) for post-order traversal\n  fileTasks.sort((a, b) => {\n    const depthA = getDirectoryDepth(path.dirname(a.path));\n    const depthB = getDirectoryDepth(path.dirname(b.path));\n    return depthB - depthA;\n  });\n\n  // Create directory tasks in post-order (deepest first)\n  // Sort directories by depth descending so children are processed before parents\n  const sortedDirs = Object.entries(directoryFileMap).sort(\n    ([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)\n  );\n\n  for (const [dir, files] of sortedDirs) {\n    const dirAbsPath = path.join(projectRoot, dir);\n    const fileTaskIds = files.map(f => `file:${f}`);\n\n    directoryTasks.push({\n      id: `dir:${dir}`,\n      type: 'directory',\n      path: dir,\n      absolutePath: dirAbsPath,\n      systemPrompt: 'Built at execution time by buildDirectoryPrompt()',\n      userPrompt: `Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.`,\n      dependencies: fileTaskIds,\n      outputPath: path.join(dirAbsPath, 'AGENTS.md'),\n      metadata: {\n        directoryFiles: files,\n        depth: getDirectoryDepth(dir),\n      },\n    });\n  }\n\n  // Create root document tasks\n  const allDirTaskIds = Object.keys(directoryFileMap).map(d => `dir:${d}`);\n\n  rootTasks.push({\n    id: 'root:CLAUDE.md',\n    type: 'root-doc',\n    path: 'CLAUDE.md',\n    absolutePath: path.join(projectRoot, 'CLAUDE.md'),\n    // Prompts are built at runtime by buildRootPrompt() in runner.ts Phase 3.\n    // These placeholders exist only for plan display and dependency tracking.\n    systemPrompt: 'Built at runtime by buildRootPrompt()',\n    userPrompt: 'Root document — prompt populated from AGENTS.md files at runtime.',\n    dependencies: allDirTaskIds,\n    outputPath: path.join(projectRoot, 'CLAUDE.md'),\n    metadata: {},\n  });\n\n  return {\n    projectRoot,\n    tasks: [...fileTasks, ...directoryTasks, ...rootTasks],\n    fileTasks,\n    directoryTasks,\n    rootTasks,\n    directoryFileMap,\n    projectStructure: plan.projectStructure,\n  };\n}\n\n/**\n * Check if all files in a directory have been analyzed (.sum files exist).\n */\nexport async function isDirectoryComplete(\n  dirPath: string,\n  expectedFiles: string[],\n  projectRoot: string\n): Promise<{ complete: boolean; missing: string[] }> {\n  const missing: string[] = [];\n\n  for (const relativePath of expectedFiles) {\n    const absolutePath = path.join(projectRoot, relativePath);\n    const exists = await sumFileExists(absolutePath);\n    if (!exists) {\n      missing.push(relativePath);\n    }\n  }\n\n  return {\n    complete: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Get all directories that are ready for AGENTS.md generation.\n * A directory is ready when all its files have .sum files.\n */\nexport async function getReadyDirectories(\n  executionPlan: ExecutionPlan\n): Promise<string[]> {\n  const ready: string[] = [];\n\n  for (const [dir, files] of Object.entries(executionPlan.directoryFileMap)) {\n    const { complete } = await isDirectoryComplete(\n      dir,\n      files,\n      executionPlan.projectRoot\n    );\n    if (complete) {\n      ready.push(dir);\n    }\n  }\n\n  return ready;\n}\n\n/**\n * Format execution plan as markdown for GENERATION-PLAN.md.\n * Uses post-order traversal (deepest directories first).\n */\nexport function formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string {\n  const lines: string[] = [];\n  const today = new Date().toISOString().split('T')[0];\n\n  // Header\n  lines.push('# Documentation Generation Plan');\n  lines.push('');\n  lines.push(`Generated: ${today}`);\n  lines.push(`Project: ${plan.projectRoot}`);\n  lines.push('');\n\n  // Summary\n  lines.push('## Summary');\n  lines.push('');\n  lines.push(`- **Total Tasks**: ${plan.tasks.length}`);\n  lines.push(`- **File Tasks**: ${plan.fileTasks.length}`);\n  lines.push(`- **Directory Tasks**: ${plan.directoryTasks.length}`);\n  lines.push(`- **Root Tasks**: ${plan.rootTasks.length}`);\n  lines.push('- **Traversal**: Post-order (children before parents)');\n  lines.push('');\n  lines.push('---');\n  lines.push('');\n\n  // Phase 1: File Analysis\n  lines.push('## Phase 1: File Analysis (Post-Order Traversal)');\n  lines.push('');\n\n  // Group files by directory, use directory task order (already post-order)\n  // Deduplicate paths\n  const filesByDir: Record<string, Set<string>> = {};\n  for (const task of plan.fileTasks) {\n    const dir = task.path.includes('/')\n      ? task.path.substring(0, task.path.lastIndexOf('/'))\n      : '.';\n    if (!filesByDir[dir]) filesByDir[dir] = new Set();\n    filesByDir[dir].add(task.path);\n  }\n\n  // Output files grouped by directory in post-order (using directoryTasks order)\n  for (const dirTask of plan.directoryTasks) {\n    const dir = dirTask.path;\n    const filesSet = filesByDir[dir];\n    if (filesSet && filesSet.size > 0) {\n      const files = Array.from(filesSet);\n      const depth = dirTask.metadata.depth ?? 0;\n      lines.push(`### Depth ${depth}: ${dir}/ (${files.length} files)`);\n      for (const file of files) {\n        lines.push(`- [ ] \\`${file}\\``);\n      }\n      lines.push('');\n    }\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 2: Directory AGENTS.md\n  lines.push(`## Phase 2: Directory AGENTS.md (Post-Order Traversal, ${plan.directoryTasks.length} directories)`);\n  lines.push('');\n\n  // Group by depth\n  const dirsByDepth: Record<number, string[]> = {};\n  for (const task of plan.directoryTasks) {\n    const depth = task.metadata.depth ?? 0;\n    if (!dirsByDepth[depth]) dirsByDepth[depth] = [];\n    dirsByDepth[depth].push(task.path);\n  }\n\n  // Output in depth order (descending)\n  const depths = Object.keys(dirsByDepth).map(Number).sort((a, b) => b - a);\n  for (const depth of depths) {\n    lines.push(`### Depth ${depth}`);\n    for (const dir of dirsByDepth[depth]) {\n      const suffix = dir === '.' ? ' (root)' : '';\n      lines.push(`- [ ] \\`${dir}/AGENTS.md\\`${suffix}`);\n    }\n    lines.push('');\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 3: Root Documents\n  lines.push('## Phase 3: Root Documents');\n  lines.push('');\n  lines.push('- [ ] `CLAUDE.md`');\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**executor.ts transforms GenerationPlan into ExecutionPlan with post-order directory traversal, dependency tracking, and directory completion predicates.**\n\n## Exported Types\n\n**ExecutionTask** represents a single AI processing unit with fields:\n- `id: string` — unique identifier (`file:path`, `dir:path`, `root:docname`)\n- `type: 'file' | 'directory' | 'root-doc'` — discriminator for task category\n- `path: string` — relative path to file or directory\n- `absolutePath: string` — resolved absolute path\n- `systemPrompt: string` — AI system message (placeholder for dir/root tasks)\n- `userPrompt: string` — AI user message (placeholder for dir/root tasks)\n- `dependencies: string[]` — array of task IDs that must complete first\n- `outputPath: string` — where to write generated content (`.sum` or `AGENTS.md` or `CLAUDE.md`)\n- `metadata: { directoryFiles?: string[], depth?: number, packageRoot?: string }` — tracking data\n\n**ExecutionPlan** contains dependency graph with fields:\n- `projectRoot: string` — absolute path to project root\n- `tasks: ExecutionTask[]` — all tasks in execution order\n- `fileTasks: ExecutionTask[]` — Phase 1 file analysis tasks (parallel eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 AGENTS.md generation tasks (post-order)\n- `rootTasks: ExecutionTask[]` — Phase 3 root document synthesis tasks (sequential)\n- `directoryFileMap: Record<string, string[]>` — maps directory paths to contained file relative paths\n- `projectStructure?: string` — compact project listing for prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** constructs three-phase task graph:\n1. Populates `directoryFileMap` by grouping files via `path.dirname()`\n2. Creates file tasks with `id: 'file:${filePath}'`, `dependencies: []`, `outputPath: '${absolutePath}.sum'`\n3. Sorts file tasks by directory depth descending via `getDirectoryDepth(path.dirname(path))`\n4. Creates directory tasks sorted by depth descending for post-order traversal, each depending on all file tasks in that directory (dependencies populated from `directoryFileMap`)\n5. Creates root tasks (`id: 'root:CLAUDE.md'`) depending on all directory task IDs\n6. Directory and root task prompts are placeholders (`'Built at runtime by buildRootPrompt()'`) — actual prompts constructed by `runner.ts` Phase 2/3 logic\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** checks readiness by calling `sumFileExists()` for each file in `expectedFiles`, returns `{ complete: true, missing: [] }` if all `.sum` files exist.\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** filters `directoryFileMap` entries by calling `isDirectoryComplete()` for each directory, returns array of directory paths ready for AGENTS.md generation.\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** generates GENERATION-PLAN.md content:\n- Header: `# Documentation Generation Plan` with ISO date and `plan.projectRoot`\n- Summary: counts for `tasks.length`, `fileTasks.length`, `directoryTasks.length`, `rootTasks.length`\n- Phase 1: groups files by directory using `directoryTasks` order (already post-order), outputs `### Depth N: dir/ (M files)` sections with `- [ ] \\`file\\`` checklist items\n- Phase 2: groups directories by `metadata.depth`, outputs `### Depth N` sections with `- [ ] \\`dir/AGENTS.md\\`` checklist items in descending depth order\n- Phase 3: outputs `- [ ] \\`CLAUDE.md\\`` checklist item\n\n## Helper Functions\n\n**getDirectoryDepth(dir: string): number** calculates depth by splitting on `path.sep` and counting segments:\n- `'.'` → `0` (root special case)\n- `'src'` → `1`\n- `'src/cli'` → `2`\n\nUsed for post-order sorting: deeper directories (higher depth) must complete before shallower parents.\n\n## Integration Points\n\nConsumes `GenerationPlan` from `orchestrator.ts` (contains `tasks[]`, `files[]`, `projectStructure` fields).\n\nConsumed by `runner.ts`:\n- `buildExecutionPlan()` called in Phase 1 initialization\n- `isDirectoryComplete()` called in Phase 2 to wait for child `.sum` files\n- `formatExecutionPlanAsMarkdown()` output written to `GENERATION-PLAN.md` via `PlanTracker`\n\nCalls `sumFileExists()` from `writers/sum.ts` to check `.sum` file presence without parsing YAML frontmatter.\n\n## Post-Order Traversal Strategy\n\nDirectory tasks sorted by depth descending (deepest first) ensures child `AGENTS.md` files exist before parent directories attempt aggregation. Sort key: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` (descending). File tasks similarly sorted to process leaf directories first. Root tasks depend on all directory task IDs, enforcing three-phase waterfall: files → directories → root.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1323,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12143,
      "latencyMs": 34463,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/executor.ts",
          "sizeBytes": 9421
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:52.580Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/banner.ts\n\n```typescript\n/**\n * ASCII banner and styled output for the installer\n *\n * Provides colored banner display, help text, and styled message helpers.\n * Uses picocolors for terminal coloring.\n */\n\nimport pc from 'picocolors';\nimport { getVersion } from '../version.js';\n\n/** Package version read from package.json */\nexport const VERSION = getVersion();\n\n/**\n * Display the ASCII banner at installer launch\n *\n * Shows big ASCII art \"ARE\" letters in green with version and tagline.\n */\nexport function displayBanner(): void {\n  const art = pc.green;\n  const dim = pc.dim;\n\n  console.log();\n  console.log(art('  █████╗ ██████╗ ███████╗'));\n  console.log(art(' ██╔══██╗██╔══██╗██╔════╝'));\n  console.log(art(' ███████║██████╔╝█████╗  '));\n  console.log(art(' ██╔══██║██╔══██╗██╔══╝  '));\n  console.log(art(' ██║  ██║██║  ██║███████╗'));\n  console.log(art(' ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝'));\n  console.log();\n  console.log(dim(` agents-reverse-engineer v${VERSION}`));\n  console.log(dim(' AI-friendly codebase documentation'));\n  console.log();\n}\n\n/**\n * Display help text showing usage, flags, and examples\n */\nexport function showHelp(): void {\n  console.log(pc.bold('Usage:') + ' npx agents-reverse-engineer [options]');\n  console.log();\n  console.log(pc.bold('Options:'));\n  console.log('  --runtime <runtime>  Select runtime: claude, opencode, gemini, or all');\n  console.log('  -g, --global         Install to global config (~/.claude, etc.)');\n  console.log('  -l, --local          Install to local project (./.claude, etc.)');\n  console.log('  -u, --uninstall      Remove installed files');\n  console.log('  --force              Overwrite existing files');\n  console.log('  -q, --quiet          Suppress banner and info messages');\n  console.log('  -h, --help           Show this help');\n  console.log();\n  console.log(pc.bold('Examples:'));\n  console.log('  npx agents-reverse-engineer');\n  console.log('    Interactive mode - prompts for runtime and location');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g');\n  console.log('    Install Claude Code commands globally');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime all -l');\n  console.log('    Install commands for all runtimes to local project');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g -u');\n  console.log('    Uninstall global Claude Code commands');\n}\n\n/**\n * Display a success message with green checkmark prefix\n *\n * @param msg - Message to display\n */\nexport function showSuccess(msg: string): void {\n  console.log(pc.green('✓') + ' ' + msg);\n}\n\n/**\n * Display an error message with red X prefix\n *\n * @param msg - Message to display\n */\nexport function showError(msg: string): void {\n  console.log(pc.red('✗') + ' ' + msg);\n}\n\n/**\n * Display a warning message with yellow exclamation prefix\n *\n * @param msg - Message to display\n */\nexport function showWarning(msg: string): void {\n  console.log(pc.yellow('!') + ' ' + msg);\n}\n\n/**\n * Display an info message with cyan arrow prefix\n *\n * @param msg - Message to display\n */\nexport function showInfo(msg: string): void {\n  console.log(pc.cyan('>') + ' ' + msg);\n}\n\n/**\n * Display post-install next steps\n *\n * Shows what to do after installation with helpful links.\n *\n * @param runtime - Which runtime was installed\n * @param filesCreated - Number of files created\n */\nexport function showNextSteps(runtime: string, filesCreated: number): void {\n  console.log();\n  console.log(pc.bold('Installation complete!'));\n  console.log(pc.dim(`${filesCreated} files installed for ${runtime}`));\n  console.log();\n  console.log(pc.bold('Next steps:'));\n  console.log('  1. Run ' + pc.cyan('/are-help') + ' in your AI assistant to verify');\n  console.log('  2. Run ' + pc.cyan('/are-init') + ' to initialize a project');\n  console.log('  3. Run ' + pc.cyan('/are-discover') + ' to create the generation plan');\n  console.log('  4. Run ' + pc.cyan('/are-generate') + ' to generate documentation');\n  console.log('  5. Run ' + pc.cyan('/are-update') + ' to update documentation after changes');\n  console.log('  6. Run ' + pc.cyan('/are-specify') + ' to generate a specification document');\n  console.log('  7. Run ' + pc.cyan('/are-clean') + ' to remove all generated artifacts');\n  console.log();\n  console.log(pc.dim('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer'));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**banner.ts provides ASCII art banner display and styled terminal output functions for the ARE installer using picocolors.**\n\n## Exported Functions\n\n`displayBanner()` renders green ASCII art spelling \"ARE\" in block letters, followed by dimmed version string (`v${VERSION}`) and tagline \"AI-friendly codebase documentation\".\n\n`showHelp()` prints usage syntax, option flags (`--runtime`, `-g/--global`, `-l/--local`, `-u/--uninstall`, `--force`, `-q/--quiet`, `-h/--help`), and example invocations for interactive mode, global/local installation, and uninstallation.\n\n`showSuccess(msg: string)` displays message prefixed with green `✓` checkmark via `pc.green('✓') + ' ' + msg`.\n\n`showError(msg: string)` displays message prefixed with red `✗` via `pc.red('✗') + ' ' + msg`.\n\n`showWarning(msg: string)` displays message prefixed with yellow `!` via `pc.yellow('!') + ' ' + msg`.\n\n`showInfo(msg: string)` displays message prefixed with cyan `>` via `pc.cyan('>') + ' ' + msg`.\n\n`showNextSteps(runtime: string, filesCreated: number)` prints installation completion summary with file count, then numbered list of next steps: `/are-help` to verify, `/are-init` to initialize, `/are-discover` for plan, `/are-generate` for docs, `/are-update` for changes, `/are-specify` for spec, `/are-clean` for cleanup, followed by GitHub docs URL.\n\n## Exported Constants\n\n`VERSION` string constant imported from `getVersion()` in `../version.js`, used in banner output.\n\n## Dependencies\n\n`picocolors` (`pc`) provides terminal color functions: `pc.green()`, `pc.red()`, `pc.yellow()`, `pc.cyan()`, `pc.dim()`, `pc.bold()`.\n\n`getVersion()` from `../version.js` reads package version at module load time.\n\n## Terminal Output Patterns\n\nAll output functions write directly to `console.log()` with no return values. Banner includes 7 lines of ASCII art using box-drawing characters (`█`, `╗`, `╔`, `═`, `║`) followed by 3 lines of metadata. Help text uses bold headings (`pc.bold('Usage:')`, `pc.bold('Options:')`) with cyan-highlighted command examples. Next steps uses bold heading, dim file count, bold subheading, then numbered list with cyan-highlighted slash commands and dim GitHub URL footer.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 611,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10606,
      "latencyMs": 27617,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/banner.ts",
          "sizeBytes": 4637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:02.873Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/detect.ts\n\n```typescript\n/**\n * Environment detection for AI coding assistants\n *\n * Detects which AI coding assistant environments are present in a project\n * by checking for their configuration directories and files.\n */\n\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { DetectedEnvironment, EnvironmentType } from './types.js';\n\n/**\n * Detect all AI coding assistant environments present in a project\n *\n * Checks for:\n * - Claude Code: .claude/ directory OR CLAUDE.md file\n * - OpenCode: .opencode/ directory\n * - Aider: .aider.conf.yml file OR .aider/ directory\n *\n * @param projectRoot - Root directory to check for environments\n * @returns Array of detected environments (may include multiple)\n */\nexport function detectEnvironments(projectRoot: string): DetectedEnvironment[] {\n  const environments: DetectedEnvironment[] = [];\n\n  // Check for Claude Code\n  const claudeDir = path.join(projectRoot, '.claude');\n  const claudeMd = path.join(projectRoot, 'CLAUDE.md');\n  if (existsSync(claudeDir) || existsSync(claudeMd)) {\n    environments.push({\n      type: 'claude',\n      configDir: '.claude',\n      detected: true,\n    });\n  }\n\n  // Check for OpenCode\n  const openCodeDir = path.join(projectRoot, '.opencode');\n  if (existsSync(openCodeDir)) {\n    environments.push({\n      type: 'opencode',\n      configDir: '.opencode',\n      detected: true,\n    });\n  }\n\n  // Check for Aider\n  const aiderConfig = path.join(projectRoot, '.aider.conf.yml');\n  const aiderDir = path.join(projectRoot, '.aider');\n  if (existsSync(aiderConfig) || existsSync(aiderDir)) {\n    environments.push({\n      type: 'aider',\n      configDir: '.aider',\n      detected: true,\n    });\n  }\n\n  return environments;\n}\n\n/**\n * Check if a specific AI coding assistant environment is present\n *\n * @param projectRoot - Root directory to check\n * @param type - Environment type to check for\n * @returns true if the environment is detected\n */\nexport function hasEnvironment(\n  projectRoot: string,\n  type: EnvironmentType\n): boolean {\n  const environments = detectEnvironments(projectRoot);\n  return environments.some((env) => env.type === type);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detectEnvironments() discovers AI coding assistant installations by checking for Claude Code (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), and Aider (`.aider.conf.yml` or `.aider/`) configuration artifacts.**\n\n## Exported Functions\n\n`detectEnvironments(projectRoot: string): DetectedEnvironment[]` returns array of detected AI assistant environments by testing existence of configuration directories and files via `existsSync()`. Checks Claude Code markers (`.claude` directory OR `CLAUDE.md` file), OpenCode marker (`.opencode` directory), and Aider markers (`.aider.conf.yml` file OR `.aider` directory). Each detected environment produces `DetectedEnvironment` object with `type`, `configDir`, and `detected: true` properties.\n\n`hasEnvironment(projectRoot: string, type: EnvironmentType): boolean` returns true if specified environment type exists in project root. Delegates to `detectEnvironments()` and tests result array via `some()` predicate matching `env.type === type`.\n\n## Detection Logic\n\nClaude detection uses dual-path OR logic: `.claude/` directory existence OR `CLAUDE.md` file existence both trigger detection with `configDir: '.claude'` in result. OpenCode requires `.opencode/` directory existence. Aider uses dual-path OR logic: `.aider.conf.yml` file existence OR `.aider/` directory existence both trigger detection with `configDir: '.aider'` in result.\n\n## Path Resolution\n\nAll checks use `path.join(projectRoot, relativePath)` for absolute path construction passed to `existsSync()`. No recursive traversal or parent directory scanning—only direct children of `projectRoot` tested.\n\n## Type Dependencies\n\nImports `DetectedEnvironment` interface and `EnvironmentType` union type from `./types.js`. `EnvironmentType` expected to constrain values `'claude' | 'opencode' | 'aider'` based on detection logic branches.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 457,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9831,
      "latencyMs": 20902,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/detect.ts",
          "sizeBytes": 2136
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:55.441Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/paths.ts\n\n```typescript\n/**\n * Path resolution for installer module\n *\n * Provides cross-platform path resolution for AI coding assistant runtimes.\n * Uses os.homedir() for global paths and path.join() for cross-platform compatibility.\n */\n\nimport * as os from 'node:os';\nimport * as path from 'node:path';\nimport { stat } from 'node:fs/promises';\nimport type { Runtime, Location, RuntimePaths } from './types.js';\n\n/**\n * Get all supported runtime identifiers (excludes 'all' meta-runtime)\n *\n * @returns Array of concrete runtime identifiers\n */\nexport function getAllRuntimes(): Array<Exclude<Runtime, 'all'>> {\n  return ['claude', 'opencode', 'gemini'];\n}\n\n/**\n * Get path configuration for a specific runtime\n *\n * Returns global and local installation paths plus settings file location.\n * All paths are cross-platform using os.homedir() and path.join().\n *\n * Environment variable overrides (in priority order):\n * - Claude: CLAUDE_CONFIG_DIR\n * - OpenCode: OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode\n * - Gemini: GEMINI_CONFIG_DIR\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Path configuration object with global, local, and settingsFile paths\n */\nexport function getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths {\n  const home = os.homedir();\n\n  switch (runtime) {\n    case 'claude': {\n      // CLAUDE_CONFIG_DIR overrides default ~/.claude\n      const globalPath = process.env.CLAUDE_CONFIG_DIR || path.join(home, '.claude');\n      return {\n        global: globalPath,\n        local: '.claude',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'opencode': {\n      // OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode > ~/.config/opencode\n      const xdgConfig = process.env.XDG_CONFIG_HOME || path.join(home, '.config');\n      const globalPath = process.env.OPENCODE_CONFIG_DIR || path.join(xdgConfig, 'opencode');\n      return {\n        global: globalPath,\n        local: '.opencode',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'gemini': {\n      // GEMINI_CONFIG_DIR overrides default ~/.gemini\n      const globalPath = process.env.GEMINI_CONFIG_DIR || path.join(home, '.gemini');\n      return {\n        global: globalPath,\n        local: '.gemini',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n  }\n}\n\n/**\n * Resolve full installation path for a runtime and location\n *\n * For global location, returns the absolute global path.\n * For local location, returns the local path joined with project root.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param projectRoot - Project root directory for local installs (defaults to cwd)\n * @returns Resolved absolute path for installation\n */\nexport function resolveInstallPath(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  projectRoot?: string,\n): string {\n  const paths = getRuntimePaths(runtime);\n\n  if (location === 'global') {\n    return paths.global;\n  }\n\n  // Local installation - join with project root or cwd\n  const root = projectRoot || process.cwd();\n  return path.join(root, paths.local);\n}\n\n/**\n * Get the settings file path for a runtime\n *\n * Settings files are used for hook registration (Claude Code uses settings.json).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Absolute path to the settings file\n */\nexport function getSettingsPath(runtime: Exclude<Runtime, 'all'>): string {\n  return getRuntimePaths(runtime).settingsFile;\n}\n\n/**\n * Check if a runtime is installed locally in a project.\n *\n * Checks for the presence of the local config directory (e.g., .claude, .opencode, .gemini).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param projectRoot - Project root directory to check\n * @returns True if the runtime's local config directory exists\n */\nexport async function isRuntimeInstalledLocally(\n  runtime: Exclude<Runtime, 'all'>,\n  projectRoot: string\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n  const localPath = path.join(projectRoot, paths.local);\n\n  try {\n    const stats = await stat(localPath);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check if a runtime is installed globally.\n *\n * Checks for the presence of the global config directory.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns True if the runtime's global config directory exists\n */\nexport async function isRuntimeInstalledGlobally(\n  runtime: Exclude<Runtime, 'all'>\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n\n  try {\n    const stats = await stat(paths.global);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get list of runtimes installed locally in a project.\n *\n * @param projectRoot - Project root directory to check\n * @returns Array of runtime identifiers that are installed locally\n */\nexport async function getInstalledRuntimes(\n  projectRoot: string\n): Promise<Array<Exclude<Runtime, 'all'>>> {\n  const runtimes = getAllRuntimes();\n  const installed: Array<Exclude<Runtime, 'all'>> = [];\n\n  for (const runtime of runtimes) {\n    if (await isRuntimeInstalledLocally(runtime, projectRoot)) {\n      installed.push(runtime);\n    }\n  }\n\n  return installed;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**paths.ts resolves cross-platform directory paths for AI runtime installations (Claude, OpenCode, Gemini) with environment variable overrides and local/global installation detection.**\n\n## Exported Functions\n\n`getAllRuntimes()` returns `Array<Exclude<Runtime, 'all'>>` containing `['claude', 'opencode', 'gemini']` as the set of concrete runtime identifiers (excludes the meta-runtime `'all'`).\n\n`getRuntimePaths(runtime: Exclude<Runtime, 'all'>)` returns `RuntimePaths` with `global`, `local`, and `settingsFile` string paths for the specified runtime. Uses `os.homedir()` for absolute paths and `path.join()` for cross-platform compatibility. Applies environment variable overrides: `CLAUDE_CONFIG_DIR` for Claude (default `~/.claude`), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME` for OpenCode (default `~/.config/opencode`), `GEMINI_CONFIG_DIR` for Gemini (default `~/.gemini`). Returns `local` paths as relative strings: `.claude`, `.opencode`, `.gemini`.\n\n`resolveInstallPath(runtime: Exclude<Runtime, 'all'>, location: Location, projectRoot?: string)` returns absolute installation path string. For `location === 'global'`, returns `paths.global` from `getRuntimePaths()`. For local installations, joins `paths.local` with `projectRoot` (defaults to `process.cwd()` if not provided).\n\n`getSettingsPath(runtime: Exclude<Runtime, 'all'>)` returns absolute path string to runtime settings file (`settings.json`) used for hook registration. Delegates to `getRuntimePaths(runtime).settingsFile`.\n\n`isRuntimeInstalledLocally(runtime: Exclude<Runtime, 'all'>, projectRoot: string)` returns `Promise<boolean>` indicating whether the runtime's local config directory exists in the project. Uses `stat()` from `node:fs/promises` to check if path is directory, returns `false` on error.\n\n`isRuntimeInstalledGlobally(runtime: Exclude<Runtime, 'all'>)` returns `Promise<boolean>` indicating whether the runtime's global config directory exists. Uses `stat()` to check `paths.global`, returns `false` on error.\n\n`getInstalledRuntimes(projectRoot: string)` returns `Promise<Array<Exclude<Runtime, 'all'>>>` containing runtime identifiers installed locally in the project. Iterates over `getAllRuntimes()` and filters using `isRuntimeInstalledLocally()`.\n\n## Environment Variable Precedence\n\nClaude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`.\n\nOpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` which overrides `~/.config/opencode`.\n\nGemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`.\n\n## Dependencies\n\nImports `os.homedir()` from `node:os`, `path.join()` from `node:path`, `stat()` from `node:fs/promises`. Imports `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 770,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10794,
      "latencyMs": 29174,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/paths.ts",
          "sizeBytes": 5356
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:08.990Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/types.ts\n\n```typescript\n/**\n * Integration types for AI coding assistant environments\n *\n * Defines types for detecting AI assistant environments (Claude Code, OpenCode, etc.)\n * and generating appropriate integration templates (command files, hooks).\n */\n\n/**\n * Supported AI coding assistant environment types\n */\nexport type EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini';\n\n/**\n * Result of detecting an AI coding assistant environment\n */\nexport interface DetectedEnvironment {\n  /** Type of AI assistant detected */\n  type: EnvironmentType;\n  /** Configuration directory for this environment (e.g., '.claude', '.opencode') */\n  configDir: string;\n  /** Whether this environment was detected in the project */\n  detected: boolean;\n}\n\n/**\n * Template for integration files (command files, hooks, etc.)\n */\nexport interface IntegrationTemplate {\n  /** File name (e.g., 'generate.md') */\n  filename: string;\n  /** Relative path from project root (e.g., '.claude/commands/ar/generate.md') */\n  path: string;\n  /** Template content to write to the file */\n  content: string;\n}\n\n/**\n * Result of generating integration files for an environment\n */\nexport interface IntegrationResult {\n  /** Environment type that was configured */\n  environment: EnvironmentType;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist) */\n  filesSkipped: string[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for AI coding assistant environment detection and integration template generation across platforms (Claude Code, OpenCode, Gemini, Aider).**\n\n## Type Definitions\n\n**`EnvironmentType`** — String literal union type enumerating supported AI assistant platforms: `'claude' | 'opencode' | 'aider' | 'gemini'`.\n\n**`DetectedEnvironment`** — Interface representing environment detection results with fields:\n- `type: EnvironmentType` — Detected AI assistant platform identifier\n- `configDir: string` — Configuration directory path (e.g., `.claude`, `.opencode`)\n- `detected: boolean` — Boolean flag indicating whether environment was found in project\n\n**`IntegrationTemplate`** — Interface representing a single integration file to be generated with fields:\n- `filename: string` — Base filename (e.g., `'generate.md'`)\n- `path: string` — Relative path from project root (e.g., `.claude/commands/ar/generate.md`)\n- `content: string` — Template content to write to filesystem\n\n**`IntegrationResult`** — Interface representing batch integration generation outcome with fields:\n- `environment: EnvironmentType` — Target AI assistant platform\n- `filesCreated: string[]` — Array of successfully written file paths\n- `filesSkipped: string[]` — Array of file paths skipped due to existing files\n\n## Integration Points\n\nConsumed by `src/integration/detect.ts` for environment detection logic and `src/integration/generate.ts` for template generation workflows. Used by `src/installer/operations.ts` to orchestrate IDE-specific command/hook installation. Referenced in `src/cli/init.ts` for initialization flows requiring platform-specific configuration.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 401,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9569,
      "latencyMs": 15902,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/types.ts",
          "sizeBytes": 1400
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:47.209Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/orchestrator.ts\n\n```typescript\n/**\n * Generation orchestrator\n *\n * Coordinates the documentation generation workflow:\n * - Discovers and prepares files for analysis\n * - Creates file analysis tasks with prompts\n * - Creates directory tasks for LLM-generated descriptions\n */\n\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport { buildFilePrompt } from './prompts/index.js';\nimport { analyzeComplexity } from './complexity.js';\nimport type { ComplexityMetrics } from './complexity.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * A file prepared for analysis.\n */\nexport interface PreparedFile {\n  /** Absolute path to the file */\n  filePath: string;\n  /** Relative path from project root */\n  relativePath: string;\n  /** File content */\n  content: string;\n}\n\n/**\n * Analysis task for a file.\n */\nexport interface AnalysisTask {\n  /** Type of task */\n  type: 'file' | 'directory';\n  /** File or directory path */\n  filePath: string;\n  /** System prompt (set for file tasks; directory prompts built at execution time) */\n  systemPrompt?: string;\n  /** User prompt (set for file tasks; directory prompts built at execution time) */\n  userPrompt?: string;\n  /** Directory info for directory tasks */\n  directoryInfo?: {\n    /** Paths of .sum files in this directory */\n    sumFiles: string[];\n    /** Number of files analyzed */\n    fileCount: number;\n  };\n}\n\n/**\n * Result of the generation planning process.\n */\nexport interface GenerationPlan {\n  /** Files to be analyzed */\n  files: PreparedFile[];\n  /** Analysis tasks to execute */\n  tasks: AnalysisTask[];\n  /** Complexity metrics */\n  complexity: ComplexityMetrics;\n  /** Compact project directory listing for bird's-eye context */\n  projectStructure?: string;\n}\n\n/**\n * Orchestrates the documentation generation workflow.\n */\nexport class GenerationOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Prepare files for analysis by reading content and detecting types.\n   */\n  async prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]> {\n    const prepared: PreparedFile[] = [];\n\n    for (let i = 0; i < discoveryResult.files.length; i++) {\n      const filePath = discoveryResult.files[i];\n      try {\n        const content = await readFile(filePath, 'utf-8');\n        const relativePath = path.relative(this.projectRoot, filePath);\n\n        prepared.push({\n          filePath,\n          relativePath,\n          content,\n        });\n      } catch {\n        // Skip files that can't be read (permission errors, etc.)\n        // Silently ignore - these files won't appear in the plan\n      }\n    }\n\n    return prepared;\n  }\n\n  /**\n   * Build a compact project structure listing from prepared files.\n   * Groups files by directory to give the AI bird's-eye context.\n   */\n  private buildProjectStructure(files: PreparedFile[]): string {\n    const byDir = new Map<string, string[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath) || '.';\n      const group = byDir.get(dir) ?? [];\n      group.push(path.basename(file.relativePath));\n      byDir.set(dir, group);\n    }\n\n    const lines: string[] = [];\n    for (const [dir, dirFiles] of [...byDir.entries()].sort(([a], [b]) => a.localeCompare(b))) {\n      lines.push(`${dir}/`);\n      for (const f of dirFiles.sort()) {\n        lines.push(`  ${f}`);\n      }\n    }\n    return lines.join('\\n');\n  }\n\n  /**\n   * Create analysis tasks for all files.\n   */\n  createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    for (const file of files) {\n      const prompt = buildFilePrompt({\n        filePath: file.filePath,\n        content: file.content,\n        projectPlan: projectStructure,\n      }, this.debug);\n\n      tasks.push({\n        type: 'file',\n        filePath: file.relativePath,\n        systemPrompt: prompt.system,\n        userPrompt: prompt.user,\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create directory tasks for LLM-generated directory descriptions.\n   * These tasks run after all files in a directory are analyzed, allowing\n   * the LLM to synthesize a richer directory overview from the .sum files.\n   * Prompts are built at execution time by buildDirectoryPrompt().\n   */\n  createDirectoryTasks(files: PreparedFile[]): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    // Group files by directory\n    const filesByDir = new Map<string, PreparedFile[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath);\n      const dirFiles = filesByDir.get(dir) ?? [];\n      dirFiles.push(file);\n      filesByDir.set(dir, dirFiles);\n    }\n\n    // Create a directory task for each directory with analyzed files\n    for (const [dir, dirFiles] of Array.from(filesByDir.entries())) {\n      const sumFilePaths = dirFiles.map(f => `${f.relativePath}.sum`);\n\n      tasks.push({\n        type: 'directory',\n        filePath: dir || '.',\n        directoryInfo: {\n          sumFiles: sumFilePaths,\n          fileCount: dirFiles.length,\n        },\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create a complete generation plan.\n   */\n  async createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'plan-creation',\n      taskCount: discoveryResult.files.length,\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Preparing files: reading and detecting types...'));\n    }\n\n    const files = await this.prepareFiles(discoveryResult);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Analyzing complexity...`));\n    }\n\n    const complexity = analyzeComplexity(\n      files.map(f => f.filePath),\n      this.projectRoot\n    );\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Complexity analysis: depth=${complexity.directoryDepth}`));\n    }\n\n    const projectStructure = this.buildProjectStructure(files);\n    const fileTasks = this.createFileTasks(files, projectStructure);\n\n    // Add directory tasks for LLM-generated directory descriptions\n    // These run after file analysis to synthesize richer directory overviews\n    const dirTasks = this.createDirectoryTasks(files);\n    const tasks = [...fileTasks, ...dirTasks];\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Generation plan: ${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)`\n        )\n      );\n    }\n\n    // Release file content from PreparedFile objects to free memory.\n    // Content has already been embedded into task prompts by createFileTasks()\n    // and is no longer needed. The runner re-reads files from disk.\n    for (const file of files) {\n      (file as { content: string }).content = '';\n    }\n\n    const plan: GenerationPlan = {\n      files,\n      tasks,\n      complexity,\n      projectStructure,\n    };\n\n    // Emit plan created event\n    // +1 for root CLAUDE.md task added later by buildExecutionPlan()\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'generate',\n      fileCount: files.length,\n      taskCount: tasks.length + 1,\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    return plan;\n  }\n}\n\n/**\n * Create a generation orchestrator with default config.\n */\nexport function createOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): GenerationOrchestrator {\n  return new GenerationOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GenerationOrchestrator coordinates three-phase documentation pipeline planning by preparing files, analyzing complexity, building file/directory task queues with embedded prompts, and emitting trace events for plan creation.**\n\n## Exported Types\n\n**PreparedFile** represents a file ready for analysis with `filePath: string` (absolute), `relativePath: string` (from project root), and `content: string` (file contents loaded via `readFile()`).\n\n**AnalysisTask** discriminated union with `type: 'file' | 'directory'`, `filePath: string`, optional `systemPrompt?: string`, optional `userPrompt?: string` (both set for file tasks, built at runtime for directory tasks), and optional `directoryInfo?: { sumFiles: string[], fileCount: number }` for directory tasks.\n\n**GenerationPlan** aggregates `files: PreparedFile[]`, `tasks: AnalysisTask[]`, `complexity: ComplexityMetrics` (from `analyzeComplexity()`), and optional `projectStructure?: string` (compact directory tree listing).\n\n**GenerationOrchestrator** class constructs via `constructor(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })` storing config, projectRoot, tracer, and debug flag.\n\n**createOrchestrator** factory function wraps `new GenerationOrchestrator(config, projectRoot, options)`.\n\n## Core Methods\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads file content via `readFile(filePath, 'utf-8')`, computes `relativePath` via `path.relative(projectRoot, filePath)`, returns array of `PreparedFile` objects, silently skips unreadable files (permission errors).\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** builds file analysis tasks by invoking `buildFilePrompt({ filePath, content, projectPlan: projectStructure }, debug)` for each `PreparedFile`, returns `AnalysisTask[]` with `type: 'file'`, `filePath` set to `relativePath`, `systemPrompt` and `userPrompt` populated from prompt builder.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files by directory via `path.dirname(relativePath)`, aggregates into `Map<string, PreparedFile[]>`, generates one `AnalysisTask` per directory with `type: 'directory'`, `filePath` set to directory path (or `'.'` for root), `directoryInfo.sumFiles` set to array of `.sum` file paths (`${relativePath}.sum`), `directoryInfo.fileCount` set to file count. Prompts built at execution time via `buildDirectoryPrompt()`.\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates plan creation by emitting `phase:start` trace event with `phase: 'plan-creation'`, calling `prepareFiles()`, invoking `analyzeComplexity(filePaths, projectRoot)`, calling `buildProjectStructure()`, invoking `createFileTasks()` and `createDirectoryTasks()`, concatenating tasks, clearing `PreparedFile.content` to free memory (content already embedded in prompts), emitting `plan:created` trace event with `fileCount`, `taskCount + 1` (+1 for root CLAUDE.md task added later by `buildExecutionPlan()`), emitting `phase:end` trace event with `durationMs` computed via `process.hrtime.bigint()` delta, returning `GenerationPlan`.\n\n## Project Structure Builder\n\n**buildProjectStructure(files: PreparedFile[]): string** private method groups files by directory via `path.dirname(relativePath) || '.'`, creates `Map<string, string[]>` of directory to basenames, sorts directories lexicographically, formats as multi-line string with pattern `${dir}/\\n  ${basename}\\n  ${basename}...`, provides bird's-eye context for AI prompts.\n\n## Memory Management\n\nMemory release pattern: `createPlan()` calls `createFileTasks()` which embeds file content into `systemPrompt` and `userPrompt` strings via `buildFilePrompt()`. After task creation, the loop `for (const file of files) { (file as { content: string }).content = '' }` clears `PreparedFile.content` fields to free heap memory. Runner re-reads files from disk when executing tasks.\n\n## Trace Events\n\nEmits `phase:start` event with `type: 'phase:start'`, `phase: 'plan-creation'`, `taskCount: discoveryResult.files.length`, `concurrency: 1`.\n\nEmits `plan:created` event with `type: 'plan:created'`, `planType: 'generate'`, `fileCount: files.length`, `taskCount: tasks.length + 1` (includes future root CLAUDE.md task).\n\nEmits `phase:end` event with `type: 'phase:end'`, `phase: 'plan-creation'`, `durationMs: Number(planEndTime - planStartTime) / 1_000_000`, `tasksCompleted: 1`, `tasksFailed: 0`.\n\n## Dependencies\n\nImports `readFile` from `node:fs/promises` for file content loading, `path` from `node:path` for path operations, `pc` (picocolors) for debug logging, `Config` from `../config/schema.js`, `DiscoveryResult` from `../types/index.js`, `buildFilePrompt` from `./prompts/index.js`, `analyzeComplexity` and `ComplexityMetrics` from `./complexity.js`, `ITraceWriter` from `../orchestration/trace.js`.\n\n## Debug Logging\n\nDebug mode controlled via `this.debug` flag. Logs messages via `console.error(pc.dim(...))` at key milestones: preparing files, analyzing complexity, complexity metrics (`depth=${complexity.directoryDepth}`), final plan stats (`${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1466,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11642,
      "latencyMs": 38101,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/orchestrator.ts",
          "sizeBytes": 8340
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:56.797Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/types.ts\n\n```typescript\n/**\n * Installer types for npx installation workflow\n *\n * Defines types for the interactive installer that copies command files and hooks\n * to runtime-specific directories (Claude Code, OpenCode, Gemini).\n */\n\n/**\n * Supported AI coding assistant runtimes for installation\n *\n * - 'claude': Claude Code (~/.claude or .claude)\n * - 'opencode': OpenCode (~/.config/opencode or .opencode)\n * - 'gemini': Gemini (~/.gemini or .gemini)\n * - 'all': Install to all supported runtimes\n */\nexport type Runtime = 'claude' | 'opencode' | 'gemini' | 'all';\n\n/**\n * Installation location target\n *\n * - 'global': User-level installation (~/.claude, ~/.config/opencode, etc.)\n * - 'local': Project-level installation (.claude, .opencode, etc.)\n */\nexport type Location = 'global' | 'local';\n\n/**\n * Arguments parsed from installer command line\n *\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n */\nexport interface InstallerArgs {\n  /** Target runtime (claude, opencode, gemini, or all) */\n  runtime?: Runtime;\n  /** Install to global/user location */\n  global: boolean;\n  /** Install to local/project location */\n  local: boolean;\n  /** Uninstall instead of install */\n  uninstall: boolean;\n  /** Force overwrite existing files */\n  force: boolean;\n  /** Show help and exit */\n  help: boolean;\n  /** Suppress banner and info messages */\n  quiet: boolean;\n}\n\n/**\n * Result of an installation operation for a single runtime/location\n */\nexport interface InstallerResult {\n  /** Whether the installation succeeded */\n  success: boolean;\n  /** Runtime that was installed */\n  runtime: Exclude<Runtime, 'all'>;\n  /** Location that was installed to */\n  location: Location;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist, no --force) */\n  filesSkipped: string[];\n  /** Error messages if any */\n  errors: string[];\n  /** Whether hook was registered in settings.json (Claude only) */\n  hookRegistered?: boolean;\n  /** Whether VERSION file was written */\n  versionWritten?: boolean;\n}\n\n/**\n * Path configuration for a specific runtime\n *\n * Contains resolved paths for global and local installation locations.\n */\nexport interface RuntimePaths {\n  /** Global installation path (e.g., ~/.claude) */\n  global: string;\n  /** Local installation path (e.g., .claude) */\n  local: string;\n  /** Path to settings.json for hook registration */\n  settingsFile: string;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces and type aliases for the npx-driven installer workflow that copies ARE commands and hooks to AI runtime configuration directories.**\n\n## Exported Types\n\n**`Runtime`** — String literal union `'claude' | 'opencode' | 'gemini' | 'all'` specifying target AI assistant runtime for installation.\n\n**`Location`** — String literal union `'global' | 'local'` distinguishing user-level installations (`~/.claude`, `~/.config/opencode`, `~/.gemini`) from project-level installations (`.claude`, `.opencode`, `.gemini`).\n\n**`InstallerArgs`** — Interface for parsed command-line arguments supporting both interactive prompts and non-interactive flag mode:\n- `runtime?: Runtime` — Target runtime or `'all'` for multi-runtime installation\n- `global: boolean` — Install to user-level configuration directory\n- `local: boolean` — Install to project-level configuration directory\n- `uninstall: boolean` — Uninstall instead of install\n- `force: boolean` — Overwrite existing files without prompting\n- `help: boolean` — Display help message and exit\n- `quiet: boolean` — Suppress banner and informational output\n\n**`InstallerResult`** — Interface capturing outcome of single runtime/location installation operation:\n- `success: boolean` — Overall operation success indicator\n- `runtime: Exclude<Runtime, 'all'>` — Installed runtime (excludes synthetic `'all'` union member)\n- `location: Location` — Installation target location\n- `filesCreated: string[]` — Absolute paths of successfully written files\n- `filesSkipped: string[]` — Paths skipped due to existing files when `force: false`\n- `errors: string[]` — Error messages from failed operations\n- `hookRegistered?: boolean` — Claude-specific flag indicating `settings.json` hook registration success\n- `versionWritten?: boolean` — Flag indicating VERSION file creation for update check system\n\n**`RuntimePaths`** — Interface defining resolved filesystem paths for runtime installation targets:\n- `global: string` — User-level configuration directory (e.g., `~/.claude`)\n- `local: string` — Project-level configuration directory (e.g., `.claude`)\n- `settingsFile: string` — Path to runtime's settings file for hook registration (e.g., `~/.claude/settings.json`)\n\n## Type Constraints\n\n`InstallerResult.runtime` uses `Exclude<Runtime, 'all'>` to ensure result objects reference concrete runtimes (`'claude'`, `'opencode'`, `'gemini'`) rather than the synthetic `'all'` selector used in `InstallerArgs.runtime`.\n\n## Integration Points\n\nConsumed by:\n- `src/installer/prompts.ts` — Interactive CLI prompts for runtime/location selection\n- `src/installer/operations.ts` — File copying, hook registration, VERSION file writing\n- `src/installer/paths.ts` — Runtime path resolution with environment variable overrides\n- `src/installer/uninstall.ts` — Removal of installed files and hook deregistration",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 721,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9893,
      "latencyMs": 28725,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/types.ts",
          "sizeBytes": 2442
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:55.718Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/prompts.ts\n\n```typescript\n/**\n * Interactive prompts module for the installer\n *\n * Provides arrow key selection in TTY mode with numbered fallback for CI/non-interactive.\n * Uses Node.js readline module with raw mode for keypress handling.\n *\n * CRITICAL: Raw mode is always cleaned up via try/finally and process exit handlers.\n */\n\nimport * as readline from 'node:readline';\nimport pc from 'picocolors';\nimport type { Runtime, Location } from './types.js';\n\n/**\n * Check if stdin is a TTY (interactive terminal)\n *\n * @returns true if running in interactive terminal, false for CI/piped input\n */\nexport function isInteractive(): boolean {\n  return process.stdin.isTTY === true;\n}\n\n/**\n * Option type for selection prompts\n */\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n\n/**\n * Raw mode state tracker for cleanup\n */\nlet rawModeActive = false;\n\n/**\n * Cleanup function to restore terminal state\n */\nfunction cleanupRawMode(): void {\n  if (rawModeActive && process.stdin.isTTY) {\n    try {\n      process.stdin.setRawMode(false);\n      process.stdin.pause();\n    } catch {\n      // Ignore errors during cleanup\n    }\n    rawModeActive = false;\n  }\n}\n\n// Register global cleanup handlers\nprocess.on('exit', cleanupRawMode);\nprocess.on('SIGINT', () => {\n  cleanupRawMode();\n  process.exit(0);\n});\n\n/**\n * Generic option selector that uses arrow keys in TTY, numbered in non-TTY\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nexport async function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  if (isInteractive()) {\n    return arrowKeySelect(prompt, options);\n  }\n  return numberedSelect(prompt, options);\n}\n\n/**\n * Arrow key selection for interactive terminals\n *\n * Uses raw mode to capture keypresses for up/down/enter navigation.\n * Always cleans up raw mode even on error or Ctrl+C.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve) => {\n    let selectedIndex = 0;\n\n    // Render the current selection state\n    const render = (clear: boolean = false): void => {\n      // Move cursor up and clear lines if re-rendering\n      if (clear) {\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n        for (let i = 0; i <= options.length; i++) {\n          process.stdout.write('\\x1b[2K\\x1b[1B');\n        }\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n      }\n\n      console.log(pc.bold(prompt));\n      options.forEach((opt, idx) => {\n        const prefix = idx === selectedIndex ? pc.cyan('> ') : '  ';\n        const label = idx === selectedIndex ? pc.cyan(opt.label) : opt.label;\n        console.log(prefix + label);\n      });\n    };\n\n    // Handle keypress events\n    const handleKeypress = (\n      _str: string | undefined,\n      key: { name?: string; ctrl?: boolean },\n    ): void => {\n      if (key.ctrl && key.name === 'c') {\n        cleanupRawMode();\n        process.exit(0);\n      }\n\n      switch (key.name) {\n        case 'up':\n          selectedIndex = Math.max(0, selectedIndex - 1);\n          render(true);\n          break;\n        case 'down':\n          selectedIndex = Math.min(options.length - 1, selectedIndex + 1);\n          render(true);\n          break;\n        case 'return':\n          // Cleanup and resolve\n          process.stdin.off('keypress', handleKeypress);\n          cleanupRawMode();\n          console.log();\n          resolve(options[selectedIndex].value);\n          break;\n      }\n    };\n\n    try {\n      // Setup raw mode for keypress handling\n      readline.emitKeypressEvents(process.stdin);\n      if (process.stdin.isTTY) {\n        process.stdin.setRawMode(true);\n        rawModeActive = true;\n      }\n      process.stdin.resume();\n      process.stdin.on('keypress', handleKeypress);\n\n      // Initial render\n      render(false);\n    } catch (err) {\n      cleanupRawMode();\n      throw err;\n    }\n  });\n}\n\n/**\n * Numbered selection for non-interactive environments\n *\n * Prints numbered options and reads a number from stdin.\n * Used in CI environments or when stdin is piped.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve, reject) => {\n    console.log(pc.bold(prompt));\n    options.forEach((opt, idx) => {\n      console.log(`  ${idx + 1}. ${opt.label}`);\n    });\n\n    const rl = readline.createInterface({\n      input: process.stdin,\n      output: process.stdout,\n    });\n\n    rl.question('Enter number: ', (answer) => {\n      rl.close();\n\n      const num = parseInt(answer, 10);\n      if (isNaN(num) || num < 1 || num > options.length) {\n        reject(new Error(`Invalid selection: ${answer}. Expected 1-${options.length}`));\n        return;\n      }\n\n      resolve(options[num - 1].value);\n    });\n  });\n}\n\n/**\n * Prompt user to select a runtime\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected runtime value\n */\nexport async function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime> {\n  const prompt = mode === 'uninstall' ? 'Select runtime to uninstall:' : 'Select runtime to install:';\n  return selectOption<Runtime>(prompt, [\n    { label: 'Claude Code', value: 'claude' },\n    { label: 'OpenCode', value: 'opencode' },\n    { label: 'Gemini CLI', value: 'gemini' },\n    { label: 'All runtimes', value: 'all' },\n  ]);\n}\n\n/**\n * Prompt user to select installation location\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected location value\n */\nexport async function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location> {\n  const prompt = mode === 'uninstall' ? 'Select uninstallation location:' : 'Select installation location:';\n  return selectOption<Location>(prompt, [\n    { label: 'Global (~/.claude, ~/.config/opencode, etc.)', value: 'global' },\n    { label: 'Local (./.claude, ./.opencode, etc.)', value: 'local' },\n  ]);\n}\n\n/**\n * Prompt user to confirm an action\n *\n * @param message - Confirmation message to display\n * @returns true if confirmed, false if declined\n */\nexport async function confirmAction(message: string): Promise<boolean> {\n  return selectOption<boolean>(message, [\n    { label: 'Yes', value: true },\n    { label: 'No', value: false },\n  ]);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts provides interactive selection prompts for the installer with arrow-key navigation in TTY mode and numbered fallback for CI/non-interactive environments, using Node.js readline with raw mode cleanup via try/finally and process exit handlers.**\n\n## Public Interface\n\n### Type Exports\n\n```typescript\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n```\n\n### Exported Functions\n\n**isInteractive(): boolean** — Returns `process.stdin.isTTY === true` to detect interactive terminal vs CI/piped input.\n\n**selectOption<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Generic option selector dispatching to `arrowKeySelect()` for TTY or `numberedSelect()` for non-TTY environments.\n\n**selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>** — Prompts for runtime selection with options: `'claude'`, `'opencode'`, `'gemini'`, `'all'`. Passes mode to customize prompt text (\"Select runtime to install:\" vs \"Select runtime to uninstall:\").\n\n**selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>** — Prompts for installation location with options: `'global'` (for `~/.claude`, `~/.config/opencode`) or `'local'` (for `./.claude`, `./.opencode`).\n\n**confirmAction(message: string): Promise<boolean>** — Displays confirmation prompt with \"Yes\"/\"No\" options returning true/false.\n\n## Interactive Terminal Handling\n\n**arrowKeySelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Private function enabling arrow key navigation (up/down/enter) via `readline.emitKeypressEvents()` and `process.stdin.setRawMode(true)`. Maintains `selectedIndex` state and re-renders option list with ANSI escape sequences: `\\x1b[${n}A` (cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (cursor down). Selected option highlighted with `pc.cyan('> ')` prefix. Keypress handler responds to `key.name === 'up' | 'down' | 'return'` and `key.ctrl && key.name === 'c'` for graceful exit.\n\n**numberedSelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Private fallback for non-TTY environments printing numbered list (1-indexed) and reading selection via `readline.createInterface()`. Validates input range `1 <= num <= options.length`, rejects with `Error('Invalid selection: ${answer}. Expected 1-${options.length}')` on invalid input.\n\n## Raw Mode Lifecycle Management\n\n**rawModeActive: boolean** — Module-level state tracker ensuring cleanup even on error/interrupt.\n\n**cleanupRawMode(): void** — Restores terminal state via `process.stdin.setRawMode(false)` and `process.stdin.pause()`, guarded by try/catch to ignore errors during cleanup. Sets `rawModeActive = false`.\n\n**Global handlers** — Registered via `process.on('exit', cleanupRawMode)` and `process.on('SIGINT', ...)` to ensure raw mode cleanup on process termination.\n\n**arrowKeySelect error handling** — Wraps raw mode setup in try/catch, calls `cleanupRawMode()` before re-throwing errors. Uses try/finally pattern implicitly via Promise resolution path.\n\n## Dependencies\n\nImports `readline` from Node.js stdlib for keypress events and interface creation, `picocolors` as `pc` for ANSI color formatting (`pc.cyan()`, `pc.bold()`), and types `Runtime`, `Location` from `./types.js`.\n\n## Behavioral Contracts\n\n**Prompt text patterns:**\n- Install mode: `\"Select runtime to install:\"`, `\"Select installation location:\"`\n- Uninstall mode: `\"Select runtime to uninstall:\"`, `\"Select uninstallation location:\"`\n\n**Runtime option labels:** `\"Claude Code\"`, `\"OpenCode\"`, `\"Gemini CLI\"`, `\"All runtimes\"` mapping to values `'claude'`, `'opencode'`, `'gemini'`, `'all'`.\n\n**Location option labels:** `\"Global (~/.claude, ~/.config/opencode, etc.)\"`, `\"Local (./.claude, ./.opencode, etc.)\"` mapping to values `'global'`, `'local'`.\n\n**ANSI escape sequences:**\n- Cursor movement: `\\x1b[${n}A` (up n lines), `\\x1b[1B` (down 1 line)\n- Line operations: `\\x1b[2K` (clear entire line)\n\n**Keypress event matching:** Checks `key.name` against strings `'up'`, `'down'`, `'return'` and compound check `key.ctrl && key.name === 'c'` for interrupt handling.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1209,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11239,
      "latencyMs": 33815,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/prompts.ts",
          "sizeBytes": 6609
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:05.858Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/index.ts\n\n```typescript\n/**\n * Main installer entry point for agents-reverse-engineer\n *\n * Provides the runInstaller function for npx installation workflow.\n * Supports interactive prompts and non-interactive flags for CI/scripted installs.\n */\n\nimport type { InstallerArgs, InstallerResult, Runtime, Location } from './types.js';\nimport { getAllRuntimes, resolveInstallPath } from './paths.js';\nimport {\n  displayBanner,\n  showHelp,\n  showSuccess,\n  showError,\n  showWarning,\n  showInfo,\n  showNextSteps,\n} from './banner.js';\nimport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\nimport { installFiles, verifyInstallation, formatInstallResult } from './operations.js';\nimport { uninstallFiles, deleteConfigFolder } from './uninstall.js';\n\n// Re-export types for external consumers\nexport type { InstallerArgs, InstallerResult, Runtime, Location, RuntimePaths } from './types.js';\nexport { getRuntimePaths, getAllRuntimes, resolveInstallPath, getSettingsPath } from './paths.js';\nexport { displayBanner, showHelp, showSuccess, showError, showWarning, showInfo, showNextSteps, VERSION } from './banner.js';\nexport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\n\n/**\n * Parse command-line arguments for the installer\n *\n * Handles both short (-g, -l, -h) and long (--global, --local, --help) flags.\n * Uses pattern from cli/index.ts for consistency.\n *\n * @param args - Command line arguments (process.argv.slice(2))\n * @returns Parsed installer arguments\n */\nexport function parseInstallerArgs(args: string[]): InstallerArgs {\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n\n    if (arg === '--runtime' && i + 1 < args.length) {\n      // --runtime requires a value\n      values.set('runtime', args[++i]);\n    } else if (arg === '-g' || arg === '--global') {\n      flags.add('global');\n    } else if (arg === '-l' || arg === '--local') {\n      flags.add('local');\n    } else if (arg === '--force') {\n      flags.add('force');\n    } else if (arg === '-q' || arg === '--quiet') {\n      flags.add('quiet');\n    } else if (arg === '-h' || arg === '--help') {\n      flags.add('help');\n    }\n  }\n\n  // Validate runtime value if provided\n  const runtimeValue = values.get('runtime');\n  const validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all'];\n  const runtime = runtimeValue && validRuntimes.includes(runtimeValue as Runtime)\n    ? (runtimeValue as Runtime)\n    : undefined;\n\n  return {\n    runtime,\n    global: flags.has('global'),\n    local: flags.has('local'),\n    uninstall: false, // Set by 'uninstall' command, not flags\n    force: flags.has('force'),\n    help: flags.has('help'),\n    quiet: flags.has('quiet'),\n  };\n}\n\n/**\n * Determine installation location from args or return undefined for prompt\n *\n * @param args - Parsed installer arguments\n * @returns Location if specified, undefined if needs prompt\n */\nfunction determineLocation(args: InstallerArgs): Location | undefined {\n  if (args.global && !args.local) {\n    return 'global';\n  }\n  if (args.local && !args.global) {\n    return 'local';\n  }\n  // Both or neither - needs interactive prompt\n  return undefined;\n}\n\n/**\n * Determine target runtimes from args\n *\n * @param runtime - Runtime from args (may be 'all' or specific runtime)\n * @returns Array of specific runtimes to install to\n */\nfunction determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>> {\n  if (!runtime) {\n    // No runtime specified - will need interactive prompt\n    return [];\n  }\n  if (runtime === 'all') {\n    return getAllRuntimes();\n  }\n  return [runtime];\n}\n\n/**\n * Run the installer workflow\n *\n * This is the main entry point for the installation process.\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n *\n * @param args - Parsed installer arguments\n * @returns Array of installation results (one per runtime/location combination)\n */\nexport async function runInstaller(args: InstallerArgs): Promise<InstallerResult[]> {\n  // Handle help flag\n  if (args.help) {\n    showHelp();\n    return [];\n  }\n\n  // Display banner unless quiet mode\n  if (!args.quiet) {\n    displayBanner();\n  }\n\n  // Determine location and runtimes from flags\n  let location = determineLocation(args);\n  const runtimeArg = args.runtime;\n\n  // Non-interactive mode: require all flags\n  if (!isInteractive()) {\n    if (!runtimeArg) {\n      showError('Missing --runtime flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n    if (!location) {\n      showError('Missing -g/--global or -l/--local flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n  }\n\n  // Interactive mode: prompt for missing values\n  const mode = args.uninstall ? 'uninstall' : 'install';\n  let selectedRuntime: Runtime | undefined = runtimeArg;\n  if (!selectedRuntime && isInteractive()) {\n    selectedRuntime = await selectRuntime(mode);\n  }\n\n  if (!location && isInteractive()) {\n    location = await selectLocation(mode);\n  }\n\n  // Safety check - should not reach here without values\n  if (!selectedRuntime || !location) {\n    showError('Unable to determine runtime and location');\n    process.exit(1);\n  }\n\n  // UNINSTALL MODE\n  if (args.uninstall) {\n    return runUninstall(selectedRuntime, location, args.quiet);\n  }\n\n  // INSTALL MODE\n  return runInstall(selectedRuntime, location, args.force, args.quiet);\n}\n\n/**\n * Run the installation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param force - Overwrite existing files\n * @param quiet - Suppress output\n * @returns Array of installation results\n */\nasync function runInstall(\n  runtime: Runtime,\n  location: Location,\n  force: boolean,\n  quiet: boolean,\n): Promise<InstallerResult[]> {\n  // Install files\n  const results = installFiles(runtime, location, { force, dryRun: false });\n\n  // Verify installation\n  const allCreatedFiles = results.flatMap((r) => r.filesCreated);\n  const verification = verifyInstallation(allCreatedFiles);\n\n  if (!verification.success) {\n    showError('Installation verification failed - some files missing:');\n    for (const missing of verification.missing) {\n      showWarning(`  Missing: ${missing}`);\n    }\n  }\n\n  // Display results\n  if (!quiet) {\n    displayInstallResults(results);\n  }\n\n  return results;\n}\n\n/**\n * Run the uninstallation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param quiet - Suppress output\n * @returns Array of uninstallation results\n */\nfunction runUninstall(\n  runtime: Runtime,\n  location: Location,\n  quiet: boolean,\n): InstallerResult[] {\n  const results = uninstallFiles(runtime, location, false);\n\n  // Delete .agents-reverse-engineer config folder (local only)\n  const configDeleted = deleteConfigFolder(location, false);\n\n  // Display results\n  if (!quiet) {\n    displayUninstallResults(results, configDeleted);\n  }\n\n  return results;\n}\n\n/**\n * Display installation results with styled output\n *\n * Shows checkmarks for successful actions, warnings for skipped files,\n * and next steps for using the installed commands.\n *\n * @param results - Array of installation results\n */\nfunction displayInstallResults(results: InstallerResult[]): void {\n  console.log();\n\n  let totalCreated = 0;\n  let totalSkipped = 0;\n  let hooksRegistered = 0;\n\n  for (const result of results) {\n    if (result.success) {\n      showSuccess(`Installed ${result.runtime} (${result.location})`);\n    } else {\n      showError(`Failed to install ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalCreated += result.filesCreated.length;\n    totalSkipped += result.filesSkipped.length;\n\n    if (result.hookRegistered) {\n      hooksRegistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalCreated > 0) {\n    showSuccess(`Created ${totalCreated} command files`);\n  }\n  if (hooksRegistered > 0) {\n    showSuccess(`Registered ${hooksRegistered} session hook(s)`);\n  }\n  if (totalSkipped > 0) {\n    showWarning(`Skipped ${totalSkipped} existing files (use --force to overwrite)`);\n  }\n\n  // Next steps\n  const primaryRuntime = results[0]?.runtime || 'claude';\n  showNextSteps(primaryRuntime, totalCreated);\n\n  // GitHub link\n  console.log();\n  showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer');\n}\n\n/**\n * Display uninstallation results with styled output\n *\n * @param results - Array of uninstallation results\n * @param configDeleted - Whether the .agents-reverse-engineer folder was deleted\n */\nfunction displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void {\n  console.log();\n\n  let totalDeleted = 0;\n  let hooksUnregistered = 0;\n\n  for (const result of results) {\n    // In uninstall context, filesCreated tracks deleted files\n    const deletedCount = result.filesCreated.length;\n    const notFoundCount = result.filesSkipped.length;\n\n    if (result.success) {\n      if (deletedCount > 0) {\n        showSuccess(`Uninstalled ${result.runtime} (${result.location}) - ${deletedCount} files removed`);\n      } else {\n        showInfo(`No ${result.runtime} files found in ${result.location}`);\n      }\n    } else {\n      showError(`Failed to uninstall ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalDeleted += deletedCount;\n\n    // hookRegistered is repurposed for uninstall to mean \"hook was unregistered\"\n    if (result.hookRegistered) {\n      hooksUnregistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalDeleted > 0) {\n    showSuccess(`Removed ${totalDeleted} files`);\n  }\n  if (hooksUnregistered > 0) {\n    showSuccess(`Unregistered ${hooksUnregistered} session hook(s)`);\n  }\n  if (configDeleted) {\n    showSuccess(`Removed .agents-reverse-engineer folder`);\n  }\n  if (totalDeleted === 0 && !configDeleted) {\n    showInfo('No files were removed');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Main installer entry point orchestrating npx installation workflow with interactive prompts, flag parsing, installation/uninstallation execution, and result display for ARE command/hook deployment.**\n\n## Exported Functions\n\n**`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>`** — Main entry point executing install/uninstall workflow with interactive prompts or non-interactive flag-based mode, returns array of per-runtime installation results.\n\n**`parseInstallerArgs(args: string[]): InstallerArgs`** — Parses command-line arguments into `InstallerArgs` structure, handles short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--help`, `--quiet`, `--force`, `--runtime <value>`), validates runtime against `['claude', 'opencode', 'gemini', 'all']`.\n\n## Re-Exported Types and Functions\n\nRe-exports from submodules: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`, `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`, `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION`, `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`.\n\n## Installation Workflow\n\n**`runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>`** — Executes installation via `installFiles()`, verifies via `verifyInstallation()` on `allCreatedFiles`, displays results unless quiet mode enabled, returns installation results array.\n\n**`runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]`** — Executes uninstallation via `uninstallFiles()`, deletes `.agents-reverse-engineer` config folder via `deleteConfigFolder(location, false)` for local installs, displays results unless quiet, returns uninstallation results array.\n\n## Argument Resolution\n\n**`determineLocation(args: InstallerArgs): Location | undefined`** — Returns `'global'` if `args.global && !args.local`, `'local'` if `args.local && !args.global`, undefined otherwise (triggers interactive prompt).\n\n**`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>`** — Returns empty array if runtime undefined (needs prompt), calls `getAllRuntimes()` if runtime is `'all'`, returns single-element array `[runtime]` for specific runtimes.\n\n## Display Functions\n\n**`displayInstallResults(results: InstallerResult[]): void`** — Shows per-runtime success/error messages via `showSuccess()`/`showError()`, aggregates `totalCreated`/`totalSkipped`/`hooksRegistered` counts, displays summary with `showSuccess()`/`showWarning()`, calls `showNextSteps(primaryRuntime, totalCreated)`, prints GitHub docs link.\n\n**`displayUninstallResults(results: InstallerResult[], configDeleted?: boolean): void`** — Shows per-runtime uninstall status (repurposes `filesCreated` as deleted file count, `filesSkipped` as not-found count), aggregates `totalDeleted`/`hooksUnregistered`, displays summary with config folder deletion status.\n\n## Non-Interactive Mode Requirements\n\nNon-interactive mode (when `!isInteractive()`) requires `--runtime` flag and either `-g`/`--global` or `-l`/`--local` flag, exits with error via `showError()` + `process.exit(1)` if missing.\n\n## Integration Dependencies\n\nImports `InstallerArgs`, `InstallerResult`, `Runtime`, `Location` from `./types.js`, `getAllRuntimes`, `resolveInstallPath` from `./paths.js`, display functions from `./banner.js`, `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive` from `./prompts.js`, `installFiles`, `verifyInstallation`, `formatInstallResult` from `./operations.js`, `uninstallFiles`, `deleteConfigFolder` from `./uninstall.js`.\n\n## Control Flow Pattern\n\n`runInstaller()` checks `args.help` first (displays help, returns empty array), displays banner unless quiet, determines location/runtime from flags, enforces non-interactive requirements, prompts for missing values in interactive mode, branches to `runUninstall()` if `args.uninstall`, otherwise calls `runInstall()`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1088,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12228,
      "latencyMs": 27409,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/index.ts",
          "sizeBytes": 10066
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:10.125Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/index.ts\n\n```typescript\n/**\n * Public API for the orchestration module.\n *\n * This barrel export is the single import point for the orchestration\n * engine. It re-exports the concurrency pool, progress reporter,\n * command runner, and all shared types.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   CommandRunner,\n *   ProgressReporter,\n *   runPool,\n * } from './orchestration/index.js';\n *\n * const runner = new CommandRunner(aiService, { concurrency: 5 });\n * const summary = await runner.executeGenerate(plan);\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\nexport type {\n  FileTaskResult,\n  RunSummary,\n  ProgressEvent,\n  CommandRunOptions,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Concurrency pool\n// ---------------------------------------------------------------------------\n\nexport { runPool } from './pool.js';\nexport type { PoolOptions, TaskResult } from './pool.js';\n\n// ---------------------------------------------------------------------------\n// Progress reporter\n// ---------------------------------------------------------------------------\n\nexport { ProgressReporter, ProgressLog } from './progress.js';\n\n// ---------------------------------------------------------------------------\n// Plan tracker\n// ---------------------------------------------------------------------------\n\nexport { PlanTracker } from './plan-tracker.js';\n\n// ---------------------------------------------------------------------------\n// Tracing\n// ---------------------------------------------------------------------------\n\nexport type { ITraceWriter, TraceEvent, TraceEventPayload } from './trace.js';\nexport { createTraceWriter, cleanupOldTraces } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Command runner\n// ---------------------------------------------------------------------------\n\nexport { CommandRunner } from './runner.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export module aggregating orchestration subsystem's public API for concurrent task execution, progress tracking, plan persistence, trace emission, and command workflow coordination.**\n\n## Exported Types\n\n**From `types.js`:**\n- `FileTaskResult` — Result envelope for individual file analysis tasks with status, path, tokens, duration, error fields\n- `RunSummary` — Aggregated execution metrics (total files, success/error counts, duration, token totals, cost estimates)\n- `ProgressEvent` — Event payload for progress updates with discriminated type field\n- `CommandRunOptions` — Configuration object threading through runner → pool → AIService (concurrency, timeout, retries, failFast, tracer, etc.)\n\n**From `pool.js`:**\n- `PoolOptions` — Worker pool configuration subset (concurrency, onProgress callback)\n- `TaskResult` — Generic result wrapper for pool tasks with `ok: boolean`, `value?`, `error?` discriminated union\n\n**From `trace.js`:**\n- `ITraceWriter` — Interface defining `emit(event, payload)` and `close()` for pluggable trace backends\n- `TraceEvent` — Union of event type strings: `'phase:start' | 'phase:end' | 'worker:start' | 'worker:end' | 'task:pickup' | 'task:done' | 'subprocess:spawn' | 'subprocess:exit' | 'retry'`\n- `TraceEventPayload` — Discriminated union of payload shapes keyed by event type\n\n## Exported Functions\n\n- `runPool<T, R>(tasks, worker, options)` — Iterator-based concurrency pool sharing single task iterator across N workers, returns `Promise<TaskResult<R>[]>`\n- `createTraceWriter(outputDir)` — Factory returning `ITraceWriter` (either `TraceWriter` writing NDJSON to `traces/` or `NullTraceWriter` stub when disabled)\n- `cleanupOldTraces(traceDir, keepCount)` — Retention enforcement deleting oldest trace files beyond `keepCount` limit\n\n## Exported Classes\n\n- `ProgressReporter` — Streaming progress tracker with ETA calculation via moving average of last 10 task durations, emits console output and writes to `progress.log`\n- `ProgressLog` — Thin wrapper appending human-readable messages to `.agents-reverse-engineer/progress.log` with timestamp prefixes\n- `PlanTracker` — Serialized file writer for `GENERATION-PLAN.md` using promise-chain pattern to prevent corruption from concurrent pool workers\n- `CommandRunner` — High-level workflow orchestrator exposing `executeGenerate(plan)`, `executeUpdate(changes)`, `executeSpecify()` methods coordinating pool execution + progress reporting + plan tracking + trace emission\n\n## Module Organization\n\nThis barrel export consolidates six submodules:\n1. **types.js** — Shared interfaces for results, summaries, events, options\n2. **pool.js** — Concurrency pool implementation with iterator-based task distribution\n3. **progress.js** — Progress reporting with ETA and file logging\n4. **plan-tracker.js** — GENERATION-PLAN.md serialized writer\n5. **trace.js** — NDJSON trace event emission with pluggable backend\n6. **runner.js** — Command workflow coordination integrating all subsystems\n\n## Integration Pattern\n\nConsumers import from `./orchestration/index.js` to access:\n- `CommandRunner` for high-level command execution (`executeGenerate`, `executeUpdate`)\n- `runPool` for direct concurrency pool usage in custom workflows\n- `ProgressReporter` for custom progress tracking\n- `createTraceWriter` for enabling trace emission via `--trace` flag\n- Type definitions for strongly-typed result handling and configuration\n\nExample from `src/cli/generate.ts`:\n```typescript\nimport { CommandRunner } from '../orchestration/index.js';\nconst runner = new CommandRunner(aiService, runOptions);\nconst summary = await runner.executeGenerate(plan);\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 943,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9598,
      "latencyMs": 28932,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/index.ts",
          "sizeBytes": 2076
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:08.399Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/generate.ts\n\n```typescript\n/**\n * Integration file generation for AI coding assistants\n *\n * Generates command files and hooks for detected AI assistant environments.\n * Handles file creation with directory creation and skip-if-exists behavior.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { IntegrationResult, EnvironmentType } from './types.js';\nimport { detectEnvironments } from './detect.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from './templates.js';\n\n/**\n * Get the path to a bundled hook file\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/integration/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Options for generating integration files\n */\nexport interface GenerateOptions {\n  /** If true, don't actually write files - just report what would be done */\n  dryRun?: boolean;\n  /** If true, overwrite existing files instead of skipping them */\n  force?: boolean;\n  /** Specific environment to generate for (bypasses auto-detection) */\n  environment?: EnvironmentType;\n}\n\n/**\n * Ensure parent directories exist for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Generate integration files for all detected AI assistant environments\n *\n * For each detected environment:\n * - Gets appropriate templates (command files)\n * - Creates files if they don't exist (or if force=true)\n * - For Claude: also creates the session-end hook\n *\n * @param projectRoot - Root directory of the project\n * @param options - Generation options\n * @returns Array of results, one per environment\n *\n * @example\n * ```typescript\n * const results = await generateIntegrationFiles('/path/to/project');\n * // [{ environment: 'claude', filesCreated: ['...'], filesSkipped: [] }]\n * ```\n */\nexport async function generateIntegrationFiles(\n  projectRoot: string,\n  options: GenerateOptions = {}\n): Promise<IntegrationResult[]> {\n  const { dryRun = false, force = false, environment: specificEnv } = options;\n  const results: IntegrationResult[] = [];\n\n  // Use specific environment if provided, otherwise auto-detect\n  let environments: { type: EnvironmentType; configDir: string }[];\n  if (specificEnv) {\n    // Map environment type to config directory\n    const configDirMap: Record<EnvironmentType, string> = {\n      claude: '.claude',\n      opencode: '.opencode',\n      aider: '.aider',\n      gemini: '.gemini',\n    };\n    environments = [{ type: specificEnv, configDir: configDirMap[specificEnv] }];\n  } else {\n    // Detect which environments are present\n    environments = detectEnvironments(projectRoot);\n  }\n\n  for (const env of environments) {\n    const result: IntegrationResult = {\n      environment: env.type,\n      filesCreated: [],\n      filesSkipped: [],\n    };\n\n    // Get templates for this environment\n    const templates = getTemplatesForEnvironment(env.type);\n\n    // Process each template\n    for (const template of templates) {\n      const fullPath = path.join(projectRoot, template.path);\n\n      if (existsSync(fullPath) && !force) {\n        // File exists and force is not set - skip it\n        result.filesSkipped.push(template.path);\n      } else {\n        // Create the file\n        if (!dryRun) {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        }\n        result.filesCreated.push(template.path);\n      }\n    }\n\n    // For Claude, also generate the hook file\n    if (env.type === 'claude') {\n      const hookPath = '.claude/hooks/are-session-end.js';\n      const fullHookPath = path.join(projectRoot, hookPath);\n\n      if (existsSync(fullHookPath) && !force) {\n        result.filesSkipped.push(hookPath);\n      } else {\n        if (!dryRun) {\n          ensureDir(fullHookPath);\n          const hookContent = readBundledHook('are-session-end.js');\n          writeFileSync(fullHookPath, hookContent, 'utf-8');\n        }\n        result.filesCreated.push(hookPath);\n      }\n    }\n\n    results.push(result);\n  }\n\n  return results;\n}\n\n/**\n * Get templates for a specific environment type\n *\n * @param type - Environment type\n * @returns Array of templates for that environment\n */\nfunction getTemplatesForEnvironment(\n  type: EnvironmentType\n): ReturnType<typeof getClaudeTemplates> {\n  switch (type) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n    case 'aider':\n      // Aider doesn't have command files yet - return empty\n      return [];\n    default:\n      return [];\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateIntegrationFiles() orchestrates AI assistant integration file creation with environment-specific template selection, bundled hook copying, and skip-if-exists behavior.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>**\n- Main entry point for integration file generation\n- Accepts `projectRoot` string and optional `GenerateOptions` with `dryRun`, `force`, and `environment` fields\n- If `options.environment` is specified, bypasses `detectEnvironments()` and targets single environment using `configDirMap: Record<EnvironmentType, string>` mapping (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n- Otherwise calls `detectEnvironments(projectRoot)` from `./detect.js` to get `{ type: EnvironmentType; configDir: string }[]` array\n- For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve template array\n- Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file if not exists or `force=true` via `writeFileSync(fullPath, template.content, 'utf-8')`\n- Tracks created/skipped paths in `IntegrationResult.filesCreated[]` and `IntegrationResult.filesSkipped[]`\n- Special case for `claude` environment: after template processing, generates hook file at `.claude/hooks/are-session-end.js` via `readBundledHook('are-session-end.js')` and `writeFileSync(fullHookPath, hookContent, 'utf-8')`\n- Returns `IntegrationResult[]` array with one entry per environment\n\n**getBundledHookPath(hookName: string): string**\n- Resolves absolute path to bundled hook file in `hooks/dist/` directory\n- Uses `fileURLToPath(import.meta.url)` to get current module path\n- Navigates from `dist/integration/` up two levels to project root, then into `hooks/dist/`\n- Returns `path.join(__dirname, '..', '..', 'hooks', 'dist', hookName)`\n\n**readBundledHook(hookName: string): string**\n- Calls `getBundledHookPath(hookName)` to resolve hook file path\n- Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false\n- Returns hook file content via `readFileSync(hookPath, 'utf-8')`\n\n**ensureDir(filePath: string): void**\n- Creates parent directories for `filePath` if they don't exist\n- Calls `mkdirSync(dir, { recursive: true })` where `dir` is `path.dirname(filePath)`\n\n**getTemplatesForEnvironment(type: EnvironmentType): ReturnType<typeof getClaudeTemplates>**\n- Switch statement dispatches on `type` to call environment-specific template getters from `./templates.js`\n- `'claude'` → `getClaudeTemplates()`\n- `'opencode'` → `getOpenCodeTemplates()`\n- `'gemini'` → `getGeminiTemplates()`\n- `'aider'` → returns empty array `[]` (no templates defined)\n- Default case returns empty array `[]`\n\n## Type Definitions\n\n**GenerateOptions** interface:\n- `dryRun?: boolean` — preview mode without file writes\n- `force?: boolean` — overwrite existing files instead of skipping\n- `environment?: EnvironmentType` — bypass auto-detection and target specific environment\n\n## Integration with Project\n\nImports `IntegrationResult` and `EnvironmentType` types from `./types.js`, `detectEnvironments()` from `./detect.js`, and template getters (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`) from `./templates.js`. Called by installer CLI commands to create `.claude/skills/`, `.opencode/commands/`, `.gemini/commands/` files and session hooks.\n\n## File System Operations\n\nAll file writes gated by `!dryRun` check to support preview mode. Uses `existsSync()` before write to implement skip-if-exists behavior (unless `force=true` overrides). Calls `ensureDir()` before every `writeFileSync()` to guarantee parent directories exist via `mkdirSync()` with `recursive: true`. Hook files read from build artifact location `hooks/dist/` which is populated by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Environment Targeting Behavior\n\nWhen `options.environment` is undefined, calls `detectEnvironments(projectRoot)` to scan for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories and generates files for all detected environments. When `options.environment` is specified, creates single-element environment array via `configDirMap` lookup, bypassing detection. This supports both multi-environment batch generation (`npx agents-reverse-engineer --runtime all`) and targeted single-environment installs (`npx agents-reverse-engineer --runtime claude -g`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1249,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10839,
      "latencyMs": 34654,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/generate.ts",
          "sizeBytes": 5521
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:18.474Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/plan-tracker.ts\n\n```typescript\n/**\n * Progress tracker that updates GENERATION-PLAN.md checkboxes during generation.\n *\n * Maintains the markdown content in memory for fast updates and serializes\n * disk writes via a promise chain to handle concurrent Phase 1 completions.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { CONFIG_DIR } from '../config/loader.js';\n\n/**\n * Tracks generation progress by ticking checkboxes in GENERATION-PLAN.md.\n *\n * Create one instance at the start of `executeGenerate()`, call `markDone()`\n * as tasks complete, and `flush()` before returning.\n */\nexport class PlanTracker {\n  private content: string;\n  private readonly planPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(projectRoot: string, initialMarkdown: string) {\n    this.planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n    this.content = initialMarkdown;\n  }\n\n  /** Write the initial plan file to disk. */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.planPath), { recursive: true });\n      await writeFile(this.planPath, this.content, 'utf8');\n    } catch {\n      // Non-critical — generation continues without tracking\n    }\n  }\n\n  /**\n   * Mark a task as done by replacing its checkbox.\n   *\n   * The caller must pass the exact path as it appears in the markdown:\n   * - File: `src/cli/init.ts`\n   * - Directory: `src/cli/AGENTS.md`  (caller appends `/AGENTS.md`)\n   * - Root doc: `CLAUDE.md`\n   */\n  markDone(itemPath: string): void {\n    const before = this.content;\n    this.content = this.content.replace(\n      `- [ ] \\`${itemPath}\\``,\n      `- [x] \\`${itemPath}\\``,\n    );\n\n    if (this.content === before) return; // no match — skip write\n\n    // Queue a serialized write so concurrent markDone() calls don't corrupt the file\n    this.writeQueue = this.writeQueue\n      .then(() => writeFile(this.planPath, this.content, 'utf8'))\n      .catch(() => {/* non-critical */});\n  }\n\n  /** Wait for all queued writes to finish. */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via promise-chain queuing to prevent file corruption during parallel Phase 1 worker completions.**\n\n## Exported Class\n\n`PlanTracker` manages generation progress by updating markdown checkboxes in `.agents-reverse-engineer/GENERATION-PLAN.md`. Constructor signature: `constructor(projectRoot: string, initialMarkdown: string)`. Three public methods: `initialize()`, `markDone(itemPath: string)`, `flush()`.\n\n## State Management\n\n`content: string` holds current markdown in memory for fast string replacement operations. `planPath: string` stores absolute file path computed as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` where `CONFIG_DIR` imported from `../config/loader.js`. `writeQueue: Promise<void>` implements serialized write queue initialized to `Promise.resolve()`.\n\n## Initialization\n\n`initialize()` creates parent directory via `mkdir(path.dirname(this.planPath), { recursive: true })` then writes initial `this.content` to disk. Catches exceptions silently since tracking is non-critical to generation pipeline success.\n\n## Checkbox Update Pattern\n\n`markDone(itemPath: string)` replaces markdown checkbox pattern `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``. Expects exact paths as they appear in markdown: file paths like `src/cli/init.ts`, directory aggregation paths with `/AGENTS.md` suffix appended by caller (e.g., `src/cli/AGENTS.md`), root documents like `CLAUDE.md`. Returns early if string replacement yields no change (no match found).\n\n## Write Serialization\n\nAfter successful string replacement, chains write operation onto `this.writeQueue` via `this.writeQueue = this.writeQueue.then(() => writeFile(this.planPath, this.content, 'utf8')).catch(() => {})`. Catch clause swallows errors silently (non-critical operation). Pattern ensures concurrent `markDone()` calls from Phase 1 worker pool execute disk writes sequentially despite parallel in-memory updates, preventing TOCTOU corruption.\n\n## Flush Mechanism\n\n`flush()` awaits `this.writeQueue` to drain pending writes before `executeGenerate()` returns. Guarantees final plan state persists to disk despite async worker completions.\n\n## Usage Protocol\n\nCreate single `PlanTracker` instance at start of `executeGenerate()` in `src/generation/executor.ts`. Call `initialize()` once after construction. Workers invoke `markDone(itemPath)` as tasks complete. Orchestrator calls `flush()` before returning to ensure all queued writes finish.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 636,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27218,
      "latencyMs": 25247,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/plan-tracker.ts",
          "sizeBytes": 2130
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:19.235Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/pool.ts\n\n```typescript\n/**\n * Iterator-based concurrency pool.\n *\n * Provides a zero-dependency concurrency limiter using the shared-iterator\n * worker pattern. Workers pull from a shared iterator so exactly N tasks\n * execute concurrently, with new tasks starting as previous ones complete.\n *\n * This avoids the \"batch\" anti-pattern where `Promise.all` on chunks of N\n * tasks idles workers while waiting for the slowest task in each batch.\n *\n * @module\n */\n\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the concurrency pool.\n */\nexport interface PoolOptions {\n  /** Maximum number of concurrent workers */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Phase label for trace events (e.g., 'phase-1-files') */\n  phaseLabel?: string;\n  /** Labels for each task by index (e.g., file paths). Used in trace events. */\n  taskLabels?: string[];\n}\n\n/**\n * Result of a single task execution within the pool.\n *\n * Indexed by the task's position in the original array so callers\n * can correlate results back to their inputs.\n */\nexport interface TaskResult<T> {\n  /** Zero-based index of the task in the original array */\n  index: number;\n  /** Whether the task completed successfully */\n  success: boolean;\n  /** The resolved value (present when success is true) */\n  value?: T;\n  /** The error (present when success is false) */\n  error?: Error;\n}\n\n// ---------------------------------------------------------------------------\n// Pool implementation\n// ---------------------------------------------------------------------------\n\n/**\n * Run an array of async task factories through a concurrency-limited pool.\n *\n * Uses the shared-iterator pattern: all workers iterate over the same\n * `entries()` iterator, so each task is picked up by exactly one worker.\n * When a worker finishes a task, it immediately pulls the next one from\n * the iterator, keeping all worker slots busy.\n *\n * @typeParam T - The resolved type of each task\n * @param tasks - Array of zero-argument async functions to execute\n * @param options - Pool configuration (concurrency, failFast, tracing)\n * @param onComplete - Optional callback invoked after each task settles\n * @returns Array of results indexed by original task position (may be sparse if aborted)\n *\n * @example\n * ```typescript\n * const results = await runPool(\n *   urls.map(url => () => fetch(url).then(r => r.json())),\n *   { concurrency: 5, failFast: false },\n *   (result) => console.log(`Task ${result.index}: ${result.success}`),\n * );\n * ```\n */\nexport async function runPool<T>(\n  tasks: Array<() => Promise<T>>,\n  options: PoolOptions,\n  onComplete?: (result: TaskResult<T>) => void,\n): Promise<TaskResult<T>[]> {\n  const results: TaskResult<T>[] = [];\n\n  if (tasks.length === 0) {\n    return results;\n  }\n\n  const tracer = options.tracer;\n  const phase = options.phaseLabel ?? 'unknown';\n  const taskLabels = options.taskLabels;\n\n  // Shared abort flag -- workers check before pulling next task\n  let aborted = false;\n\n  // Active task counter for trace snapshots\n  let activeTasks = 0;\n\n  async function worker(\n    iterator: IterableIterator<[number, () => Promise<T>]>,\n    workerId: number,\n  ): Promise<void> {\n    let tasksExecuted = 0;\n\n    tracer?.emit({ type: 'worker:start', workerId, phase });\n\n    for (const [index, task] of iterator) {\n      if (aborted) break;\n\n      activeTasks++;\n      const taskStart = Date.now();\n      const label = taskLabels?.[index] ?? `task-${index}`;\n\n      tracer?.emit({\n        type: 'task:pickup',\n        workerId,\n        taskIndex: index,\n        taskLabel: label,\n        activeTasks,\n      });\n\n      try {\n        const value = await task();\n        activeTasks--;\n        tasksExecuted++;\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: true, value };\n        results[index] = result;\n        onComplete?.(result);\n      } catch (err) {\n        activeTasks--;\n        tasksExecuted++;\n\n        const error = err instanceof Error ? err : new Error(String(err));\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error.message,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: false, error };\n        results[index] = result;\n        onComplete?.(result);\n\n        if (options.failFast) {\n          aborted = true;\n          break;\n        }\n      }\n    }\n\n    tracer?.emit({ type: 'worker:end', workerId, phase, tasksExecuted });\n  }\n\n  // Spawn workers, capped at the number of available tasks\n  const effectiveConcurrency = Math.min(options.concurrency, tasks.length);\n  const iterator = tasks.entries();\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) =>\n    worker(iterator, workerId),\n  );\n\n  await Promise.allSettled(workers);\n\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Iterator-based concurrency pool implementing shared-iterator worker pattern for zero-dependency concurrent task execution with N-bounded parallelism, fail-fast abort semantics, and NDJSON trace emission.**\n\n## Exported Interface\n\n`runPool<T>(tasks, options, onComplete?)` executes array of async task factories through concurrency-limited pool. Returns `Promise<TaskResult<T>[]>` where results indexed by original task position. Parameters:\n- `tasks: Array<() => Promise<T>>` — zero-argument async factory functions\n- `options: PoolOptions` — pool configuration with required `concurrency: number`, optional `failFast?: boolean`, `tracer?: ITraceWriter`, `phaseLabel?: string`, `taskLabels?: string[]`\n- `onComplete?: (result: TaskResult<T>) => void` — callback invoked after each task settles\n\n`TaskResult<T>` discriminated by `success: boolean`, contains `index: number`, optional `value?: T` (when success true), optional `error?: Error` (when success false).\n\n`PoolOptions` configures maximum `concurrency: number`, optional `failFast?: boolean` stops pulling new tasks on first error, optional `tracer?: ITraceWriter` for concurrency debugging (imported from `'./trace.js'`), optional `phaseLabel?: string` for trace events (example: `'phase-1-files'`), optional `taskLabels?: string[]` indexed by task position for trace event labels (example: file paths).\n\n## Shared-Iterator Worker Pattern\n\nAll workers iterate over same `tasks.entries()` iterator. Each `worker()` consumes `[index, task]` pairs via `for...of` loop over shared iterator. JavaScript iterator protocol ensures each task picked up by exactly one worker (atomic `.next()` calls). When worker finishes task, immediately pulls next from iterator, keeping all worker slots busy without batch idle periods.\n\nEffective concurrency capped via `Math.min(options.concurrency, tasks.length)` to prevent spawning more workers than tasks. Workers spawned via `Array.from({ length: effectiveConcurrency }, (_, workerId) => worker(iterator, workerId))` and awaited via `Promise.allSettled(workers)`.\n\n## Abort Semantics\n\nShared `aborted` flag checked before pulling next task via `if (aborted) break` at loop start. Flag set when `options.failFast` true and task throws error via `aborted = true; break`. Workers in-flight during abort complete their current task before checking flag.\n\n## Trace Event Emission\n\nEmits trace events via `tracer?.emit()` optional chaining when `options.tracer` provided:\n- `{ type: 'worker:start', workerId, phase }` at worker spawn\n- `{ type: 'worker:end', workerId, phase, tasksExecuted }` at worker termination\n- `{ type: 'task:pickup', workerId, taskIndex, taskLabel, activeTasks }` before task execution\n- `{ type: 'task:done', workerId, taskIndex, taskLabel, durationMs, success, activeTasks, error? }` after task settles\n\nTask labels resolved via `taskLabels?.[index] ?? 'task-${index}'`. Duration computed via `Date.now() - taskStart`. Global `activeTasks` counter incremented before execution, decremented after completion for snapshot visibility.\n\n## Result Collection\n\nResults array sparse-populated via `results[index] = result` to preserve task position. Returns `TaskResult<T>[]` where index correlates back to input task array position. Sparse array may contain holes when abort occurs before all tasks execute.\n\nError coercion via `err instanceof Error ? err : new Error(String(err))` normalizes non-Error throws.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 876,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10709,
      "latencyMs": 25647,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/pool.ts",
          "sizeBytes": 5458
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:55.169Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/operations.ts\n\n```typescript\n/**\n * File operations for installer module\n *\n * Handles copying command/hook files to runtime directories,\n * verifying installations, and registering hooks in settings.json.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Options for install operations\n */\nexport interface InstallOptions {\n  /** Overwrite existing files */\n  force: boolean;\n  /** Preview mode - don't write files */\n  dryRun: boolean;\n}\n\n/**\n * Ensure directory exists for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Get the path to a bundled hook file\n *\n * Hooks are bundled in hooks/dist/ during npm prepublishOnly.\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  // Navigate from dist/installer/operations.js to hooks/dist/\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/installer/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Install files for one or all runtimes\n *\n * If runtime is 'all', installs to all supported runtimes.\n * Otherwise, installs to the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Array of installation results (one per runtime)\n */\nexport function installFiles(\n  runtime: Runtime,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => installFilesForRuntime(r, location, options));\n  }\n  return [installFilesForRuntime(runtime, location, options)];\n}\n\n/**\n * Install files for a specific runtime\n *\n * Copies command templates and hook files to the installation directory.\n * Skips existing files unless force=true.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Installation result with files created/skipped\n */\nfunction installFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = [];\n  const filesSkipped: string[] = [];\n  const errors: string[] = [];\n\n  // Install command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath) && !options.force) {\n      filesSkipped.push(fullPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath);\n    }\n  }\n\n  // Install hooks/plugins based on runtime\n  let hookRegistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Claude and Gemini: install session hooks\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath) && !options.force) {\n        filesSkipped.push(hookPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(hookPath);\n            const hookContent = readBundledHook(hookDef.filename);\n            writeFileSync(hookPath, hookContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Register hooks in settings.json\n    hookRegistered = registerHooks(basePath, runtime, options.dryRun);\n\n    // Register permissions for Claude (reduces friction for users)\n    if (runtime === 'claude') {\n      const settingsPath = path.join(basePath, 'settings.json');\n      registerPermissions(settingsPath, options.dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // OpenCode: install plugins (auto-loaded from plugins/ directory)\n    for (const pluginDef of ARE_PLUGINS) {\n      const pluginPath = path.join(basePath, 'plugins', pluginDef.destFilename);\n      if (existsSync(pluginPath) && !options.force) {\n        filesSkipped.push(pluginPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(pluginPath);\n            const pluginContent = readBundledHook(pluginDef.srcFilename);\n            writeFileSync(pluginPath, pluginContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookRegistered = true;\n        }\n      }\n    }\n  }\n\n  // Write VERSION file if files were created and not dry run\n  let versionWritten = false;\n  if (filesCreated.length > 0 && !options.dryRun) {\n    try {\n      writeVersionFile(basePath, options.dryRun);\n      versionWritten = true;\n    } catch {\n      // Non-fatal, don't add to errors\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated,\n    filesSkipped,\n    errors,\n    hookRegistered,\n    versionWritten,\n  };\n}\n\n/**\n * Verify that installed files exist\n *\n * @param files - Array of file paths to verify\n * @returns Object with success status and list of missing files\n */\nexport function verifyInstallation(files: string[]): { success: boolean; missing: string[] } {\n  const missing = files.filter((f) => !existsSync(f));\n  return {\n    success: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Session hook configuration for settings.json\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (Claude and Gemini)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n  name: string; // For Gemini format\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  // { event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' },\n  // { event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }, // Disabled - causing issues\n];\n\n/**\n * Plugin definitions for ARE (OpenCode)\n *\n * OpenCode uses a plugin system (.opencode/plugins/) instead of hooks.\n * Plugins are JS/TS modules that export async functions returning event handlers.\n */\ninterface PluginDefinition {\n  /** Source filename in hooks/dist/ (prefixed with opencode-) */\n  srcFilename: string;\n  /** Destination filename in .opencode/plugins/ */\n  destFilename: string;\n}\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // { srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }, // Disabled - causing issues\n];\n\n/**\n * Register ARE hooks in settings.json\n *\n * Registers both SessionStart (update check) and SessionEnd (auto-update) hooks.\n * Supports both Claude Code and Gemini CLI formats.\n * Merges with existing hooks, doesn't overwrite.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was added, false if all already existed\n */\nexport function registerHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  // Only for Claude and Gemini installations\n  if (runtime !== 'claude' && runtime !== 'gemini') {\n    return false;\n  }\n\n  const settingsPath = path.join(basePath, 'settings.json');\n  const runtimeDir = runtime === 'claude' ? '.claude' : '.gemini';\n\n  if (runtime === 'gemini') {\n    return registerGeminiHooks(settingsPath, runtimeDir, dryRun);\n  }\n\n  return registerClaudeHooks(settingsPath, runtimeDir, dryRun);\n}\n\n/**\n * Register ARE hooks in Claude Code settings.json format\n */\nfunction registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((event) =>\n      event.hooks?.some((h) => h.command === hookCommand),\n    );\n\n    if (!hookExists) {\n      // Define our hook (Claude format: nested hooks array)\n      const newHook: HookEvent = {\n        hooks: [\n          {\n            type: 'command',\n            command: hookCommand,\n          },\n        ],\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Permissions to auto-allow for ARE commands\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n\n/**\n * Register ARE permissions in Claude Code settings.json\n *\n * Adds bash command permissions for ARE commands to reduce friction.\n *\n * @param settingsPath - Path to settings.json\n * @param dryRun - If true, don't write changes\n * @returns true if permissions were added, false if already existed\n */\nexport function registerPermissions(settingsPath: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure permissions structure exists\n  if (!settings.permissions) {\n    settings.permissions = {};\n  }\n  if (!settings.permissions.allow) {\n    settings.permissions.allow = [];\n  }\n\n  // Add any missing ARE permissions\n  let addedAny = false;\n  for (const perm of ARE_PERMISSIONS) {\n    if (!settings.permissions.allow.includes(perm)) {\n      settings.permissions.allow.push(perm);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Register ARE hooks in Gemini CLI settings.json format\n */\nfunction registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: GeminiSettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as GeminiSettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((h) => h.command === hookCommand);\n\n    if (!hookExists) {\n      // Define our hook (Gemini format: flat object with name)\n      const newHook: GeminiHook = {\n        name: hookDef.name,\n        type: 'command',\n        command: hookCommand,\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Get package version from package.json\n *\n * @returns Version string or 'unknown' if can't read\n */\nexport function getPackageVersion(): string {\n  try {\n    // Navigate from dist/installer/operations.js to package.json\n    // In ESM, we need to use import.meta.url\n    const __filename = fileURLToPath(import.meta.url);\n    const __dirname = path.dirname(__filename);\n    // From src/installer/ go up two levels to project root\n    const packagePath = path.join(__dirname, '..', '..', 'package.json');\n    const content = readFileSync(packagePath, 'utf-8');\n    const pkg = JSON.parse(content) as { version?: string };\n    return pkg.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n/**\n * Write ARE-VERSION file to track installed version\n *\n * @param basePath - Base installation path\n * @param dryRun - If true, don't write the file\n */\nexport function writeVersionFile(basePath: string, dryRun: boolean): void {\n  if (dryRun) {\n    return;\n  }\n\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  const version = getPackageVersion();\n\n  ensureDir(versionPath);\n  writeFileSync(versionPath, version, 'utf-8');\n}\n\n/**\n * Format installation result for display\n *\n * Generates human-readable lines showing created/skipped files.\n *\n * @param result - Installation result to format\n * @returns Array of formatted lines for display\n */\nexport function formatInstallResult(result: InstallerResult): string[] {\n  const lines: string[] = [];\n\n  // Header with runtime and location\n  lines.push(`  ${result.runtime} (${result.location}):`);\n\n  // Created files\n  for (const file of result.filesCreated) {\n    lines.push(`    Created: ${file}`);\n  }\n\n  // Skipped files\n  for (const file of result.filesSkipped) {\n    lines.push(`    Skipped: ${file} (already exists)`);\n  }\n\n  // Hook registration status (Claude only)\n  if (result.hookRegistered) {\n    lines.push(`    Registered: SessionEnd hook in settings.json`);\n  }\n\n  // Summary line\n  const created = result.filesCreated.length;\n  const skipped = result.filesSkipped.length;\n  lines.push(`    ${created} files installed, ${skipped} skipped`);\n\n  return lines;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**operations.ts orchestrates file copying, settings.json hook registration, and permission configuration for IDE command/hook installation across Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Exported Functions\n\n**installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]** — installs command templates and hooks for one or all runtimes, returning array of InstallerResult objects. When runtime='all', iterates getAllRuntimes() and calls installFilesForRuntime() for each. Otherwise returns single-element array from installFilesForRuntime().\n\n**verifyInstallation(files: string[]): { success: boolean; missing: string[] }** — checks existsSync() for each file path, returns missing array of non-existent files.\n\n**registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean** — updates settings.json with SessionStart/SessionEnd hook registrations, returns true if any hook added. Routes to registerClaudeHooks() or registerGeminiHooks() based on runtime.\n\n**registerPermissions(settingsPath: string, dryRun: boolean): boolean** — adds ARE_PERMISSIONS array entries to settings.json permissions.allow for Claude Code, returns true if any permission added.\n\n**getPackageVersion(): string** — reads package.json from `__dirname/../../package.json` via fileURLToPath(import.meta.url), returns version field or 'unknown'.\n\n**writeVersionFile(basePath: string, dryRun: boolean): void** — writes getPackageVersion() result to `${basePath}/ARE-VERSION`.\n\n**formatInstallResult(result: InstallerResult): string[]** — generates human-readable lines showing created/skipped files, hook registration status, summary counts.\n\n## Exported Interfaces\n\n**InstallOptions** — `{ force: boolean; dryRun: boolean }` controls overwrite behavior and preview mode.\n\n## Internal Architecture\n\n**installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult** — main worker function handling:\n1. Resolves basePath via resolveInstallPath()\n2. Fetches templates via getTemplatesForRuntime()\n3. Iterates templates, writes to `${basePath}/${relativePath}` after extracting path component past runtime directory (e.g., `commands/are/generate.md` from `.claude/commands/are/generate.md`)\n4. For claude/gemini: copies ARE_HOOKS entries from getBundledHookPath() to `${basePath}/hooks/`, then calls registerHooks() and optionally registerPermissions() for Claude\n5. For opencode: copies ARE_PLUGINS entries from srcFilename to `${basePath}/plugins/${destFilename}`, sets hookRegistered=true\n6. Calls writeVersionFile() if filesCreated.length > 0 and not dryRun\n7. Returns InstallerResult with success, filesCreated, filesSkipped, errors, hookRegistered, versionWritten\n\n**getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)** — switches on runtime to return getClaudeTemplates(), getOpenCodeTemplates(), or getGeminiTemplates() from `../integration/templates.js`.\n\n**getBundledHookPath(hookName: string): string** — constructs path from `__dirname/../../hooks/dist/${hookName}` where `__dirname` is derived via fileURLToPath(import.meta.url). Hooks bundled during npm prepublishOnly via build-hooks.js.\n\n**readBundledHook(hookName: string): string** — reads hook content from getBundledHookPath(), throws Error if not found.\n\n**ensureDir(filePath: string): void** — calls mkdirSync(path.dirname(filePath), { recursive: true }) if directory doesn't exist.\n\n## Hook Registration Formats\n\n**registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean** — parses settings.json as SettingsJson schema, ensures settings.hooks[event] exists for each ARE_HOOKS entry, appends HookEvent with nested hooks array containing `{ type: 'command', command: 'node ${runtimeDir}/hooks/${filename}' }`. Writes JSON with 2-space indentation.\n\n**registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean** — parses settings.json as GeminiSettingsJson schema, appends flat GeminiHook objects with name field to settings.hooks[event] arrays. Command format identical to Claude: `node ${runtimeDir}/hooks/${filename}`.\n\n**SettingsJson** — Claude Code schema with `hooks?: { SessionStart?: HookEvent[]; SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[]; deny?: string[] }`. HookEvent contains `hooks: SessionHook[]` where SessionHook is `{ type: 'command'; command: string }`.\n\n**GeminiSettingsJson** — simpler schema with `hooks?: { SessionStart?: GeminiHook[]; SessionEnd?: GeminiHook[] }` where GeminiHook is `{ name: string; type: 'command'; command: string }`.\n\n## Hook and Plugin Definitions\n\n**ARE_HOOKS: HookDefinition[]** — currently empty array (both SessionStart and SessionEnd entries commented out due to \"causing issues\"). HookDefinition schema: `{ event: 'SessionStart' | 'SessionEnd'; filename: string; name: string }`. Intended entries:\n- `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`\n- `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**ARE_PLUGINS: PluginDefinition[]** — OpenCode plugin mappings with one active entry and one commented. PluginDefinition schema: `{ srcFilename: string; destFilename: string }`. Active entry maps `opencode-are-check-update.js` → `are-check-update.js`. Commented entry maps `opencode-are-session-end.js` → `are-session-end.js`.\n\n**ARE_PERMISSIONS: string[]** — array of 7 bash command patterns for Claude Code auto-approval:\n- `Bash(npx agents-reverse-engineer@latest init*)`\n- `Bash(npx agents-reverse-engineer@latest discover*)`\n- `Bash(npx agents-reverse-engineer@latest generate*)`\n- `Bash(npx agents-reverse-engineer@latest update*)`\n- `Bash(npx agents-reverse-engineer@latest clean*)`\n- `Bash(rm -f .agents-reverse-engineer/progress.log*)`\n- `Bash(sleep *)`\n\n## Error Handling\n\nFile write failures append to errors array with format `Failed to write ${fullPath}: ${err}` or `Failed to write hook ${hookPath}: ${err}`. Subsequent filesCreated filtering checks `errors.some((e) => e.includes(hookPath))` to prevent listing failed files as created. JSON parse failures in registerHooks/registerPermissions silently reset settings to empty object `{}`. Missing bundled hooks throw Error in readBundledHook(). Version file write failures non-fatal (caught without adding to errors). InstallerResult.success = errors.length === 0.\n\n## File System Conventions\n\nTemplate paths contain runtime directory prefix (e.g., `.claude/commands/are/generate.md`). Relative path extraction via `template.path.split('/').slice(1).join('/')` removes first segment to yield `commands/are/generate.md`. Full installation path: `${basePath}/${relativePath}` where basePath from resolveInstallPath(runtime, location). Hook paths: `${basePath}/hooks/${filename}`. OpenCode plugin paths: `${basePath}/plugins/${destFilename}`. Settings path: `${basePath}/settings.json`. Version file: `${basePath}/ARE-VERSION`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1933,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14627,
      "latencyMs": 52445,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/operations.ts",
          "sizeBytes": 17303
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:25.530Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/output/logger.ts\n\n```typescript\n/**\n * Terminal logger for agents-reverse\n *\n * Provides colored output.\n * Output format follows CONTEXT.md human-readable specification.\n */\n\nimport pc from 'picocolors';\n\n/**\n * Logger interface for CLI output.\n */\nexport interface Logger {\n  /** Log an informational message */\n  info(message: string): void;\n\n  /** Log a discovered file */\n  file(path: string): void;\n\n  /** Log an excluded file with reason */\n  excluded(path: string, reason: string, filter: string): void;\n\n  /** Log discovery summary */\n  summary(included: number, excluded: number): void;\n\n  /** Log a warning message */\n  warn(message: string): void;\n\n  /** Log an error message */\n  error(message: string): void;\n}\n\n/**\n * Options for creating a logger instance.\n */\nexport interface LoggerOptions {\n  /**\n   * Use colors in terminal output.\n   * @default true\n   */\n  colors: boolean;\n}\n\n/**\n * Color functions type - either picocolors or identity functions\n */\ninterface ColorFunctions {\n  green: (s: string) => string;\n  dim: (s: string) => string;\n  red: (s: string) => string;\n  bold: (s: string) => string;\n  yellow: (s: string) => string;\n}\n\n/**\n * Identity function for no-color mode\n */\nconst identity = (s: string): string => s;\n\n/**\n * No-color formatter that returns strings unchanged\n */\nconst noColor: ColorFunctions = {\n  green: identity,\n  dim: identity,\n  red: identity,\n  bold: identity,\n  yellow: identity,\n};\n\n/**\n * Create a logger instance with the given options.\n *\n * Output format per CONTEXT.md (human-readable):\n * - file: green \"  +\" prefix + relative path\n * - excluded: dim \"  -\" prefix + path + reason (when shown)\n * - summary: bold count + dim excluded count\n * - warn: yellow \"Warning:\" prefix\n * - error: red \"Error:\" prefix\n *\n * @param options - Logger configuration\n * @returns Logger instance\n *\n * @example\n * ```typescript\n * const log = createLogger({ colors: true });\n *\n * log.file('src/index.ts');\n * log.summary(42, 10);\n * ```\n */\nexport function createLogger(options: LoggerOptions): Logger {\n  const c: ColorFunctions = options.colors ? pc : noColor;\n\n  return {\n    info(message: string): void {\n      console.log(message);\n    },\n\n    file(path: string): void {\n      console.log(c.green('  +') + ' ' + path);\n    },\n\n    excluded(path: string, reason: string, filter: string): void {\n      console.log(c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`));\n    },\n\n    summary(included: number, excluded: number): void {\n      console.log(\n        c.bold(`\\nDiscovered ${included} files`) +\n          c.dim(` (${excluded} excluded)`)\n      );\n    },\n\n    warn(message: string): void {\n      console.warn(c.yellow('Warning: ') + message);\n    },\n\n    error(message: string): void {\n      console.error(c.red('Error: ') + message);\n    },\n  };\n}\n\n/**\n * Create a silent logger that produces no output.\n *\n * Useful for testing or programmatic usage.\n *\n * @returns Logger instance with all no-op methods\n */\nexport function createSilentLogger(): Logger {\n  const noop = (): void => {};\n  return {\n    info: noop,\n    file: noop,\n    excluded: noop,\n    summary: noop,\n    warn: noop,\n    error: noop,\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Terminal logging interface with picocolors-based output formatting for CLI progress reporting, file discovery, and error messages.**\n\n## Exported Types\n\n`Logger` interface defines six output methods: `info(message: string): void` for general messages, `file(path: string): void` for discovered files, `excluded(path: string, reason: string, filter: string): void` for skipped files with justification, `summary(included: number, excluded: number): void` for discovery totals, `warn(message: string): void` for warnings, `error(message: string): void` for errors.\n\n`LoggerOptions` interface specifies `colors: boolean` configuration (defaults to `true` per JSDoc).\n\n`ColorFunctions` internal interface wraps five picocolors methods: `green`, `dim`, `red`, `bold`, `yellow`, each accepting `string` and returning `string`.\n\n## Factory Functions\n\n`createLogger(options: LoggerOptions): Logger` returns configured logger instance. When `options.colors` is `true`, assigns `picocolors` to `ColorFunctions` variable `c`; when `false`, assigns `noColor` identity function wrapper. Implements output format specification:\n- `file`: renders `c.green('  +') + ' ' + path`\n- `excluded`: renders `c.dim('  -') + ' ' + path + c.dim(\\` (\\${reason}: \\${filter})\\`)`\n- `summary`: renders `c.bold(\\`\\\\nDiscovered \\${included} files\\`) + c.dim(\\` (\\${excluded} excluded)\\`)`\n- `warn`: renders `c.yellow('Warning: ') + message` via `console.warn()`\n- `error`: renders `c.red('Error: ') + message` via `console.error()`\n\n`createSilentLogger(): Logger` returns no-op logger for testing. Creates `noop = (): void => {}` and assigns to all six Logger methods.\n\n## Color Mode Abstraction\n\n`noColor` constant implements `ColorFunctions` with `identity` function for all five color methods. `identity` function returns input string unchanged: `(s: string): string => s`.\n\nConditional logic `options.colors ? pc : noColor` enables compile-time type safety for color-enabled/disabled paths without runtime string inspection.\n\n## Output Format Specification\n\nReferences CONTEXT.md for human-readable format rules:\n- File discovery prefixes: `  +` (green) for included, `  -` (dim) for excluded\n- Summary format: bold count + dim excluded count with leading newline\n- Error/warning prefixes: `Error:` (red), `Warning:` (yellow)\n\n## Integration Points\n\nConsumed by CLI commands (discover, generate, update) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging and `src/orchestration/progress.ts` for phase execution updates.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 668,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10181,
      "latencyMs": 26919,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/output/logger.ts",
          "sizeBytes": 3144
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:33.272Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/types.ts\n\n```typescript\n/**\n * Shared types for quality analysis: inconsistency detection and density measurement.\n */\n\n/** Severity level for detected inconsistencies. */\nexport type InconsistencySeverity = 'info' | 'warning' | 'error';\n\n/**\n * Inconsistency between .sum documentation and source code.\n * Detected by comparing exported symbols against .sum content.\n */\nexport interface CodeDocInconsistency {\n  type: 'code-vs-doc';\n  severity: InconsistencySeverity;\n  /** Path to the source file */\n  filePath: string;\n  /** Path to the corresponding .sum file */\n  sumPath: string;\n  description: string;\n  details: {\n    /** Symbols exported in source but not mentioned in .sum */\n    missingFromDoc: string[];\n    /** Symbols mentioned in .sum but not found in source */\n    missingFromCode: string[];\n    /** Purpose statement that contradicts observable behavior */\n    purposeMismatch?: string;\n  };\n}\n\n/**\n * Inconsistency across multiple source files.\n * Detected by comparing patterns and exports across files.\n */\nexport interface CodeCodeInconsistency {\n  type: 'code-vs-code';\n  severity: InconsistencySeverity;\n  /** Paths to the conflicting files */\n  files: string[];\n  description: string;\n  /** Pattern that was detected (e.g. 'duplicate-export') */\n  pattern: string;\n}\n\n/**\n * Path reference in generated documentation that doesn't resolve to a real file/directory.\n */\nexport interface PhantomPathInconsistency {\n  type: 'phantom-path';\n  severity: InconsistencySeverity;\n  /** Path to the AGENTS.md file containing the phantom reference */\n  agentsMdPath: string;\n  description: string;\n  details: {\n    /** The phantom path as written in the document */\n    referencedPath: string;\n    /** What it was resolved against (project root or AGENTS.md location) */\n    resolvedTo: string;\n    /** The line of text containing the phantom reference */\n    context: string;\n  };\n}\n\n/** Union of all inconsistency types. */\nexport type Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency;\n\n/** Structured report produced by inconsistency analysis. */\nexport interface InconsistencyReport {\n  /** Run metadata */\n  metadata: {\n    timestamp: string;\n    projectRoot: string;\n    filesChecked: number;\n    durationMs: number;\n  };\n  /** All detected inconsistencies */\n  issues: Inconsistency[];\n  /** Summary counts */\n  summary: {\n    total: number;\n    codeVsDoc: number;\n    codeVsCode: number;\n    phantomPaths: number;\n    errors: number;\n    warnings: number;\n    info: number;\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines discriminated union types for quality analysis inconsistency detection and reporting across code-vs-doc, code-vs-code, and phantom-path validation domains.**\n\n## Exported Types\n\n**InconsistencySeverity**: String literal union `'info' | 'warning' | 'error'` categorizing inconsistency impact levels.\n\n**CodeDocInconsistency**: Interface with `type: 'code-vs-doc'` discriminant representing mismatches between source code exports and `.sum` documentation content. Contains:\n- `severity: InconsistencySeverity`\n- `filePath: string` — source file path\n- `sumPath: string` — corresponding `.sum` file path\n- `description: string`\n- `details.missingFromDoc: string[]` — exported symbols absent from `.sum` summary\n- `details.missingFromCode: string[]` — symbols mentioned in `.sum` but not found in source\n- `details.purposeMismatch?: string` — purpose statement contradicting observable behavior\n\n**CodeCodeInconsistency**: Interface with `type: 'code-vs-code'` discriminant representing conflicts across multiple source files (e.g., duplicate exports). Contains:\n- `severity: InconsistencySeverity`\n- `files: string[]` — paths to conflicting files\n- `description: string`\n- `pattern: string` — detected pattern identifier (e.g., `'duplicate-export'`)\n\n**PhantomPathInconsistency**: Interface with `type: 'phantom-path'` discriminant representing unresolvable path references in `AGENTS.md` files. Contains:\n- `severity: InconsistencySeverity`\n- `agentsMdPath: string` — path to `AGENTS.md` containing phantom reference\n- `description: string`\n- `details.referencedPath: string` — phantom path as written in document\n- `details.resolvedTo: string` — resolution attempt context (project root or `AGENTS.md` location)\n- `details.context: string` — line of text containing phantom reference\n\n**Inconsistency**: Discriminated union type `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` enabling exhaustive pattern matching via `type` discriminant.\n\n**InconsistencyReport**: Structured analysis output interface containing:\n- `metadata.timestamp: string` — ISO 8601 execution timestamp\n- `metadata.projectRoot: string` — absolute project root path\n- `metadata.filesChecked: number` — count of analyzed files\n- `metadata.durationMs: number` — analysis duration in milliseconds\n- `issues: Inconsistency[]` — array of detected inconsistencies\n- `summary.total: number` — total inconsistency count\n- `summary.codeVsDoc: number` — code-vs-doc inconsistency count\n- `summary.codeVsCode: number` — code-vs-code inconsistency count\n- `summary.phantomPaths: number` — phantom path inconsistency count\n- `summary.errors: number` — error-severity count\n- `summary.warnings: number` — warning-severity count\n- `summary.info: number` — info-severity count\n\n## Integration Points\n\nConsumed by validators in `src/quality/inconsistency/` (`code-vs-doc.ts`, `code-vs-code.ts`) and `src/quality/phantom-paths/validator.ts` for constructing typed inconsistency objects. `InconsistencyReport` returned by `src/quality/inconsistency/reporter.ts` aggregation logic and logged via `src/orchestration/progress.ts` streaming reporter.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 857,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9923,
      "latencyMs": 20213,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/types.ts",
          "sizeBytes": 2518
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:25.323Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/types.ts\n\n```typescript\n/**\n * Shared types for the orchestration module.\n *\n * These types are used across the concurrency pool, progress reporter,\n * and command runner to represent task results, run summaries, progress\n * events, and command options.\n *\n * @module\n */\n\nimport type { InconsistencyReport } from '../quality/index.js';\nimport type { ProgressLog } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// File task result\n// ---------------------------------------------------------------------------\n\n/**\n * Result of processing a single file through AI analysis.\n *\n * Produced by the command runner for each file task, carrying token\n * counts and timing data needed for the run summary.\n */\nexport interface FileTaskResult {\n  /** Relative path to the source file */\n  path: string;\n  /** Whether the AI call succeeded */\n  success: boolean;\n  /** Number of input tokens consumed (non-cached) */\n  tokensIn: number;\n  /** Number of output tokens generated */\n  tokensOut: number;\n  /** Number of cache read input tokens */\n  cacheReadTokens: number;\n  /** Number of cache creation input tokens */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Model identifier used for this call */\n  model: string;\n  /** Error message if the call failed */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Run summary\n// ---------------------------------------------------------------------------\n\n/**\n * Aggregated summary of a command run.\n *\n * Produced at the end of a generate or update command execution,\n * combining per-file results into totals for display and telemetry.\n */\nexport interface RunSummary {\n  /** agents-reverse-engineer version that produced this run */\n  version: string;\n  /** Number of files that were successfully processed */\n  filesProcessed: number;\n  /** Number of files that failed processing */\n  filesFailed: number;\n  /** Number of files that were skipped (e.g., dry-run) */\n  filesSkipped: number;\n  /** Total number of AI calls made */\n  totalCalls: number;\n  /** Sum of input tokens across all calls */\n  totalInputTokens: number;\n  /** Sum of output tokens across all calls */\n  totalOutputTokens: number;\n  /** Sum of cache read tokens across all calls */\n  totalCacheReadTokens: number;\n  /** Sum of cache creation tokens across all calls */\n  totalCacheCreationTokens: number;\n  /** Total wall-clock duration in milliseconds */\n  totalDurationMs: number;\n  /** Number of errors encountered */\n  errorCount: number;\n  /** Number of retries that occurred */\n  retryCount: number;\n  /** Total file reads across all calls */\n  totalFilesRead: number;\n  /** Unique files read (deduped by path) */\n  uniqueFilesRead: number;\n  /** Number of code-vs-doc inconsistencies detected */\n  inconsistenciesCodeVsDoc?: number;\n  /** Number of code-vs-code inconsistencies detected */\n  inconsistenciesCodeVsCode?: number;\n  /** Number of phantom path references detected in AGENTS.md files */\n  phantomPaths?: number;\n  /** Full inconsistency report (undefined if no checks ran) */\n  inconsistencyReport?: InconsistencyReport;\n}\n\n// ---------------------------------------------------------------------------\n// Progress events\n// ---------------------------------------------------------------------------\n\n/**\n * Event emitted by the command runner to the progress reporter.\n *\n * Each event type carries different optional fields:\n * - `start`: filePath, index, total\n * - `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n * - `error`: filePath, index, total, error\n * - `dir-done`: filePath (directory path)\n * - `root-done`: filePath (root document path)\n */\nexport interface ProgressEvent {\n  /** Event type */\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  /** File or directory path */\n  filePath: string;\n  /** Zero-based index of this task in the current phase */\n  index: number;\n  /** Total number of tasks in the current phase */\n  total: number;\n  /** Wall-clock duration in milliseconds (for 'done' events) */\n  durationMs?: number;\n  /** Input tokens consumed (for 'done' events) */\n  tokensIn?: number;\n  /** Output tokens generated (for 'done' events) */\n  tokensOut?: number;\n  /** Model identifier (for 'done' events) */\n  model?: string;\n  /** Error message (for 'error' events) */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Command run options\n// ---------------------------------------------------------------------------\n\n/**\n * Options that control how commands execute.\n *\n * These are populated from a combination of config file defaults\n * and CLI flag overrides.\n */\nexport interface CommandRunOptions {\n  /** Maximum number of concurrent AI calls */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Show debug information (exact prompts sent) */\n  debug?: boolean;\n  /** List files that would be processed without executing */\n  dryRun?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Progress log for file-based output mirroring (tail -f monitoring) */\n  progressLog?: ProgressLog;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces for orchestration module communication: task results, run summaries, progress events, and command options that flow between worker pools, progress reporters, and command runners.**\n\n## Exported Interfaces\n\n**FileTaskResult** represents the outcome of processing a single file through AI analysis:\n- `path: string` — relative path to source file\n- `success: boolean` — whether AI call succeeded\n- `tokensIn: number` — input tokens consumed (non-cached)\n- `tokensOut: number` — output tokens generated\n- `cacheReadTokens: number` — cache read input tokens\n- `cacheCreationTokens: number` — cache creation input tokens\n- `durationMs: number` — wall-clock duration in milliseconds\n- `model: string` — model identifier used for this call\n- `error?: string` — error message if call failed\n\n**RunSummary** aggregates command execution metrics for telemetry and display:\n- `version: string` — agents-reverse-engineer version\n- `filesProcessed: number` — successfully processed file count\n- `filesFailed: number` — failed file count\n- `filesSkipped: number` — skipped file count (e.g., dry-run)\n- `totalCalls: number` — AI call count\n- `totalInputTokens: number` — sum of input tokens across calls\n- `totalOutputTokens: number` — sum of output tokens across calls\n- `totalCacheReadTokens: number` — sum of cache read tokens\n- `totalCacheCreationTokens: number` — sum of cache creation tokens\n- `totalDurationMs: number` — total wall-clock duration\n- `errorCount: number` — error count\n- `retryCount: number` — retry count\n- `totalFilesRead: number` — total file reads across calls\n- `uniqueFilesRead: number` — unique files read (deduped by path)\n- `inconsistenciesCodeVsDoc?: number` — code-vs-doc inconsistency count\n- `inconsistenciesCodeVsCode?: number` — code-vs-code inconsistency count\n- `phantomPaths?: number` — phantom path reference count in AGENTS.md files\n- `inconsistencyReport?: InconsistencyReport` — full report (undefined if checks didn't run)\n\n**ProgressEvent** emitted by command runners to progress reporters with discriminated `type` field:\n- `type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done'` — event type\n- `filePath: string` — file or directory path\n- `index: number` — zero-based task index in current phase\n- `total: number` — total tasks in current phase\n- `durationMs?: number` — wall-clock duration (present on `'done'` events)\n- `tokensIn?: number` — input tokens consumed (present on `'done'` events)\n- `tokensOut?: number` — output tokens generated (present on `'done'` events)\n- `model?: string` — model identifier (present on `'done'` events)\n- `error?: string` — error message (present on `'error'` events)\n\n**CommandRunOptions** controls command execution behavior populated from config defaults and CLI flags:\n- `concurrency: number` — maximum concurrent AI calls\n- `failFast?: boolean` — stop pulling tasks on first error\n- `debug?: boolean` — show exact prompts sent\n- `dryRun?: boolean` — list files without executing\n- `tracer?: ITraceWriter` — trace writer for NDJSON event emission (no-op when tracing disabled)\n- `progressLog?: ProgressLog` — file-based output mirror for `tail -f` monitoring\n\n## Import Dependencies\n\nImports `InconsistencyReport` from `../quality/index.js` for quality validation results embedded in RunSummary.\n\nImports `ProgressLog` from `./progress.js` for file-based progress logging.\n\nImports `ITraceWriter` from `./trace.js` for NDJSON trace event serialization.\n\n## Event Type Semantics\n\nProgressEvent carries phase-specific optional fields:\n- `'start'` events populate `filePath`, `index`, `total`\n- `'done'` events populate `filePath`, `index`, `total`, `durationMs`, `tokensIn`, `tokensOut`, `model`\n- `'error'` events populate `filePath`, `index`, `total`, `error`\n- `'dir-done'` events populate `filePath` with directory path\n- `'root-done'` events populate `filePath` with root document path",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1094,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10570,
      "latencyMs": 30596,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/types.ts",
          "sizeBytes": 5350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:28:57.777Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/uninstall.ts\n\n```typescript\n/**\n * Uninstall module for agents-reverse-engineer installer\n *\n * Handles removing installed command files, hooks, and hook registrations.\n * Mirrors the installation logic in operations.ts for clean reversal.\n */\n\nimport { existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes, getRuntimePaths } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Session hook configuration for settings.json (matches operations.ts)\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (must match operations.ts)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  { event: 'SessionStart', filename: 'are-check-update.js' },\n  { event: 'SessionEnd', filename: 'are-session-end.js' },\n];\n\n/**\n * Plugin definitions for ARE OpenCode (must match operations.ts)\n */\nconst ARE_PLUGIN_FILENAMES = [\n  'are-check-update.js',\n  'are-session-end.js',\n];\n\n/**\n * Permissions to remove during uninstall (must match operations.ts)\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n];\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Uninstall files for one or all runtimes\n *\n * If runtime is 'all', uninstalls from all supported runtimes.\n * Otherwise, uninstalls from the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Array of uninstallation results (one per runtime)\n */\nexport function uninstallFiles(\n  runtime: Runtime,\n  location: Location,\n  dryRun: boolean = false,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => uninstallFilesForRuntime(r, location, dryRun));\n  }\n  return [uninstallFilesForRuntime(runtime, location, dryRun)];\n}\n\n/**\n * Uninstall files for a specific runtime\n *\n * Removes command templates, hook files, and VERSION file from the installation directory.\n * Also unregisters hooks from settings.json for Claude global installs.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Uninstallation result with files deleted\n */\nfunction uninstallFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  dryRun: boolean,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = []; // In uninstall context, this tracks files deleted\n  const filesSkipped: string[] = []; // Files that didn't exist\n  const errors: string[] = [];\n\n  // Remove command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(fullPath);\n        } catch (err) {\n          errors.push(`Failed to delete ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath); // Track as \"deleted\"\n    } else {\n      filesSkipped.push(fullPath); // File didn't exist\n    }\n  }\n\n  // Remove hooks/plugins based on runtime\n  let hookUnregistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Remove all ARE hook files\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(hookPath);\n          } catch (err) {\n            errors.push(`Failed to delete hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Unregister hooks from settings.json\n    hookUnregistered = unregisterHooks(basePath, runtime, dryRun);\n\n    // Unregister permissions from settings.json (Claude only)\n    if (runtime === 'claude') {\n      unregisterPermissions(basePath, dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // Remove all ARE plugin files\n    for (const pluginFilename of ARE_PLUGIN_FILENAMES) {\n      const pluginPath = path.join(basePath, 'plugins', pluginFilename);\n      if (existsSync(pluginPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(pluginPath);\n          } catch (err) {\n            errors.push(`Failed to delete plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookUnregistered = true;\n        }\n      }\n    }\n  }\n\n  // Remove ARE-VERSION file if exists\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  if (existsSync(versionPath)) {\n    if (!dryRun) {\n      try {\n        unlinkSync(versionPath);\n      } catch (err) {\n        errors.push(`Failed to delete ARE-VERSION: ${err}`);\n      }\n    }\n    if (!errors.some((e) => e.includes('ARE-VERSION'))) {\n      filesCreated.push(versionPath);\n    }\n  }\n\n  // Try to clean up empty directories\n  if (!dryRun) {\n    if (runtime === 'claude') {\n      // Claude uses skills format: clean up skills/are-* directories\n      const skillsDir = path.join(basePath, 'skills');\n      cleanupAreSkillDirs(skillsDir);\n      cleanupEmptyDirs(skillsDir);\n    } else if (runtime === 'gemini') {\n      // Gemini uses flat commands/ directory for TOML files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n      // Clean up legacy files from old installations\n      cleanupLegacyGeminiFiles(commandsDir);\n    } else {\n      // OpenCode uses commands format with flat .md files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n    }\n\n    // Clean up hooks/plugins directory if empty\n    if (runtime === 'claude' || runtime === 'gemini') {\n      const hooksDir = path.join(basePath, 'hooks');\n      cleanupEmptyDirs(hooksDir);\n    } else if (runtime === 'opencode') {\n      const pluginsDir = path.join(basePath, 'plugins');\n      cleanupEmptyDirs(pluginsDir);\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated, // Actually files deleted in uninstall context\n    filesSkipped, // Files that didn't exist\n    errors,\n    hookRegistered: hookUnregistered, // Repurpose: true if hook was unregistered\n  };\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Unregister ARE hooks from settings.json\n *\n * Removes all ARE hook entries from SessionStart and SessionEnd arrays.\n * Cleans up empty hooks structures. Handles both old and new hook paths.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was removed, false if none found\n */\nexport function unregisterHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  if (runtime === 'gemini') {\n    return unregisterGeminiHooks(basePath, dryRun);\n  }\n  return unregisterClaudeHooks(basePath, dryRun);\n}\n\n/**\n * Build hook command patterns for matching (includes legacy paths)\n */\nfunction getHookPatterns(runtimeDir: string): string[] {\n  const patterns: string[] = [];\n  for (const hookDef of ARE_HOOKS) {\n    // Current path format\n    patterns.push(`node ${runtimeDir}/hooks/${hookDef.filename}`);\n    // Legacy path format\n    patterns.push(`node hooks/${hookDef.filename}`);\n  }\n  return patterns;\n}\n\n/**\n * Unregister ARE hooks from Claude Code settings.json\n */\nfunction unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.claude');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (event) => !event.hooks?.some((h) => hookPatterns.includes(h.command)),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE permissions from Claude Code settings.json\n *\n * Removes all ARE-related bash command permissions from the allow list.\n *\n * @param basePath - Base installation path (e.g., ~/.claude)\n * @param dryRun - If true, don't write changes\n * @returns true if any permissions were removed, false if none found\n */\nexport function unregisterPermissions(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  // Check if permissions.allow exists\n  if (!settings.permissions?.allow) {\n    return false;\n  }\n\n  const originalLength = settings.permissions.allow.length;\n\n  // Remove all ARE permissions\n  settings.permissions.allow = settings.permissions.allow.filter(\n    (perm) => !ARE_PERMISSIONS.includes(perm),\n  );\n\n  // Check if we actually removed something\n  if (settings.permissions.allow.length === originalLength) {\n    return false;\n  }\n\n  // Clean up empty structures\n  if (settings.permissions.allow.length === 0) {\n    delete settings.permissions.allow;\n  }\n\n  if (settings.permissions && Object.keys(settings.permissions).length === 0) {\n    delete settings.permissions;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE hooks from Gemini CLI settings.json\n */\nfunction unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: GeminiSettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as GeminiSettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.gemini');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (h) => !hookPatterns.includes(h.command),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Clean up ARE skill directories\n *\n * Removes all empty are-* skill directories from the skills folder.\n *\n * @param skillsDir - Path to the skills directory\n */\nfunction cleanupAreSkillDirs(skillsDir: string): void {\n  try {\n    if (!existsSync(skillsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(skillsDir);\n    for (const entry of entries) {\n      // Only clean up are-* directories\n      if (entry.startsWith('are-')) {\n        const skillDir = path.join(skillsDir, entry);\n        cleanupEmptyDirs(skillDir);\n      }\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Clean up empty directories recursively\n *\n * Removes a directory if it's empty, then tries parent directories.\n * Stops when hitting a non-empty directory or the runtime root.\n *\n * @param dirPath - Directory path to check and potentially remove\n */\nfunction cleanupEmptyDirs(dirPath: string): void {\n  try {\n    if (!existsSync(dirPath)) {\n      return;\n    }\n\n    const entries = readdirSync(dirPath);\n    if (entries.length === 0) {\n      rmdirSync(dirPath);\n\n      // Try parent directory (but don't go above runtime root)\n      const parent = path.dirname(dirPath);\n      // Stop at common runtime roots (.claude, .opencode, .gemini)\n      const baseName = path.basename(parent);\n      if (\n        baseName !== '.claude' &&\n        baseName !== '.opencode' &&\n        baseName !== '.gemini' &&\n        baseName !== '.config'\n      ) {\n        cleanupEmptyDirs(parent);\n      }\n    }\n  } catch {\n    // Ignore errors - directory might be in use or we don't have permissions\n  }\n}\n\n/**\n * Clean up legacy Gemini files from old installations\n *\n * Removes:\n * - Old are-*.md files from .gemini/commands/ (pre-TOML format)\n * - Old .toml files from .gemini/commands/are/ (pre-flat structure)\n *\n * @param commandsDir - Path to the commands directory\n */\nfunction cleanupLegacyGeminiFiles(commandsDir: string): void {\n  try {\n    if (!existsSync(commandsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(commandsDir);\n    for (const entry of entries) {\n      // Remove legacy are-*.md files\n      if (entry.startsWith('are-') && entry.endsWith('.md')) {\n        const filePath = path.join(commandsDir, entry);\n        try {\n          unlinkSync(filePath);\n        } catch {\n          // Ignore errors\n        }\n      }\n    }\n\n    // Remove legacy nested are/ directory (old /are:* namespace structure)\n    const areDir = path.join(commandsDir, 'are');\n    if (existsSync(areDir)) {\n      const areEntries = readdirSync(areDir);\n      for (const entry of areEntries) {\n        if (entry.endsWith('.toml')) {\n          try {\n            unlinkSync(path.join(areDir, entry));\n          } catch {\n            // Ignore errors\n          }\n        }\n      }\n      cleanupEmptyDirs(areDir);\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Configuration directory name (matches config/loader.ts)\n */\nconst CONFIG_DIR = '.agents-reverse-engineer';\n\n/**\n * Delete the .agents-reverse-engineer configuration folder\n *\n * Only applicable for local installations. Removes the entire folder\n * including configuration files and generation plans.\n *\n * @param location - Installation location (only 'local' triggers deletion)\n * @param dryRun - If true, don't actually delete\n * @returns true if folder was deleted, false if not found or not local\n */\nexport function deleteConfigFolder(location: Location, dryRun: boolean): boolean {\n  // Only delete for local installations\n  if (location !== 'local') {\n    return false;\n  }\n\n  const configPath = path.join(process.cwd(), CONFIG_DIR);\n\n  if (!existsSync(configPath)) {\n    return false;\n  }\n\n  if (!dryRun) {\n    try {\n      rmSync(configPath, { recursive: true, force: true });\n    } catch {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**uninstall.ts reverses installer operations by removing ARE command templates, hook files, VERSION markers, and settings.json registrations for Claude/Gemini/OpenCode runtimes with directory cleanup and legacy artifact removal.**\n\n## Exported Functions\n\n**uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]** removes all ARE artifacts from specified runtime (or all runtimes if `runtime === 'all'`), returns array of `InstallerResult` objects where `filesCreated` repurposed to track deleted files and `hookRegistered` repurposed to indicate hook unregistration success.\n\n**unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean** removes ARE hook entries from `settings.json` at `basePath` by dispatching to `unregisterClaudeHooks()` or `unregisterGeminiHooks()` based on runtime, returns true if any hooks removed.\n\n**unregisterPermissions(basePath: string, dryRun: boolean): boolean** removes all `ARE_PERMISSIONS` entries from Claude Code `settings.json` permissions.allow array, cleans up empty permission structures, returns true if any permissions removed.\n\n**deleteConfigFolder(location: Location, dryRun: boolean): boolean** deletes `.agents-reverse-engineer` directory via `rmSync({ recursive: true, force: true })` only when `location === 'local'`, returns true if folder existed and was deleted.\n\n## Hook and Permission Constants\n\n**ARE_HOOKS: HookDefinition[]** defines two hook registrations: `{ event: 'SessionStart', filename: 'are-check-update.js' }` and `{ event: 'SessionEnd', filename: 'are-session-end.js' }`, must match `operations.ts` definitions.\n\n**ARE_PLUGIN_FILENAMES: string[]** lists OpenCode plugin files: `['are-check-update.js', 'are-session-end.js']`.\n\n**ARE_PERMISSIONS: string[]** contains five Bash permission patterns for ARE commands (init/discover/generate/update/clean) formatted as `'Bash(npx agents-reverse-engineer@latest <command>*)'`.\n\n**CONFIG_DIR: string** equals `'.agents-reverse-engineer'`, matches `config/loader.ts` constant.\n\n## Settings.json Schema Types\n\n**SettingsJson** interface models Claude Code settings structure with `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`, and index signature for other fields.\n\n**HookEvent** wraps `hooks: SessionHook[]` array, where **SessionHook** defines `{ type: 'command', command: string }`.\n\n**GeminiSettingsJson** uses simpler hook format with `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`, where **GeminiHook** includes `name: string` field alongside `type` and `command`.\n\n**HookDefinition** specifies `event: 'SessionStart' | 'SessionEnd'` and `filename: string` for ARE hook configuration.\n\n## Uninstallation Logic\n\n**uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult** orchestrates removal sequence: fetches templates via `getTemplatesForRuntime()`, deletes command files by joining `basePath` with template path stripped of runtime prefix, removes hook files from `basePath/hooks/` for Claude/Gemini or plugin files from `basePath/plugins/` for OpenCode, calls `unregisterHooks()` and `unregisterPermissions()` for Claude global installs, deletes `ARE-VERSION` file, invokes directory cleanup helpers unless `dryRun === true`.\n\n**getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)** switches on runtime to return `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` from `integration/templates.ts`.\n\n**getHookPatterns(runtimeDir: string): string[]** builds array of hook command strings by combining each `ARE_HOOKS` filename with current path format `node ${runtimeDir}/hooks/${filename}` and legacy format `node hooks/${filename}` for backward compatibility.\n\n## Hook Unregistration\n\n**unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean** loads `settings.json` from `basePath`, filters `hooks.SessionStart` and `hooks.SessionEnd` arrays to remove entries where any hook command matches `getHookPatterns('.claude')`, deletes empty event arrays and hooks object, writes updated JSON with 2-space indent via `JSON.stringify(settings, null, 2)` unless dry run, returns true if any hooks removed.\n\n**unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean** mirrors Claude logic but filters `GeminiHook[]` arrays directly by matching `h.command` against `getHookPatterns('.gemini')`, handles simpler schema without nested `event.hooks` structure.\n\n## Directory Cleanup\n\n**cleanupAreSkillDirs(skillsDir: string): void** iterates entries in `skillsDir`, calls `cleanupEmptyDirs()` recursively on any directory name starting with `'are-'`, used for Claude skills format.\n\n**cleanupEmptyDirs(dirPath: string): void** recursively removes empty directories via `rmdirSync()`, walks up parent chain via `path.dirname()` until hitting non-empty directory or runtime root (`.claude`, `.opencode`, `.gemini`, `.config`), ignores errors from missing directories or permission issues.\n\n**cleanupLegacyGeminiFiles(commandsDir: string): void** removes legacy `are-*.md` files from flat commands directory (pre-TOML format), deletes old `.toml` files from nested `commands/are/` subdirectory (pre-flat structure), calls `cleanupEmptyDirs()` on are/ subdirectory after file deletion.\n\n## Integration Points\n\nUses `resolveInstallPath()`, `getAllRuntimes()`, `getRuntimePaths()` from `./paths.js` for path resolution. Fetches command templates via `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `../integration/templates.js`. Returns `InstallerResult` type with runtime/location/filesCreated/filesSkipped/errors/hookRegistered fields defined in `./types.js`. Deletes config folder matching `CONFIG_DIR` constant from `config/loader.ts`. Must maintain symmetry with `operations.ts` hook/permission definitions to ensure complete uninstall.\n\n## Error Handling\n\nAll filesystem operations wrapped in try-catch blocks that push error messages to `errors[]` array on failure. Missing `settings.json` or JSON parse failures return false from unregister functions. Directory cleanup helpers silently ignore errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` options to handle missing paths gracefully. Dry run mode skips all write operations (`unlinkSync`, `writeFileSync`, `rmSync`) while still performing validation and tracking.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1727,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14730,
      "latencyMs": 59488,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/uninstall.ts",
          "sizeBytes": 17689
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:24.626Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/runner.ts.annex.md\n\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: runner.ts\n\nReproduction-critical source content from `runner.ts`.\nReferenced by `runner.ts.sum`.\n\n```\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Annex document containing verbatim source code of `runner.ts` for reproduction of the three-phase AI-driven documentation generation pipeline.**\n\n## Purpose\n\nThis `.annex.md` file stores the complete source code of `runner.ts` to preserve reproduction-critical implementation details that cannot be adequately summarized in `runner.ts.sum`. The annex enables exact reproduction of concurrency orchestration patterns, quality validation logic, trace emission sequences, and subprocess management protocols.\n\n## Structure\n\nThe annex contains a single fenced code block with the entire TypeScript source of `runner.ts` (approximately 750 lines). This includes:\n\n- `CommandRunner` class definition with `executeGenerate()` and `executeUpdate()` methods\n- Five-phase execution flow: pre-phase-1-cache, phase-1-files, post-phase-1-quality, phase-2-dirs-depth-N, phase-3-root\n- Worker pool orchestration via `runPool()` with phase-specific concurrency and task grouping\n- `ITraceWriter` integration for NDJSON trace emission at phase/task boundaries\n- Quality validation pipelines: code-vs-doc (old/new), code-vs-code, phantom-paths\n- Helper functions: `stripPreamble()`, `extractPurpose()` with preamble prefix array\n\n## Referenced By\n\nThe summary file `runner.ts.sum` references this annex document to avoid inline reproduction of multi-phase orchestration logic, retry patterns, cache management strategies, and quality validation implementations.\n\n## Behavioral Contracts Preserved\n\n- **Preamble detection prefixes**: `PREAMBLE_PREFIXES` constant array with 12 conversational markers: `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`\n- **Phase labels**: Exact phase identifiers for trace events: `'pre-phase-1-cache'`, `'phase-1-files'`, `'post-phase-1-quality'`, `'phase-2-dirs-depth-N'`, `'phase-3-root'`, `'update-phase-1-files'`, `'update-post-phase-1-quality'`\n- **Concurrency limits**: Pre-phase-1 cache reads throttled to 20, quality checks throttled to 10, root tasks sequential (concurrency=1)\n- **Purpose truncation**: `purpose.slice(0, 117) + '...'` for purposes exceeding 120 characters\n- **Preamble separator pattern**: `'\\n---\\n'` detection within first 500 bytes\n- **Bold purpose regex**: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` for extracting purpose from AI responses\n\n## Integration Points\n\n- Imports `runPool()` from `./pool.js` for iterator-based worker pool execution\n- Imports `ITraceWriter` from `./trace.js` for NDJSON event emission\n- Imports quality validators from `../quality/index.js`: `checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`, `buildInconsistencyReport`, `formatReportForCli`\n- Imports prompt builders from `../generation/prompts/index.js`: `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`\n- Imports file writers from `../generation/writers/sum.js`: `writeSumFile`, `readSumFile`, `writeAnnexFile`\n- Imports `PlanTracker` from `./plan-tracker.js` for GENERATION-PLAN.md checkbox management\n- Imports `ProgressReporter` from `./progress.js` for streaming ETA calculation and console output\n\n## Annex References\n\nThis file IS an annex document — it does not reference additional annexes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 931,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 19263,
      "latencyMs": 32988,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/runner.ts.annex.md",
          "sizeBytes": 35428
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:29.542Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/index.ts\n\n```typescript\n/**\n * Public API for the quality analysis module.\n *\n * Re-exports all quality analysis types and functions from the\n * inconsistency detection and density validation submodules.\n *\n * @module\n */\n\n// ---------------------------------------------------------------------------\n// Shared types\n// ---------------------------------------------------------------------------\n\nexport type {\n  InconsistencySeverity,\n  CodeDocInconsistency,\n  CodeCodeInconsistency,\n  PhantomPathInconsistency,\n  Inconsistency,\n  InconsistencyReport,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-doc\n// ---------------------------------------------------------------------------\n\nexport { extractExports, checkCodeVsDoc } from './inconsistency/code-vs-doc.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-code\n// ---------------------------------------------------------------------------\n\nexport { checkCodeVsCode } from './inconsistency/code-vs-code.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency reporting\n// ---------------------------------------------------------------------------\n\nexport { buildInconsistencyReport, formatReportForCli } from './inconsistency/reporter.js';\n\n// ---------------------------------------------------------------------------\n// Phantom path detection\n// ---------------------------------------------------------------------------\n\nexport { checkPhantomPaths } from './phantom-paths/index.js';\n\n// ---------------------------------------------------------------------------\n// Density validation\n// ---------------------------------------------------------------------------\n\nexport { validateFindability } from './density/validator.js';\nexport type { FindabilityResult } from './density/validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`src/quality/index.ts` re-exports quality analysis types and functions from inconsistency detection, phantom path validation, and density validation submodules.**\n\n## Exported Types\n\nRe-exports from `./types.js`:\n- `InconsistencySeverity` — severity level enumeration for quality issues\n- `CodeDocInconsistency` — reports exports missing from `.sum` documentation\n- `CodeCodeInconsistency` — reports duplicate symbol definitions across files\n- `PhantomPathInconsistency` — reports unresolvable path references in `AGENTS.md`\n- `Inconsistency` — discriminated union of all inconsistency types\n- `InconsistencyReport` — aggregated quality report with metadata, issues array, and summary counts\n\n## Code-vs-Doc Validation\n\nRe-exports from `./inconsistency/code-vs-doc.js`:\n- `extractExports(sourceCode: string): string[]` — parses exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — verifies all extracted exports appear in `.sum` summary text via substring search\n\n## Code-vs-Code Validation\n\nRe-exports from `./inconsistency/code-vs-code.js`:\n- `checkCodeVsCode(directoryPath: string, exports: Map<string, string[]>): CodeCodeInconsistency[]` — detects duplicate symbols across multiple files within a directory, returns issues with `pattern: 'duplicate-export'`\n\n## Inconsistency Reporting\n\nRe-exports from `./inconsistency/reporter.js`:\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — constructs report with timestamp, projectRoot, filesChecked, durationMs, and summary counts by type/severity\n- `formatReportForCli(report: InconsistencyReport): string` — renders human-readable CLI output with picocolors formatting\n\n## Phantom Path Detection\n\nRe-exports from `./phantom-paths/index.js`:\n- `checkPhantomPaths(agentsMdPath: string, projectRoot: string): PhantomPathInconsistency[]` — extracts path-like strings from `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback\n\n## Density Validation\n\nRe-exports from `./density/validator.js`:\n- `validateFindability(sumContent: string): FindabilityResult` — validates identifier density in summaries (currently disabled after structured `publicInterface` removal from schema)\n- `FindabilityResult` — type representing density validation results\n\n## Module Purpose\n\nServes as barrel export aggregating all quality validation capabilities: code-documentation consistency checks (`extractExports`, `checkCodeVsDoc`), duplicate symbol detection (`checkCodeVsCode`), phantom path resolution (`checkPhantomPaths`), report synthesis (`buildInconsistencyReport`, `formatReportForCli`), and density validation (`validateFindability`). Called by Phase 1 post-processing to emit `InconsistencyReport` with counts logged to `.agents-reverse-engineer/progress.log` and included in telemetry run logs.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 822,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9548,
      "latencyMs": 28230,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/index.ts",
          "sizeBytes": 1936
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:08.591Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/templates.ts\n\n```typescript\n/**\n * Template generators for AI coding assistant integration files\n *\n * Generates command file templates for Claude Code, OpenCode, Gemini CLI, and session hooks.\n */\n\nimport type { IntegrationTemplate } from './types.js';\n\n// =============================================================================\n// Shared Command Content\n// =============================================================================\n\nconst COMMANDS = {\n  generate: {\n    description: 'Generate AI-friendly documentation for the entire codebase',\n    argumentHint: '[path] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Generate comprehensive documentation for this codebase using agents-reverse-engineer.\n\n<execution>\nRun the generate command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the generate command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest generate $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"32/96 files analyzed, ~12m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of files analyzed and any failures\n   - Number of directories documented\n   - Root and per-package documents generated\n   - Any inconsistency warnings from the quality report\n\nThis executes a three-phase pipeline:\n\n1. **Discovery & Planning**: Walks the directory tree, applies filters (gitignore, vendor, binary, custom), detects file types, and creates a generation plan.\n\n2. **File Analysis** (concurrent): Analyzes each source file via AI and writes \\`.sum\\` summary files with YAML frontmatter (\\`content_hash\\`, \\`file_type\\`, \\`purpose\\`, \\`public_interface\\`, \\`dependencies\\`, \\`patterns\\`).\n\n3. **Directory & Root Documents** (sequential):\n   - Generates \\`AGENTS.md\\` per directory in post-order traversal (deepest first, so child summaries feed into parents)\n   - Creates root document: \\`CLAUDE.md\\`\n\n**Options:**\n- \\`--dry-run\\`: Preview the plan without making AI calls\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  update: {\n    description: 'Incrementally update documentation for changed files',\n    argumentHint: '[path] [--uncommitted] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Update documentation for files that changed since last run.\n\n<execution>\nRun the update command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the update command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest update $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"12/30 files updated, ~5m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Files updated\n   - Files unchanged\n   - Any orphaned docs cleaned up\n\n**Options:**\n- \\`--uncommitted\\`: Include staged but uncommitted changes\n- \\`--dry-run\\`: Show what would be updated without writing\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  init: {\n    description: 'Initialize agents-reverse-engineer configuration',\n    argumentHint: '',\n    content: `Initialize agents-reverse-engineer configuration in this project.\n\n<execution>\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. Run the agents-reverse-engineer init command:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest init\n\\`\\`\\`\n\nThis creates \\`.agents-reverse-engineer/config.yaml\\` configuration file.\n</execution>`,\n  },\n\n  discover: {\n    description: 'Discover files in codebase',\n    argumentHint: '[path] [--debug] [--trace]',\n    content: `List files that would be analyzed for documentation.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest discover $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXdiscover\\`, run with ZERO flags\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the discover command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest discover $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~10 seconds (use \\`sleep 10\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and report number of files found.\n</execution>`,\n  },\n\n  clean: {\n    description: 'Delete all generated documentation artifacts (.sum, AGENTS.md, plan)',\n    argumentHint: '[path] [--dry-run]',\n    content: `Remove all documentation artifacts generated by agents-reverse-engineer.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest clean $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXclean\\`, run with ZERO flags\n\n**Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest clean $ARGUMENTS\n\\`\\`\\`\n\nReport number of files deleted.\n</execution>`,\n  },\n\n  specify: {\n    description: 'Generate project specification from AGENTS.md docs',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--multi-file] [--force] [--debug] [--trace]',\n    content: `Generate a project specification from existing AGENTS.md documentation.\n\n<execution>\nRun the specify command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the specify command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest specify $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of AGENTS.md files collected\n   - Output file(s) written\n\nThis collects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification.\n\nIf no AGENTS.md files exist, it will auto-run \\`generate\\` first.\n\n**Options:**\n- \\`--dry-run\\`: Show input statistics without making AI calls\n- \\`--output <path>\\`: Custom output path (default: specs/SPEC.md)\n- \\`--multi-file\\`: Split specification into multiple files\n- \\`--force\\`: Overwrite existing specification\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  rebuild: {\n    description: 'Reconstruct project from specification documents',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--force] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Reconstruct a project from specification documents using agents-reverse-engineer.\n\n<execution>\nRun the rebuild command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the rebuild command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest rebuild $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"4/12 rebuild units completed\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of rebuild units processed\n   - Files generated and output directory\n   - Any failures or partial completions\n\nThis reads spec files from \\`specs/\\`, partitions them into ordered rebuild units, and processes each via AI to generate source files.\n\n**Options:**\n- \\`--dry-run\\`: Show rebuild plan without making AI calls\n- \\`--output <path>\\`: Output directory (default: rebuild/)\n- \\`--force\\`: Wipe output directory and start fresh\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n\n**Exit codes:** 0 (success), 1 (partial failure), 2 (total failure)\n</execution>`,\n  },\n\n  help: {\n    description: 'Show available ARE commands and usage guide',\n    argumentHint: '',\n    // Content uses COMMAND_PREFIX placeholder, replaced per platform\n    content: `<objective>\nDisplay the complete ARE command reference.\n\n**First**: Read \\`VERSION_FILE_PATH\\` and show the user the version: \\`agents-reverse-engineer vX.Y.Z\\`\n\n**Then**: Output ONLY the reference content below. Do NOT add:\n- Project-specific analysis\n- Git status or file context\n- Next-step suggestions\n- Any commentary beyond the reference\n</objective>\n\n<reference>\n# agents-reverse-engineer (ARE) Command Reference\n\n**ARE** generates AI-friendly documentation for codebases, creating structured summaries optimized for AI assistants.\n\n## Quick Start\n\n1. \\`COMMAND_PREFIXinit\\` — Create configuration file\n2. \\`COMMAND_PREFIXgenerate\\` — Generate documentation for the codebase\n3. \\`COMMAND_PREFIXupdate\\` — Keep docs in sync after code changes\n\n## Commands Reference\n\n### \\`COMMAND_PREFIXinit\\`\nInitialize configuration in this project.\n\nCreates \\`.agents-reverse-engineer/config.yaml\\` with customizable settings.\n\n**Usage:** \\`COMMAND_PREFIXinit\\`\n**CLI:** \\`npx are init\\`\n\n---\n\n### \\`COMMAND_PREFIXdiscover\\`\nDiscover files that would be analyzed for documentation.\n\nShows included files, excluded files with reasons, and generates a \\`GENERATION-PLAN.md\\` execution plan.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--debug\\` | Show verbose debug output |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXdiscover\\` — Discover files and generate execution plan\n\n**CLI:**\n\\`\\`\\`bash\nnpx are discover\nnpx are discover ./src\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXgenerate\\`\nGenerate comprehensive documentation for the codebase.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--dry-run\\` | Show what would be generated without writing |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXgenerate\\` — Generate docs\n- \\`COMMAND_PREFIXgenerate --dry-run\\` — Preview without writing\n- \\`COMMAND_PREFIXgenerate --concurrency 3\\` — Limit parallel AI calls\n\n**CLI:**\n\\`\\`\\`bash\nnpx are generate\nnpx are generate --dry-run\nnpx are generate ./my-project --concurrency 3\nnpx are generate --debug --trace\n\\`\\`\\`\n\n**How it works:**\n1. Discovers files, applies filters, detects file types, and creates a generation plan\n2. Analyzes each file via concurrent AI calls, writes \\`.sum\\` summary files\n3. Generates \\`AGENTS.md\\` for each directory (post-order traversal)\n4. Creates root document: \\`CLAUDE.md\\`\n\n---\n\n### \\`COMMAND_PREFIXupdate\\`\nIncrementally update documentation for changed files.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--uncommitted\\` | Include staged but uncommitted changes |\n| \\`--dry-run\\` | Show what would be updated without writing |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXupdate\\` — Update docs for committed changes\n- \\`COMMAND_PREFIXupdate --uncommitted\\` — Include uncommitted changes\n\n**CLI:**\n\\`\\`\\`bash\nnpx are update\nnpx are update --uncommitted\nnpx are update --dry-run\nnpx are update ./my-project --concurrency 3\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXspecify\\`\nGenerate a project specification from AGENTS.md documentation.\n\nCollects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification. Auto-runs \\`generate\\` if no AGENTS.md files exist.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Custom output path (default: specs/SPEC.md) |\n| \\`--multi-file\\` | Split specification into multiple files |\n| \\`--force\\` | Overwrite existing specification |\n| \\`--dry-run\\` | Show input statistics without making AI calls |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXspecify\\` — Generate specification\n- \\`COMMAND_PREFIXspecify --dry-run\\` — Preview without calling AI\n- \\`COMMAND_PREFIXspecify --output ./docs/spec.md --force\\` — Custom output path\n\n**CLI:**\n\\`\\`\\`bash\nnpx are specify\nnpx are specify --dry-run\nnpx are specify --output ./docs/spec.md --force\nnpx are specify --multi-file\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXrebuild\\`\nReconstruct a project from specification documents.\n\nReads spec files from \\`specs/\\`, partitions them into ordered rebuild units, processes each via AI, and writes generated source files to an output directory. Supports checkpoint-based session continuity for resumable long-running rebuilds.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Output directory (default: rebuild/) |\n| \\`--force\\` | Wipe output directory and start fresh |\n| \\`--dry-run\\` | Show rebuild plan without making AI calls |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXrebuild --dry-run\\` — Preview rebuild plan\n- \\`COMMAND_PREFIXrebuild --output ./out --force\\` — Rebuild to custom directory\n\n**CLI:**\n\\`\\`\\`bash\nnpx are rebuild --dry-run\nnpx are rebuild --output ./out --force\nnpx are rebuild --concurrency 3\n\\`\\`\\`\n\n**How it works:**\n1. Reads all spec files from \\`specs/\\` directory\n2. Partitions specs into ordered rebuild units (from Build Plan phases or top-level headings)\n3. Processes units in order: sequentially between groups, concurrently within each group\n4. Accumulates context (export signatures) after each group for dependent phases\n5. Writes generated source files via \\`===FILE:===\\` delimited output parsing\n\n**Exit codes:** 0 (success), 1 (partial failure), 2 (total failure)\n\n---\n\n### \\`COMMAND_PREFIXclean\\`\nRemove all generated documentation artifacts.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--dry-run\\` | Show what would be deleted without deleting |\n\n**What gets deleted:**\n- \\`.agents-reverse-engineer/GENERATION-PLAN.md\\`\n- All \\`*.sum\\` files\n- All \\`AGENTS.md\\` files\n- Root docs: \\`CLAUDE.md\\`\n\n**Usage:**\n- \\`COMMAND_PREFIXclean --dry-run\\` — Preview deletions\n- \\`COMMAND_PREFIXclean\\` — Delete all artifacts\n\n**CLI:**\n\\`\\`\\`bash\nnpx are clean --dry-run\nnpx are clean\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXhelp\\`\nShow this command reference.\n\n## CLI Installation\n\nInstall ARE commands to your AI assistant:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer install              # Interactive mode\nnpx agents-reverse-engineer install --runtime claude -g  # Global Claude\nnpx agents-reverse-engineer install --runtime claude -l  # Local project\nnpx agents-reverse-engineer install --runtime all -g     # All runtimes\n\\`\\`\\`\n\n**Install/Uninstall Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--runtime <name>\\` | Target: \\`claude\\`, \\`opencode\\`, \\`gemini\\`, \\`all\\` |\n| \\`-g, --global\\` | Install to global config directory |\n| \\`-l, --local\\` | Install to current project directory |\n| \\`--force\\` | Overwrite existing files (install only) |\n\n## Configuration\n\n**File:** \\`.agents-reverse-engineer/config.yaml\\`\n\n\\`\\`\\`yaml\n# Exclusion patterns\nexclude:\n  patterns:\n    - \"**/*.test.ts\"\n    - \"**/__mocks__/**\"\n  vendorDirs:\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:\n    - .png\n    - .jpg\n    - .pdf\n\n# Options\noptions:\n  followSymlinks: false\n  maxFileSize: 100000\n\n# Output settings\noutput:\n  colors: true\n\\`\\`\\`\n\n## Generated Files\n\n### Per Source File\n\n**\\`*.sum\\`** — File summaries with YAML frontmatter + detailed prose.\n\n\\`\\`\\`yaml\n---\nfile_type: service\ngenerated_at: 2025-01-15T10:30:00Z\ncontent_hash: abc123...\npurpose: Handles user authentication and session management\npublic_interface: [login(), logout(), refreshToken(), AuthService]\ndependencies: [express, jsonwebtoken, ./user-model]\npatterns: [singleton, factory, observer]\nrelated_files: [./types.ts, ./middleware.ts]\n---\n\n<300-500 word summary covering implementation, patterns, edge cases>\n\\`\\`\\`\n\n### Per Directory\n\n**\\`AGENTS.md\\`** — Directory overview synthesized from \\`.sum\\` files. Groups files by purpose and links to subdirectories.\n\n### Root Documents\n\n| File | Purpose |\n|------|---------|\n| \\`CLAUDE.md\\` | Project entry point — synthesizes all AGENTS.md |\n\n## Common Workflows\n\n**Initial documentation:**\n\\`\\`\\`\nCOMMAND_PREFIXinit\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**After code changes:**\n\\`\\`\\`\nCOMMAND_PREFIXupdate\n\\`\\`\\`\n\n**Full regeneration:**\n\\`\\`\\`\nCOMMAND_PREFIXclean\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**Preview before generating:**\n\\`\\`\\`\nCOMMAND_PREFIXdiscover                   # Check files and exclusions\nCOMMAND_PREFIXgenerate --dry-run         # Preview generation\n\\`\\`\\`\n\n## Tips\n\n- **Custom exclusions**: Edit \\`.agents-reverse-engineer/config.yaml\\` to skip files\n- **Hook auto-update**: Install creates a session-end hook that auto-runs update\n\n## Resources\n\n- **Repository:** https://github.com/GeoloeG-IsT/agents-reverse-engineer\n- **Update:** \\`npx agents-reverse-engineer@latest install --force\\`\n</reference>`,\n  },\n} as const;\n\n// =============================================================================\n// Platform-specific template generators\n// =============================================================================\n\ntype Platform = 'claude' | 'opencode' | 'gemini';\n\ninterface PlatformConfig {\n  commandPrefix: string; // /are- (claude, opencode) or /are: (gemini)\n  pathPrefix: string; // .claude/commands/are/ or .opencode/commands/ etc\n  filenameSeparator: string; // . or -\n  extraFrontmatter?: string; // e.g., \"agent: build\" for OpenCode\n  usesName: boolean; // Claude uses \"name:\" in frontmatter\n  versionFilePath: string; // .claude/ARE-VERSION, .opencode/ARE-VERSION, etc.\n}\n\nconst PLATFORM_CONFIGS: Record<Platform, PlatformConfig> = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands/',\n    filenameSeparator: '-',\n    extraFrontmatter: 'agent: build',\n    usesName: false,\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands/',\n    filenameSeparator: '-',\n    usesName: false,\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n\nfunction buildFrontmatter(\n  platform: Platform,\n  commandName: string,\n  description: string\n): string {\n  const config = PLATFORM_CONFIGS[platform];\n  const lines = ['---'];\n\n  if (config.usesName) {\n    lines.push(`name: are-${commandName}`);\n  }\n\n  lines.push(`description: ${description}`);\n\n  if (config.extraFrontmatter) {\n    lines.push(config.extraFrontmatter);\n  }\n\n  lines.push('---');\n  return lines.join('\\n');\n}\n\n/**\n * Build TOML content for Gemini CLI commands\n *\n * Gemini uses TOML format with description and prompt fields.\n * See: https://geminicli.com/docs/cli/custom-commands/\n */\nfunction buildGeminiToml(\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): string {\n  const config = PLATFORM_CONFIGS.gemini;\n  // Replace placeholders in content\n  const promptContent = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  // Build TOML content\n  // Use triple quotes for multi-line prompt\n  const lines = [`description = \"${command.description}\"`];\n\n  if (command.argumentHint) {\n    lines.push(`# Arguments: ${command.argumentHint}`);\n  }\n\n  lines.push(`prompt = \"\"\"`);\n  lines.push(promptContent);\n  lines.push(`\"\"\"`);\n\n  return lines.join('\\n');\n}\n\nfunction buildTemplate(\n  platform: Platform,\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): IntegrationTemplate {\n  const config = PLATFORM_CONFIGS[platform];\n\n  // Platform-specific file naming:\n  // - Claude: .claude/skills/are-{command}/SKILL.md\n  // - OpenCode: .opencode/commands/are-{command}.md\n  // - Gemini: .gemini/commands/are-{command}.toml (TOML format)\n  if (platform === 'gemini') {\n    const filename = `are-${commandName}.toml`;\n    const path = `${config.pathPrefix}${filename}`;\n    const content = buildGeminiToml(commandName, command);\n\n    return {\n      filename,\n      path,\n      content: `${content}\\n`,\n    };\n  }\n\n  const filename = platform === 'claude' ? 'SKILL.md' : `are-${commandName}.md`;\n  const path =\n    platform === 'claude'\n      ? `${config.pathPrefix}are-${commandName}/${filename}`\n      : `${config.pathPrefix}${filename}`;\n\n  const frontmatter = buildFrontmatter(\n    platform,\n    commandName,\n    command.description\n  );\n\n  // Replace placeholders in content\n  const content = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  return {\n    filename,\n    path,\n    content: `${frontmatter}\\n\\n${content}\\n`,\n  };\n}\n\nfunction getTemplatesForPlatform(platform: Platform): IntegrationTemplate[] {\n  return Object.entries(COMMANDS).map(([name, command]) =>\n    buildTemplate(platform, name, command)\n  );\n}\n\n// =============================================================================\n// Public API\n// =============================================================================\n\n/**\n * Get Claude Code command file templates\n */\nexport function getClaudeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('claude');\n}\n\n/**\n * Get OpenCode command file templates\n */\nexport function getOpenCodeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('opencode');\n}\n\n/**\n * Get Gemini CLI command file templates\n */\nexport function getGeminiTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('gemini');\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Generates platform-specific command templates for Claude Code, OpenCode, and Gemini CLI integration with AI-driven documentation workflow instructions.**\n\n## Exported Functions\n\n- `getClaudeTemplates()` — Returns `IntegrationTemplate[]` for Claude Code (.claude/skills/are-{command}/SKILL.md format with frontmatter `name:` field)\n- `getOpenCodeTemplates()` — Returns `IntegrationTemplate[]` for OpenCode (.opencode/commands/are-{command}.md format with `agent: build` frontmatter)\n- `getGeminiTemplates()` — Returns `IntegrationTemplate[]` for Gemini CLI (.gemini/commands/are-{command}.toml format with `description` and triple-quoted `prompt` fields)\n\n## Template Generation Architecture\n\n`buildTemplate(platform, commandName, command)` constructs `IntegrationTemplate` objects with platform-specific file paths, frontmatter schemas, and placeholder substitution. `buildFrontmatter(platform, commandName, description)` generates Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure with `description = \"...\"` and `prompt = \"\"\"...\"\"\"` multiline strings per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n`getTemplatesForPlatform(platform)` maps all entries in `COMMANDS` constant to `buildTemplate()` calls, returning array of fully materialized templates. Each template includes `filename`, `path`, and `content` fields populated via `PLATFORM_CONFIGS` lookup and placeholder replacement for `COMMAND_PREFIX` (`/are-` for all platforms) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n## Command Definitions\n\n`COMMANDS` constant defines seven command templates: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. Each entry contains `description`, `argumentHint`, and `content` fields.\n\n`generate` template embeds three-phase pipeline documentation: Discovery → File Analysis (concurrent .sum file generation) → Directory/Root Documents (sequential AGENTS.md and CLAUDE.md synthesis). Includes background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, and completion summarization covering file counts, failures, and inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n\n`update` template describes incremental change detection workflow with `--uncommitted` flag for staged changes, hash-based change comparison, orphan cleanup, and affected directory regeneration. Monitoring pattern identical to `generate`.\n\n`discover` template enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay, reports file count on completion.\n\n`clean` template mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview support.\n\n`specify` template orchestrates AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n\n`rebuild` template describes spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation after each group, ===FILE:=== delimited output parsing, and exit code semantics (0=success, 1=partial failure, 2=total failure). Supports checkpoint-based resumability. Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n\n`help` template outputs command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, and repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n## Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines `PlatformConfig` objects for `claude`, `opencode`, `gemini` keys with fields:\n- `commandPrefix` — Slash command prefix (`/are-` for all platforms)\n- `pathPrefix` — Installation directory (.claude/skills/, .opencode/commands/, .gemini/commands/)\n- `filenameSeparator` — Path component delimiter (`.` for Claude, `-` for OpenCode/Gemini)\n- `extraFrontmatter` — Optional platform-specific metadata (`agent: build` for OpenCode only)\n- `usesName` — Boolean controlling frontmatter `name:` field emission (true for Claude only)\n- `versionFilePath` — Platform-specific version cache path (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION)\n\nClaude uses nested directory structure (.claude/skills/are-{command}/SKILL.md), while OpenCode and Gemini use flat file naming (.opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml).\n\n## Placeholder Substitution\n\nAll `content` strings undergo two replacements before template materialization:\n1. `COMMAND_PREFIX` → platform-specific slash command prefix (`/are-`)\n2. `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`\n\n## Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Type Dependencies\n\nImports `IntegrationTemplate` from `./types.js` with structure:\n```typescript\ninterface IntegrationTemplate {\n  filename: string;\n  path: string;\n  content: string;\n}\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1614,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 17057,
      "latencyMs": 50273,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/templates.ts",
          "sizeBytes": 26312
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:43.055Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/index.ts\n\n```typescript\n/**\n * Rebuild module barrel export.\n *\n * Re-exports all public types, schemas, functions, and classes\n * for the rebuild pipeline.\n *\n * @module\n */\n\nexport type {\n  RebuildCheckpoint,\n  RebuildUnit,\n  RebuildPlan,\n  RebuildResult,\n} from './types.js';\n\nexport { RebuildCheckpointSchema } from './types.js';\n\nexport { readSpecFiles, partitionSpec } from './spec-reader.js';\n\nexport { parseModuleOutput } from './output-parser.js';\n\nexport { CheckpointManager } from './checkpoint.js';\n\nexport { REBUILD_SYSTEM_PROMPT, buildRebuildPrompt } from './prompts.js';\n\nexport { executeRebuild, type RebuildExecutionOptions } from './orchestrator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`src/rebuild/index.ts` re-exports all public types, schemas, functions, and classes for the rebuild pipeline through a barrel export pattern.**\n\n## Exported Types\n\n- **`RebuildCheckpoint`** — Type representing a rebuild checkpoint state from `./types.js`\n- **`RebuildUnit`** — Type representing a unit of work in the rebuild process from `./types.js`\n- **`RebuildPlan`** — Type representing a complete rebuild execution plan from `./types.js`\n- **`RebuildResult`** — Type representing the outcome of a rebuild operation from `./types.js`\n\n## Exported Schemas\n\n- **`RebuildCheckpointSchema`** — Zod schema for validating `RebuildCheckpoint` structures from `./types.js`\n\n## Exported Functions\n\n- **`readSpecFiles(specPath: string): Promise<...>`** — Reads specification files from the given path from `./spec-reader.js`\n- **`partitionSpec(...): ...`** — Partitions a specification into rebuild units from `./spec-reader.js`\n- **`parseModuleOutput(...): ...`** — Parses AI-generated module output into structured data from `./output-parser.js`\n- **`buildRebuildPrompt(...): string`** — Constructs prompts for the rebuild AI subprocess from `./prompts.js`\n- **`executeRebuild(options: RebuildExecutionOptions): Promise<RebuildResult>`** — Orchestrates the complete rebuild pipeline execution from `./orchestrator.js`\n\n## Exported Classes\n\n- **`CheckpointManager`** — Manages checkpoint persistence and recovery for incremental rebuild progress from `./checkpoint.js`\n\n## Exported Constants\n\n- **`REBUILD_SYSTEM_PROMPT`** — System prompt string template for rebuild AI interactions from `./prompts.js`\n\n## Exported Type Aliases\n\n- **`RebuildExecutionOptions`** — Type alias for options passed to `executeRebuild()` from `./orchestrator.js`\n\n## Module Organization\n\nThis barrel export aggregates the rebuild pipeline's public interface across six internal modules: type definitions (`types.js`), specification parsing (`spec-reader.js`), AI output parsing (`output-parser.js`), checkpoint management (`checkpoint.js`), prompt engineering (`prompts.js`), and orchestration logic (`orchestrator.js`). Consumers import from this index file rather than individual submodules to access rebuild functionality.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 577,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9404,
      "latencyMs": 17031,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/index.ts",
          "sizeBytes": 645
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:44.886Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/output-parser.ts\n\n```typescript\n/**\n * AI output parser for multi-file rebuild responses.\n *\n * Extracts individual files from AI-generated responses using\n * `===FILE: path===` / `===END_FILE===` delimiters, with a fallback\n * to markdown fenced code blocks with file path annotations.\n *\n * @module\n */\n\n/**\n * Parse multi-file AI output into a Map of file paths to contents.\n *\n * Primary format: `===FILE: path===` / `===END_FILE===` delimiters.\n * Fallback format: Markdown fenced code blocks with `language:path` annotation.\n *\n * File paths are trimmed. File content is NOT trimmed (preserves indentation).\n * Returns an empty Map if neither format matches (caller handles error case).\n *\n * @param responseText - Raw AI response text\n * @returns Map of relative file paths to file contents\n */\nexport function parseModuleOutput(responseText: string): Map<string, string> {\n  // Primary: ===FILE: path=== / ===END_FILE=== delimiters\n  const files = parseDelimiterFormat(responseText);\n  if (files.size > 0) return files;\n\n  // Fallback: markdown fenced code blocks with file path annotation\n  return parseFencedBlockFormat(responseText);\n}\n\n/**\n * Parse `===FILE: path===` / `===END_FILE===` delimited output.\n */\nfunction parseDelimiterFormat(text: string): Map<string, string> {\n  const files = new Map<string, string>();\n  const pattern = /===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g;\n\n  let match: RegExpExecArray | null;\n  while ((match = pattern.exec(text)) !== null) {\n    const filePath = match[1].trim();\n    const content = match[2];\n    files.set(filePath, content);\n  }\n\n  return files;\n}\n\n/**\n * Parse markdown fenced code blocks with file path annotations.\n *\n * Matches blocks like:\n * ```language:path/to/file\n * content\n * ```\n */\nfunction parseFencedBlockFormat(text: string): Map<string, string> {\n  const files = new Map<string, string>();\n  const pattern = /```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g;\n\n  let match: RegExpExecArray | null;\n  while ((match = pattern.exec(text)) !== null) {\n    const filePath = match[1].trim();\n    const content = match[2];\n    files.set(filePath, content);\n  }\n\n  return files;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Parses AI-generated multi-file rebuild responses using delimiter-based and markdown fence fallback strategies to extract file paths and contents into a `Map<string, string>`.**\n\n## Exported Interface\n\n**`parseModuleOutput(responseText: string): Map<string, string>`** — Primary entry point that attempts delimiter-based parsing via `parseDelimiterFormat()`, falling back to `parseFencedBlockFormat()` if no matches found, returning empty Map on total failure (caller handles error case).\n\n## Parsing Strategies\n\n**Primary Format (Delimiter-based):**\n`parseDelimiterFormat()` uses regex `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g` to extract files between `===FILE: path===` and `===END_FILE===` markers. File paths undergo `.trim()`, content preserves whitespace verbatim (no trimming).\n\n**Fallback Format (Markdown Fenced Blocks):**\n`parseFencedBlockFormat()` matches code blocks with file path annotations using `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g` pattern. Expects format:\n````\n```language:path/to/file\ncontent\n```\n````\n\n## Behavioral Contracts\n\n**Delimiter pattern (primary):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block pattern (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Content preservation:** File content captured without trimming to preserve indentation and trailing whitespace. Only file paths undergo `.trim()` normalization.\n\n**Empty Map semantics:** Returns `new Map<string, string>()` when neither format matches, requiring caller to validate non-empty result.\n\n## Integration Points\n\nUsed by rebuild orchestrator (`src/rebuild/orchestrator.ts`) to parse AI responses during multi-file project reconstruction from specification documents. Output Map keys are relative file paths, values are raw file contents for filesystem writes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 481,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9869,
      "latencyMs": 20053,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/output-parser.ts",
          "sizeBytes": 2105
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:24.905Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/trace.ts\n\n```typescript\n/**\n * Concurrency tracing system for debugging task/subprocess lifecycle.\n *\n * Produces append-only NDJSON trace files in `.agents-reverse-engineer/traces/`\n * when the `--trace` CLI flag is set. When disabled, the {@link NullTraceWriter}\n * ensures zero overhead -- every call site can unconditionally call `emit()`\n * without branching.\n *\n * Uses promise-chain serialization (same pattern as {@link PlanTracker}) to\n * handle concurrent writes from multiple pool workers safely.\n *\n * @module\n */\n\nimport { open, mkdir, readdir, unlink } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n// ---------------------------------------------------------------------------\n// Trace directory\n// ---------------------------------------------------------------------------\n\n/** Directory for trace files (relative to project root) */\nconst TRACES_DIR = '.agents-reverse-engineer/traces';\n\n// ---------------------------------------------------------------------------\n// Trace event types\n// ---------------------------------------------------------------------------\n\n/** Common fields present on every trace event */\ninterface TraceEventBase {\n  /** Monotonically increasing sequence number (per-run) */\n  seq: number;\n  /** ISO 8601 timestamp at event creation time */\n  ts: string;\n  /** process.pid of the Node.js parent process */\n  pid: number;\n  /** High-resolution elapsed time since run start (ms, fractional) */\n  elapsedMs: number;\n}\n\n/** Emitted when a phase begins execution */\ninterface PhaseStartEvent extends TraceEventBase {\n  type: 'phase:start';\n  phase: string;\n  taskCount: number;\n  concurrency: number;\n}\n\n/** Emitted when a phase completes */\ninterface PhaseEndEvent extends TraceEventBase {\n  type: 'phase:end';\n  phase: string;\n  durationMs: number;\n  tasksCompleted: number;\n  tasksFailed: number;\n}\n\n/** Emitted when a pool worker starts pulling from the shared iterator */\ninterface WorkerStartEvent extends TraceEventBase {\n  type: 'worker:start';\n  workerId: number;\n  phase: string;\n}\n\n/** Emitted when a pool worker exhausts the iterator or is aborted */\ninterface WorkerEndEvent extends TraceEventBase {\n  type: 'worker:end';\n  workerId: number;\n  phase: string;\n  tasksExecuted: number;\n}\n\n/** Emitted when a worker picks up a task from the iterator */\ninterface TaskPickupEvent extends TraceEventBase {\n  type: 'task:pickup';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  activeTasks: number;\n}\n\n/** Emitted when a task completes (success or failure) */\ninterface TaskDoneEvent extends TraceEventBase {\n  type: 'task:done';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  durationMs: number;\n  success: boolean;\n  error?: string;\n  activeTasks: number;\n}\n\n/** Emitted when a child process is spawned */\ninterface SubprocessSpawnEvent extends TraceEventBase {\n  type: 'subprocess:spawn';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n}\n\n/** Emitted when a child process exits */\ninterface SubprocessExitEvent extends TraceEventBase {\n  type: 'subprocess:exit';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n  exitCode: number;\n  signal: string | null;\n  durationMs: number;\n  timedOut: boolean;\n}\n\n/** Emitted before a retry attempt */\ninterface RetryEvent extends TraceEventBase {\n  type: 'retry';\n  attempt: number;\n  taskLabel: string;\n  errorCode: string;\n}\n\n/** Emitted when a non-pool task starts execution */\ninterface TaskStartEvent extends TraceEventBase {\n  type: 'task:start';\n  taskLabel: string;\n  phase: string;\n}\n\n/** Emitted when file discovery begins */\ninterface DiscoveryStartEvent extends TraceEventBase {\n  type: 'discovery:start';\n  targetPath: string;\n}\n\n/** Emitted when file discovery completes */\ninterface DiscoveryEndEvent extends TraceEventBase {\n  type: 'discovery:end';\n  filesIncluded: number;\n  filesExcluded: number;\n  durationMs: number;\n}\n\n/** Emitted when a filter is applied during discovery */\ninterface FilterAppliedEvent extends TraceEventBase {\n  type: 'filter:applied';\n  filterName: string;\n  filesMatched: number;\n  filesRejected: number;\n}\n\n/** Emitted when a generation/update plan is created */\ninterface PlanCreatedEvent extends TraceEventBase {\n  type: 'plan:created';\n  planType: 'generate' | 'update';\n  fileCount: number;\n  taskCount: number;\n}\n\n/** Emitted when configuration is loaded */\ninterface ConfigLoadedEvent extends TraceEventBase {\n  type: 'config:loaded';\n  configPath: string;\n  model: string;\n  concurrency: number;\n}\n\n/** Discriminated union of all trace event types */\nexport type TraceEvent =\n  | PhaseStartEvent\n  | PhaseEndEvent\n  | WorkerStartEvent\n  | WorkerEndEvent\n  | TaskPickupEvent\n  | TaskDoneEvent\n  | TaskStartEvent\n  | SubprocessSpawnEvent\n  | SubprocessExitEvent\n  | RetryEvent\n  | DiscoveryStartEvent\n  | DiscoveryEndEvent\n  | FilterAppliedEvent\n  | PlanCreatedEvent\n  | ConfigLoadedEvent;\n\n/** Keys auto-populated by the trace writer */\ntype BaseKeys = 'seq' | 'ts' | 'pid' | 'elapsedMs';\n\n/** Distributive Omit that works correctly across union members */\ntype DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never;\n\n/** Event payload without auto-populated base fields */\nexport type TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>;\n\n// ---------------------------------------------------------------------------\n// ITraceWriter interface\n// ---------------------------------------------------------------------------\n\n/**\n * Public interface for trace event emission.\n *\n * All consumers depend only on this interface, allowing the no-op\n * implementation to be swapped in when tracing is disabled.\n */\nexport interface ITraceWriter {\n  /** Emit a trace event. Base fields (seq, ts, pid, elapsedMs) are auto-populated. */\n  emit(event: TraceEventPayload): void;\n  /** Flush all pending writes and close the file handle. */\n  finalize(): Promise<void>;\n  /** Absolute path to the trace file (empty string for NullTraceWriter). */\n  readonly filePath: string;\n}\n\n// ---------------------------------------------------------------------------\n// NullTraceWriter (no-op)\n// ---------------------------------------------------------------------------\n\n/**\n * No-op trace writer. Returned when `--trace` is not set.\n * All methods are empty -- zero overhead at call sites.\n */\nclass NullTraceWriter implements ITraceWriter {\n  readonly filePath = '';\n  emit(): void { /* no-op */ }\n  async finalize(): Promise<void> { /* no-op */ }\n}\n\n// ---------------------------------------------------------------------------\n// TraceWriter (real implementation)\n// ---------------------------------------------------------------------------\n\n/**\n * Append-only NDJSON trace writer.\n *\n * Each `emit()` call serializes the event to a single JSON line and\n * enqueues a file append via a promise chain. This guarantees correct\n * ordering even when multiple pool workers emit concurrently.\n */\nclass TraceWriter implements ITraceWriter {\n  readonly filePath: string;\n\n  private seq = 0;\n  private readonly nodePid = process.pid;\n  private readonly startHr = process.hrtime.bigint();\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(filePath: string) {\n    this.filePath = filePath;\n  }\n\n  emit(partial: TraceEventPayload): void {\n    const event = {\n      ...partial,\n      seq: this.seq++,\n      ts: new Date().toISOString(),\n      pid: this.nodePid,\n      elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000,\n    };\n    const line = JSON.stringify(event) + '\\n';\n\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'a');\n        }\n        await this.fd.write(line);\n      })\n      .catch(() => { /* non-critical -- trace loss is acceptable */ });\n  }\n\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a trace writer.\n *\n * Returns a {@link NullTraceWriter} when `enabled` is false (zero overhead).\n * Otherwise returns a {@link TraceWriter} that appends NDJSON to\n * `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param enabled - Whether tracing is enabled (typically from `--trace` flag)\n * @returns A trace writer instance\n */\nexport function createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter {\n  if (!enabled) return new NullTraceWriter();\n  const safeTimestamp = new Date().toISOString().replace(/[:.]/g, '-');\n  const filePath = path.join(projectRoot, TRACES_DIR, `trace-${safeTimestamp}.ndjson`);\n  return new TraceWriter(filePath);\n}\n\n// ---------------------------------------------------------------------------\n// Cleanup\n// ---------------------------------------------------------------------------\n\n/**\n * Remove old trace files, keeping only the most recent ones.\n *\n * Mirrors the pattern in `src/ai/telemetry/cleanup.ts`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent trace files to retain (default: 500)\n * @returns Number of files deleted\n */\nexport async function cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number> {\n  const tracesDir = path.join(projectRoot, TRACES_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await readdir(tracesDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('trace-') && name.endsWith('.ndjson'),\n    );\n  } catch (error) {\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort newest first (ISO timestamps sort lexicographically)\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await unlink(path.join(tracesDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**trace.ts implements append-only NDJSON trace event emission for debugging task/subprocess lifecycle with promise-chain serialization to handle concurrent writes from pool workers.**\n\n## Exported Interface\n\n`ITraceWriter` — Public interface for trace event emission with three methods:\n- `emit(event: TraceEventPayload): void` — Emits trace event with auto-populated base fields (seq, ts, pid, elapsedMs)\n- `finalize(): Promise<void>` — Flushes pending writes and closes file handle\n- `filePath: string` — Absolute path to trace file (empty string for NullTraceWriter)\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — Factory returning NullTraceWriter when `enabled` is false, otherwise TraceWriter writing to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — Removes old trace files keeping only most recent ones, returns count deleted\n\n## Event Schema\n\n`TraceEvent` — Discriminated union of 15 event types:\n- `PhaseStartEvent` — type `'phase:start'`, fields: `phase`, `taskCount`, `concurrency`\n- `PhaseEndEvent` — type `'phase:end'`, fields: `phase`, `durationMs`, `tasksCompleted`, `tasksFailed`\n- `WorkerStartEvent` — type `'worker:start'`, fields: `workerId`, `phase`\n- `WorkerEndEvent` — type `'worker:end'`, fields: `workerId`, `phase`, `tasksExecuted`\n- `TaskPickupEvent` — type `'task:pickup'`, fields: `workerId`, `taskIndex`, `taskLabel`, `activeTasks`\n- `TaskDoneEvent` — type `'task:done'`, fields: `workerId`, `taskIndex`, `taskLabel`, `durationMs`, `success`, `error?`, `activeTasks`\n- `TaskStartEvent` — type `'task:start'`, fields: `taskLabel`, `phase`\n- `SubprocessSpawnEvent` — type `'subprocess:spawn'`, fields: `childPid`, `command`, `taskLabel`\n- `SubprocessExitEvent` — type `'subprocess:exit'`, fields: `childPid`, `command`, `taskLabel`, `exitCode`, `signal`, `durationMs`, `timedOut`\n- `RetryEvent` — type `'retry'`, fields: `attempt`, `taskLabel`, `errorCode`\n- `DiscoveryStartEvent` — type `'discovery:start'`, fields: `targetPath`\n- `DiscoveryEndEvent` — type `'discovery:end'`, fields: `filesIncluded`, `filesExcluded`, `durationMs`\n- `FilterAppliedEvent` — type `'filter:applied'`, fields: `filterName`, `filesMatched`, `filesRejected`\n- `PlanCreatedEvent` — type `'plan:created'`, fields: `planType`, `fileCount`, `taskCount`\n- `ConfigLoadedEvent` — type `'config:loaded'`, fields: `configPath`, `model`, `concurrency`\n\n`TraceEventBase` — Common fields auto-populated by TraceWriter: `seq` (monotonic number), `ts` (ISO 8601 string), `pid` (process.pid), `elapsedMs` (high-resolution fractional milliseconds since run start via `process.hrtime.bigint()`)\n\n`TraceEventPayload` — Type alias using `DistributiveOmit<TraceEvent, BaseKeys>` to strip auto-populated fields from event payloads\n\n`DistributiveOmit<T, K extends PropertyKey>` — Distributive conditional type `T extends unknown ? Omit<T, K> : never` ensuring Omit correctly distributes across discriminated union members\n\n## Implementation Classes\n\n`NullTraceWriter implements ITraceWriter` — No-op implementation with empty methods and empty `filePath`, ensures zero overhead when `--trace` flag absent\n\n`TraceWriter implements ITraceWriter` — Real implementation using promise-chain serialization pattern:\n- Private fields: `seq` (counter), `nodePid` (process.pid), `startHr` (process.hrtime.bigint() at construction), `writeQueue` (Promise<void> chain), `fd` (FileHandle | null)\n- `emit()` enriches payload with base fields, serializes to JSON line, chains write via `this.writeQueue = this.writeQueue.then(async () => {...})`\n- Lazy file handle opening: first write creates parent directory via `mkdir(path.dirname(this.filePath), { recursive: true })` and opens file in append mode\n- Write errors caught and silently ignored (trace loss acceptable, non-critical)\n- `finalize()` awaits `writeQueue` then closes `fd` if open\n\n## Trace File Management\n\n`TRACES_DIR` — Constant `'.agents-reverse-engineer/traces'` defines trace directory relative to project root\n\n`createTraceWriter()` generates timestamp via `new Date().toISOString().replace(/[:.]/g, '-')` for filename `trace-{timestamp}.ndjson`\n\n`cleanupOldTraces()` mirrors `src/ai/telemetry/cleanup.ts` pattern: filters `trace-*.ndjson` files, sorts lexicographically (ISO timestamps), reverses to newest-first, deletes beyond `keepCount` (default 500)\n\n## Integration Points\n\nThreaded via `CommandRunOptions.tracer` field to pool workers (`src/orchestration/pool.ts`), AIService (`src/ai/service.ts`), phase runners (`src/orchestration/runner.ts`)\n\nPhase orchestration emits `phase:start`/`phase:end`, worker pool emits `worker:start`/`worker:end`/`task:pickup`/`task:done`, subprocess manager (`src/ai/subprocess.ts`) emits `subprocess:spawn`/`subprocess:exit`, retry logic (`src/ai/retry.ts`) emits `retry`\n\nDiscovery (`src/discovery/run.ts`) emits `discovery:start`/`discovery:end`/`filter:applied`, plan creation emits `plan:created`, config loading emits `config:loaded`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1507,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12074,
      "latencyMs": 41158,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/trace.ts",
          "sizeBytes": 10277
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:47.619Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/prompts.ts\n\n```typescript\n/**\n * Prompt templates for AI-driven project reconstruction.\n *\n * Provides the system prompt that instructs the AI to generate source files\n * using `===FILE:===` / `===END_FILE===` delimiters, and a per-unit user\n * prompt builder that combines the full spec, current phase, and already-built\n * context.\n *\n * @module\n */\n\nimport type { RebuildUnit } from './types.js';\n\n/**\n * System prompt for AI-driven project reconstruction.\n *\n * Instructs the model to emit source files using `===FILE: path===` /\n * `===END_FILE===` delimiters with production-quality code that follows\n * the spec's architecture and type definitions.\n */\nexport const REBUILD_SYSTEM_PROMPT = `You reconstruct source code from a project specification.\n\nTASK:\nGenerate all source files for the described module/phase. The code must be complete, compilable, and production-ready.\n\nOUTPUT FORMAT:\nUse this exact delimiter format for EVERY file:\n\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n\nGenerate ONLY the file content between delimiters. No markdown fencing, no commentary, no explanations outside the file delimiters.\n\nQUALITY:\n- Code must compile. Use exact type names, function signatures, and constants from the spec.\n- Follow the architecture and patterns described in the specification.\n- Imports must reference real modules described in the spec.\n- Generate production code only (no tests, no stubs, no placeholders).\n- Do not invent features not in the spec.\n- Do not add comments explaining what the spec says — write the code the spec describes.\n\nSTRICT COMPLIANCE:\n- When the specification defines exact names for functions, methods, types, classes, or constants, you MUST use those exact names. Do not invent synonyms (e.g., if the spec says done(), do not write reportSuccess()).\n- Pay close attention to the \"Interfaces for This Phase\" section in the current phase — it contains the exact signatures you must implement.\n- When \"Already Built\" context shows an exported symbol, import and use it. Do not redefine it.\n\nCONTEXT AWARENESS:\nWhen \"Already Built\" context is provided, import from those modules and use their exported types/functions. Do not redefine types that already exist in built modules.\nWhen \"Already Built\" context provides a function or method signature, your code MUST call it using the exact name shown. Match the API precisely.`;\n\n/**\n * Build the system + user prompt pair for a single rebuild unit.\n *\n * The user prompt includes:\n * 1. Full specification for reference\n * 2. Current phase/module to build\n * 3. Already-built context (exported signatures from prior groups)\n * 4. Output format reminder\n *\n * @param unit - The rebuild unit to generate code for\n * @param fullSpec - Concatenated content of all spec files\n * @param builtContext - Exported type signatures from previously built modules\n * @returns Prompt pair with system and user strings\n */\nexport function buildRebuildPrompt(\n  unit: RebuildUnit,\n  fullSpec: string,\n  builtContext: string | undefined,\n): { system: string; user: string } {\n  const sections: string[] = [\n    'Reconstruct the following module from this specification.',\n    '',\n    '## Full Specification',\n    '',\n    fullSpec,\n    '',\n    '## Current Phase',\n    '',\n    'Build the module described in this phase:',\n    '',\n    unit.specContent,\n  ];\n\n  if (builtContext) {\n    sections.push(\n      '',\n      '## Already Built',\n      '',\n      'The following modules have been built. Import from them as needed:',\n      '',\n      builtContext,\n    );\n  }\n\n  sections.push(\n    '',\n    '## Output Format',\n    '',\n    'Emit each file using:',\n    '===FILE: path/to/file.ext===',\n    '[content]',\n    '===END_FILE===',\n    '',\n    'Generate ALL files needed for this phase. Use relative paths from the project root.',\n  );\n\n  return {\n    system: REBUILD_SYSTEM_PROMPT,\n    user: sections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts defines system and user prompt construction for AI-driven project reconstruction, emitting source files with `===FILE:===` / `===END_FILE===` delimiters.**\n\n## Exported Constants\n\n`REBUILD_SYSTEM_PROMPT` is a string constant containing the system prompt that instructs the AI model to reconstruct source code from a project specification. The prompt enforces:\n\n- **Output format:** Each file must be wrapped in `===FILE: relative/path.ext===` and `===END_FILE===` delimiters with no markdown fencing or external commentary\n- **Quality requirements:** Code must compile, use exact type names/function signatures/constants from spec, follow described architecture, import only real modules from spec, generate production code only (no tests/stubs/placeholders), avoid inventing features or adding explanatory comments\n- **Strict compliance:** When spec defines exact names for functions/methods/types/classes/constants, those exact names MUST be used (no synonyms like `reportSuccess()` for spec's `done()`), \"Interfaces for This Phase\" section signatures must be implemented precisely, exported symbols from \"Already Built\" context must be imported and used without redefinition\n- **Context awareness:** Import from previously built modules shown in \"Already Built\" context, use their exported types/functions, match API signatures exactly\n\n## Exported Functions\n\n`buildRebuildPrompt(unit: RebuildUnit, fullSpec: string, builtContext: string | undefined): { system: string; user: string }` constructs the prompt pair for a single `RebuildUnit`. Returns object with `system` field set to `REBUILD_SYSTEM_PROMPT` and `user` field containing concatenated sections:\n\n1. **Full Specification** section with `fullSpec` parameter content\n2. **Current Phase** section with `unit.specContent` describing module to build\n3. **Already Built** section (conditionally included when `builtContext` provided) listing exported type signatures from previously built modules for import resolution\n4. **Output Format** section reminding of `===FILE:===` / `===END_FILE===` delimiter syntax and relative path requirements\n\n## Integration Points\n\nImports `RebuildUnit` type from `./types.js` which defines the structure for individual rebuild tasks. `buildRebuildPrompt` consumes `unit.specContent` field containing phase-specific specification content to isolate current build target within larger project context.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 544,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10230,
      "latencyMs": 18448,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/prompts.ts",
          "sizeBytes": 3880
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:20.202Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/progress.ts\n\n```typescript\n/**\n * Streaming build-log progress reporter with ETA calculation.\n *\n * Outputs one line per event (start, done, fail, dir-done, root-done)\n * using `console.log` for atomic, non-corrupting concurrent output.\n * Each line shows progress counter, status, file path, timing, and\n * token counts using colored output via `picocolors`.\n *\n * ETA is computed via a moving average of the last 10 completion times,\n * displayed after 2 or more files have completed.\n *\n * Optionally mirrors all output to a plain-text progress log file\n * (`.agents-reverse-engineer/progress.log`) via {@link ProgressLog},\n * enabling `tail -f` monitoring when running inside buffered environments\n * (e.g. Claude Code's Bash tool).\n *\n * @module\n */\n\nimport { open, mkdir } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { RunSummary } from './types.js';\n\n// ---------------------------------------------------------------------------\n// ANSI stripping\n// ---------------------------------------------------------------------------\n\n/** Strip ANSI escape sequences from a string for plain-text output. */\nfunction stripAnsi(str: string): string {\n  // Matches all common ANSI escape codes (SGR, cursor, erase, etc.)\n  // eslint-disable-next-line no-control-regex\n  return str.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// ---------------------------------------------------------------------------\n// ProgressLog\n// ---------------------------------------------------------------------------\n\n/** Relative path for the progress log file */\nconst PROGRESS_LOG_FILENAME = 'progress.log';\n\n/**\n * Plain-text progress log file writer.\n *\n * Mirrors console progress output to `.agents-reverse-engineer/progress.log`\n * without ANSI escape codes, enabling real-time monitoring via `tail -f`\n * when the CLI runs inside buffered environments (e.g. Claude Code).\n *\n * Uses promise-chain serialization (same pattern as {@link TraceWriter})\n * to handle concurrent writes from multiple pool workers safely.\n *\n * @example\n * ```typescript\n * const log = new ProgressLog('/project/.agents-reverse-engineer/progress.log');\n * log.write('=== ARE Generate (2026-02-09) ===');\n * log.write('[1/10] ANALYZING src/index.ts');\n * await log.finalize();\n * ```\n */\nexport class ProgressLog {\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(private readonly filePath: string) {}\n\n  /**\n   * Create a ProgressLog for a project root.\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns A new ProgressLog instance\n   */\n  static create(projectRoot: string): ProgressLog {\n    return new ProgressLog(\n      path.join(projectRoot, '.agents-reverse-engineer', PROGRESS_LOG_FILENAME),\n    );\n  }\n\n  /**\n   * Append a line to the progress log file.\n   *\n   * On first call, creates the parent directory and opens the file\n   * in truncate mode ('w'). Subsequent writes append to the open handle.\n   * Write failures are silently swallowed (non-critical telemetry).\n   */\n  write(line: string): void {\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'w');\n        }\n        await this.fd.write(line + '\\n');\n      })\n      .catch(() => { /* non-critical -- progress log loss is acceptable */ });\n  }\n\n  /** Flush all pending writes and close the file handle. */\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// ProgressReporter\n// ---------------------------------------------------------------------------\n\n/**\n * Streaming build-log progress reporter.\n *\n * Create one instance per command run. Call the event methods as files\n * are processed. Call {@link printSummary} at the end of the run.\n *\n * @example\n * ```typescript\n * const reporter = new ProgressReporter(fileCount);\n * reporter.onFileStart('src/index.ts');\n * reporter.onFileDone('src/index.ts', 1200, 500, 300, 'sonnet');\n * reporter.printSummary(summary);\n * ```\n */\nexport class ProgressReporter {\n  /** Total number of file tasks in this run */\n  private readonly totalFiles: number;\n\n  /** Number of files that have started processing */\n  private started: number = 0;\n\n  /** Number of files completed successfully */\n  private completed: number = 0;\n\n  /** Number of files that failed */\n  private failed: number = 0;\n\n  /** Sliding window of recent completion durations for ETA */\n  private readonly completionTimes: number[] = [];\n\n  /** Maximum window size for ETA moving average */\n  private readonly windowSize: number = 10;\n\n  /** Timestamp when the reporter was created */\n  private readonly startTime: number = Date.now();\n\n  /** Total number of directory tasks in this run */\n  private totalDirectories: number = 0;\n\n  /** Number of directory tasks that have started */\n  private dirStarted: number = 0;\n\n  /** Number of directory tasks completed */\n  private dirCompleted: number = 0;\n\n  /** Sliding window of recent directory completion durations for ETA */\n  private readonly dirCompletionTimes: number[] = [];\n\n  /** Optional file-based progress log for tail -f monitoring */\n  private readonly progressLog: ProgressLog | null;\n\n  /**\n   * Create a new progress reporter.\n   *\n   * @param totalFiles - Total number of file tasks to process\n   * @param totalDirectories - Total number of directory tasks to process\n   * @param progressLog - Optional progress log for file-based output mirroring\n   */\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog) {\n    this.totalFiles = totalFiles;\n    this.totalDirectories = totalDirectories;\n    this.progressLog = progressLog ?? null;\n  }\n\n  /**\n   * Log the start of file analysis.\n   *\n   * Output format: `[X/Y] ANALYZING path`\n   *\n   * @param filePath - Relative path to the file being analyzed\n   */\n  onFileStart(filePath: string): void {\n    this.started++;\n    const line = `${pc.dim(`[${this.started}/${this.totalFiles}]`)} ${pc.cyan('ANALYZING')} ${filePath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the successful completion of file analysis.\n   *\n   * Output format: `[X/Y] DONE path Xs in/out tok ~Ns remaining`\n   *\n   * Records the completion time for ETA calculation.\n   *\n   * @param filePath - Relative path to the completed file\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onFileDone(\n    filePath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.completed++;\n\n    // Record completion time for ETA\n    this.completionTimes.push(durationMs);\n    if (this.completionTimes.length > this.windowSize) {\n      this.completionTimes.shift();\n    }\n\n    const counter = pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatETA();\n\n    const line = `${counter} ${pc.green('DONE')} ${filePath} ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log a file analysis failure.\n   *\n   * Output format: `[X/Y] FAIL path error`\n   *\n   * @param filePath - Relative path to the failed file\n   * @param error - Error message describing the failure\n   */\n  onFileError(filePath: string, error: string): void {\n    this.failed++;\n\n    const line = `${pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`)} ${pc.red('FAIL')} ${filePath} ${pc.dim(error)}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the start of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n   *\n   * @param dirPath - Path to the directory\n   */\n  onDirectoryStart(dirPath: string): void {\n    this.dirStarted++;\n    const line = `${pc.dim(`[dir ${this.dirStarted}/${this.totalDirectories}]`)} ${pc.cyan('ANALYZING')} ${dirPath}/AGENTS.md`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n   *\n   * @param dirPath - Path to the directory\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onDirectoryDone(\n    dirPath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.dirCompleted++;\n\n    // Record completion time for directory ETA\n    this.dirCompletionTimes.push(durationMs);\n    if (this.dirCompletionTimes.length > this.windowSize) {\n      this.dirCompletionTimes.shift();\n    }\n\n    const counter = pc.dim(`[dir ${this.dirCompleted}/${this.totalDirectories}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatDirectoryETA();\n\n    const line = `${counter} ${pc.blue('DONE')} ${dirPath}/AGENTS.md ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of a root document generation.\n   *\n   * Output format: `[root] DONE docPath`\n   *\n   * @param docPath - Path to the root document\n   */\n  onRootDone(docPath: string): void {\n    const line = `${pc.dim('[root]')} ${pc.blue('DONE')} ${docPath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Print the end-of-run summary.\n   *\n   * Shows files processed, token counts, files read with unique dedup,\n   * time elapsed, errors, and retries.\n   *\n   * @param summary - Aggregated run summary\n   */\n  printSummary(summary: RunSummary): void {\n    const elapsed = ((Date.now() - this.startTime) / 1000).toFixed(1);\n\n    const lines: string[] = [];\n    lines.push('');\n    lines.push(pc.bold('=== Run Summary ==='));\n    lines.push(`  ARE version:     ${summary.version}`);\n    lines.push(`  Files processed: ${pc.green(String(summary.filesProcessed))}`);\n    if (summary.filesFailed > 0) {\n      lines.push(`  Files failed:    ${pc.red(String(summary.filesFailed))}`);\n    }\n    if (summary.filesSkipped > 0) {\n      lines.push(`  Files skipped:   ${pc.yellow(String(summary.filesSkipped))}`);\n    }\n    lines.push(`  Total calls:     ${summary.totalCalls}`);\n    const totalIn = summary.totalInputTokens + summary.totalCacheReadTokens + summary.totalCacheCreationTokens;\n    lines.push(`  Tokens:          ${totalIn} in / ${summary.totalOutputTokens} out`);\n    if (summary.totalCacheReadTokens > 0) {\n      lines.push(`  Cache:           ${summary.totalCacheReadTokens} read / ${summary.totalCacheCreationTokens} created`);\n    }\n\n    if (summary.totalFilesRead > 0) {\n      lines.push(`  Files read:      ${summary.totalFilesRead} (${summary.uniqueFilesRead} unique)`);\n    }\n\n    lines.push(`  Total time:      ${elapsed}s`);\n    lines.push(`  Errors:          ${summary.errorCount}`);\n    if (summary.retryCount > 0) {\n      lines.push(`  Retries:         ${summary.retryCount}`);\n    }\n\n    for (const line of lines) {\n      console.log(line);\n      this.progressLog?.write(stripAnsi(line));\n    }\n  }\n\n  // -------------------------------------------------------------------------\n  // ETA calculation\n  // -------------------------------------------------------------------------\n\n  /**\n   * Compute and format the estimated time remaining.\n   *\n   * Uses a moving average of the last 10 completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   *\n   * @returns Formatted ETA string like ` ~12s remaining` or ` ~2m 30s remaining`\n   */\n  private formatETA(): string {\n    if (this.completionTimes.length < 2) return '';\n\n    const avg =\n      this.completionTimes.reduce((a, b) => a + b, 0) /\n      this.completionTimes.length;\n    const remaining = this.totalFiles - this.completed - this.failed;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n\n  /**\n   * Compute and format the estimated time remaining for directory tasks.\n   *\n   * Uses a moving average of the last 10 directory completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   */\n  private formatDirectoryETA(): string {\n    if (this.dirCompletionTimes.length < 2) return '';\n\n    const avg =\n      this.dirCompletionTimes.reduce((a, b) => a + b, 0) /\n      this.dirCompletionTimes.length;\n    const remaining = this.totalDirectories - this.dirCompleted;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ProgressReporter provides streaming build-log progress output with ETA calculation via moving averages, colored terminal formatting, and optional plain-text file mirroring for real-time monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog** — Plain-text progress log file writer mirroring console output to `.agents-reverse-engineer/progress.log` without ANSI escape codes, enabling `tail -f` monitoring when CLI runs inside buffered environments like Claude Code's Bash tool.\n\n- `constructor(filePath: string)` — Creates writer with promise-chain serialization for concurrent-safe writes\n- `static create(projectRoot: string): ProgressLog` — Factory creating instance with path `<projectRoot>/.agents-reverse-engineer/progress.log`\n- `write(line: string): void` — Appends line to log file, creating parent directory and opening file handle ('w' truncate mode) on first call, silently swallowing write failures\n- `finalize(): Promise<void>` — Flushes pending writes and closes file handle\n\nPrivate fields: `writeQueue: Promise<void>` (serialization chain), `fd: FileHandle | null` (open file handle)\n\n**ProgressReporter** — Streaming build-log reporter tracking file/directory task progress with ETA calculation from sliding window of last 10 completion times, outputting colored console logs and optional plain-text file mirroring.\n\n- `constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)` — Initializes reporter with task totals and optional file-based log\n- `onFileStart(filePath: string): void` — Logs file analysis start with format `[X/Y] ANALYZING path`\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — Logs file completion with format `[X/Y] DONE path Xs in/out tok model ~Ns remaining`, records duration for ETA calculation, computes total input tokens as `tokensIn + cacheReadTokens + cacheCreationTokens`\n- `onFileError(filePath: string, error: string): void` — Logs file failure with format `[X/Y] FAIL path error`\n- `onDirectoryStart(dirPath: string): void` — Logs directory AGENTS.md generation start with format `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — Logs directory completion with format `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`, records duration for directory ETA calculation\n- `onRootDone(docPath: string): void` — Logs root document completion with format `[root] DONE docPath`\n- `printSummary(summary: RunSummary): void` — Outputs end-of-run summary with version, files processed/failed/skipped, total calls, token counts (input/output/cache read/cache created), files read (total and unique count), elapsed time, error count, retry count\n\nPrivate fields: `totalFiles: number`, `started: number`, `completed: number`, `failed: number`, `completionTimes: number[]` (sliding window for file ETA), `windowSize: number = 10` (max window size), `startTime: number` (timestamp for elapsed calculation), `totalDirectories: number`, `dirStarted: number`, `dirCompleted: number`, `dirCompletionTimes: number[]` (sliding window for directory ETA), `progressLog: ProgressLog | null`\n\nPrivate methods:\n- `formatETA(): string` — Computes file ETA from moving average of `completionTimes`, returns empty string if fewer than 2 completions, formats as `~12s remaining` or `~2m 30s remaining`\n- `formatDirectoryETA(): string` — Computes directory ETA from moving average of `dirCompletionTimes`, returns empty string if fewer than 2 completions\n\n## Exported Functions\n\n**stripAnsi(str: string): string** — Strips ANSI escape sequences from string for plain-text output via regex pattern `/\\x1b\\[[0-9;]*m/g` (matches SGR, cursor, erase codes).\n\n## Dependencies\n\n**picocolors (`pc`)** — Used for terminal color formatting: `pc.dim()`, `pc.cyan()`, `pc.green()`, `pc.red()`, `pc.blue()`, `pc.bold()`, `pc.yellow()`\n\n**node:fs/promises** — `open()` for file handle creation, `mkdir()` for directory creation, `FileHandle` type\n\n**./types.js** — Imports `RunSummary` type containing aggregated run statistics (version, filesProcessed, filesFailed, filesSkipped, totalCalls, token counts, files read, elapsed time, error/retry counts)\n\n## Output Format Specifications\n\n**File progress lines:**\n- Start: `[X/Y] ANALYZING path`\n- Done: `[X/Y] DONE path Xs in/out tok model ~Ns remaining`\n- Fail: `[X/Y] FAIL path error`\n\n**Directory progress lines:**\n- Start: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- Done: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n\n**Root progress lines:**\n- Done: `[root] DONE docPath`\n\n**Summary format:**\n```\n=== Run Summary ===\n  ARE version:     <version>\n  Files processed: <count>\n  Files failed:    <count>  (if > 0)\n  Files skipped:   <count>  (if > 0)\n  Total calls:     <count>\n  Tokens:          <totalIn> in / <totalOut> out\n  Cache:           <cacheRead> read / <cacheCreated> created  (if cacheRead > 0)\n  Files read:      <total> (<unique> unique)  (if totalFilesRead > 0)\n  Total time:      <elapsed>s\n  Errors:          <count>\n  Retries:         <count>  (if > 0)\n```\n\n## Constants\n\n**PROGRESS_LOG_FILENAME** — `'progress.log'` (relative path for log file)\n\n## Implementation Patterns\n\n**Promise-chain serialization** — `ProgressLog.writeQueue` uses same pattern as `TraceWriter` to serialize concurrent writes from pool workers: `this.writeQueue = this.writeQueue.then(async () => { ... }).catch(() => {})`, preventing NDJSON corruption\n\n**Moving average ETA** — `completionTimes` and `dirCompletionTimes` arrays maintain sliding window (max size `windowSize = 10`) of recent task durations, compute average via `reduce((a, b) => a + b, 0) / length`, multiply by remaining tasks to estimate time remaining\n\n**Atomic console output** — Uses `console.log()` for each line instead of buffered streams to prevent interleaved output from concurrent pool workers\n\n**ANSI stripping** — `stripAnsi()` removes color codes before writing to `ProgressLog` via regex `/\\x1b\\[[0-9;]*m/g` for plain-text tail monitoring\n\n**Lazy file handle creation** — `ProgressLog.fd` remains null until first write, then opens in truncate mode ('w') and reuses handle for subsequent appends\n\n**Graceful degradation** — Progress log write failures are silently caught (`catch(() => {})`) as non-critical telemetry loss is acceptable",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1871,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13553,
      "latencyMs": 49196,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/progress.ts",
          "sizeBytes": 14428
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:55.922Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/index.ts\n\n```typescript\nexport { buildSpecPrompt } from './prompts.js';\nexport type { SpecPrompt } from './prompts.js';\n\nexport { writeSpec, SpecExistsError } from './writer.js';\nexport type { WriteSpecOptions } from './writer.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting specification synthesis API comprising prompt builder (`buildSpecPrompt`), filesystem writer (`writeSpec`, `SpecExistsError`), and associated types (`SpecPrompt`, `WriteSpecOptions`).**\n\n## Exported Symbols\n\n- **`buildSpecPrompt`** — Function from `./prompts.js` constructing prompts for AI-driven project specification synthesis from `AGENTS.md` corpus\n- **`SpecPrompt`** — Type from `./prompts.js` defining prompt structure for specification generation\n- **`writeSpec`** — Function from `./writer.js` writing synthesized specification to filesystem (single-file or multi-file output)\n- **`SpecExistsError`** — Error class from `./writer.js` thrown when target specification files already exist without `force` flag\n- **`WriteSpecOptions`** — Type from `./writer.js` defining configuration for `writeSpec` (output mode, force overwrite, target directory)\n\n## Module Purpose\n\nConsolidates specification synthesis functionality into single import point for `src/cli/specify.ts` command implementation. Specification synthesis consumes all `AGENTS.md` directory documentation files and produces `specs/SPEC.md` (single-file) or `specs/<dirname>.md` (multi-file) project specifications via AI-driven aggregation. Prompt builder likely includes constraints enforcing synthesis-only behavior (no invention of features not present in source documents).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 329,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9262,
      "latencyMs": 19145,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/index.ts",
          "sizeBytes": 208
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:57.783Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/types/index.ts\n\n```typescript\n/**\n * Shared types used across modules\n */\n\n/**\n * Information about an excluded file\n */\nexport interface ExcludedFile {\n  /** Absolute or relative path to the excluded file */\n  path: string;\n  /** Reason for exclusion (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\") */\n  reason: string;\n}\n\n/**\n * Result of the file discovery process\n */\nexport interface DiscoveryResult {\n  /** List of file paths that should be analyzed */\n  files: string[];\n  /** List of files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Statistics about the discovery process\n */\nexport interface DiscoveryStats {\n  /** Total number of files found */\n  totalFiles: number;\n  /** Number of files included for analysis */\n  includedFiles: number;\n  /** Number of files excluded */\n  excludedFiles: number;\n  /** Breakdown of exclusion reasons */\n  exclusionReasons: Record<string, number>;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines core TypeScript interfaces for file discovery results, exclusion metadata, and discovery statistics shared across discovery and change-detection modules.**\n\n## Exported Interfaces\n\n**ExcludedFile** represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to the excluded file\n- `reason: string` — Exclusion rationale string (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** encapsulates file discovery output consumed by generation orchestrators:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics and reporting)\n\n**DiscoveryStats** aggregates discovery metrics for progress reporting and telemetry:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Map of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n`DiscoveryResult` is returned by `discoverFiles()` in `src/discovery/run.ts` and consumed by `generateDocumentation()` in `src/generation/orchestrator.ts`.\n\n`DiscoveryStats` is computed from `DiscoveryResult` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, then logged by `src/output/logger.ts` and written to `GENERATION-PLAN.md`.\n\n`ExcludedFile` is produced by filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) and aggregated into `DiscoveryResult.excluded`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 455,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9451,
      "latencyMs": 20025,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/types/index.ts",
          "sizeBytes": 909
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:53.489Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/types.ts\n\n```typescript\n/**\n * Types for the rebuild module.\n *\n * Provides Zod-validated checkpoint schema and interfaces for rebuild\n * units, plans, and results.\n *\n * @module\n */\n\nimport { z } from 'zod';\n\n/**\n * Zod schema for the rebuild checkpoint file.\n *\n * Validated when reading from disk to detect corruption or version mismatch.\n */\nexport const RebuildCheckpointSchema = z.object({\n  version: z.string(),\n  createdAt: z.string(),\n  updatedAt: z.string(),\n  outputDir: z.string(),\n  specHashes: z.record(z.string()),\n  modules: z.record(z.object({\n    status: z.enum(['pending', 'done', 'failed']),\n    completedAt: z.string().optional(),\n    error: z.string().optional(),\n    filesWritten: z.array(z.string()).optional(),\n  })),\n});\n\n/**\n * Checkpoint state persisted to `.rebuild-checkpoint` inside the output directory.\n *\n * Tracks per-module completion status and spec file hashes for drift detection.\n */\nexport type RebuildCheckpoint = z.infer<typeof RebuildCheckpointSchema>;\n\n/**\n * A single rebuild unit representing one AI call.\n *\n * Each unit produces all files for a logical module/phase of the project.\n */\nexport interface RebuildUnit {\n  /** Unit name derived from spec section heading */\n  name: string;\n  /** The spec section content for this unit */\n  specContent: string;\n  /** Execution order from Build Plan phase numbering */\n  order: number;\n}\n\n/**\n * The full rebuild plan computed from spec files before execution.\n */\nexport interface RebuildPlan {\n  /** Spec files read from specs/ directory */\n  specFiles: Array<{ relativePath: string; content: string }>;\n  /** Ordered rebuild units extracted from spec content */\n  units: RebuildUnit[];\n  /** Output directory for rebuilt project */\n  outputDir: string;\n}\n\n/**\n * Result of rebuilding a single unit.\n */\nexport interface RebuildResult {\n  /** Name of the rebuild unit */\n  unitName: string;\n  /** Whether the rebuild succeeded */\n  success: boolean;\n  /** Relative paths of files written within the output directory */\n  filesWritten: string[];\n  /** Input tokens consumed */\n  tokensIn: number;\n  /** Output tokens produced */\n  tokensOut: number;\n  /** Cache read tokens */\n  cacheReadTokens: number;\n  /** Cache creation tokens */\n  cacheCreationTokens: number;\n  /** Duration in milliseconds */\n  durationMs: number;\n  /** Model used for generation */\n  model: string;\n  /** Error message if rebuild failed */\n  error?: string;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines Zod-validated checkpoint schema and TypeScript interfaces for the rebuild module's state persistence, unit planning, and execution results.**\n\n## Exported Types and Schema\n\n`RebuildCheckpointSchema` validates checkpoint JSON structure with fields:\n- `version` (string) — checkpoint format version\n- `createdAt` (string) — ISO 8601 creation timestamp\n- `updatedAt` (string) — ISO 8601 last update timestamp\n- `outputDir` (string) — absolute path to rebuild target directory\n- `specHashes` (Record<string, string>) — SHA-256 hashes of spec files for drift detection\n- `modules` (Record<string, ModuleStatus>) — per-module completion state with nested object schema:\n  - `status` (enum: `'pending' | 'done' | 'failed'`)\n  - `completedAt` (optional string) — ISO 8601 completion timestamp\n  - `error` (optional string) — error message for failed modules\n  - `filesWritten` (optional string[]) — relative paths of generated files\n\n`RebuildCheckpoint` type inferred from `RebuildCheckpointSchema` via `z.infer<>`, represents state persisted to `.rebuild-checkpoint` file inside output directory.\n\n`RebuildUnit` interface defines single rebuild work unit:\n- `name` (string) — derived from spec section heading\n- `specContent` (string) — spec section markdown content for AI prompt\n- `order` (number) — execution sequence from Build Plan phase numbering\n\n`RebuildPlan` interface defines full rebuild plan computed before execution:\n- `specFiles` (Array<{relativePath: string, content: string}>) — spec files read from `specs/` directory\n- `units` (RebuildUnit[]) — ordered rebuild units extracted from spec content\n- `outputDir` (string) — target directory for rebuilt project\n\n`RebuildResult` interface captures single unit execution outcome:\n- `unitName` (string) — name of rebuild unit\n- `success` (boolean) — whether rebuild succeeded\n- `filesWritten` (string[]) — relative paths within output directory\n- `tokensIn` (number) — input tokens consumed\n- `tokensOut` (number) — output tokens produced\n- `cacheReadTokens` (number) — cache read tokens\n- `cacheCreationTokens` (number) — cache creation tokens\n- `durationMs` (number) — execution duration in milliseconds\n- `model` (string) — AI model used for generation\n- `error` (optional string) — error message if rebuild failed\n\n## Checkpoint Persistence Pattern\n\n`RebuildCheckpoint` serializes to `.rebuild-checkpoint` JSON file enabling resume-on-failure workflow. `specHashes` field enables detection of spec file changes requiring re-execution. `modules` record maps unit names to status enums tracking completion state across interrupted runs.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 672,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9896,
      "latencyMs": 24355,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/types.ts",
          "sizeBytes": 2406
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:43.728Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/orchestrator.ts\n\n```typescript\n/**\n * Rebuild execution orchestrator.\n *\n * Standalone async function that wires together the spec reader, checkpoint\n * manager, AI service, concurrency pool, and progress reporter into a\n * working rebuild pipeline. Processes rebuild units grouped by order value:\n * all units in a group run concurrently via runPool, and groups execute\n * sequentially to respect ordering dependencies.\n *\n * After each order group completes, exported type signatures are extracted\n * from the generated files and accumulated as context for subsequent groups.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { writeFile, mkdir, readFile, rm } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport { runPool, ProgressReporter, type ProgressLog, type ITraceWriter } from '../orchestration/index.js';\nimport { CheckpointManager } from './checkpoint.js';\nimport { readSpecFiles, partitionSpec } from './spec-reader.js';\nimport { parseModuleOutput } from './output-parser.js';\nimport type { RebuildUnit, RebuildResult } from './types.js';\nimport { buildRebuildPrompt } from './prompts.js';\n\n// ---------------------------------------------------------------------------\n// Options\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the rebuild execution pipeline.\n */\nexport interface RebuildExecutionOptions {\n  /** Absolute path to the output directory */\n  outputDir: string;\n  /** Maximum concurrent AI calls within each order group */\n  concurrency: number;\n  /** Stop on first failure */\n  failFast?: boolean;\n  /** Wipe output directory and start fresh */\n  force?: boolean;\n  /** Enable verbose debug logging */\n  debug?: boolean;\n  /** Trace writer for concurrency debugging */\n  tracer?: ITraceWriter;\n  /** Progress log for tail -f monitoring */\n  progressLog?: ProgressLog;\n}\n\n// ---------------------------------------------------------------------------\n// Context accumulation\n// ---------------------------------------------------------------------------\n\n/** Default character limit before truncating older group context */\nconst BUILT_CONTEXT_LIMIT = 100_000;\n\n/** Number of lines to keep from truncated files (typically imports + type declarations) */\nconst TRUNCATED_HEAD_LINES = 20;\n\n// ---------------------------------------------------------------------------\n// executeRebuild\n// ---------------------------------------------------------------------------\n\n/**\n * Execute the rebuild pipeline.\n *\n * Reads spec files, partitions into units, loads/creates checkpoint,\n * processes units grouped by order value (sequential groups, concurrent\n * within each group), accumulates built context, and returns summary.\n *\n * @param aiService - Configured AI service instance\n * @param projectRoot - Absolute path to the project root\n * @param options - Rebuild execution options\n * @returns Summary with counts of processed, failed, and skipped modules\n */\nexport async function executeRebuild(\n  aiService: AIService,\n  projectRoot: string,\n  options: RebuildExecutionOptions,\n): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }> {\n  const { outputDir, concurrency, tracer, progressLog } = options;\n\n  // 1. Read specs\n  const specFiles = await readSpecFiles(projectRoot);\n\n  // 2. Partition into units\n  const units = partitionSpec(specFiles);\n\n  if (options.debug) {\n    console.error(`[debug] Rebuild units (${units.length}):`);\n    for (const unit of units) {\n      console.error(`[debug]   order=${unit.order} name=\"${unit.name}\"`);\n    }\n  }\n\n  // 3. Handle --force: wipe output directory\n  if (options.force) {\n    await rm(outputDir, { recursive: true, force: true });\n  }\n\n  // Ensure output directory exists\n  await mkdir(outputDir, { recursive: true });\n\n  // 4. Load/create checkpoint\n  const unitNames = units.map((u) => u.name);\n  const { manager: checkpoint, isResume } = await CheckpointManager.load(\n    outputDir,\n    specFiles,\n    unitNames,\n  );\n\n  if (isResume) {\n    const pending = checkpoint.getPendingUnits();\n    const done = unitNames.length - pending.length;\n    console.log(`Resuming from checkpoint: ${done} of ${unitNames.length} modules already complete`);\n    progressLog?.write(`Resuming from checkpoint: ${done} of ${unitNames.length} modules already complete`);\n  }\n\n  // 5. Initialize checkpoint (write to disk)\n  await checkpoint.initialize();\n\n  // 6. Filter to pending units\n  let modulesSkipped = 0;\n  const pendingUnits: RebuildUnit[] = [];\n  for (const unit of units) {\n    if (checkpoint.isDone(unit.name)) {\n      modulesSkipped++;\n    } else {\n      pendingUnits.push(unit);\n    }\n  }\n\n  if (pendingUnits.length === 0) {\n    console.log('All modules already complete. Nothing to rebuild.');\n    progressLog?.write('All modules already complete. Nothing to rebuild.');\n    return { modulesProcessed: 0, modulesFailed: 0, modulesSkipped };\n  }\n\n  // 7. Create progress reporter\n  const reporter = new ProgressReporter(pendingUnits.length, 0, progressLog);\n\n  // 8. Concatenate full spec for prompt context\n  const fullSpec = specFiles.map((f) => f.content).join('\\n\\n');\n\n  // 9. Context accumulator for export signatures\n  let builtContext = '';\n\n  // 10. Group units by order value\n  const orderGroups = new Map<number, RebuildUnit[]>();\n  for (const unit of pendingUnits) {\n    const group = orderGroups.get(unit.order) ?? [];\n    group.push(unit);\n    orderGroups.set(unit.order, group);\n  }\n\n  // Sort order keys ascending\n  const sortedOrders = [...orderGroups.keys()].sort((a, b) => a - b);\n\n  let modulesProcessed = 0;\n  let modulesFailed = 0;\n\n  // 11. Process each order group sequentially\n  for (const orderValue of sortedOrders) {\n    const groupUnits = orderGroups.get(orderValue)!;\n\n    tracer?.emit({\n      type: 'phase:start',\n      phase: `rebuild-order-${orderValue}`,\n      taskCount: groupUnits.length,\n      concurrency,\n    });\n\n    const groupStart = Date.now();\n    const filesWrittenInGroup: string[] = [];\n\n    // Create pool tasks for this group\n    const groupTasks = groupUnits.map((unit) => async (): Promise<RebuildResult> => {\n      reporter.onFileStart(unit.name);\n      const callStart = Date.now();\n\n      const prompt = buildRebuildPrompt(unit, fullSpec, builtContext || undefined);\n      const response: AIResponse = await aiService.call({\n        prompt: prompt.user,\n        systemPrompt: prompt.system,\n        taskLabel: `rebuild:${unit.name}`,\n      });\n\n      // Parse files from response\n      const files = parseModuleOutput(response.text);\n      if (files.size === 0) {\n        throw new Error(\n          `AI produced no files for unit \"${unit.name}\". Response may have used unexpected format.`,\n        );\n      }\n\n      // Write files to output directory\n      const filesWritten: string[] = [];\n      for (const [filePath, content] of files) {\n        const absolutePath = path.join(outputDir, filePath);\n        await mkdir(path.dirname(absolutePath), { recursive: true });\n        await writeFile(absolutePath, content, 'utf-8');\n        filesWritten.push(filePath);\n      }\n\n      // Update checkpoint\n      checkpoint.markDone(unit.name, filesWritten);\n\n      const durationMs = Date.now() - callStart;\n      return {\n        unitName: unit.name,\n        success: true,\n        filesWritten,\n        tokensIn: response.inputTokens,\n        tokensOut: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        durationMs,\n        model: response.model,\n      };\n    });\n\n    // 12. Run pool with onComplete callback\n    await runPool(\n      groupTasks,\n      {\n        concurrency,\n        failFast: options.failFast,\n        tracer,\n        phaseLabel: `rebuild-order-${orderValue}`,\n        taskLabels: groupUnits.map((u) => u.name),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          modulesProcessed++;\n          filesWrittenInGroup.push(...v.filesWritten);\n          reporter.onFileDone(\n            v.unitName,\n            v.durationMs,\n            v.tokensIn,\n            v.tokensOut,\n            v.model,\n            v.cacheReadTokens,\n            v.cacheCreationTokens,\n          );\n        } else {\n          modulesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const unitName = groupUnits[result.index]?.name ?? `unit-${result.index}`;\n          checkpoint.markFailed(unitName, errorMsg);\n          reporter.onFileError(unitName, errorMsg);\n        }\n      },\n    );\n\n    tracer?.emit({\n      type: 'phase:end',\n      phase: `rebuild-order-${orderValue}`,\n      durationMs: Date.now() - groupStart,\n      tasksCompleted: modulesProcessed,\n      tasksFailed: modulesFailed,\n    });\n\n    // 13. After group completes, accumulate full file content as context\n    for (const filePath of filesWrittenInGroup) {\n      // Skip non-source files (configs, docs)\n      if (filePath.endsWith('.md') || filePath.endsWith('.json') || filePath.endsWith('.yml')) continue;\n      try {\n        const content = await readFile(path.join(outputDir, filePath), 'utf-8');\n        builtContext += `\\n// === ${filePath} ===\\n${content}\\n`;\n      } catch {\n        // Non-critical: skip unreadable files\n      }\n    }\n\n    // Truncate older context if it exceeds the limit\n    if (builtContext.length > BUILT_CONTEXT_LIMIT) {\n      const sections = builtContext.split(/\\n\\/\\/ === /);\n      // Keep the first empty section prefix, truncate older sections\n      const recentCount = Math.max(1, Math.floor(sections.length / 2));\n      const olderSections = sections.slice(1, sections.length - recentCount);\n      const recentSections = sections.slice(sections.length - recentCount);\n\n      let truncated = '';\n      for (const section of olderSections) {\n        const lines = section.split('\\n');\n        const head = lines.slice(0, TRUNCATED_HEAD_LINES).join('\\n');\n        truncated += `\\n// === ${head}\\n// ... (truncated)\\n`;\n      }\n      for (const section of recentSections) {\n        truncated += `\\n// === ${section}`;\n      }\n      builtContext = truncated;\n\n      if (options.debug) {\n        console.error(`[debug] Built context truncated: keeping last ${recentCount} groups in full`);\n      }\n    }\n  }\n\n  // 14. Flush checkpoint\n  await checkpoint.flush();\n\n  // 15. Return summary\n  return { modulesProcessed, modulesFailed, modulesSkipped };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orchestrator.ts orchestrates the rebuild pipeline by partitioning spec files into ordered groups (RebuildUnit[]), executing them sequentially by order value with concurrent AI-driven file generation within each group via runPool, accumulating export signatures as built context for subsequent groups, and managing checkpoint persistence for resumability.**\n\n## Exported Interface\n\n**executeRebuild(aiService: AIService, projectRoot: string, options: RebuildExecutionOptions): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }>**\nMain pipeline executor that reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads/creates checkpoint via `CheckpointManager.load()`, groups units by `order` field, processes each order group sequentially while running units within each group concurrently via `runPool()`, accumulates built context from generated files, and returns summary counts.\n\n**RebuildExecutionOptions**\nConfiguration interface with fields:\n- `outputDir: string` — absolute path to output directory\n- `concurrency: number` — max concurrent AI calls within each order group\n- `failFast?: boolean` — abort on first failure\n- `force?: boolean` — wipe output directory and start fresh\n- `debug?: boolean` — enable verbose debug logging\n- `tracer?: ITraceWriter` — trace writer for concurrency debugging\n- `progressLog?: ProgressLog` — progress log for tail -f monitoring\n\n## Rebuild Pipeline Stages\n\n**Stage 1: Spec Loading and Partitioning**\nCalls `readSpecFiles(projectRoot)` to load `.md` spec files from `specs/` directory, then `partitionSpec(specFiles)` to extract RebuildUnit objects with `name`, `order`, `outPath`, `dependencies`, `specContent` fields.\n\n**Stage 2: Checkpoint Management**\nInvokes `CheckpointManager.load(outputDir, specFiles, unitNames)` returning `{ manager, isResume }`. If `isResume === true`, logs count of already-complete modules via `checkpoint.getPendingUnits()`. Calls `checkpoint.initialize()` to write `.rebuild-checkpoint.json` to disk. Filters units via `checkpoint.isDone(unit.name)` to build `pendingUnits[]` array, increments `modulesSkipped` for completed units.\n\n**Stage 3: Order Group Execution**\nGroups pendingUnits by `unit.order` value into `Map<number, RebuildUnit[]>`, sorts keys ascending via `[...orderGroups.keys()].sort((a, b) => a - b)`. For each orderValue sequentially:\n1. Emits `tracer.emit({ type: 'phase:start', phase: 'rebuild-order-${orderValue}', taskCount, concurrency })`\n2. Creates pool tasks mapping each unit to async function calling `buildRebuildPrompt(unit, fullSpec, builtContext)` → `aiService.call()` → `parseModuleOutput(response.text)` → `writeFile()` for each parsed file → `checkpoint.markDone(unit.name, filesWritten)` → returns RebuildResult\n3. Calls `runPool(groupTasks, { concurrency, failFast, tracer, phaseLabel, taskLabels }, onCompleteCallback)`\n4. onComplete callback increments `modulesProcessed`/`modulesFailed`, calls `reporter.onFileDone()` or `reporter.onFileError()`, invokes `checkpoint.markFailed(unitName, errorMsg)` on failure\n5. Emits `tracer.emit({ type: 'phase:end', phase, durationMs, tasksCompleted, tasksFailed })`\n\n**Stage 4: Context Accumulation**\nAfter each order group completes, reads all `filesWrittenInGroup` via `readFile(path.join(outputDir, filePath))`, skips non-source files (`.md`, `.json`, `.yml`), appends content to `builtContext` string with `// === ${filePath} ===` delimiter. If `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits by `\\n// === ` delimiter, keeps recent sections in full, truncates older sections to first `TRUNCATED_HEAD_LINES` (20) lines with `// ... (truncated)` marker.\n\n**Stage 5: Finalization**\nCalls `checkpoint.flush()` to persist final state, returns summary object with `modulesProcessed`, `modulesFailed`, `modulesSkipped` counts.\n\n## Progress Reporting\n\nCreates `ProgressReporter(pendingUnits.length, 0, progressLog)` for streaming output. Calls `reporter.onFileStart(unit.name)` before AI call, `reporter.onFileDone(unitName, durationMs, tokensIn, tokensOut, model, cacheReadTokens, cacheCreationTokens)` on success, `reporter.onFileError(unitName, errorMsg)` on failure. All progress events mirrored to `progressLog.write()` for `tail -f` monitoring.\n\n## Dependencies\n\nImports `runPool` and `ProgressReporter` from `../orchestration/index.js`, `CheckpointManager` from `./checkpoint.js`, `readSpecFiles` and `partitionSpec` from `./spec-reader.js`, `parseModuleOutput` from `./output-parser.js`, `buildRebuildPrompt` from `./prompts.js`, `AIService` and `AIResponse` from `../ai/` types. Uses `writeFile`, `mkdir`, `readFile`, `rm` from `node:fs/promises` for disk I/O.\n\n## Error Handling\n\nThrows if `parseModuleOutput()` returns empty Map: `\"AI produced no files for unit \"${unit.name}\". Response may have used unexpected format.\"` Pool failures captured in runPool onComplete callback with `result.error?.message ?? 'Unknown error'`, logged via `checkpoint.markFailed()` and `reporter.onFileError()`. Non-critical errors during context accumulation (unreadable files) silently skipped via try-catch.\n\n## Configuration Constants\n\n**BUILT_CONTEXT_LIMIT = 100_000**\nCharacter threshold before truncating older group context to prevent unbounded memory growth.\n\n**TRUNCATED_HEAD_LINES = 20**\nNumber of lines kept from truncated files (typically imports + type declarations) when context exceeds limit.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1512,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12138,
      "latencyMs": 34198,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/orchestrator.ts",
          "sizeBytes": 10509
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:39.061Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/checkpoint.ts\n\n```typescript\n/**\n * Checkpoint manager for rebuild session continuity.\n *\n * Persists per-module completion status and spec file hashes inside the\n * output directory. Uses promise-chain write serialization (same pattern\n * as PlanTracker) to handle concurrent pool worker updates safely.\n *\n * @module\n */\n\nimport { writeFile, readFile, mkdir, rm } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport { RebuildCheckpointSchema, type RebuildCheckpoint } from './types.js';\nimport { getVersion } from '../version.js';\n\n/**\n * Manages rebuild checkpoint state for session continuity.\n *\n * Create via the static `load()` or `createFresh()` factory methods.\n * Call `markDone()` / `markFailed()` as modules complete, and `flush()`\n * before returning to ensure all writes finish.\n */\nexport class CheckpointManager {\n  private data: RebuildCheckpoint;\n  private readonly checkpointPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(outputDir: string, initialData: RebuildCheckpoint) {\n    this.checkpointPath = path.join(outputDir, '.rebuild-checkpoint');\n    this.data = initialData;\n  }\n\n  /**\n   * Load an existing checkpoint or create a new one.\n   *\n   * If a checkpoint file exists and is valid, checks for spec drift by\n   * comparing stored hashes against current spec file content hashes.\n   * Returns `isResume: true` only if the checkpoint is valid and specs\n   * haven't changed.\n   *\n   * @param outputDir - Absolute path to the output directory\n   * @param specFiles - Current spec files with content for drift detection\n   * @param unitNames - Names of all rebuild units (for fresh checkpoint initialization)\n   * @returns CheckpointManager instance and whether this is a resume\n   */\n  static async load(\n    outputDir: string,\n    specFiles: Array<{ relativePath: string; content: string }>,\n    unitNames: string[],\n  ): Promise<{ manager: CheckpointManager; isResume: boolean }> {\n    const checkpointPath = path.join(outputDir, '.rebuild-checkpoint');\n\n    let raw: string;\n    try {\n      raw = await readFile(checkpointPath, 'utf-8');\n    } catch {\n      // No checkpoint file -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    // Parse and validate\n    let parsed: unknown;\n    try {\n      parsed = JSON.parse(raw);\n    } catch {\n      // Corrupted JSON -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    const result = RebuildCheckpointSchema.safeParse(parsed);\n    if (!result.success) {\n      // Schema validation failed -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    const checkpoint = result.data;\n\n    // Check for spec drift\n    const currentHashes: Record<string, string> = {};\n    for (const spec of specFiles) {\n      currentHashes[spec.relativePath] = computeContentHashFromString(spec.content);\n    }\n\n    // Compare hash counts (files added or removed)\n    const storedPaths = Object.keys(checkpoint.specHashes);\n    const currentPaths = Object.keys(currentHashes);\n    if (storedPaths.length !== currentPaths.length) {\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    // Compare individual hashes\n    for (const specPath of currentPaths) {\n      if (checkpoint.specHashes[specPath] !== currentHashes[specPath]) {\n        const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n        return { manager, isResume: false };\n      }\n    }\n\n    // Valid checkpoint, no drift -- resume\n    const manager = new CheckpointManager(outputDir, checkpoint);\n    return { manager, isResume: true };\n  }\n\n  /**\n   * Create a fresh checkpoint with all modules set to pending.\n   *\n   * @param outputDir - Absolute path to the output directory\n   * @param specFiles - Spec files with content for hash computation\n   * @param unitNames - Names of all rebuild units\n   * @returns New CheckpointManager instance\n   */\n  static createFresh(\n    outputDir: string,\n    specFiles: Array<{ relativePath: string; content: string }>,\n    unitNames: string[],\n  ): CheckpointManager {\n    const specHashes: Record<string, string> = {};\n    for (const spec of specFiles) {\n      specHashes[spec.relativePath] = computeContentHashFromString(spec.content);\n    }\n\n    const modules: RebuildCheckpoint['modules'] = {};\n    for (const name of unitNames) {\n      modules[name] = { status: 'pending' };\n    }\n\n    const now = new Date().toISOString();\n    const data: RebuildCheckpoint = {\n      version: getVersion(),\n      createdAt: now,\n      updatedAt: now,\n      outputDir,\n      specHashes,\n      modules,\n    };\n\n    return new CheckpointManager(outputDir, data);\n  }\n\n  /**\n   * Mark a unit as successfully completed.\n   *\n   * Queues a serialized write to the checkpoint file.\n   */\n  markDone(unitName: string, filesWritten: string[]): void {\n    this.data.modules[unitName] = {\n      status: 'done',\n      completedAt: new Date().toISOString(),\n      filesWritten,\n    };\n    this.data.updatedAt = new Date().toISOString();\n    this.queueWrite();\n  }\n\n  /**\n   * Mark a unit as failed.\n   *\n   * Queues a serialized write to the checkpoint file.\n   */\n  markFailed(unitName: string, error: string): void {\n    this.data.modules[unitName] = {\n      status: 'failed',\n      error,\n    };\n    this.data.updatedAt = new Date().toISOString();\n    this.queueWrite();\n  }\n\n  /**\n   * Get names of units that are pending or failed (eligible for execution).\n   */\n  getPendingUnits(): string[] {\n    return Object.entries(this.data.modules)\n      .filter(([, mod]) => mod.status === 'pending' || mod.status === 'failed')\n      .map(([name]) => name);\n  }\n\n  /**\n   * Check if a unit has been completed.\n   */\n  isDone(unitName: string): boolean {\n    return this.data.modules[unitName]?.status === 'done';\n  }\n\n  /**\n   * Wait for all queued writes to finish.\n   */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n\n  /**\n   * Create the output directory if needed and write the initial checkpoint file.\n   */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.checkpointPath), { recursive: true });\n      await writeFile(\n        this.checkpointPath,\n        JSON.stringify(this.data, null, 2),\n        'utf-8',\n      );\n    } catch {\n      // Non-critical -- rebuild continues without persistent checkpoint\n    }\n  }\n\n  /**\n   * Return the current checkpoint data (for dry-run display or inspection).\n   */\n  getData(): RebuildCheckpoint {\n    return this.data;\n  }\n\n  /**\n   * Queue a serialized write to prevent file corruption from concurrent calls.\n   *\n   * Follows the PlanTracker promise-chain pattern.\n   */\n  private queueWrite(): void {\n    this.writeQueue = this.writeQueue\n      .then(() =>\n        writeFile(\n          this.checkpointPath,\n          JSON.stringify(this.data, null, 2),\n          'utf-8',\n        ),\n      )\n      .catch(() => {\n        /* non-critical */\n      });\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CheckpointManager provides session continuity for rebuild operations via serialized writes of per-module completion status and spec file hash-based drift detection.**\n\n## Exported Symbols\n\n**CheckpointManager** — Main class managing rebuild checkpoint persistence with promise-chain write serialization to prevent corruption from concurrent worker updates.\n\n### Static Factory Methods\n\n- `static async load(outputDir: string, specFiles: Array<{ relativePath: string; content: string }>, unitNames: string[]): Promise<{ manager: CheckpointManager; isResume: boolean }>` — Loads existing checkpoint from `<outputDir>/.rebuild-checkpoint`, validates schema via `RebuildCheckpointSchema.safeParse()`, compares stored spec hashes against current hashes via `computeContentHashFromString()`, returns `isResume: true` only if checkpoint exists and no spec drift detected, otherwise creates fresh checkpoint.\n\n- `static createFresh(outputDir: string, specFiles: Array<{ relativePath: string; content: string }>, unitNames: string[]): CheckpointManager` — Constructs new checkpoint with all modules set to `status: 'pending'`, computes spec content hashes via `computeContentHashFromString()`, initializes `RebuildCheckpoint.modules` record from `unitNames[]`, sets `createdAt`/`updatedAt` to current ISO timestamp.\n\n### Instance Methods\n\n- `markDone(unitName: string, filesWritten: string[]): void` — Updates `modules[unitName]` to `{ status: 'done', completedAt: ISO timestamp, filesWritten }`, updates `data.updatedAt`, queues serialized write via `queueWrite()`.\n\n- `markFailed(unitName: string, error: string): void` — Updates `modules[unitName]` to `{ status: 'failed', error }`, updates `data.updatedAt`, queues serialized write via `queueWrite()`.\n\n- `getPendingUnits(): string[]` — Returns array of module names with `status: 'pending'` or `'failed'` (eligible for execution).\n\n- `isDone(unitName: string): boolean` — Returns true if `modules[unitName].status === 'done'`.\n\n- `async flush(): Promise<void>` — Waits for `writeQueue` promise chain to complete, ensuring all pending writes finish before returning.\n\n- `async initialize(): Promise<void>` — Creates output directory via `mkdir()` with `recursive: true`, writes initial checkpoint JSON to `<outputDir>/.rebuild-checkpoint`, swallows errors (non-critical failure).\n\n- `getData(): RebuildCheckpoint` — Returns current checkpoint data for inspection or dry-run display.\n\n### Private Implementation\n\n- `private writeQueue: Promise<void>` — Promise chain for serializing concurrent writes from pool workers (same pattern as `PlanTracker` and `TraceWriter`).\n\n- `private queueWrite(): void` — Appends serialized write to `writeQueue` via `.then()` chain, writes `JSON.stringify(this.data, null, 2)` to `checkpointPath`, swallows errors with empty catch block (non-critical).\n\n## Data Structure\n\n**RebuildCheckpoint** (from `./types.js`) — Validated via `RebuildCheckpointSchema` Zod schema, contains:\n- `version: string` — ARE version via `getVersion()`\n- `createdAt: string` / `updatedAt: string` — ISO 8601 timestamps\n- `outputDir: string` — Absolute path to rebuild output directory\n- `specHashes: Record<string, string>` — Map of spec file relative paths to SHA-256 content hashes (drift detection)\n- `modules: Record<string, ModuleStatus>` — Per-unit completion tracking\n\n**ModuleStatus** discriminated union:\n- `{ status: 'pending' }` — Not yet processed\n- `{ status: 'done', completedAt: string, filesWritten: string[] }` — Successfully completed with output file list\n- `{ status: 'failed', error: string }` — Failed with error message\n\n## Drift Detection Algorithm\n\n1. `load()` reads checkpoint JSON from `.rebuild-checkpoint`\n2. Parses and validates via `RebuildCheckpointSchema.safeParse()`, returns fresh checkpoint on failure\n3. Computes `currentHashes` map via `computeContentHashFromString()` for each spec file\n4. Compares hash count: if `Object.keys(checkpoint.specHashes).length !== Object.keys(currentHashes).length`, returns fresh checkpoint\n5. Compares individual hashes: if any `checkpoint.specHashes[path] !== currentHashes[path]`, returns fresh checkpoint\n6. Returns `isResume: true` only if all checks pass\n\n## Concurrency Safety\n\nFollows promise-chain serialization pattern from `PlanTracker` and `TraceWriter` modules to handle concurrent `markDone()`/`markFailed()` calls from worker pool:\n- All writes queued via `this.writeQueue = this.writeQueue.then(...)` pattern\n- Each `queueWrite()` appends to chain, ensuring FIFO execution order\n- `flush()` awaits `writeQueue` to guarantee completion before process exit\n- Non-critical failures swallowed with empty catch blocks (checkpoint corruption does not halt rebuild)\n\n## Dependencies\n\n- `node:fs/promises` — `writeFile()`, `readFile()`, `mkdir()`, `rm()`\n- `node:path` — Path joining for `.rebuild-checkpoint` resolution\n- `../change-detection/index.js` — `computeContentHashFromString()` for spec drift detection\n- `./types.js` — `RebuildCheckpointSchema`, `RebuildCheckpoint` type\n- `../version.js` — `getVersion()` for checkpoint version stamping\n\n## Checkpoint File Format\n\n**Location:** `<outputDir>/.rebuild-checkpoint`\n\n**Content:** Pretty-printed JSON (`JSON.stringify(data, null, 2)`) with schema:\n```typescript\n{\n  version: string,           // ARE version\n  createdAt: string,        // ISO 8601 timestamp\n  updatedAt: string,        // ISO 8601 timestamp\n  outputDir: string,        // Absolute path\n  specHashes: {             // Drift detection\n    \"specs/SPEC.md\": \"a3f5d8e9...\",\n    \"specs/module.md\": \"7b2c1a4f...\"\n  },\n  modules: {                // Per-unit tracking\n    \"ModuleName\": { status: 'pending' | 'done' | 'failed', ... }\n  }\n}\n```\n\n## Usage Pattern\n\nConstructor is private — clients must use `load()` or `createFresh()` factory methods:\n\n```typescript\nconst { manager, isResume } = await CheckpointManager.load(outputDir, specFiles, unitNames);\nif (!isResume) {\n  await manager.initialize();\n}\n\n// In worker pool callbacks:\nmanager.markDone(unitName, filesWritten);\n// or\nmanager.markFailed(unitName, errorMsg);\n\n// Before exit:\nawait manager.flush();\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1693,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11314,
      "latencyMs": 40511,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/checkpoint.ts",
          "sizeBytes": 7304
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:58.877Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/index.ts\n\n```typescript\n/**\n * Update module\n *\n * Provides incremental documentation update functionality.\n * Coordinates state management, change detection, and orphan cleanup.\n */\nexport {\n  UpdateOrchestrator,\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from './orchestrator.js';\n\nexport {\n  cleanupOrphans,\n  cleanupEmptyDirectoryDocs,\n  getAffectedDirectories,\n} from './orphan-cleaner.js';\n\nexport type {\n  UpdateOptions,\n  UpdateResult,\n  UpdateProgress,\n  CleanupResult,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export module exposing UpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, and TypeScript types for incremental documentation updates.**\n\n## Exported Symbols\n\n**UpdateOrchestrator** — class exported from `./orchestrator.js` coordinating incremental update workflow with change detection, orphan cleanup, and selective regeneration\n\n**createUpdateOrchestrator** — factory function exported from `./orchestrator.js` constructing UpdateOrchestrator instances with dependency injection\n\n**UpdatePlan** — type exported from `./orchestrator.js` representing planned update operations (files to analyze, skip, delete; affected directories)\n\n**cleanupOrphans** — function exported from `./orphan-cleaner.js` removing stale `.sum` files for deleted source files or renamed oldPaths\n\n**cleanupEmptyDirectoryDocs** — function exported from `./orphan-cleaner.js` deleting `AGENTS.md` files from directories with no remaining source files after cleanup\n\n**getAffectedDirectories** — function exported from `./orphan-cleaner.js` computing directories requiring `AGENTS.md` regeneration by walking parent paths of changed files\n\n**UpdateOptions** — type exported from `./types.js` parameterizing update behavior (includeUncommitted flag, baseCommit, concurrency, timeout, retries)\n\n**UpdateResult** — type exported from `./types.js` representing update outcome with counts for analyzed/skipped/deleted files, affected directories, success status\n\n**UpdateProgress** — type exported from `./types.js` describing update progress events emitted during execution (phase transitions, file completions, error reports)\n\n**CleanupResult** — type exported from `./types.js` representing orphan cleanup outcome with arrays of deleted `.sum` files and removed directory docs\n\n## Module Purpose\n\nCentralizes incremental update API surface as public interface for `src/cli/update.ts` command entry point. Separates orchestration logic (UpdateOrchestrator in `./orchestrator.js`) from cleanup utilities (orphan-cleaner functions) and type definitions (`./types.js`). Enables consumers to import only required symbols without loading implementation details from sibling modules.\n\n## Integration Context\n\nInvoked by `src/cli/update.ts` command handler which constructs UpdateOrchestrator via createUpdateOrchestrator factory, passes CommandRunOptions with tracer/progress reporters, and executes UpdateOrchestrator.update() method. UpdateOrchestrator internally calls `src/change-detection/detector.ts` for SHA-256 hash comparison, cleanupOrphans for `.sum` deletion, getAffectedDirectories for directory scope calculation, and `src/generation/executor.ts` for Phase 1 file regeneration and Phase 2 directory aggregation (skips Phase 3 root synthesis).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 653,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9351,
      "latencyMs": 25033,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/index.ts",
          "sizeBytes": 482
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:06.090Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/types.ts\n\n```typescript\n/**\n * Types for incremental update workflow\n */\nimport type { FileChange } from '../change-detection/types.js';\n\n/**\n * Result of orphan cleanup\n */\nexport interface CleanupResult {\n  /** .sum files that were deleted */\n  deletedSumFiles: string[];\n  /** AGENTS.md files that were deleted (from empty directories) */\n  deletedAgentsMd: string[];\n}\n\n/**\n * Options for the update command\n */\nexport interface UpdateOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  includeUncommitted?: boolean;\n  /** Dry run - show what would change without making changes */\n  dryRun?: boolean;\n}\n\n/**\n * Result of an update run\n */\nexport interface UpdateResult {\n  /** Files that were analyzed (added or modified) */\n  analyzedFiles: string[];\n  /** Files that were skipped (unchanged) */\n  skippedFiles: string[];\n  /** Cleanup result (deleted .sum and AGENTS.md files) */\n  cleanup: CleanupResult;\n  /** Directories whose AGENTS.md was regenerated */\n  regeneratedDirs: string[];\n  /** Git commit hash at start of update */\n  baseCommit: string;\n  /** Git commit hash at end of update */\n  currentCommit: string;\n  /** Whether this was a dry run */\n  dryRun: boolean;\n}\n\n/**\n * Progress callback for update operations\n */\nexport interface UpdateProgress {\n  /** Called when a file is about to be processed */\n  onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void;\n  /** Called when a file is done processing */\n  onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void;\n  /** Called when cleanup deletes a file */\n  onCleanup?: (path: string, type: 'sum' | 'agents-md') => void;\n  /** Called when a directory AGENTS.md is regenerated */\n  onDirRegenerate?: (path: string) => void;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for incremental update workflow results, options, and progress callbacks.**\n\n## Exported Types\n\n**CleanupResult** — Result of orphan cleanup operation with `deletedSumFiles: string[]` (removed `.sum` files) and `deletedAgentsMd: string[]` (removed `AGENTS.md` files from empty directories).\n\n**UpdateOptions** — Configuration for update command with `includeUncommitted?: boolean` (include staged + working directory changes) and `dryRun?: boolean` (preview mode without file writes).\n\n**UpdateResult** — Comprehensive update execution result containing `analyzedFiles: string[]` (added/modified files), `skippedFiles: string[]` (unchanged files), `cleanup: CleanupResult` (orphan deletion summary), `regeneratedDirs: string[]` (directories with rebuilt `AGENTS.md`), `baseCommit: string` (git SHA at start), `currentCommit: string` (git SHA at end), and `dryRun: boolean` (execution mode flag).\n\n**UpdateProgress** — Progress callback interface with optional handlers: `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` (file processing start), `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` (file processing completion), `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` (orphan file deletion), `onDirRegenerate?: (path: string) => void` (directory aggregation).\n\n## Dependencies\n\nImports `FileChange` from `../change-detection/types.js` to represent git-based file modifications (used in orchestrator implementations, not directly exposed in this type module).\n\n## Design Pattern\n\nStatus discriminators use string literal unions (`'analyzing' | 'skipping'`, `'analyzed' | 'skipped' | 'error'`, `'sum' | 'agents-md'`) for type-safe progress event routing and UI rendering in streaming progress reporter.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 483,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27080,
      "latencyMs": 19639,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/types.ts",
          "sizeBytes": 1741
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:06.106Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/docs/INPUT.md\n\n```markdown\n# Agents Reverse Engineer:\n\nA light-weight and powerful tool to reverse engineer a brownfield project and generate adequate AGENTS.md and CLAUDE.md among others... Compatible with Claude Code, OpenCode and many other agent tools.\n\n## Inception\n\nAgents Reverse Engineer was born out of the need to streamline the process of documenting and understanding existing projects. It aims to provide agents with a clear and concise overview of their codebase, making it easier to maintain the project over time.\nIt can be used in conjunction with other tools to enhance the overall development workflow like:\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done (GSD): https://github.com/glittercowboy/get-shit-done\n\n## Features\n\n- It should provide a command that will execute script usin Recursive Language Model with Claude Calude or and other LLM Agents tool. For Claude, it will be a command, ffor other, the available alternative.\n- It should be possible to add a hook on the end of a session to update the impacted files\n- It should generate AGENTS.md file in every directory of the structure\n- Each AGENTS.md file will be describing the content of the current directory and sub-structure. They can reference other files like ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md, etc...\n- The RLM will work as follow:\n    * build the project structure tree\n    * with start by executing the call at the first leaf and build recursively backward\n    * since the leaf is a file, analyse the file and generate a summary {filename}.sum\n    * when all leaf of a directory are summarized, analyse the directory and generate AGENTS.md file and other files if needed\n    * continue recursively until the root of the project is reached\n\n## Reaserch\n\nAnalyse in details GSD and BMAD.\n\nWe'll use a similar repository structure mainly to GSD and a bit of BMAD.\n\nLook at how they are approaching a brownfiled project, speical command and details about how they are doing. Check their codebase in details if needed.\n\n## Usage\n\nTo use Agents Reverse Engineer, simply run the following command in your terminal:\n\n```bash\n/are-generate\n/are-update\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**INPUT.md defines the original vision and requirements specification for the agents-reverse-engineer project, establishing the Recursive Language Model (RLM) algorithm for brownfield documentation generation via post-order traversal of project trees.**\n\n## Vision Statement\n\nThe document establishes agents-reverse-engineer as a lightweight tool to reverse engineer brownfield projects and generate `AGENTS.md`, `CLAUDE.md`, and platform-specific documentation compatible with Claude Code, OpenCode, and other agent tools. Born from the need to streamline brownfield project documentation, providing agents with clear codebase overviews for long-term maintenance.\n\n## Recursive Language Model Algorithm\n\nThe RLM algorithm executes as follows:\n\n1. Build complete project structure tree via directory traversal\n2. Start execution at first leaf node (file level)\n3. Analyze each file and generate `{filename}.sum` summary files\n4. When all leaf files in a directory are summarized, analyze the directory and generate `AGENTS.md` plus supplementary files if needed\n5. Continue recursively backward until reaching project root\n\nThis post-order traversal ensures child summaries exist before parent directory analysis, enabling bottom-up documentation synthesis where directory docs can reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nEach `AGENTS.md` file describes current directory content and substructure. May reference supplementary documentation:\n\n- `ARCHITECTURE.md` — System design and component relationships\n- `STRUCTURE.md` — Directory organization rationale\n- `STACK.md` — Technology stack and dependencies\n- `INTEGRATIONS.md` — External service integrations\n- `INFRASTRUCTURE.md` — Deployment and runtime configuration\n- `CONVENTIONS.md` — Coding standards and naming patterns\n- `TESTING.md` — Test strategy and frameworks\n- `PATTERNS.md` — Design patterns and architectural idioms\n- `CONCERNS.md` — Known issues and technical debt\n\n## Required Features\n\n- Command execution using Recursive Language Model with Claude Code or alternative LLM agent tools (commands for Claude, available alternatives for others)\n- Session-end hook for incremental updates of impacted files\n- Generation of `AGENTS.md` in every project directory\n- Multi-platform support: Claude Code, OpenCode, and other agent tools\n\n## Usage Commands\n\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references two methodologies for analysis:\n\n- **GSD (Get Shit Done)**: https://github.com/glittercowboy/get-shit-done\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD\n\nRecommends analyzing repository structure (primarily GSD, partially BMAD), brownfield project approaches, special commands, and codebase implementation details.\n\n## Complementary Tooling\n\nDesigned for workflow integration with:\n\n- **SpecKit**: https://github.com/github/spec-kit\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD  \n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 739,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9765,
      "latencyMs": 22170,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "docs/INPUT.md",
          "sizeBytes": 2272
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:57.623Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/writer.ts\n\n```typescript\nimport { writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport * as path from 'node:path';\n\n/**\n * Options controlling spec output writing behavior.\n */\nexport interface WriteSpecOptions {\n  /** Full path to the output file (e.g., /project/specs/SPEC.md). */\n  outputPath: string;\n  /** Overwrite existing files without error. */\n  force: boolean;\n  /** Split AI output into multiple files by top-level `# ` headings. */\n  multiFile: boolean;\n}\n\n/**\n * Thrown when writeSpec() detects existing file(s) and force=false.\n * Callers should catch this and present a user-friendly message.\n */\nexport class SpecExistsError extends Error {\n  /** Paths of the files that already exist. */\n  readonly paths: string[];\n\n  constructor(paths: string[]) {\n    const list = paths.map((p) => `  - ${p}`).join('\\n');\n    super(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`);\n    this.name = 'SpecExistsError';\n    this.paths = paths;\n  }\n}\n\n/**\n * Check whether a file exists at the given path.\n */\nasync function fileExists(filePath: string): Promise<boolean> {\n  try {\n    await access(filePath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Sanitize a heading string into a filename-safe slug.\n *\n * Lowercases, replaces whitespace with hyphens, strips non-alphanumeric\n * characters (except hyphens), collapses consecutive hyphens, and trims\n * leading/trailing hyphens.\n */\nfunction slugify(heading: string): string {\n  return heading\n    .toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^a-z0-9-]/g, '')\n    .replace(/-+/g, '-')\n    .replace(/^-|-$/g, '');\n}\n\n/**\n * Split markdown content on top-level `# ` headings into named sections.\n *\n * Returns an array of `{ filename, content }` pairs. Any content before\n * the first `# ` heading is placed into `00-preamble.md`.\n */\nfunction splitByHeadings(content: string): Array<{ filename: string; content: string }> {\n  const sections: Array<{ filename: string; content: string }> = [];\n\n  // Split on lines that start with exactly \"# \" (top-level heading)\n  const parts = content.split(/^(?=# )/m);\n\n  for (const part of parts) {\n    const trimmed = part.trimEnd();\n    if (!trimmed) continue;\n\n    const headingMatch = trimmed.match(/^# (.+)/);\n    if (headingMatch) {\n      const slug = slugify(headingMatch[1]);\n      const filename = slug ? `${slug}.md` : '00-preamble.md';\n      sections.push({ filename, content: trimmed + '\\n' });\n    } else {\n      // Content before the first heading\n      sections.push({ filename: '00-preamble.md', content: trimmed + '\\n' });\n    }\n  }\n\n  return sections;\n}\n\n/**\n * Write spec output to disk with overwrite protection.\n *\n * In single-file mode, writes content directly to `outputPath`.\n * In multi-file mode, splits content on top-level `# ` headings and\n * writes each section to a separate file in the directory of `outputPath`.\n *\n * @param content - The full AI-generated spec markdown\n * @param options - Output path, force flag, and multi-file mode\n * @returns Array of absolute paths to all written files\n * @throws SpecExistsError if files exist and force=false\n */\nexport async function writeSpec(content: string, options: WriteSpecOptions): Promise<string[]> {\n  const { outputPath, force, multiFile } = options;\n\n  if (!multiFile) {\n    // Single-file mode\n    if (!force && await fileExists(outputPath)) {\n      throw new SpecExistsError([outputPath]);\n    }\n\n    await mkdir(path.dirname(outputPath), { recursive: true });\n    await writeFile(outputPath, content, 'utf-8');\n    return [outputPath];\n  }\n\n  // Multi-file mode: split on top-level headings\n  const outputDir = path.dirname(outputPath);\n  const sections = splitByHeadings(content);\n\n  // Check all target files for existence before writing any\n  if (!force) {\n    const conflicts: string[] = [];\n    for (const section of sections) {\n      const filePath = path.join(outputDir, section.filename);\n      if (await fileExists(filePath)) {\n        conflicts.push(filePath);\n      }\n    }\n    if (conflicts.length > 0) {\n      throw new SpecExistsError(conflicts);\n    }\n  }\n\n  // Create output directory and write all sections\n  await mkdir(outputDir, { recursive: true });\n\n  const writtenPaths: string[] = [];\n  for (const section of sections) {\n    const filePath = path.join(outputDir, section.filename);\n    await writeFile(filePath, section.content, 'utf-8');\n    writtenPaths.push(filePath);\n  }\n\n  return writtenPaths;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeSpec() orchestrates single-file or multi-file specification output with overwrite protection, slugified heading-based splitting, and existence-check guardrails for the `/are-specify` command.**\n\n## Exported Interface\n\n**WriteSpecOptions** configures output behavior:\n- `outputPath: string` — absolute path to target file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — bypass existence checks to overwrite files\n- `multiFile: boolean` — split AI output on top-level `# ` headings into separate files\n\n**SpecExistsError** extends `Error` with `paths: string[]` property listing conflicting file paths. Constructor message formats paths as bullet list with `\"Use --force to overwrite.\"` suffix.\n\n**writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>** writes specification markdown to disk:\n- Single-file mode: writes `content` directly to `outputPath` after `mkdir(dirname(outputPath), { recursive: true })`\n- Multi-file mode: calls `splitByHeadings(content)` to partition on `/^# /m` regex, writes each section to `dirname(outputPath)/<slug>.md`\n- Pre-write existence check: aggregates conflicts via `fileExists()` and throws `SpecExistsError` unless `force=true`\n- Returns array of absolute paths for all written files\n\n## Heading-Based Splitting\n\n**splitByHeadings(content: string): Array<{ filename: string; content: string }>** partitions markdown on top-level headings:\n- Splits via `/^(?=# )/m` regex (positive lookahead for lines starting with `# `)\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify()`\n- Pre-heading content assigned to `00-preamble.md` filename\n- Each section object contains `{ filename, content }` with trailing newline appended\n\n**slugify(heading: string): string** sanitizes heading text into filename-safe slug:\n- Lowercases via `.toLowerCase()`\n- Replaces whitespace with hyphens via `/\\s+/g` → `'-'`\n- Strips non-alphanumeric characters (except hyphens) via `/[^a-z0-9-]/g` → `''`\n- Collapses consecutive hyphens via `/-+/g` → `'-'`\n- Trims leading/trailing hyphens via `/^-|-$/g` → `''`\n\n## File System Operations\n\n**fileExists(filePath: string): Promise<boolean>** checks file existence:\n- Invokes `access(filePath, constants.F_OK)` from `node:fs/promises`\n- Returns `true` on success, `false` on rejection (file not found)\n\n**Multi-file conflict detection:** Before writing any files in multi-file mode, iterates all sections to check existence via `fileExists()`. Aggregates conflicts into array and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write operation (all-or-nothing).\n\n## Integration Context\n\nConsumed by `src/cli/specify.ts` after AI synthesis completes. The `outputPath` parameter receives value from `config.output.specPath` (default: `specs/SPEC.md`). The `--force` CLI flag maps to `WriteSpecOptions.force`. The `--multi-file` CLI flag maps to `WriteSpecOptions.multiFile`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 818,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10580,
      "latencyMs": 31837,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/writer.ts",
          "sizeBytes": 4490
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:15.069Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Hook\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\n// Check for disable flag\nif (process.env.ARE_DISABLE_HOOK === '1') {\n  process.exit(0);\n}\n\n// Check config file for permanent disable\nconst configPath = '.agents-reverse-engineer.yaml';\nif (existsSync(configPath)) {\n  const config = readFileSync(configPath, 'utf-8');\n  if (config.includes('hook_enabled: false')) {\n    process.exit(0);\n  }\n}\n\n// Check git status - skip if no changes\ntry {\n  const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n  if (!status.trim()) {\n    // No changes since last run - exit silently\n    process.exit(0);\n  }\n} catch {\n  // Not a git repo or git not available - exit silently\n  process.exit(0);\n}\n\n// Run update in background (don't block session close)\nconst child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n});\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-session-end.js executes `npx agents-reverse-engineer@latest update --quiet` as a detached background process when session ends if uncommitted changes are detected via `git status --porcelain`.**\n\n## Disable Mechanisms\n\nExits immediately (status 0) if:\n- `process.env.ARE_DISABLE_HOOK === '1'` environment variable is set\n- `.agents-reverse-engineer.yaml` file exists and contains substring `'hook_enabled: false'` (no YAML parser used)\n\n## Change Detection\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exits silently if:\n- `status.trim()` returns empty string (no changes)\n- `execSync()` throws error (not a git repo or git unavailable)\n\n## Background Execution Pattern\n\nSpawns detached process to avoid blocking session close:\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n}).unref();\n```\n\n## Module Type\n\nES module using `import` syntax: `import { execSync, spawn } from 'child_process'` and `import { existsSync, readFileSync } from 'fs'`.\n\n## Shebang\n\n`#!/usr/bin/env node` enables direct execution as CLI script.\n\n## Integration Context\n\nRegistered as SessionEnd hook for Claude Code (`~/.claude/hooks/`) and Gemini CLI (`~/.gemini/hooks/`) via installer (`src/installer/operations.ts`). OpenCode integration uses wrapper plugin (`opencode-are-session-end.js`) that imports this script's logic.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 410,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9609,
      "latencyMs": 16507,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-session-end.js",
          "sizeBytes": 1276
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:04.949Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orphan-cleaner.ts\n\n```typescript\n/**\n * Orphan cleanup for stale .sum and AGENTS.md files\n *\n * Handles:\n * - Deleting .sum files when source files are deleted\n * - Deleting .sum files for renamed files (at old path)\n * - Deleting AGENTS.md from directories with no remaining source files\n */\nimport { unlink, readdir, stat } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { FileChange } from '../change-detection/types.js';\nimport type { CleanupResult } from './types.js';\n\n/**\n * Files to ignore when checking if a directory has source files.\n * These are generated files, not source files.\n */\nconst GENERATED_FILES = new Set([\n  'AGENTS.md',\n  'CLAUDE.md',\n]);\n\n/**\n * Clean up orphaned .sum files and empty AGENTS.md files.\n *\n * @param projectRoot - Absolute path to project root\n * @param changes - List of file changes (deleted and renamed files need cleanup)\n * @param dryRun - If true, don't actually delete files\n * @returns Cleanup result with lists of deleted files\n */\nexport async function cleanupOrphans(\n  projectRoot: string,\n  changes: FileChange[],\n  dryRun: boolean = false\n): Promise<CleanupResult> {\n  const result: CleanupResult = {\n    deletedSumFiles: [],\n    deletedAgentsMd: [],\n  };\n\n  // Collect paths that need .sum cleanup\n  const pathsToClean: string[] = [];\n\n  for (const change of changes) {\n    if (change.status === 'deleted') {\n      pathsToClean.push(change.path);\n    } else if (change.status === 'renamed' && change.oldPath) {\n      // For renames, clean up the old path's .sum\n      pathsToClean.push(change.oldPath);\n    }\n  }\n\n  // Delete .sum and .annex.md files for deleted/renamed source files\n  for (const relativePath of pathsToClean) {\n    const sumPath = path.join(projectRoot, `${relativePath}.sum`);\n    const deleted = await deleteIfExists(sumPath, dryRun);\n    if (deleted) {\n      result.deletedSumFiles.push(relativePath + '.sum');\n    }\n    const annexPath = path.join(projectRoot, `${relativePath}.annex.md`);\n    const annexDeleted = await deleteIfExists(annexPath, dryRun);\n    if (annexDeleted) {\n      result.deletedSumFiles.push(relativePath + '.annex.md');\n    }\n  }\n\n  // Collect affected directories for AGENTS.md cleanup\n  const affectedDirs = new Set<string>();\n  for (const relativePath of pathsToClean) {\n    const dir = path.dirname(relativePath);\n    if (dir && dir !== '.') {\n      affectedDirs.add(dir);\n    }\n  }\n\n  // Check each affected directory for empty AGENTS.md\n  for (const dir of affectedDirs) {\n    const dirPath = path.join(projectRoot, dir);\n    const cleaned = await cleanupEmptyDirectoryDocs(dirPath, dryRun);\n    if (cleaned) {\n      result.deletedAgentsMd.push(path.join(dir, 'AGENTS.md'));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Delete a file if it exists.\n *\n * @returns true if file was deleted (or would be in dry run)\n */\nasync function deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean> {\n  try {\n    await stat(filePath);\n    if (!dryRun) {\n      await unlink(filePath);\n    }\n    return true;\n  } catch {\n    // File doesn't exist\n    return false;\n  }\n}\n\n/**\n * Check if a directory has any source files remaining.\n * If not, delete its AGENTS.md.\n *\n * @param dirPath - Absolute path to directory\n * @param dryRun - If true, don't actually delete\n * @returns true if AGENTS.md was deleted\n */\nexport async function cleanupEmptyDirectoryDocs(\n  dirPath: string,\n  dryRun: boolean = false\n): Promise<boolean> {\n  try {\n    const entries = await readdir(dirPath);\n\n    // Check if directory has any source files\n    // Source files are: not .sum files, not generated docs, not hidden files\n    const hasSourceFiles = entries.some(entry => {\n      // Skip hidden files\n      if (entry.startsWith('.')) return false;\n      // Skip .sum and .annex.md files\n      if (entry.endsWith('.sum')) return false;\n      if (entry.endsWith('.annex.md')) return false;\n      // Skip known generated files\n      if (GENERATED_FILES.has(entry)) return false;\n      // Everything else counts as a source file\n      return true;\n    });\n\n    if (!hasSourceFiles) {\n      const agentsPath = path.join(dirPath, 'AGENTS.md');\n      return await deleteIfExists(agentsPath, dryRun);\n    }\n\n    return false;\n  } catch {\n    // Directory doesn't exist or can't be read\n    return false;\n  }\n}\n\n/**\n * Get list of directories that should have their AGENTS.md regenerated.\n *\n * Includes parent directories of changed files up to project root.\n *\n * @param changes - List of file changes (non-deleted files)\n * @returns Set of relative directory paths\n */\nexport function getAffectedDirectories(changes: FileChange[]): Set<string> {\n  const dirs = new Set<string>();\n\n  for (const change of changes) {\n    // Skip deleted files - they don't affect directory docs\n    if (change.status === 'deleted') continue;\n\n    // Add all parent directories up to root\n    let dir = path.dirname(change.path);\n    while (dir && dir !== '.' && !path.isAbsolute(dir)) {\n      dirs.add(dir);\n      dir = path.dirname(dir);\n    }\n    // Include root directory\n    dirs.add('.');\n  }\n\n  return dirs;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orphan-cleaner.ts deletes stale `.sum` files, `.annex.md` files, and `AGENTS.md` files when source files are deleted or renamed, ensuring documentation artifacts match the current codebase structure.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun: boolean = false): Promise<CleanupResult>`** — Deletes `.sum` and `.annex.md` files for deleted or renamed source files, then removes `AGENTS.md` from directories that no longer contain source files. Processes `FileChange` entries with `status === 'deleted'` or `status === 'renamed'`, adding their paths (or `oldPath` for renames) to cleanup queue. Returns `CleanupResult` containing `deletedSumFiles[]` and `deletedAgentsMd[]` arrays with relative paths of removed files.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun: boolean = false): Promise<boolean>`** — Checks if directory contains any source files by reading entries via `readdir()`, filtering out hidden files (starting with `.`), `.sum` files, `.annex.md` files, and entries in `GENERATED_FILES` set. Returns `true` if `AGENTS.md` was deleted, `false` otherwise. Used to remove directory-level documentation when no source files remain.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** — Extracts parent directory paths from non-deleted `FileChange` entries by walking up via `path.dirname()` until reaching root (`.`). Skips changes with `status === 'deleted'` since deleted files don't require directory doc regeneration. Returns `Set<string>` of relative directory paths that need `AGENTS.md` regeneration during incremental updates.\n\n## Private Implementation\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** — Calls `stat()` to check file existence, then `unlink()` if not in dry-run mode. Returns `true` if file existed (regardless of actual deletion), `false` if `stat()` throws. Swallows all errors via empty catch block.\n\n**`GENERATED_FILES`** — `Set` containing `'AGENTS.md'` and `'CLAUDE.md'`, used by `cleanupEmptyDirectoryDocs()` to distinguish generated documentation from source files during directory emptiness checks.\n\n## Cleanup Logic\n\n**Deletion triggers:** For each `FileChange` with `status === 'deleted'`, constructs `.sum` and `.annex.md` paths via `path.join(projectRoot, \\`${relativePath}.sum\\`)`. For `status === 'renamed'` changes, targets `oldPath` instead of `path` to remove artifacts at previous location.\n\n**Directory cleanup strategy:** Collects affected directories via `path.dirname()` of each cleaned path, skipping root directory (`.`). For each affected directory, calls `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` if no source files remain. Source file detection excludes hidden files (`entry.startsWith('.')`) + `.sum` + `.annex.md` + `GENERATED_FILES` entries.\n\n**Dry-run support:** When `dryRun === true`, skips `unlink()` calls but still executes `stat()` checks and returns `true` for would-be deletions. Used for preview mode in update workflow.\n\n## Integration Context\n\nInvoked by `src/update/orchestrator.ts` after change detection to remove stale artifacts before regenerating modified files. Paired with `getAffectedDirectories()` to compute which directories need `AGENTS.md` regeneration during Phase 2 of incremental update workflow. Return value `CleanupResult` logged to `.agents-reverse-engineer/progress.log` for user visibility into cleanup operations.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 909,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10775,
      "latencyMs": 27498,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orphan-cleaner.ts",
          "sizeBytes": 5065
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:09.407Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Hook\n *\n * Checks for ARE updates in background, writes result to cache.\n * Called by SessionStart hook - runs once per session.\n *\n * Cache file: ~/.claude/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn, execSync } from 'child_process';\n\nconst homeDir = homedir();\nconst cwd = process.cwd();\nconst cacheDir = join(homeDir, '.claude', 'cache');\nconst cacheFile = join(cacheDir, 'are-update-check.json');\n\n// ARE-VERSION file locations (check project first, then global)\nconst projectVersionFile = join(cwd, '.claude', 'ARE-VERSION');\nconst globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION');\n\n// Ensure cache directory exists\nif (!existsSync(cacheDir)) {\n  mkdirSync(cacheDir, { recursive: true });\n}\n\n// Run check in background (spawn background process)\nconst child = spawn(process.execPath, ['-e', `\n  const fs = require('fs');\n  const { execSync } = require('child_process');\n\n  const cacheFile = ${JSON.stringify(cacheFile)};\n  const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n  const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n  // Check project directory first (local install), then global\n  let installed = '0.0.0';\n  try {\n    if (fs.existsSync(projectVersionFile)) {\n      installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n    } else if (fs.existsSync(globalVersionFile)) {\n      installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n    }\n  } catch (e) {}\n\n  let latest = null;\n  try {\n    latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n  } catch (e) {}\n\n  const result = {\n    update_available: latest && installed !== latest,\n    installed,\n    latest: latest || 'unknown',\n    checked: Math.floor(Date.now() / 1000)\n  };\n\n  fs.writeFileSync(cacheFile, JSON.stringify(result));\n`], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n});\n\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-check-update.js spawns a detached background process that queries npm for the latest agents-reverse-engineer version, compares it against installed version from ARE-VERSION files, and writes update availability status to ~/.claude/cache/are-update-check.json.**\n\n## Execution Context\n\nInvoked by SessionStart hook in Claude Code runtime. Runs once per session as non-blocking background check. Executable via shebang `#!/usr/bin/env node`.\n\n## Cache Management\n\nWrites JSON to `~/.claude/cache/are-update-check.json` with schema:\n```javascript\n{\n  update_available: boolean,  // true if installed !== latest\n  installed: string,          // version from ARE-VERSION file or '0.0.0'\n  latest: string,             // npm registry version or 'unknown'\n  checked: number             // Unix timestamp (seconds)\n}\n```\n\nCreates cache directory via `mkdirSync(cacheDir, { recursive: true })` if missing at `~/.claude/cache/`.\n\n## Version Resolution\n\nPrioritizes project-local ARE-VERSION over global:\n1. `.claude/ARE-VERSION` in current working directory (`process.cwd()`)\n2. `~/.claude/ARE-VERSION` in home directory (`homedir()`)\n\nFalls back to `'0.0.0'` if neither file exists.\n\n## Background Execution Pattern\n\nUses detached spawn to prevent blocking:\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n}).unref()\n```\n\nChild process executes inline script via `-e` flag performing npm query and cache write. Parent process exits immediately after spawn without waiting for completion.\n\n## npm Registry Query\n\nExecutes `npm view agents-reverse-engineer version` via `execSync()` with:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n- Catches errors silently, sets `latest: 'unknown'` on failure\n\n## File System Operations\n\n**Imports:** `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync` from `fs`; `homedir` from `os`; `join` from `path`; `spawn`, `execSync` from `child_process`.\n\n**Constants:**\n- `cacheDir` = `~/.claude/cache`\n- `cacheFile` = `~/.claude/cache/are-update-check.json`\n- `projectVersionFile` = `<cwd>/.claude/ARE-VERSION`\n- `globalVersionFile` = `~/.claude/ARE-VERSION`\n\nNo exports — script executes immediately on load.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 658,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9864,
      "latencyMs": 23914,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-check-update.js",
          "sizeBytes": 2097
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:57.270Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/prompts.ts\n\n```typescript\nimport type { AgentsDocs } from '../generation/collector.js';\n\n/**\n * Prompt pair for spec generation: system instructions and user content.\n */\nexport interface SpecPrompt {\n  system: string;\n  user: string;\n}\n\n/**\n * System prompt for AI-driven specification synthesis.\n *\n * Enforces conceptual grouping by concern, prohibits folder-mirroring\n * and exact file path prescription, and targets AI agent consumption.\n */\nexport const SPEC_SYSTEM_PROMPT = `You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence with explicit interface contracts per phase:\n   - Each phase MUST include a \"Defines:\" list naming the exact types, interfaces, classes, and functions this phase must export (use the exact names from section 3 Public API Surface)\n   - Each phase MUST include a \"Consumes:\" list naming the exact types and functions from earlier phases that this phase imports\n   - Include dependency ordering and implementation tasks as before\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Build Plan phases MUST cross-reference the Public API Surface: every type/function in the API Surface section must appear in exactly one phase's \"Defines:\" list\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.`;\n\n/**\n * Build the system + user prompt pair for spec generation.\n *\n * Injects all collected AGENTS.md content with section delimiters.\n *\n * @param docs - Collected AGENTS.md documents from collectAgentsDocs()\n * @returns SpecPrompt with system and user prompt strings\n */\nexport function buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt {\n  const agentsSections = docs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  const userSections: string[] = [\n    'Generate a comprehensive project specification from the following documentation.',\n    '',\n    `## AGENTS.md Files (${docs.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (annexFiles && annexFiles.length > 0) {\n    const annexSections = annexFiles.map(\n      (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n    );\n    userSections.push(\n      '',\n      `## Annex Files (${annexFiles.length} reproduction-critical source files)`,\n      '',\n      ...annexSections,\n    );\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The specification MUST include these sections in order:',\n    '1. Project Overview (purpose, value, tech stack)',\n    '2. Architecture (module boundaries, data flow, design decisions)',\n    '3. Public API Surface (all exported interfaces, full type signatures)',\n    '4. Data Structures & State (types, schemas, config objects)',\n    '5. Configuration (options, types, defaults, validation)',\n    '6. Dependencies (each with version and rationale)',\n    '7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)',\n    '8. Test Contracts (per-module test scenarios and edge cases)',\n    '9. Build Plan (phased implementation order with dependencies, each phase listing \"Defines:\" and \"Consumes:\" with exact names from API Surface)',\n    '10. Prompt Templates & System Instructions (FULL verbatim text from annex content)',\n    '11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim from annex content)',\n    '',\n    'Sections 10 and 11 MUST reproduce annex content verbatim.',\n    'Do NOT summarize prompt templates or IDE templates into prose descriptions.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: SPEC_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts defines the system prompt template and prompt construction logic for AI-driven specification synthesis from AGENTS.md documentation corpus.**\n\n## Exported Interface\n\n`SpecPrompt` interface encapsulates prompt pair with `system: string` and `user: string` properties for AI service consumption.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` constructs complete prompt pair by:\n- Mapping `docs` array to markdown sections with `### ${doc.relativePath}` headers and embedded `doc.content`\n- Conditionally appending annex sections when `annexFiles` provided (reproduction-critical source files like templates)\n- Injecting 11-point section requirement list into user prompt: Project Overview, Architecture, Public API Surface, Data Structures & State, Configuration, Dependencies, Behavioral Contracts (split into Runtime Behavior and Implementation Contracts with verbatim regex/format/constant requirements), Test Contracts, Build Plan (with \"Defines:\" and \"Consumes:\" interface contract lists), Prompt Templates & System Instructions (verbatim reproduction), IDE Integration & Installer (verbatim templates)\n- Returns `SpecPrompt` with `SPEC_SYSTEM_PROMPT` as system and constructed markdown sections as user\n\n## System Prompt Template\n\n`SPEC_SYSTEM_PROMPT` constant defines 600+ character system instruction enforcing:\n- **Audience**: AI agents (LLMs) requiring actionable, instruction-oriented language\n- **Organization mandate**: Group by concern (not directory structure), use 11 conceptual sections in prescribed order\n- **Anti-patterns**: Prohibits folder-mirroring, exact file path prescription, section headings derived from directory names\n- **Module boundary focus**: Describe interfaces and exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for all external dependencies\n- **Build Plan requirements**: Phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface section, dependency ordering\n- **Behavioral contract mandates**: Exact error types/codes with throw conditions, verbatim regex patterns in backticks, format strings and output templates with exact structure, magic constants and sentinel values with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical content handling**: Sections 10-11 must reproduce annex file content verbatim (prompt templates, IDE templates, installer configs) without summarization or paraphrasing\n- **Output format**: Raw markdown with no preamble, meta-commentary, or conversational prefix\n\n## Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompt via:\n1. Header: `\"Generate a comprehensive project specification from the following documentation.\"`\n2. Section delimiter: `\"## AGENTS.md Files (${docs.length} directories)\"`\n3. Per-document subsections: `agentsSections` array mapping each doc to `### ${relativePath}\\n\\n${content}`\n4. Conditional annex block: When `annexFiles` non-empty, appends `\"## Annex Files (${annexFiles.length} reproduction-critical source files)\"` followed by annex subsections\n5. Requirement checklist: `\"## Output Requirements\"` followed by 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n6. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim. Do NOT summarize prompt templates or IDE templates into prose descriptions.\"`\n7. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\n## Integration Points\n\nConsumed by `src/specify/index.ts` specification synthesis command via `AIService.call()` invocation. Receives `AgentsDocs` from `collectAgentsDocs()` in `src/generation/collector.ts` (recursive AGENTS.md traversal). Optional `annexFiles` parameter populated from `.annex.md` companion files containing verbatim source code for reproduction-critical modules (prompt templates, IDE command templates, installer configs).\n\n## Behavioral Contracts\n\nUser prompt sections joined with `'\\n'` separator producing multiline string. Section 7 split into subsections: `\"7a. Runtime Behavior\"` and `\"7b. Implementation Contracts\"` targeting distinct documentation domains (execution semantics vs. reproduction-critical constants).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1027,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11096,
      "latencyMs": 39738,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/prompts.ts",
          "sizeBytes": 8216
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:19.577Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/version.ts\n\n```typescript\nimport { readFileSync } from 'node:fs';\nimport { dirname, join } from 'node:path';\nimport { fileURLToPath } from 'node:url';\n\n/**\n * Get package version from package.json.\n */\nexport function getVersion(): string {\n  try {\n    const __dirname = dirname(fileURLToPath(import.meta.url));\n    const packagePath = join(__dirname, '..', 'package.json');\n    const packageJson = JSON.parse(readFileSync(packagePath, 'utf-8'));\n    return packageJson.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exports `getVersion()` to extract package version from package.json via filesystem path resolution from ES module URL.**\n\n## Public Interface\n\n```typescript\nfunction getVersion(): string\n```\n\nReturns the `version` field from `package.json` located one directory above the compiled module location, or `'unknown'` string literal if read fails or version field missing.\n\n## Implementation Details\n\n`getVersion()` resolves the package.json path via:\n1. `import.meta.url` → `fileURLToPath()` converts ES module URL to filesystem path\n2. `dirname()` extracts directory containing the compiled `.js` file\n3. `join(__dirname, '..', 'package.json')` navigates up one level to project root\n\nUses synchronous `readFileSync(packagePath, 'utf-8')` followed by `JSON.parse()` to extract `packageJson.version`. Try-catch block returns `'unknown'` on any error (file not found, invalid JSON, missing version field).\n\n## Runtime Constraints\n\nAssumes compiled output in `dist/` with `package.json` at `dist/../package.json` (project root). Build process must preserve this relative path structure. Path resolution works for both development (`tsx src/cli/index.ts`) and production (`node dist/cli/index.js`) via `import.meta.url` dynamic resolution.\n\n## Integration Points\n\nVersion string consumed by:\n- CLI `--version` flag output (see `src/cli/index.ts`)\n- Session lifecycle hooks for update checking (`hooks/are-check-update.js` compares against `npm view agents-reverse-engineer version`)\n- Telemetry run logs metadata (`src/ai/telemetry/run-log.ts` includes version in run context)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 401,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9357,
      "latencyMs": 17983,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/version.ts",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:17.929Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/scripts/build-hooks.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * Build Hooks Script\n *\n * Copies hook source files from hooks/ to hooks/dist/ for npm bundling.\n * Run via: npm run build:hooks\n * Called automatically during: npm run prepublishOnly\n */\n\nimport { copyFileSync, existsSync, mkdirSync, readdirSync } from 'fs';\nimport { join, dirname } from 'path';\nimport { fileURLToPath } from 'url';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\nconst projectRoot = join(__dirname, '..');\n\nconst HOOKS_SRC = join(projectRoot, 'hooks');\nconst HOOKS_DIST = join(projectRoot, 'hooks', 'dist');\n\n// Ensure dist directory exists\nif (!existsSync(HOOKS_DIST)) {\n  mkdirSync(HOOKS_DIST, { recursive: true });\n}\n\n// Copy all .js files from hooks/ to hooks/dist/\nconst hookFiles = readdirSync(HOOKS_SRC).filter(\n  (f) => f.endsWith('.js') && f !== 'dist'\n);\n\nconsole.log('Building hooks...');\nfor (const file of hookFiles) {\n  const src = join(HOOKS_SRC, file);\n  const dest = join(HOOKS_DIST, file);\n  copyFileSync(src, dest);\n  console.log(`  Copied: ${file} -> hooks/dist/${file}`);\n}\n\nconsole.log(`Done. ${hookFiles.length} hook(s) built.`);\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**build-hooks.js copies session lifecycle hook files from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Execution Context\n\nRuns as Node.js script via shebang `#!/usr/bin/env node`, invoked by `npm run build:hooks` and automatically during `npm run prepublishOnly` (see package.json).\n\n## Build Algorithm\n\n1. Resolves `projectRoot` via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..')`\n2. Defines `HOOKS_SRC = join(projectRoot, 'hooks')` and `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')`\n3. Creates `HOOKS_DIST` directory via `mkdirSync(HOOKS_DIST, { recursive: true })` if not `existsSync(HOOKS_DIST)`\n4. Filters `readdirSync(HOOKS_SRC)` for `.js` files excluding `'dist'` directory itself\n5. Iterates `hookFiles` array, calling `copyFileSync(src, dest)` for each file where `src = join(HOOKS_SRC, file)` and `dest = join(HOOKS_DIST, file)`\n6. Logs each copy operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n7. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Copied Hook Files\n\nBased on project structure, copies four session lifecycle hooks:\n- `are-check-update.js` — SessionStart version check hook for Claude/Gemini\n- `are-session-end.js` — SessionEnd auto-update hook for Claude/Gemini\n- `opencode-are-check-update.js` — OpenCode plugin for version checking\n- `opencode-are-session-end.js` — OpenCode plugin for session-end updates\n\n## Integration with Package Distribution\n\nEnsures `hooks/dist/` contains all hook files before `npm publish` executes, allowing `package.json` `files` field to include `\"hooks/dist\"` for tarball inclusion. The `hooks/dist/` directory is the source for installer operations in `src/installer/operations.ts` which copies hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 563,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9590,
      "latencyMs": 19828,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/build-hooks.js",
          "sizeBytes": 1141
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:00.092Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orchestrator.ts\n\n```typescript\n/**\n * Update orchestrator\n *\n * Coordinates incremental documentation updates using frontmatter-based change detection:\n * 1. Check git repository status\n * 2. Scan for existing .sum files\n * 3. Compare content hashes from frontmatter with current file hashes\n * 4. Clean up orphaned .sum files\n * 5. Prepare analysis tasks for changed files\n * 6. Track affected directories for AGENTS.md regeneration\n */\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport {\n  isGitRepo,\n  getCurrentCommit,\n  computeContentHash,\n  type FileChange,\n} from '../change-detection/index.js';\nimport { cleanupOrphans, getAffectedDirectories } from './orphan-cleaner.js';\nimport { readSumFile, getSumPath } from '../generation/writers/sum.js';\nimport type { UpdateOptions, CleanupResult } from './types.js';\nimport { discoverFiles as runDiscovery } from '../discovery/run.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Result of update preparation (before analysis).\n */\nexport interface UpdatePlan {\n  /** Files to analyze (added or modified) */\n  filesToAnalyze: FileChange[];\n  /** Files to skip (unchanged based on content hash) */\n  filesToSkip: string[];\n  /** Cleanup result (files to delete) */\n  cleanup: CleanupResult;\n  /** Directories that need AGENTS.md regeneration */\n  affectedDirs: string[];\n  /** Base commit (not used in frontmatter mode, kept for compatibility) */\n  baseCommit: string;\n  /** Current commit */\n  currentCommit: string;\n  /** Whether this is first run (no .sum files exist) */\n  isFirstRun: boolean;\n}\n\n/**\n * Orchestrates incremental documentation updates using frontmatter-based change detection.\n */\nexport class UpdateOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Close resources (no-op in frontmatter mode, kept for API compatibility).\n   */\n  close(): void {\n    // No database to close in frontmatter mode\n  }\n\n  /**\n   * Check prerequisites for update.\n   *\n   * @throws Error if not in a git repository\n   */\n  async checkPrerequisites(): Promise<void> {\n    const isRepo = await isGitRepo(this.projectRoot);\n    if (!isRepo) {\n      throw new Error(\n        `Not a git repository: ${this.projectRoot}\\n` +\n        'The update command requires a git repository for change detection.'\n      );\n    }\n  }\n\n  /**\n   * Discover all source files in the project.\n   */\n  private async discoverFiles(): Promise<string[]> {\n    const filterResult = await runDiscovery(this.projectRoot, this.config, {\n      tracer: this.tracer,\n      debug: this.debug,\n    });\n\n    // Walker returns absolute paths; convert to relative for consistent usage\n    return filterResult.included.map((f: string) => path.relative(this.projectRoot, f));\n  }\n\n  /**\n   * Prepare update plan without executing analysis.\n   *\n   * Uses frontmatter-based change detection:\n   * - Reads content_hash from each .sum file\n   * - Compares with current file content hash\n   * - Files with mismatched hashes need re-analysis\n   *\n   * @param options - Update options\n   * @returns Update plan with files to analyze and cleanup actions\n   */\n  async preparePlan(options: UpdateOptions = {}): Promise<UpdatePlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-plan-creation',\n      taskCount: 0, // Will be determined after discovery\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Creating update plan with change detection...'));\n    }\n\n    await this.checkPrerequisites();\n\n    // Get current commit for reference\n    const currentCommit = await getCurrentCommit(this.projectRoot);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Git commit: ${currentCommit.slice(0, 7)}`));\n    }\n\n    // Discover all source files\n    if (this.debug) {\n      console.error(pc.dim('[debug] Discovering files...'));\n    }\n\n    const allFiles = await this.discoverFiles();\n\n    const filesToAnalyze: FileChange[] = [];\n    const filesToSkip: string[] = [];\n    const deletedOrRenamed: FileChange[] = [];\n\n    // Track which .sum files we've seen (to detect orphans)\n    const seenSumFiles = new Set<string>();\n\n    // Check each file against its .sum file\n    for (const relativePath of allFiles) {\n      const filePath = path.join(this.projectRoot, relativePath);\n      const sumPath = getSumPath(filePath);\n      seenSumFiles.add(sumPath);\n\n      try {\n        // Read existing .sum file\n        const sumContent = await readSumFile(sumPath);\n\n        if (!sumContent) {\n          // No .sum file exists - file needs analysis\n          filesToAnalyze.push({ path: relativePath, status: 'added' });\n          continue;\n        }\n\n        // Compare content hashes\n        const currentHash = await computeContentHash(filePath);\n        const storedHash = sumContent.contentHash;\n\n        if (!storedHash || storedHash !== currentHash) {\n          // Hash mismatch or no hash stored - file needs re-analysis\n          filesToAnalyze.push({ path: relativePath, status: 'modified' });\n        } else {\n          // Hash matches - skip this file\n          filesToSkip.push(relativePath);\n        }\n      } catch {\n        // Error reading file - skip it\n        filesToSkip.push(relativePath);\n      }\n    }\n\n    // Cleanup orphans (deleted files whose .sum files still exist)\n    const cleanup = await cleanupOrphans(\n      this.projectRoot,\n      deletedOrRenamed,\n      options.dryRun ?? false\n    );\n\n    // Get directories affected by changes (for AGENTS.md regeneration)\n    // Sort by depth descending (deepest first) so children are processed before parents\n    const affectedDirs = Array.from(getAffectedDirectories(filesToAnalyze))\n      .sort((a, b) => {\n        const depthA = a === '.' ? 0 : a.split(path.sep).length;\n        const depthB = b === '.' ? 0 : b.split(path.sep).length;\n        return depthB - depthA;\n      });\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned`\n        )\n      );\n      console.error(pc.dim(`[debug] Affected directories: ${affectedDirs.length}`));\n    }\n\n    // Emit plan created event\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'update',\n      fileCount: filesToAnalyze.length,\n      taskCount: filesToAnalyze.length + affectedDirs.length, // File tasks + dir regen tasks\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    // Determine if this is first run (no files to skip means no existing .sum files)\n    const isFirstRun = filesToSkip.length === 0 && filesToAnalyze.length > 0;\n    return {\n      filesToAnalyze,\n      filesToSkip,\n      cleanup,\n      affectedDirs,\n      baseCommit: currentCommit, // Not used in frontmatter mode\n      currentCommit,\n      isFirstRun,\n    };\n  }\n\n  /**\n   * Record file analyzed (no-op in frontmatter mode - hash is stored in .sum file).\n   * Kept for API compatibility.\n   */\n  async recordFileAnalyzed(\n    _relativePath: string,\n    _contentHash: string,\n    _currentCommit: string\n  ): Promise<void> {\n    // No-op: content hash is stored in .sum file frontmatter\n  }\n\n  /**\n   * Remove file from state (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async removeFileState(_relativePath: string): Promise<void> {\n    // No-op: .sum file cleanup is handled separately\n  }\n\n  /**\n   * Record a completed update run (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async recordRun(\n    _commitHash: string,\n    _filesAnalyzed: number,\n    _filesSkipped: number\n  ): Promise<number> {\n    // No-op: no run history in frontmatter mode\n    return 0;\n  }\n\n  /**\n   * Get last run information (not available in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async getLastRun(): Promise<undefined> {\n    // No run history in frontmatter mode\n    return undefined;\n  }\n\n  /**\n   * Check if this is the first run.\n   * In frontmatter mode, checks if any .sum files exist.\n   */\n  async isFirstRun(): Promise<boolean> {\n    const plan = await this.preparePlan({ dryRun: true });\n    return plan.isFirstRun;\n  }\n}\n\n/**\n * Create an update orchestrator.\n */\nexport function createUpdateOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): UpdateOrchestrator {\n  return new UpdateOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**UpdateOrchestrator implements frontmatter-based incremental documentation updates by comparing SHA-256 content hashes stored in `.sum` file YAML frontmatter against current file content, generating UpdatePlan objects identifying files requiring re-analysis, orphaned `.sum` cleanup targets, and affected directories needing `AGENTS.md` regeneration.**\n\n## Exported Interface\n\n**UpdateOrchestrator** class with constructor signature `(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })`. Provides instance methods:\n\n- `checkPrerequisites(): Promise<void>` — Validates git repository via `isGitRepo()`, throws Error if not a git repo\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — Discovers files, compares content hashes, returns plan with `filesToAnalyze`, `filesToSkip`, `cleanup`, `affectedDirs`, `baseCommit`, `currentCommit`, `isFirstRun`\n- `close(): void` — No-op for API compatibility (no database in frontmatter mode)\n- `recordFileAnalyzed(relativePath: string, contentHash: string, currentCommit: string): Promise<void>` — No-op, kept for compatibility\n- `removeFileState(relativePath: string): Promise<void>` — No-op, kept for compatibility\n- `recordRun(commitHash: string, filesAnalyzed: number, filesSkipped: number): Promise<number>` — No-op, returns 0\n- `getLastRun(): Promise<undefined>` — No-op, returns undefined (no run history)\n- `isFirstRun(): Promise<boolean>` — Checks if any `.sum` files exist via `preparePlan({ dryRun: true })`\n\n**createUpdateOrchestrator** factory function with signature `(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean }): UpdateOrchestrator`.\n\n**UpdatePlan** interface with fields:\n- `filesToAnalyze: FileChange[]` — Added or modified files requiring analysis\n- `filesToSkip: string[]` — Unchanged files based on hash comparison\n- `cleanup: CleanupResult` — Orphaned `.sum` files to delete\n- `affectedDirs: string[]` — Directories needing `AGENTS.md` regeneration (sorted deepest-first)\n- `baseCommit: string` — Not used in frontmatter mode, kept for compatibility\n- `currentCommit: string` — Current git commit SHA\n- `isFirstRun: boolean` — True when `filesToSkip.length === 0 && filesToAnalyze.length > 0`\n\n## Change Detection Algorithm\n\n`preparePlan()` orchestrates hash-based change detection:\n\n1. Calls `checkPrerequisites()` ensuring git repository\n2. Calls `getCurrentCommit()` to retrieve current commit SHA\n3. Invokes `discoverFiles()` → `runDiscovery()` returning absolute paths, converts to relative via `path.relative()`\n4. Iterates discovered files, for each:\n   - Constructs `.sum` path via `getSumPath(filePath)`\n   - Adds to `seenSumFiles` Set for orphan detection\n   - Calls `readSumFile(sumPath)` to extract `SumFileContent` with `contentHash` from frontmatter\n   - If no `.sum` exists: pushes `{ path, status: 'added' }` to `filesToAnalyze`\n   - If `.sum` exists: calls `computeContentHash(filePath)` for current SHA-256 hash\n   - Hash mismatch or missing: pushes `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: pushes path to `filesToSkip`\n5. Calls `cleanupOrphans(projectRoot, deletedOrRenamed, dryRun)` returning `CleanupResult` with `deletedSumFiles`\n6. Calls `getAffectedDirectories(filesToAnalyze)` returning Set of directory paths\n7. Sorts `affectedDirs` by depth descending: `depthB - depthA` where depth = `path.split(path.sep).length`\n8. Emits `plan:created` trace event with `fileCount` and `taskCount`\n\n## Trace Emission Points\n\nEmits trace events via optional `ITraceWriter`:\n\n- `phase:start` at plan creation start with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after plan completion with `phase: 'update-plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n\nDuration computed via `process.hrtime.bigint()` delta converted to milliseconds: `Number(endTime - startTime) / 1_000_000`.\n\n## Dependencies\n\nImports from `../change-detection/index.js`: `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type.\n\nImports from `./orphan-cleaner.js`: `cleanupOrphans()`, `getAffectedDirectories()`.\n\nImports from `../generation/writers/sum.js`: `readSumFile()`, `getSumPath()`.\n\nImports from `../discovery/run.js`: `discoverFiles` as `runDiscovery()`.\n\nImports from `../config/schema.js`: `Config` type.\n\nImports from `../orchestration/trace.js`: `ITraceWriter` interface.\n\nImports from `./types.js`: `UpdateOptions`, `CleanupResult` types.\n\n## Debug Logging\n\nWhen `debug: true`, emits picocolors-formatted stderr messages:\n\n- `[debug] Creating update plan with change detection...` at plan start\n- `[debug] Git commit: ${currentCommit.slice(0, 7)}` after commit retrieval\n- `[debug] Discovering files...` before discovery\n- `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned` after hash comparison\n- `[debug] Affected directories: ${affectedDirs.length}` after directory computation\n\n## Frontmatter Mode Design\n\nNo-op methods (`recordFileAnalyzed`, `removeFileState`, `recordRun`, `getLastRun`) retain signatures for API compatibility with database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by `writeSumFile()` from `../generation/writers/sum.js`. Run history tracking unavailable in frontmatter mode (`getLastRun()` returns `undefined`, `recordRun()` returns `0`).\n\n## Error Handling\n\n`checkPrerequisites()` throws Error with message `Not a git repository: ${projectRoot}\\nThe update command requires a git repository for change detection.` when `isGitRepo()` returns false.\n\nHash comparison wrapped in try-catch: file read errors cause path addition to `filesToSkip` (silent failure for inaccessible files).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1766,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11926,
      "latencyMs": 38147,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orchestrator.ts",
          "sizeBytes": 9165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:17.822Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Plugin for OpenCode\n *\n * Checks for ARE updates in background, writes result to cache.\n * OpenCode equivalent of the Claude/Gemini SessionStart hook.\n *\n * Cache file: ~/.config/opencode/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn } from 'child_process';\n\nexport const AreCheckUpdate = async () => {\n  return {\n    event: {\n      'session.created': async () => {\n        const homeDir = homedir();\n        const cwd = process.cwd();\n        const cacheDir = join(homeDir, '.config', 'opencode', 'cache');\n        const cacheFile = join(cacheDir, 'are-update-check.json');\n\n        // ARE-VERSION file locations (check project first, then global)\n        const projectVersionFile = join(cwd, '.opencode', 'ARE-VERSION');\n        const globalVersionFile = join(homeDir, '.config', 'opencode', 'ARE-VERSION');\n\n        // Ensure cache directory exists\n        if (!existsSync(cacheDir)) {\n          mkdirSync(cacheDir, { recursive: true });\n        }\n\n        // Run check in background (spawn background process)\n        const child = spawn(process.execPath, ['-e', `\n          const fs = require('fs');\n          const { execSync } = require('child_process');\n\n          const cacheFile = ${JSON.stringify(cacheFile)};\n          const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n          const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n          // Check project directory first (local install), then global\n          let installed = '0.0.0';\n          try {\n            if (fs.existsSync(projectVersionFile)) {\n              installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n            } else if (fs.existsSync(globalVersionFile)) {\n              installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n            }\n          } catch (e) {}\n\n          let latest = null;\n          try {\n            latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n          } catch (e) {}\n\n          const result = {\n            update_available: latest && installed !== latest,\n            installed,\n            latest: latest || 'unknown',\n            checked: Math.floor(Date.now() / 1000)\n          };\n\n          fs.writeFileSync(cacheFile, JSON.stringify(result));\n        `], {\n          stdio: 'ignore',\n          detached: true,\n          windowsHide: true,\n        });\n\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCode plugin factory exporting `AreCheckUpdate()` async function that returns `session.created` event handler for background version check spawning detached Node.js subprocess writing update availability to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n**`AreCheckUpdate()`** — Async factory function returning plugin object with event handlers. Returns `Promise<{ event: { 'session.created': AsyncFunction } }>`.\n\n## Event Handler Contract\n\nThe `session.created` handler executes synchronously at OpenCode session start without blocking. Implements detached background spawn pattern via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true }).unref()`.\n\n## Version File Resolution\n\nChecks two locations in priority order:\n1. Project-local: `<cwd>/.opencode/ARE-VERSION`\n2. Global: `~/.config/opencode/ARE-VERSION`\n\nReads first existing file via `readFileSync()`, defaults to `'0.0.0'` if neither exists.\n\n## Cache File Structure\n\nWrites JSON object to `~/.config/opencode/cache/are-update-check.json`:\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": string,\n  \"latest\": string,\n  \"checked\": number\n}\n```\n\nThe `update_available` field compares `installed !== latest` after trimming whitespace. The `checked` field stores Unix timestamp via `Math.floor(Date.now() / 1000)`. The `latest` field defaults to `'unknown'` on `npm view` failure.\n\n## NPM Registry Query\n\nSpawned subprocess executes `npm view agents-reverse-engineer version` with `execSync()` options:\n- `encoding: 'utf8'`\n- `timeout: 10000` (10s)\n- `windowsHide: true`\n\nWraps in try-catch; network/timeout failures set `latest = null` → `'unknown'` in cache output.\n\n## Cache Directory Creation\n\nEnsures `~/.config/opencode/cache/` exists via `mkdirSync(cacheDir, { recursive: true })` before spawning background process. Parent directories created automatically.\n\n## Subprocess Detachment Pattern\n\nBackground script injected as inline Node.js string via `-e` flag. Uses `JSON.stringify()` to pass file path constants (`cacheFile`, `projectVersionFile`, `globalVersionFile`) from parent scope into subprocess. The `child.unref()` call allows parent process to exit without waiting for background check completion.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 612,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9942,
      "latencyMs": 20805,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-check-update.js",
          "sizeBytes": 2627
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:25.732Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LICENSE\n\n```text\nMIT License\n\nCopyright (c) 2026 GeoloeG-IsT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LICENSE defines MIT License terms granting unrestricted usage, modification, and distribution rights for agents-reverse-engineer project with liability disclaimer.**\n\n## License Terms\n\nGrants permissions without restriction:\n- Use, copy, modify, merge, publish, distribute, sublicense, sell\n- Applies to \"Software\" (this codebase and associated documentation files)\n- Requires copyright notice and permission notice inclusion in all copies or substantial portions\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT\n\n## Liability and Warranty\n\nSoftware provided \"AS IS\" without warranty of any kind (express or implied). No warranties for MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, or NONINFRINGEMENT. Authors not liable for any claims, damages, or liabilities arising from the software.\n\n## Integration Context\n\nReferenced in `package.json` via `\"license\": \"MIT\"` field. Governs distribution via npm registry (publish workflow in `.github/workflows/publish.yml`). No programmatic parsing required—human-readable legal text only.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 231,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9425,
      "latencyMs": 13038,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LICENSE",
          "sizeBytes": 1068
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:17.861Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Plugin for OpenCode\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * OpenCode equivalent of the Claude/Gemini SessionEnd hook.\n *\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\nexport const AreSessionEnd = async () => {\n  return {\n    event: {\n      'session.deleted': async () => {\n        // Check for disable flag\n        if (process.env.ARE_DISABLE_HOOK === '1') return;\n\n        // Check config file for permanent disable\n        const configPath = '.agents-reverse-engineer.yaml';\n        if (existsSync(configPath)) {\n          const config = readFileSync(configPath, 'utf-8');\n          if (config.includes('hook_enabled: false')) return;\n        }\n\n        // Check git status - skip if no changes\n        try {\n          const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n          if (!status.trim()) return;\n        } catch {\n          // Not a git repo or git not available - exit silently\n          return;\n        }\n\n        // Run update in background (don't block session close)\n        const child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n          stdio: 'ignore',\n          detached: true,\n        });\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**opencode-are-session-end.js exports `AreSessionEnd` async factory returning OpenCode plugin that triggers `npx agents-reverse-engineer@latest update --quiet` as detached background process when `session.deleted` event fires if git working tree has uncommitted changes.**\n\n## Exported Interface\n\n`AreSessionEnd` — Async function returning plugin object with shape `{ event: { 'session.deleted': AsyncFunction } }`. No parameters. Plugin conforms to OpenCode event handler contract.\n\n## Event Hook Behavior\n\n`event['session.deleted']` handler executes four-stage gate sequence:\n\n1. Environment gate: Returns early if `process.env.ARE_DISABLE_HOOK === '1'`\n2. Config file gate: Reads `.agents-reverse-engineer.yaml` via `existsSync()` + `readFileSync()`, returns early if content `includes('hook_enabled: false')` (substring search, no YAML parser)\n3. Git status gate: Executes `execSync('git status --porcelain', { encoding: 'utf-8' })`, returns early if output `trim()` is empty string, catches and returns on error (non-git repo)\n4. Background spawn: Launches `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })`, calls `child.unref()` to prevent blocking Node.js exit\n\n## Process Lifecycle Pattern\n\nDetached spawn pattern prevents session close blocking. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion.\n\n## Disable Mechanisms\n\n- **Environment**: `ARE_DISABLE_HOOK=1` environment variable\n- **Config**: `hook_enabled: false` anywhere in `.agents-reverse-engineer.yaml` content (not YAML-parsed, simple string inclusion check)\n\n## Platform Context\n\nOpenCode plugin system equivalent to Claude/Gemini SessionEnd hooks (`are-session-end.js`). Event name `session.deleted` maps to OpenCode lifecycle, whereas Claude/Gemini use `SessionEnd` trigger. Identical git change detection and background spawn logic across all three platform variants.\n\n## Installation Path\n\nInstalled to `~/.config/opencode/plugins/` (or `$OPENCODE_CONFIG_DIR/plugins/`) via `src/installer/operations.ts` when user runs `npx agents-reverse-engineer --runtime opencode -g`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 582,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9647,
      "latencyMs": 23902,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-session-end.js",
          "sizeBytes": 1550
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:52.453Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/spec-reader.ts\n\n```typescript\n/**\n * Spec file reader and partitioner for the rebuild module.\n *\n * Reads spec files from `specs/` and partitions them into rebuild units\n * based on the Build Plan section or top-level headings.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport type { RebuildUnit } from './types.js';\n\n/**\n * Read all `.md` spec files from the `specs/` directory.\n *\n * Files are returned sorted alphabetically by filename.\n * Throws a descriptive error if `specs/` doesn't exist or has no `.md` files.\n *\n * @param projectRoot - Absolute path to the project root\n * @returns Array of spec file objects with relative path and content\n */\nexport async function readSpecFiles(\n  projectRoot: string,\n): Promise<Array<{ relativePath: string; content: string }>> {\n  const specsDir = path.join(projectRoot, 'specs');\n\n  let entries: string[];\n  try {\n    entries = await readdir(specsDir);\n  } catch {\n    throw new Error(\n      'No spec files found in specs/. Run \"are specify\" first.',\n    );\n  }\n\n  const mdFiles = entries.filter((e) => e.endsWith('.md')).sort();\n\n  if (mdFiles.length === 0) {\n    throw new Error(\n      'No spec files found in specs/. Run \"are specify\" first.',\n    );\n  }\n\n  const results: Array<{ relativePath: string; content: string }> = [];\n  for (const file of mdFiles) {\n    const filePath = path.join(specsDir, file);\n    const content = await readFile(filePath, 'utf-8');\n    results.push({ relativePath: `specs/${file}`, content });\n  }\n\n  return results;\n}\n\n/**\n * Partition spec content into ordered rebuild units.\n *\n * Strategy:\n * 1. Concatenate all spec file contents\n * 2. Look for a \"Build Plan\" section with phase headings\n * 3. Each phase becomes a RebuildUnit with context from Architecture and Public API Surface\n * 4. Falls back to splitting on top-level `## ` headings if no Build Plan found\n *\n * Throws a descriptive error if no rebuild units can be extracted.\n * Logs a warning and skips units with empty content.\n *\n * @param specFiles - Spec files from readSpecFiles()\n * @returns Ordered array of rebuild units\n */\nexport function partitionSpec(\n  specFiles: Array<{ relativePath: string; content: string }>,\n): RebuildUnit[] {\n  const fullContent = specFiles.map((f) => f.content).join('\\n\\n');\n\n  // Try Build Plan strategy first\n  let units = extractFromBuildPlan(fullContent);\n\n  // Fall back to top-level headings\n  if (units.length === 0) {\n    units = extractFromTopLevelHeadings(fullContent);\n  }\n\n  // Validate: must have at least one unit\n  if (units.length === 0) {\n    throw new Error(\n      'Could not extract rebuild units from spec files. Expected either a \"## Build Plan\" section with \"### Phase N:\" subsections, or top-level \"## \" headings. Check your spec file format.',\n    );\n  }\n\n  // Filter out empty units with warning\n  const validUnits: RebuildUnit[] = [];\n  for (const unit of units) {\n    if (!unit.specContent.trim()) {\n      console.error(`[warn] Skipping empty spec section: \"${unit.name}\"`);\n      continue;\n    }\n    validUnits.push(unit);\n  }\n\n  // Re-validate after filtering\n  if (validUnits.length === 0) {\n    throw new Error(\n      'Could not extract rebuild units from spec files. Expected either a \"## Build Plan\" section with \"### Phase N:\" subsections, or top-level \"## \" headings. Check your spec file format.',\n    );\n  }\n\n  return validUnits.sort((a, b) => a.order - b.order);\n}\n\n/**\n * Extract rebuild units from the Build Plan section.\n *\n * Looks for `## 9. Build Plan` or `## Build Plan`, then extracts\n * `### Phase N:` subsections. Each phase gets context from the\n * Architecture and Public API Surface sections.\n */\nfunction extractFromBuildPlan(fullContent: string): RebuildUnit[] {\n  // Find the Build Plan section\n  const buildPlanMatch = fullContent.match(\n    /^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m,\n  );\n  if (!buildPlanMatch) return [];\n\n  const buildPlanStart = buildPlanMatch.index!;\n\n  // Find the end of the Build Plan section (next ## heading or end of content)\n  const afterBuildPlan = fullContent.slice(\n    buildPlanStart + buildPlanMatch[0].length,\n  );\n  const nextH2Match = afterBuildPlan.match(/^## /m);\n  const buildPlanContent = nextH2Match\n    ? afterBuildPlan.slice(0, nextH2Match.index)\n    : afterBuildPlan;\n\n  // Extract Architecture section for context (always included in full)\n  const architectureContent = extractSection(fullContent, 'Architecture');\n\n  // Extract Public API Surface section for targeted injection\n  const apiContent = extractSection(fullContent, 'Public API Surface');\n  const apiSubsections = apiContent ? extractSubsections(apiContent) : new Map<string, string>();\n\n  // Extract Data Structures and Behavioral Contracts for targeted injection\n  const dataStructuresContent = extractSection(fullContent, 'Data Structures');\n  const dataSubsections = dataStructuresContent ? extractSubsections(dataStructuresContent) : new Map<string, string>();\n\n  const behavioralContent = extractSection(fullContent, 'Behavioral Contracts');\n  const behavioralSubsections = behavioralContent ? extractSubsections(behavioralContent) : new Map<string, string>();\n\n  // Check if any phase has Defines:/Consumes: lists (Change 2 format)\n  const hasDefinesConsumes = buildPlanContent.match(/^\\*\\*Defines:\\*\\*|^Defines:/m) !== null;\n\n  // Extract phase subsections (### Phase N: ...)\n  const phasePattern = /^### Phase (\\d+):\\s*(.+)$/gm;\n  const phases: Array<{ number: number; name: string; startIndex: number }> = [];\n\n  let phaseMatch: RegExpExecArray | null;\n  while ((phaseMatch = phasePattern.exec(buildPlanContent)) !== null) {\n    phases.push({\n      number: parseInt(phaseMatch[1], 10),\n      name: phaseMatch[2].trim(),\n      startIndex: phaseMatch.index,\n    });\n  }\n\n  if (phases.length === 0) return [];\n\n  // Extract content for each phase with targeted API injection\n  const units: RebuildUnit[] = [];\n  for (let i = 0; i < phases.length; i++) {\n    const phase = phases[i];\n    const contentStart = phase.startIndex;\n    const contentEnd = i + 1 < phases.length\n      ? phases[i + 1].startIndex\n      : buildPlanContent.length;\n    const phaseContent = buildPlanContent.slice(contentStart, contentEnd).trim();\n\n    // Build context prefix per phase\n    const contextParts: string[] = [];\n    if (architectureContent) {\n      contextParts.push(`## Architecture\\n\\n${architectureContent}`);\n    }\n\n    if (hasDefinesConsumes && apiSubsections.size > 0) {\n      // Targeted injection: only relevant API subsections\n      const relevantApi = findRelevantSubsections(phaseContent, apiSubsections);\n      if (relevantApi) {\n        contextParts.push(`## Interfaces for This Phase\\n\\n${relevantApi}`);\n      }\n      const relevantData = findRelevantSubsections(phaseContent, dataSubsections);\n      if (relevantData) {\n        contextParts.push(`## Data Structures for This Phase\\n\\n${relevantData}`);\n      }\n      const relevantBehavior = findRelevantSubsections(phaseContent, behavioralSubsections);\n      if (relevantBehavior) {\n        contextParts.push(`## Behavioral Contracts for This Phase\\n\\n${relevantBehavior}`);\n      }\n    } else {\n      // Graceful degradation: full API Surface for older specs without Defines:/Consumes:\n      if (apiContent) {\n        contextParts.push(`## Public API Surface\\n\\n${apiContent}`);\n      }\n    }\n\n    const contextPrefix = contextParts.length > 0\n      ? contextParts.join('\\n\\n') + '\\n\\n'\n      : '';\n\n    units.push({\n      name: `Phase ${phase.number}: ${phase.name}`,\n      specContent: contextPrefix + phaseContent,\n      order: phase.number,\n    });\n  }\n\n  return units;\n}\n\n/**\n * Fallback: extract rebuild units from top-level `## ` headings.\n *\n * Each `## ` section becomes a unit with order matching its position.\n */\nfunction extractFromTopLevelHeadings(fullContent: string): RebuildUnit[] {\n  const headingPattern = /^## (.+)$/gm;\n  const headings: Array<{ name: string; startIndex: number }> = [];\n\n  let headingMatch: RegExpExecArray | null;\n  while ((headingMatch = headingPattern.exec(fullContent)) !== null) {\n    headings.push({\n      name: headingMatch[1].trim(),\n      startIndex: headingMatch.index,\n    });\n  }\n\n  if (headings.length === 0) return [];\n\n  const units: RebuildUnit[] = [];\n  for (let i = 0; i < headings.length; i++) {\n    const heading = headings[i];\n    const contentStart = heading.startIndex;\n    const contentEnd = i + 1 < headings.length\n      ? headings[i + 1].startIndex\n      : fullContent.length;\n    const sectionContent = fullContent.slice(contentStart, contentEnd).trim();\n\n    units.push({\n      name: heading.name,\n      specContent: sectionContent,\n      order: i + 1,\n    });\n  }\n\n  return units;\n}\n\n/**\n * Parse a section's content into subsections keyed by `### ` headings.\n *\n * @param sectionContent - Content of a spec section (without the `## ` heading)\n * @returns Map from heading text to full subsection content (including the heading)\n */\nfunction extractSubsections(sectionContent: string): Map<string, string> {\n  const result = new Map<string, string>();\n  const pattern = /^### (.+)$/gm;\n  const headings: Array<{ key: string; startIndex: number }> = [];\n\n  let match: RegExpExecArray | null;\n  while ((match = pattern.exec(sectionContent)) !== null) {\n    headings.push({ key: match[1].trim(), startIndex: match.index });\n  }\n\n  for (let i = 0; i < headings.length; i++) {\n    const heading = headings[i];\n    const end = i + 1 < headings.length\n      ? headings[i + 1].startIndex\n      : sectionContent.length;\n    result.set(heading.key, sectionContent.slice(heading.startIndex, end).trim());\n  }\n\n  return result;\n}\n\n/**\n * Find subsections relevant to a phase based on its content.\n *\n * Matches using:\n * 1. \"Defines:\" and \"Consumes:\" lists (exact symbol names)\n * 2. File paths and module names mentioned in the phase's task list\n *\n * @param phaseContent - Raw text of the phase section\n * @param subsections - Map of subsection heading → content\n * @returns Concatenated matching subsections, or null if none match\n */\nfunction findRelevantSubsections(\n  phaseContent: string,\n  subsections: Map<string, string>,\n): string | null {\n  if (subsections.size === 0) return null;\n\n  // Extract words from Defines: and Consumes: lines\n  const definesMatch = phaseContent.match(/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m);\n  const consumesMatch = phaseContent.match(/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m);\n\n  const keywords: string[] = [];\n  if (definesMatch) {\n    // Split on commas and semicolons, extract identifiers\n    keywords.push(...definesMatch[1].split(/[,;]/).map((s) => s.trim()).filter(Boolean));\n  }\n  if (consumesMatch) {\n    keywords.push(...consumesMatch[1].split(/[,;]/).map((s) => s.trim()).filter(Boolean));\n  }\n\n  // Also extract file paths and module-like references from the phase text\n  const pathRefs = phaseContent.match(/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g);\n  if (pathRefs) {\n    keywords.push(...pathRefs);\n  }\n\n  if (keywords.length === 0) {\n    // No structured references found — return all subsections as fallback\n    return [...subsections.values()].join('\\n\\n');\n  }\n\n  // Lowercase the keywords for case-insensitive matching\n  const lowerKeywords = keywords.map((k) => k.toLowerCase());\n\n  const matched: string[] = [];\n  for (const [key, content] of subsections) {\n    const lowerKey = key.toLowerCase();\n    // Check if any keyword appears in the subsection heading\n    const isRelevant = lowerKeywords.some((kw) =>\n      lowerKey.includes(kw) || kw.includes(lowerKey) ||\n      // Fuzzy: check individual words from the keyword against the key\n      kw.split(/[\\s/()]+/).some((word) => word.length > 3 && lowerKey.includes(word)),\n    );\n    if (isRelevant) {\n      matched.push(content);\n    }\n  }\n\n  return matched.length > 0 ? matched.join('\\n\\n') : null;\n}\n\n/**\n * Extract a named section's content from the full spec.\n *\n * Looks for `## N. Name` or `## Name` and returns content up to next `## `.\n */\nfunction extractSection(\n  fullContent: string,\n  sectionName: string,\n): string | null {\n  const pattern = new RegExp(\n    `^## (?:\\\\d+\\\\.\\\\s*)?${sectionName}\\\\s*Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/rebuild/spec-reader.ts\n\n```typescript\n,\n    'm',\n  );\n  const match = fullContent.match(pattern);\n  if (!match) return null;\n\n  const sectionStart = match.index! + match[0].length;\n  const afterSection = fullContent.slice(sectionStart);\n  const nextH2 = afterSection.match(/^## /m);\n  const content = nextH2\n    ? afterSection.slice(0, nextH2.index)\n    : afterSection;\n\n  return content.trim() || null;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**spec-reader.ts reads specification markdown files from `specs/` directory and partitions them into ordered `RebuildUnit[]` arrays for incremental reconstruction workflows, extracting phase-level content from Build Plan sections with targeted context injection or falling back to top-level heading splits.**\n\n## Exported Functions\n\n### readSpecFiles\n\n```typescript\nasync function readSpecFiles(\n  projectRoot: string\n): Promise<Array<{ relativePath: string; content: string }>>\n```\n\nReads all `.md` files from `specs/` subdirectory, returning sorted array of `{ relativePath: 'specs/filename.md', content: string }` objects. Throws error with message `\"No spec files found in specs/. Run \\\"are specify\\\" first.\"` if directory missing or empty. Uses `readdir()` for discovery, `readFile(..., 'utf-8')` for content loading, sorts filenames alphabetically via `.sort()`.\n\n### partitionSpec\n\n```typescript\nfunction partitionSpec(\n  specFiles: Array<{ relativePath: string; content: string }>\n): RebuildUnit[]\n```\n\nConverts spec file array into ordered `RebuildUnit[]` by concatenating contents and applying two extraction strategies: (1) `extractFromBuildPlan()` seeking `## Build Plan` or `## 9. Build Plan` with `### Phase N:` subsections, (2) `extractFromTopLevelHeadings()` splitting on `^## (.+)$` regex as fallback. Validates non-empty units, logs warnings via `console.error()` for empty sections, throws error if zero units extracted: `\"Could not extract rebuild units from spec files. Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings. Check your spec file format.\"`. Sorts output by `RebuildUnit.order` ascending.\n\n## Internal Extraction Functions\n\n### extractFromBuildPlan\n\n```typescript\nfunction extractFromBuildPlan(fullContent: string): RebuildUnit[]\n```\n\nMatches Build Plan section via regex `/^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m`, extracts `### Phase (\\d+):\\s*(.+)$` subsections, detects **Change 2 format** presence via `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern to enable targeted API injection. Calls `extractSection()` to retrieve Architecture, Public API Surface, Data Structures, Behavioral Contracts sections. Calls `extractSubsections()` to parse `### ` headings within API/Data/Behavioral content. For phases in Change 2 format (with `Defines:`/`Consumes:` lists), invokes `findRelevantSubsections()` to filter API/Data/Behavioral subsections matching phase keywords, injecting only relevant content under headings `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase`. Falls back to full `## Public API Surface` inclusion for older specs without Defines/Consumes lists (graceful degradation). Prepends Architecture and filtered API content to each phase's `RebuildUnit.specContent`. Returns empty array if no `## Build Plan` heading or no phases found.\n\n### extractFromTopLevelHeadings\n\n```typescript\nfunction extractFromTopLevelHeadings(fullContent: string): RebuildUnit[]\n```\n\nFallback strategy matching all `^## (.+)$/gm` patterns, slicing content between consecutive headings. Assigns `RebuildUnit.order` as sequential position (1-indexed). Returns empty array if no headings found.\n\n### extractSubsections\n\n```typescript\nfunction extractSubsections(sectionContent: string): Map<string, string>\n```\n\nParses section content into `Map<subsectionHeading, content>` by matching `^### (.+)$/gm` and slicing text between consecutive `### ` headings. Returns map with subsection heading text (trimmed) as keys and full content (including heading line) as values.\n\n### findRelevantSubsections\n\n```typescript\nfunction findRelevantSubsections(\n  phaseContent: string,\n  subsections: Map<string, string>\n): string | null\n```\n\nExtracts keywords from phase text via three sources: (1) `Defines:` list matching `/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m`, (2) `Consumes:` list matching `/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m`, (3) file path references via `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`. Splits Defines/Consumes on `[,;]` delimiters. Performs case-insensitive fuzzy matching: checks if subsection heading contains keyword, keyword contains heading, or individual words (length >3) from keyword appear in heading. Returns concatenated matched subsections joined with `\\n\\n`, or all subsections if no keywords found (fallback), or `null` if subsection map empty.\n\n### extractSection\n\n```typescript\nfunction extractSection(\n  fullContent: string,\n  sectionName: string\n): string | null\n```\n\nExtracts named section content between `## N. SectionName` or `## SectionName` heading and next `^## ` heading via regex `/^## (?:\\\\d+\\\\.\\\\s*)?${sectionName}\\s*$/m`. Returns trimmed content string or `null` if heading not found or content empty.\n\n## Data Flow Integration\n\nCalled by `orchestrator.ts` in `src/rebuild/` module during reconstruction workflow. `readSpecFiles()` locates spec markdown files generated by `are specify` command. `partitionSpec()` transforms concatenated spec content into executable units consumed by rebuild orchestrator's sequential phase execution. Each `RebuildUnit` contains `name` (e.g., `\"Phase 3: Core Data Structures\"`), `specContent` (phase description + context sections), `order` (numeric sequencing).\n\n## Behavioral Contracts\n\n**Build Plan phase extraction regex**: `/^### Phase (\\d+):\\s*(.+)$/gm` matches phase headings with capture groups for number and name.\n\n**Change 2 format detection pattern**: `/^\\*\\*Defines:\\*\\*|^Defines:/m` identifies specs using Defines/Consumes lists for targeted API injection.\n\n**Section heading pattern**: `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m` matches numbered or unnumbered section headings.\n\n**Subsection heading pattern**: `/^### (.+)$/gm` extracts all third-level headings for subsection parsing.\n\n**Keyword extraction from lists**: Split on `/[,;]/` delimiters, trim whitespace.\n\n**File path reference extraction**: `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g` captures source file paths and module names from phase text.\n\n**Error messages**:\n- `\"No spec files found in specs/. Run \\\"are specify\\\" first.\"` (directory missing or no .md files)\n- `\"Could not extract rebuild units from spec files. Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings. Check your spec file format.\"` (zero units after extraction)\n\n**Warning format**: `[warn] Skipping empty spec section: \"${unit.name}\"` logged via `console.error()` for units with empty trimmed content.\n\n## Context Injection Strategy\n\n**Change 2 format** (with Defines/Consumes lists): Injects only subsections matching phase keywords under section headings:\n- `## Interfaces for This Phase`\n- `## Data Structures for This Phase`\n- `## Behavioral Contracts for This Phase`\n\n**Legacy format** (no Defines/Consumes): Injects full `## Public API Surface` content for all phases (graceful degradation).\n\n**Always injected**: `## Architecture` section prepended to all phases regardless of format.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1928,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13161,
      "latencyMs": 50605,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/spec-reader.ts",
          "sizeBytes": 12508
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:29:23.779Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/runner.ts\n\n```typescript\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent file analysis, post-order directory aggregation, and sequential root document synthesis with quality validation, trace emission, and progress tracking.**\n\n## Exported Class\n\n`CommandRunner` — main orchestration class holding AIService reference and CommandRunOptions for executing generate/update workflows.\n\n### Constructor\n\n```typescript\nconstructor(aiService: AIService, options: CommandRunOptions)\n```\n\nWires tracer into AIService via `setTracer()` if `options.tracer` provided.\n\n### Primary Methods\n\n`executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` — runs full three-phase pipeline: file analysis (concurrent), directory AGENTS.md (post-order by depth), root documents (sequential). Returns aggregated RunSummary with token counts, inconsistency metrics, duration.\n\n`executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` — runs Phase 1 only for changed files, skips Phase 2/3. Supports incremental updates via hash-based change detection.\n\n## Three-Phase Execution Pipeline\n\n### Pre-Phase 1: Cache Existing .sum Content\n\nThrottled parallel read (concurrency=20) of existing `.sum` files via `readSumFile()` into `oldSumCache: Map<string, SumFileContent>`. Used for stale documentation detection in quality validation. Emits `phase:start` and `phase:end` trace events with `phase: 'pre-phase-1-cache'`.\n\n### Phase 1: File Analysis (Concurrent)\n\nMaps `plan.fileTasks` to async functions calling:\n1. `readFile(task.absolutePath)` for source content\n2. `aiService.call()` with `buildFilePrompt()` user/system prompts\n3. `computeContentHashFromString()` for SHA-256 hash\n4. `stripPreamble()` on response text\n5. `extractPurpose()` for one-line metadata\n6. `writeSumFile()` with SumFileContent (summary, generatedAt, contentHash)\n7. `writeAnnexFile()` if response contains `## Annex References` marker\n\nCaches source content in `sourceContentCache: Map<string, string>` for reuse in quality validation. Executes via `runPool()` with `options.concurrency` workers. Updates `PlanTracker.markDone()` and `ProgressReporter.onFileDone()` per completion. Emits `phase:start`, `phase:end` traces with `phase: 'phase-1-files'`.\n\n### Post-Phase 1: Quality Validation\n\nGroups files by directory via `path.dirname()`, runs throttled parallel checks (concurrency=10):\n\n**Code-vs-doc**: `checkCodeVsDoc()` twice per file — once against `oldSumCache` (stale detection), once against freshly written `.sum` (omission detection). Regex-extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Code-vs-code**: `checkCodeVsCode()` aggregates exports per directory group into `Map<symbol, string[]>`, detects duplicates.\n\n**Reporting**: Builds InconsistencyReport via `buildInconsistencyReport()`, prints via `formatReportForCli()`, assigns counts to `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`. Non-throwing — logs errors to stderr on failure. Clears `sourceContentCache` after validation to free memory.\n\nEmits `phase:start`, `phase:end` traces with `phase: 'post-phase-1-quality'`.\n\n### Phase 2: Directory Aggregation (Post-Order by Depth)\n\nGroups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`. Processes depth levels descending (`sort((a,b) => b-a)`) to ensure children precede parents.\n\nPer depth level:\n1. Computes `dirConcurrency = Math.min(options.concurrency, dirsAtDepth.length)`\n2. Calls `buildDirectoryPrompt()` with knownDirs filter (`Set` from `plan.directoryTasks.map(t => t.path)`)\n3. Invokes `aiService.call()` with directory prompt\n4. Writes via `writeAgentsMd()` which merges user-authored `AGENTS.local.md` if present\n5. Updates `PlanTracker.markDone()` and `ProgressReporter.onDirectoryDone()`\n\nEmits `phase:start`, `phase:end` traces per depth with `phase: 'phase-2-dirs-depth-${depth}'`.\n\n### Post-Phase 2: Phantom Path Validation\n\nReads each `AGENTS.md` via `readFile()`, calls `checkPhantomPaths()` to extract path-like strings via three regex patterns:\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\nResolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`. Builds PhantomPathInconsistency report, assigns `phantomPathCount`. Non-throwing — logs errors on failure.\n\n### Phase 3: Root Documents (Sequential)\n\nIterates `plan.rootTasks` with concurrency=1:\n1. Calls `buildRootPrompt()` which injects all `AGENTS.md` via `collectAgentsDocs()`\n2. Invokes `aiService.call()` with `maxTurns: 1` (no tool use)\n3. Strips conversational preamble via markdown start detection (`indexOf('# ')`)\n4. Writes to `rootTask.outputPath` via `writeFile()`\n5. Updates `PlanTracker.markDone()` and `ProgressReporter.onRootDone()`\n\nEmits `task:start`, `task:done` (with success/error), `phase:start`, `phase:end` traces with `phase: 'phase-3-root'`. Re-throws errors to maintain existing error handling.\n\n## Progress Tracking Infrastructure\n\n`PlanTracker` — writes `GENERATION-PLAN.md` with checkbox syntax via `formatExecutionPlanAsMarkdown()`, updates via `markDone()`, flushes via promise chain.\n\n`ProgressReporter` — streams human-readable progress to console and `options.progressLog` file via `onFileStart()`, `onFileDone()`, `onFileError()`, `onDirectoryStart()`, `onDirectoryDone()`, `onRootDone()`, `printSummary()`. Calculates ETA via moving average.\n\nTrace emission via `this.tracer?.emit()` for events: `phase:start`, `phase:end`, `task:start`, `task:done`, `worker:start`, `worker:end`. Tracer threaded through `runPool()` options and `aiService.setTracer()`.\n\n## Update Workflow (Incremental)\n\n`executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`:\n1. Reads existing project plan from `.agents-reverse-engineer/GENERATION-PLAN.md` for context (optional)\n2. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode\n3. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code)\n4. Skips Phase 2/3 — caller (`src/update/orchestrator.ts`) handles `AGENTS.md` regeneration for `affectedDirs`\n\nReturns RunSummary with `filesSkipped: 0` (update mode doesn't skip).\n\n## Helper Functions\n\n`stripPreamble(responseText: string): string` — removes LLM conversational preamble via two patterns:\n1. Content after `\\n---\\n` separator within first 500 chars\n2. Content starting with bold purpose `**[A-Z]` if preceding text is <300 chars and lacks `##`\n\n`extractPurpose(responseText: string): string` — scans lines skipping:\n- Empty, `#` headers, `---` separators\n- Lines starting with `PREAMBLE_PREFIXES` (case-insensitive): `['now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`\n\nStrips bold wrapper `**...**`, truncates to 120 chars with `...` suffix.\n\n## Dependencies\n\n`AIService` — `call()`, `setTracer()`, `addFilesReadToLastEntry()`, `getSummary()` methods.\n\n`ExecutionPlan` — fileTasks, directoryTasks, rootTasks arrays with metadata (depth, absolutePath, path, userPrompt, systemPrompt, outputPath).\n\n`runPool()` — iterator-based concurrency pool from `src/orchestration/pool.ts` with `onResult` callback for progress updates.\n\n`buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` — prompt construction from `src/generation/prompts/`.\n\n`writeSumFile()`, `readSumFile()`, `writeAnnexFile()` — YAML frontmatter `.sum` file I/O from `src/generation/writers/sum.ts`.\n\n`writeAgentsMd()` — directory doc writer from `src/generation/writers/agents-md.ts`.\n\n`checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()` — quality validators from `src/quality/`.\n\n`computeContentHashFromString()` — SHA-256 hashing from `src/change-detection/`.\n\n`PlanTracker`, `ProgressReporter`, `ITraceWriter` — progress tracking from `src/orchestration/`.\n\n`getVersion()` — package version from `src/version.ts`.\n\n## Trace Events Emitted\n\nAll events auto-populated with `seq`, `ts`, `pid`, `elapsedMs` by TraceWriter.\n\n- `phase:start` — taskCount, concurrency, phase label\n- `phase:end` — durationMs, tasksCompleted, tasksFailed, phase label\n- `task:start` — taskLabel, phase (Phase 3 only)\n- `task:done` — workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks (Phase 3 only)\n\n## RunSummary Schema\n\n```typescript\n{\n  version: string,\n  filesProcessed: number,\n  filesFailed: number,\n  filesSkipped: number,\n  totalCalls: number,\n  totalInputTokens: number,\n  totalOutputTokens: number,\n  totalCacheReadTokens: number,\n  totalCacheCreationTokens: number,\n  totalDurationMs: number,\n  errorCount: number,\n  retryCount: number,\n  totalFilesRead: number,\n  uniqueFilesRead: number,\n  inconsistenciesCodeVsDoc: number,\n  inconsistenciesCodeVsCode: number,\n  phantomPaths?: number,\n  inconsistencyReport?: InconsistencyReport\n}\n```\n\n## Error Handling\n\nQuality validation (code-vs-doc, code-vs-code, phantom-paths) wrapped in try-catch with `console.error()` — failures logged but don't abort pipeline.\n\nPhase 3 re-throws errors after emitting `task:done` with `success: false` to preserve existing error propagation.\n\nPool failures handled via `onResult` callback checking `result.success` — updates `filesFailed` counter, logs via `reporter.onFileError()`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2973,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 19212,
      "latencyMs": 79292,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/runner.ts",
          "sizeBytes": 35266
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:23.915Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LANGUAGES-MANIFEST.md\n\n```markdown\n# Package Manifest Files by Language\n\n| Language | Manifest File(s) | Package Manager | Supported |\n|----------|------------------|-----------------|-----------|\n| JavaScript/TypeScript | `package.json` | npm, yarn, pnpm | ✓ |\n| Python | `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` | pip, poetry, pipenv | ✓ |\n| Ruby | `Gemfile` | bundler | |\n| Go | `go.mod` | go modules | ✓ |\n| Rust | `Cargo.toml` | cargo | ✓ |\n| Java | `pom.xml` (Maven), `build.gradle` (Gradle) | Maven, Gradle | |\n| Kotlin | `build.gradle.kts`, `build.gradle` | Gradle | |\n| C#/.NET | `*.csproj`, `packages.config`, `*.fsproj` | NuGet | |\n| PHP | `composer.json` | Composer | |\n| Swift | `Package.swift` | Swift PM | |\n| Elixir | `mix.exs` | Mix | |\n| Erlang | `rebar.config` | rebar3 | |\n| Scala | `build.sbt` | sbt | |\n| Clojure | `deps.edn`, `project.clj` | clj, Leiningen | |\n| Haskell | `package.yaml`, `*.cabal`, `stack.yaml` | cabal, stack | |\n| Dart/Flutter | `pubspec.yaml` | pub | |\n| Lua | `*.rockspec` | LuaRocks | |\n| R | `DESCRIPTION` | CRAN | |\n| Julia | `Project.toml` | Pkg | |\n| Zig | `build.zig.zon` | zig | |\n| Nim | `*.nimble` | nimble | |\n| OCaml | `dune-project`, `*.opam` | dune, opam | |\n| C/C++ | `CMakeLists.txt`, `conanfile.txt`, `vcpkg.json` | CMake, Conan, vcpkg | |\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LANGUAGES-MANIFEST.md catalogues package manifest file patterns across 25 programming languages with support status indicators for the ARE directory aggregation phase.**\n\n## Reference Table Structure\n\nThe markdown table contains four columns: `Language`, `Manifest File(s)`, `Package Manager`, and `Supported`. Each row maps a language ecosystem to its dependency declaration files. The `Supported` column contains `✓` marks for languages currently handled by `detectManifestFiles()` in the directory aggregation phase (Phase 2).\n\n## Supported Manifest Detection\n\nLanguages marked with `✓` in the `Supported` column correspond to patterns in `src/generation/prompts/builder.ts`:\n- **JavaScript/TypeScript**: `package.json` detected via npm/yarn/pnpm\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` detected via pip/poetry/pipenv\n- **Go**: `go.mod` detected via go modules\n- **Rust**: `Cargo.toml` detected via cargo\n\nAdditional detected patterns without explicit `✓` marks:\n- **Java**: `pom.xml` (Maven), `build.gradle` (Gradle)\n- **PHP**: `composer.json` (Composer)\n- **C/C++**: `CMakeLists.txt`, `Makefile` (CMake, make)\n\nThe `detectManifestFiles()` function searches for exactly nine manifest types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, and `CMakeLists.txt`/`Makefile`. Discrepancies exist between the `✓` marks in this table and actual detection logic (e.g., Ruby `Gemfile` is detected but unmarked; Python `setup.py`/`Pipfile` are marked but not in the detection array).\n\n## Unsupported Languages\n\nLanguages without `✓` marks have documented manifest patterns but no active detection in the codebase:\n- **Ruby**: `Gemfile` (bundler) — detection exists but unmarked\n- **Kotlin**: `build.gradle.kts`, `build.gradle` (Gradle)\n- **C#/.NET**: `*.csproj`, `packages.config`, `*.fsproj` (NuGet)\n- **Swift**: `Package.swift` (Swift PM)\n- **Elixir**: `mix.exs` (Mix)\n- **Erlang**: `rebar.config` (rebar3)\n- **Scala**: `build.sbt` (sbt)\n- **Clojure**: `deps.edn`, `project.clj` (clj/Leiningen)\n- **Haskell**: `package.yaml`, `*.cabal`, `stack.yaml` (cabal/stack)\n- **Dart/Flutter**: `pubspec.yaml` (pub)\n- **Lua**: `*.rockspec` (LuaRocks)\n- **R**: `DESCRIPTION` (CRAN)\n- **Julia**: `Project.toml` (Pkg)\n- **Zig**: `build.zig.zon` (zig)\n- **Nim**: `*.nimble` (nimble)\n- **OCaml**: `dune-project`, `*.opam` (dune/opam)\n- **C/C++**: `conanfile.txt`, `vcpkg.json` (Conan/vcpkg) — `CMakeLists.txt` is detected\n\n## Integration with Directory Aggregation\n\nDuring Phase 2 directory aggregation (`buildDirectoryPrompt()` in `src/generation/prompts/builder.ts`), `detectManifestFiles()` scans the directory for manifest patterns and includes detected files in the AI prompt context. The prompt instructs the AI to recognize these files as dependency declarations and incorporate package information into the `AGENTS.md` directory summary.\n\nThis reference document serves as the canonical list for future manifest detection expansion. Adding support for additional languages requires:\n1. Updating the `COMMON_MANIFEST_FILES` array in `src/generation/prompts/builder.ts`\n2. Marking the language row with `✓` in this table",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1002,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9736,
      "latencyMs": 23563,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LANGUAGES-MANIFEST.md",
          "sizeBytes": 1293
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:29.516Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/package.json\n\n```json\n{\n  \"name\": \"agents-reverse-engineer\",\n  \"version\": \"0.7.1\",\n  \"description\": \"CLI tool for reverse-engineering codebase documentation for AI agents\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"agents-reverse-engineer\": \"dist/cli/index.js\",\n    \"are\": \"dist/cli/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"build:hooks\": \"node scripts/build-hooks.js\",\n    \"prepack\": \"rm -f LICENSE.sum README.md.sum\",\n    \"prepublishOnly\": \"npm run build && npm run build:hooks\",\n    \"dev\": \"tsx watch src/cli/index.ts\"\n  },\n  \"dependencies\": {\n    \"fast-glob\": \"^3.3.3\",\n    \"ignore\": \"^7.0.3\",\n    \"isbinaryfile\": \"^5.0.4\",\n    \"ora\": \"^8.1.1\",\n    \"picocolors\": \"^1.1.1\",\n    \"simple-git\": \"^3.27.0\",\n    \"yaml\": \"^2.7.0\",\n    \"zod\": \"^3.24.1\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.7\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.3\"\n  },\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  },\n  \"keywords\": [\n    \"documentation\",\n    \"codebase\",\n    \"ai\",\n    \"agents\",\n    \"reverse-engineering\"\n  ],\n  \"author\": \"GeoloeG-IsT\",\n  \"license\": \"MIT\",\n  \"main\": \"dist/cli/index.js\",\n  \"files\": [\n    \"dist\",\n    \"hooks/dist\",\n    \"README.md\",\n    \"LICENSE\"\n  ],\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"\n  },\n  \"homepage\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**package.json defines npm package metadata, dependencies, build scripts, and distribution configuration for the agents-reverse-engineer CLI tool.**\n\n## Package Identity\n\n**name**: `\"agents-reverse-engineer\"`  \n**version**: `\"0.7.1\"`  \n**description**: `\"CLI tool for reverse-engineering codebase documentation for AI agents\"`  \n**author**: `\"GeoloeG-IsT\"`  \n**license**: `\"MIT\"`  \n**type**: `\"module\"` (ES module package)\n\n## Binary Entry Points\n\nExports two CLI binaries mapping to `dist/cli/index.js`:\n- `agents-reverse-engineer` — full command name\n- `are` — short alias\n\n## Runtime Requirements\n\n**engines.node**: `\">=18.0.0\"` (minimum Node.js version)\n\n## Build Scripts\n\n**build**: `\"tsc\"` — compiles TypeScript source tree to `dist/` directory  \n**build:hooks**: `\"node scripts/build-hooks.js\"` — copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion  \n**prepack**: `\"rm -f LICENSE.sum README.md.sum\"` — removes documentation artifacts before tarball creation  \n**prepublishOnly**: `\"npm run build && npm run build:hooks\"` — sequential compilation before npm publish  \n**dev**: `\"tsx watch src/cli/index.ts\"` — hot-reload development mode via tsx watch\n\n## Production Dependencies\n\n**fast-glob** (`^3.3.3`) — file discovery with glob patterns  \n**ignore** (`^7.0.3`) — gitignore parsing  \n**isbinaryfile** (`^5.0.4`) — binary file detection  \n**ora** (`^8.1.1`) — terminal spinner UI  \n**picocolors** (`^1.1.1`) — ANSI color formatting  \n**simple-git** (`^3.27.0`) — git diff parsing for change detection  \n**yaml** (`^2.7.0`) — YAML config parsing  \n**zod** (`^3.24.1`) — schema validation\n\n## Development Dependencies\n\n**@types/node** (`^22.10.7`) — Node.js type definitions  \n**tsx** (`^4.19.2`) — TypeScript execution with watch mode  \n**typescript** (`^5.7.3`) — TypeScript compiler\n\n## Distribution Configuration\n\n**main**: `\"dist/cli/index.js\"` — CommonJS entry point fallback  \n**files**: `[\"dist\", \"hooks/dist\", \"README.md\", \"LICENSE\"]` — npm tarball inclusion list excludes source (`src/`), tests, and configuration files\n\n## Repository Links\n\n**repository.url**: `\"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"`  \n**bugs.url**: `\"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"`  \n**homepage**: `\"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"`\n\n## NPM Keywords\n\nSearch tags: `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 751,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9797,
      "latencyMs": 21025,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "package.json",
          "sizeBytes": 1445
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:31.657Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**TypeScript compiler configuration defining ES2022 target with NodeNext module resolution, strict type-checking, and source map generation for the agents-reverse-engineer CLI tool.**\n\n## Compiler Options\n\n**target**: `\"ES2022\"` — Emits JavaScript compatible with ECMAScript 2022 runtime (requires Node.js ≥18.0.0).\n\n**module**: `\"NodeNext\"` — Uses Node.js native ES module resolution with package.json `\"type\": \"module\"` support.\n\n**moduleResolution**: `\"NodeNext\"` — Resolves imports using Node.js package exports field and .js extension requirements for relative imports.\n\n**lib**: `[\"ES2022\"]` — Includes ES2022 standard library type definitions (Promise, Array methods, Object.hasOwn, etc.).\n\n**outDir**: `\"dist\"` — Compiles all TypeScript output to `dist/` directory (referenced by package.json `\"main\"` field and `bin` entry points).\n\n**rootDir**: `\"src\"` — Establishes `src/` as base directory for computing relative output paths in `dist/`.\n\n**strict**: `true` — Enables all strict type-checking flags (noImplicitAny, strictNullChecks, strictFunctionTypes, strictBindCallApply, strictPropertyInitialization, noImplicitThis, alwaysStrict).\n\n**esModuleInterop**: `true` — Enables CommonJS/ES module interoperability helpers for default imports from CJS modules.\n\n**skipLibCheck**: `true` — Skips type-checking of .d.ts files in node_modules to improve compilation speed.\n\n**forceConsistentCasingInFileNames**: `true` — Enforces case-sensitive file path imports (prevents Windows/macOS case-insensitivity bugs).\n\n**declaration**: `true` — Generates .d.ts type declaration files alongside compiled JavaScript (enables TypeScript consumers to import with type safety).\n\n**declarationMap**: `true` — Emits .d.ts.map files mapping type declarations back to original TypeScript source for IDE navigation.\n\n**sourceMap**: `true` — Generates .js.map files for debugging with original TypeScript line numbers in stack traces.\n\n**resolveJsonModule**: `true` — Allows importing JSON files with type inference (used for package.json version reading in src/version.ts).\n\n**isolatedModules**: `true` — Ensures each file can be transpiled independently without cross-file type information (required for build tools like esbuild/swc).\n\n## File Inclusion\n\n**include**: `[\"src/**/*\"]` — Compiles all TypeScript files recursively under `src/` directory (matches src/cli/index.ts, src/ai/service.ts, src/generation/prompts/templates.ts, etc.).\n\n**exclude**: `[\"node_modules\", \"dist\"]` — Excludes third-party dependencies and compiled output from compilation scope (prevents reprocessing of emitted .d.ts files).\n\n## Build Integration\n\nReferenced by `npm run build` script which invokes `tsc` to compile src/ → dist/. The `prepublishOnly` hook chains `build` + `build:hooks` before npm publish. Binary entry points in package.json (`bin.are`, `bin.agents-reverse-engineer`) resolve to `dist/cli/index.js` after compilation.\n\n## NodeNext Module Implications\n\nRequires explicit `.js` extensions in relative imports despite source files using `.ts` extension (e.g., `import { foo } from './bar.js'` resolves to `src/bar.ts` at compile time, `dist/bar.js` at runtime). Package.json must include `\"type\": \"module\"` to enable ES module semantics in emitted JavaScript.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 868,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9378,
      "latencyMs": 27694,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "tsconfig.json",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:30:28.281Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/README.md\n\n```markdown\n<div align=\"center\">\n\n# AGENTS REVERSE ENGINEER (ARE)\n\n**Reverse engineer your codebase into AI-friendly documentation.**\n\n**Generate `.sum` files, `AGENTS.md`, and root docs for Claude Code, OpenCode, and any AI assistant that supports `AGENTS.md`.**\n\n[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)\n[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)\n\n<br>\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\n**Interactive installer with runtime and location selection.**\n\n**Works on Mac, Windows, and Linux.**\n\n<br>\n\n_\"Finally, my AI assistant actually understands my codebase structure.\"_\n\n_\"No more explaining the same architecture in every conversation.\"_\n\n<br>\n\n[Why This Exists](#why-this-exists) · [How It Works](#how-it-works) · [Commands](#commands) · [Generated Docs](#generated-documentation)\n\n</div>\n\n---\n\n## Why This Exists\n\nAI coding assistants are powerful, but they don't know your codebase. Every session starts fresh. You explain the same architecture, the same patterns, the same file locations — over and over.\n\n**agents-reverse-engineer** fixes that. It generates documentation that AI assistants actually read:\n\n- **`.sum` files** — Per-file summaries with purpose, exports, dependencies\n- **`AGENTS.md`** — Per-directory overviews with file organization (standard format)\n- **`CLAUDE.md`** / **`GEMINI.md`** / **`OPENCODE.md`** — Runtime-specific project entry points\n\nThe result: Your AI assistant understands your codebase from the first message.\n\n---\n\n## Who This Is For\n\nDevelopers using AI coding assistants (Claude Code, OpenCode, Gemini CLI, or any tool supporting `AGENTS.md`) who want their assistant to actually understand their project structure — without manually writing documentation or repeating context every session.\n\n---\n\n## Getting Started\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nThe interactive installer prompts you to:\n\n1. **Select runtime** — Claude Code, OpenCode, Gemini CLI, or all\n2. **Select location** — Global (`~/.claude/`) or local (`./.claude/`)\n\nThis installs:\n\n- **Commands** — `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`\n- **Session hook** — Auto-updates docs when session ends (Claude/Gemini)\n\n### 2. Initialize Configuration\n\nAfter installation, create the configuration file in your AI assistant:\n\n```bash\n/are-init\n```\n\nThis creates `.agents-reverse-engineer/config.yaml` with default settings.\n\n### 3. Generate Documentation\n\nIn your AI assistant:\n\n```\n/are-discover\n/are-generate\n```\n\nThe assistant creates the plan and generates all documentation.\n\n### Non-Interactive Installation\n\n```bash\n# Install for Claude Code globally\nnpx agents-reverse-engineer@latest --runtime claude -g\n\n# Install for all runtimes locally\nnpx agents-reverse-engineer@latest --runtime all -l\n```\n\n### Uninstall\n\n```bash\nnpx agents-reverse-engineer@latest uninstall\n```\n\nRemoves:\n- Command files (`/are-*` commands)\n- Session hooks (Claude/Gemini)\n- ARE permissions from settings.json\n- `.agents-reverse-engineer` folder (local installs only)\n\nUse `--runtime` and `-g`/`-l` flags for specific targets.\n\n### Checking Version\n\n```bash\nnpx agents-reverse-engineer@latest --version\n```\n\n---\n\n## How It Works\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nInteractive installer installs commands and hooks for your chosen runtime(s).\n\n**Runtimes:** Claude Code, OpenCode, Gemini CLI (or all at once)\n\n---\n\n### 2. Initialize Configuration\n\n```\n/are-init\n```\n\nCreates `.agents-reverse-engineer/config.yaml` with exclusion patterns and options.\n\n---\n\n### 3. Discover & Plan\n\n```\n/are-discover\n```\n\nScans your codebase (respecting `.gitignore`), detects file types, and creates `GENERATION-PLAN.md` with all files to analyze.\n\nUses **post-order traversal** — deepest directories first, so child documentation exists before parent directories are documented.\n\n---\n\n### 4. Generate (in your AI assistant)\n\n```\n/are-generate\n```\n\nYour AI assistant executes the plan:\n\n1. **File Analysis** — Creates `.sum` file for each source file\n2. **Directory Docs** — Creates `AGENTS.md` for each directory\n3. **Root Docs** — Creates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`\n\n---\n\n### 5. Update Incrementally\n\n```\n/are-update\n```\n\nOnly regenerates documentation for files that changed since last run.\n\n---\n\n### 6. Generate Specification\n\n```\n/are-specify\n```\n\nSynthesizes all AGENTS.md documentation into a single project specification document (`specs/SPEC.md`). Use `--multi-file` to split into separate files, or `--dry-run` to preview without calling the AI.\n\n---\n\n## Commands\n\n| Command                         | Description                      |\n| ------------------------------- | -------------------------------- |\n| `are`                           | Interactive installer (default)  |\n| `are install`                   | Install with prompts             |\n| `are install --runtime <rt> -g` | Install to runtime globally      |\n| `are install --runtime <rt> -l` | Install to runtime locally       |\n| `are install -u`                | Uninstall (remove files/hooks)   |\n| `are init`                      | Create configuration file        |\n| `are discover`                  | List files that will be analyzed |\n| `are discover --plan`           | Create GENERATION-PLAN.md        |\n| `are discover --show-excluded`  | Show excluded files with reasons |\n| `are generate`                  | Generate all documentation       |\n| `are update`                    | Update changed files only        |\n| `are specify`                   | Generate project specification   |\n| `are clean`                     | Remove all generated docs        |\n\n**Runtimes:** `claude`, `opencode`, `gemini`, `all`\n\n### AI Assistant Commands\n\n| Command         | Description                    | Supported Runtimes       |\n| --------------- | ------------------------------ | ------------------------ |\n| `/are-init`     | Initialize config and commands | Claude, OpenCode, Gemini |\n| `/are-discover` | Rediscover and regenerate plan | Claude, OpenCode, Gemini |\n| `/are-generate` | Generate all documentation     | Claude, OpenCode, Gemini |\n| `/are-update`   | Update changed files only      | Claude, OpenCode, Gemini |\n| `/are-specify`  | Generate project specification | Claude, OpenCode, Gemini |\n| `/are-clean`    | Remove all generated docs      | Claude, OpenCode, Gemini |\n\n---\n\n## Generated Documentation\n\n### `.sum` Files (Per File)\n\n```yaml\n---\nfile_type: service\ngenerated_at: 2026-01-30T12:00:00Z\n---\n\n## Purpose\nHandles user authentication via JWT tokens.\n\n## Public Interface\n- `authenticate(token: string): User`\n- `generateToken(user: User): string`\n\n## Dependencies\n- jsonwebtoken: Token signing/verification\n- ./user-repository: User data access\n\n## Implementation Notes\nTokens expire after 24 hours. Refresh handled by client.\n```\n\n### `AGENTS.md` (Per Directory)\n\nDirectory overview with:\n\n- Description of the directory's role\n- Files grouped by purpose (Types, Services, Utils, etc.)\n- Subdirectories with brief descriptions\n\n### Root Documents\n\n- **`CLAUDE.md`** — Project entry point for Claude Code (auto-loaded)\n- **`GEMINI.md`** — Project entry point for Gemini CLI\n- **`OPENCODE.md`** — Project entry point for OpenCode\n- **`AGENTS.md`** — Root directory overview (universal format)\n\n---\n\n## Configuration\n\nEdit `.agents-reverse-engineer/config.yaml`:\n\n```yaml\n# File and directory exclusions\nexclude:\n  patterns: []              # Custom glob patterns (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Directories to skip\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:         # File types to skip\n    - .png\n    - .jpg\n    - .pdf\n\n# Discovery options\noptions:\n  followSymlinks: false     # Follow symbolic links during traversal\n  maxFileSize: 1048576      # Max file size in bytes (1MB default)\n\n# Output formatting\noutput:\n  colors: true              # Use colors in terminal output\n  verbose: true             # Show each file as processed\n\n# AI service configuration\nai:\n  backend: auto             # Backend: 'claude', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Model identifier (backend-specific)\n  timeoutMs: 300000         # Subprocess timeout in ms (5 minutes)\n  maxRetries: 3             # Max retries for transient errors\n  concurrency: 5            # Parallel AI calls (1-10, lower for WSL/constrained envs)\n\n  telemetry:\n    keepRuns: 50            # Number of run logs to keep\n    costThresholdUsd: 10.0  # Optional: warn when cost exceeds this (USD)\n\n  # Optional: Custom model pricing (override defaults)\n  pricing:\n    claude-opus-4:\n      inputCostPerMTok: 15.0    # USD per 1M input tokens\n      outputCostPerMTok: 75.0   # USD per 1M output tokens\n```\n\n### Key Config Options\n\n**Concurrency (`ai.concurrency`)**\n- Default: `5` (changed from 5 to 2 in WSL environments)\n- Range: `1-10`\n- Lower values recommended for resource-constrained environments\n- Higher values speed up generation but use more memory\n\n**Timeout (`ai.timeoutMs`)**\n- Default: `300000` (5 minutes)\n- AI subprocess timeout for each file analysis\n- Increase for very large files or slow connections\n\n---\n\n## Requirements\n\n- **Node.js 18+**\n- **AI Coding Assistant** — One of:\n  - [Claude Code](https://claude.ai/claude-code) (full support + session hooks)\n  - [Gemini CLI](https://github.com/google-gemini/gemini-cli) (full support + session hooks)\n  - [OpenCode](https://github.com/opencode-ai/opencode) (AGENTS.md supported)\n  - Any assistant supporting `AGENTS.md` format\n\n---\n\n## License\n\nMIT\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**README.md serves as the primary user-facing documentation entry point, providing installation workflows, command reference, configuration schema, and generated documentation structure for the agents-reverse-engineer CLI tool.**\n\n## Installation Patterns\n\nInteractive installer invoked via `npx agents-reverse-engineer@latest` prompts for runtime selection (`claude`, `opencode`, `gemini`, `all`) and location (`-g` global or `-l` local). Non-interactive mode uses `--runtime <rt>` with `-g`/`-l` flags. Uninstallation via `npx agents-reverse-engineer@latest uninstall` removes command files, session hooks, ARE permissions from `settings.json`, and `.agents-reverse-engineer` folder (local installs only). Version check via `--version` flag.\n\n## Command Interface\n\nCLI commands: `are install` (interactive or flagged), `are init` (creates `.agents-reverse-engineer/config.yaml`), `are discover` (scans files with `--plan` to generate `GENERATION-PLAN.md`, `--show-excluded` to display exclusion reasons), `are generate` (three-phase documentation generation), `are update` (incremental regeneration), `are specify` (synthesizes `specs/SPEC.md` with `--multi-file` split option and `--dry-run` preview), `are clean` (removes generated artifacts).\n\nAI assistant skill commands: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` available in Claude Code, OpenCode, and Gemini CLI runtimes.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` structure:\n- `exclude.patterns` (custom glob patterns like `[\"*.log\", \"temp/**\"]`)\n- `exclude.vendorDirs` (third-party directories: `node_modules`, `dist`, `.git`)\n- `exclude.binaryExtensions` (file types: `.png`, `.jpg`, `.pdf`)\n- `options.followSymlinks` (boolean, default `false`)\n- `options.maxFileSize` (bytes, default `1048576` for 1MB)\n- `output.colors` (boolean terminal ANSI flag)\n- `output.verbose` (boolean per-file logging)\n- `ai.backend` (enum: `'claude'`, `'gemini'`, `'opencode'`, `'auto'`)\n- `ai.model` (string, backend-specific identifier like `sonnet`)\n- `ai.timeoutMs` (number, default `300000` for 5-minute subprocess limit)\n- `ai.maxRetries` (number, default `3` for exponential backoff)\n- `ai.concurrency` (number 1-10, default `5`, reduced to `2` for WSL/resource-constrained environments)\n- `ai.telemetry.keepRuns` (number, default `50` log retention count)\n- `ai.telemetry.costThresholdUsd` (number, optional USD warning threshold)\n- `ai.pricing.claude-opus-4.inputCostPerMTok` (USD per 1M input tokens, default `15.0`)\n- `ai.pricing.claude-opus-4.outputCostPerMTok` (USD per 1M output tokens, default `75.0`)\n\n## Generated Documentation Formats\n\n`.sum` file structure includes YAML frontmatter with `file_type`, `generated_at` timestamp, followed by markdown sections: `## Purpose` (single-line role statement), `## Public Interface` (exported functions/classes with signatures), `## Dependencies` (library usage with purpose), `## Implementation Notes` (critical runtime behaviors).\n\n`AGENTS.md` per-directory format includes directory role description, files grouped by purpose (Types, Services, Utils), subdirectories with brief descriptions.\n\nRoot documents: `CLAUDE.md` (auto-loaded Claude Code entry point), `GEMINI.md` (Gemini CLI entry point), `OPENCODE.md` (OpenCode entry point), `AGENTS.md` (universal format root directory overview).\n\n## Workflow Sequence\n\nInstallation workflow: run `npx agents-reverse-engineer@latest` → select runtime(s) → select location → installs commands and session hooks. Documentation generation workflow: run `/are-init` → `/are-discover` (creates `GENERATION-PLAN.md` with post-order traversal plan) → `/are-generate` (Phase 1: file analysis creates `.sum` files, Phase 2: directory docs creates `AGENTS.md`, Phase 3: root docs creates `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) → `/are-update` (incremental regeneration for changed files only). Project specification synthesis via `/are-specify` aggregates all `AGENTS.md` into `specs/SPEC.md`.\n\n## Requirements\n\nNode.js 18+ runtime required. Supported AI assistants: Claude Code (full support with session hooks), Gemini CLI (full support with session hooks), OpenCode (AGENTS.md format supported), any assistant supporting `AGENTS.md` format.\n\n## Badge Links\n\n`[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)` links to npm package page. `[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)` links to LICENSE file.\n\n## Concurrency Tuning\n\n`ai.concurrency` range 1-10 controls parallel AI subprocess count. Default `5` (changed to `2` in WSL environments per v0.4.8+ resource management constraints). Lower values recommended for resource-constrained environments (reduces memory pressure), higher values accelerate generation (increase throughput). Timeout `ai.timeoutMs` default `300000` (5 minutes) per file analysis subprocess, increase for large files or slow network connections.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1409,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11875,
      "latencyMs": 34764,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "README.md",
          "sizeBytes": 9778
        }
      ]
    },
    {
      "timestamp": "2026-02-09T21:31:03.093Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/density\" (density)\n\n## File Summaries (1 files)\n\n### validator.ts\n**Purpose:** validator.ts implements stub findability validation for AGENTS.md symbol coverage, currently disabled after removal o...\n\n**validator.ts implements stub findability validation for AGENTS.md symbol coverage, currently disabled after removal of structured `publicInterface` metadata from SumFileContent schema.**\n\n## Exported Interfaces\n\n`FindabilityResult` represents validation outcome for a single `.sum` file with fields:\n- `filePath: string` — path to validated `.sum` file\n- `symbolsTested: string[]` — symbol names checked for presence\n- `symbolsFound: string[]` — symbols found in AGENTS.md content\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md content\n- `score: number` — ratio of found to tested symbols (0.0 to 1.0)\n\n## Exported Functions\n\n`validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>): FindabilityResult[]` returns empty array `[]` unconditionally. Function signature preserved for future re-implementation via post-processing passes that extract structured symbols from `.sum` files. Parameters prefixed with underscore indicate unused status.\n\n## Design Context\n\nModule originally validated that exported symbols from `.sum` files appeared in parent `AGENTS.md` content via string-based substring matching (no LLM calls). Validation logic removed after `SumFileContent.metadata.publicInterface` field was deleted from schema (referenced in comment: \"Previously relied on metadata.publicInterface which has been removed\"). Retention rationale stated in comment: \"This module is retained for future structured extraction support.\"\n\n## Integration Points\n\nImported by `src/quality/index.ts` which aggregates quality validators. `SumFileContent` type imported from `src/generation/writers/sum.ts`. Return type `FindabilityResult[]` consumed by quality reporting pipeline but produces no findings in current implementation.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub findability validation module, currently disabled after removal of structured `publicInterface` metadata from `.sum` file schema. Preserved for future re-implementation via post-processing symbol extraction.\n\n## Contents\n\n### [validator.ts](./validator.ts)\nExports `validateFindability()` stub returning empty `FindabilityResult[]` array and `FindabilityResult` interface defining validation outcome structure with `filePath`, `symbolsTested`, `symbolsFound`, `symbolsMissing`, `score` fields.\n\n## Architecture\n\n**Validation Approach (Disabled):** Originally performed string-based substring matching to verify exported symbols from `.sum` files appeared in parent `AGENTS.md` content without LLM calls. Logic removed when `SumFileContent.metadata.publicInterface` field was deleted from schema.\n\n**Current State:** `validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>)` unconditionally returns `[]` with parameters prefixed by underscores indicating unused status. Function signature retained for future structured extraction support via post-processing passes.\n\n## Integration Points\n\nConsumed by `src/quality/index.ts` which aggregates quality validators (`validateCodeDocConsistency`, `validateCodeCodeConsistency`, `validatePhantomPaths`, `validateFindability`). Produces no findings in current implementation but return type `FindabilityResult[]` preserved in reporting pipeline.\n\n**Type Dependencies:**\n- `SumFileContent` imported from `../../generation/writers/sum.js` (verified path)\n- `FindabilityResult` consumed by quality reporting aggregator\n\n## Behavioral Contracts\n\n**Return Value:** Always `[]` (empty array) indicating zero validation findings.\n\n**Score Calculation (Historical):** Ratio of `symbolsFound.length / symbolsTested.length` yielding `0.0` (no symbols) to `1.0` (all symbols present) when validation was active.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 460,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9564,
      "latencyMs": 21316,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.098Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/phantom-paths\" (phantom-paths)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** Re-exports `checkPhantomPaths` validator function from `./validator.js` as the public interface for phantom path dete...\n\n**Re-exports `checkPhantomPaths` validator function from `./validator.js` as the public interface for phantom path detection.**\n\n## Exported Interface\n\nExports `checkPhantomPaths` function from `./validator.js` module, providing access to the phantom path validation subsystem that detects unresolvable file references in generated `AGENTS.md` documentation.\n\n## Module Role\n\nServes as the barrel export for the `src/quality/phantom-paths/` subsystem within the larger quality validation pipeline (`src/quality/index.ts`). This pattern centralizes the phantom path detection capability behind a single import point while keeping implementation details isolated in `validator.ts`.\n### validator.ts\n**Purpose:** validator.ts extracts path-like strings from AGENTS.md content via three regex patterns, resolves them relative to bo...\n\n**validator.ts extracts path-like strings from AGENTS.md content via three regex patterns, resolves them relative to both the AGENTS.md directory and project root with .ts/.js extension fallback, and reports unresolved references as PhantomPathInconsistency issues.**\n\n## Exported Function\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` scans AGENTS.md content for path references, attempts multi-strategy resolution, and returns phantom path issues with severity='warning', type='phantom-path', including referencedPath, resolvedTo, and contextLine details.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` array contains three RegExp patterns for extracting path-like references:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets starting with `.`\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths starting with `src/`, `./`, or `../` ending in 1-4 letter extension\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded `src/` paths following keywords \"from\", \"in\", \"by\", \"via\", or \"see\"\n\n## Skip Filters\n\n`SKIP_PATTERNS` array excludes non-file references:\n- `/node_modules/` — Third-party dependencies\n- `/\\.git\\//` — Git internal paths\n- `/^https?:/` — HTTP/HTTPS URLs\n- `/\\{\\{/` — Template placeholder delimiters\n- `/\\$\\{/` — Template literal expressions\n- `/\\*/` — Glob wildcard patterns\n- `/\\{[^}]*,[^}]*\\}/` — Brace expansion syntax (e.g., `{a,b,c}`)\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths` attempts resolution in four locations:\n1. `path.resolve(agentsMdDir, rawPath)` — Relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — Relative to project root (handles `src/` paths)\n3. `.ts` extension fallback for `.js` paths resolved from agentsMdDir\n4. `.ts` extension fallback for `.js` paths resolved from projectRoot\n\n`existsSync(p)` validates each candidate path via filesystem check.\n\n## Issue Context Extraction\n\nWhen unresolved, `checkPhantomPaths` searches content line-by-line via `lines.find((l) => l.includes(rawPath))` to extract contextLine containing the reference, trims it, slices to 120 characters maximum, and includes in `PhantomPathInconsistency.details.context` for debugging.\n\n## Deduplication\n\n`seen` Set tracks already-processed rawPath strings to prevent duplicate issues for the same path reference appearing multiple times in content.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../types.js → PhantomPathInconsistency (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable file path references in generated `AGENTS.md` documentation through regex extraction, multi-strategy filesystem resolution with TypeScript/JavaScript extension fallback, and `PhantomPathInconsistency` issue reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Re-exports `checkPhantomPaths` from `validator.js` as barrel export for phantom path detection subsystem within parent `src/quality/` pipeline.\n\n**[validator.ts](./validator.ts)** — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` scanning AGENTS.md text via `PATH_PATTERNS` regex array (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location resolution (agentsMdDir-relative, projectRoot-relative, .ts fallback for both), filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), extracts 120-char contextLine from content on resolution failure, deduplicates via Set, returns `PhantomPathInconsistency[]` with severity='warning', type='phantom-path', referencedPath, resolvedTo, contextLine details.\n\n## Path Extraction Strategy\n\n`PATH_PATTERNS` contains three RegExp patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link syntax\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures code-formatted paths starting with src/, ./, ../\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures src/ paths following trigger keywords\n\n`SKIP_PATTERNS` excludes seven non-file reference types: node_modules, .git, HTTP(S) URLs, template placeholders (`{{`, `${`), glob wildcards, brace expansion syntax.\n\n## Resolution Protocol\n\nFor each extracted path, `checkPhantomPaths` attempts `existsSync()` validation at four filesystem locations in sequence:\n1. `path.resolve(agentsMdDir, rawPath)` — relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — relative to project root (handles absolute-style src/ paths)\n3. agentsMdDir-relative path with .js → .ts extension substitution\n4. projectRoot-relative path with .js → .ts extension substitution\n\nStops at first successful resolution; reports unresolved path as `PhantomPathInconsistency` with contextLine extracted via `lines.find((l) => l.includes(rawPath))`, trimmed and sliced to 120-char maximum.\n\n## Behavioral Contracts\n\n**Path extraction patterns (validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 864,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11338,
      "latencyMs": 24521,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.114Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/inconsistency\" (inconsistency)\n\n## File Summaries (3 files)\n\n### code-vs-code.ts\n**Purpose:** checkCodeVsCode() detects duplicate exports across a scoped file group by aggregating symbol names via extractExports...\n\n**checkCodeVsCode() detects duplicate exports across a scoped file group by aggregating symbol names via extractExports() and flagging any symbol exported from multiple files within the group.**\n\n## Exported Function\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\n\nAccepts array of file objects with `path` and `content` properties, extracts exported symbols from each file via `extractExports()` (imported from `./code-vs-doc.js`), builds aggregation map `Map<string, string[]>` where key is symbol name and value is array of file paths exporting that symbol, returns `CodeCodeInconsistency[]` array containing entries for symbols appearing in more than one file.\n\n## Algorithm\n\nInitializes `exportMap` as `Map<string, string[]>`, iterates `files` array calling `extractExports(file.content)` for each entry, populates map by appending `file.path` to existing array or creating new entry with single-element array. Second pass iterates map entries via `for (const [name, paths] of exportMap)`, pushes `CodeCodeInconsistency` object to output array when `paths.length > 1`.\n\n## Inconsistency Shape\n\nEach returned `CodeCodeInconsistency` object contains:\n- `type: 'code-vs-code'` discriminant\n- `severity: 'warning'` static level\n- `files: paths` array of all file paths exporting the duplicate symbol\n- `description: \\`Symbol \"${name}\" exported from ${paths.length} files\\`` template\n- `pattern: 'duplicate-export'` static sentinel\n\n## Scoping Constraint\n\nCaller must scope input to per-directory file groups to prevent false positives across unrelated modules (e.g., intentional re-exports, monorepo package boundaries, namespace collisions across domains). No directory validation enforced within function—relies on caller to partition files appropriately before invocation.\n\n## Dependencies\n\nImports `extractExports()` from `./code-vs-doc.js` for regex-based export symbol extraction via pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Imports `CodeCodeInconsistency` type from `../types.js`.\n\n## Design Pattern\n\nPure heuristic analysis with no AI service calls, operates entirely on static regex matching of export statements. Detection granularity limited to symbol names—no AST analysis to distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions.\n### code-vs-doc.ts\n**Purpose:** code-vs-doc.ts detects documentation drift by comparing exported identifiers from TypeScript/JavaScript source files ...\n\n**code-vs-doc.ts detects documentation drift by comparing exported identifiers from TypeScript/JavaScript source files against their corresponding .sum file content, flagging symbols that exist in code but are undocumented.**\n\n## Exported Functions\n\n**extractExports(sourceContent: string): string[]** — Extracts exported identifier names from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Captures identifiers from export declarations including `export function`, `export const`, `export default class`, `export type`, `export interface`, and `export enum`. Returns array of identifier strings, ignoring re-exports, comments, and non-exported declarations.\n\n**checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null** — Compares exports extracted via `extractExports()` against `sumContent.summary` text using case-sensitive substring matching. Returns `CodeDocInconsistency` with `type: 'code-vs-doc'` and `severity: 'warning'` when exports are missing from documentation. Populates `details.missingFromDoc` with undocumented export names and `details.missingFromCode` as empty array (legacy field). Returns `null` when all exports appear in `.sum` text.\n\n## Type Dependencies\n\nConsumes `SumFileContent` from `../../generation/writers/sum.js` providing `summary: string` field for substring search. Returns `CodeDocInconsistency` from `../types.js` with discriminated union fields: `type`, `severity`, `filePath`, `sumPath`, `description`, and `details: { missingFromDoc: string[], missingFromCode: string[] }`.\n\n## Detection Strategy\n\nUses heuristic substring presence check: `sumText.includes(e)` determines whether export identifier `e` appears anywhere in `.sum` markdown content. False negatives occur when identifier appears in prose unrelated to API surface (e.g., \"the function exports data\"). No AST analysis or semantic validation performed.\n\n## Regex Limitations\n\nPattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` misses:\n- Destructured exports: `export const { foo, bar } = obj`\n- Namespace exports: `export * as ns from './mod'`\n- Re-exported identifiers: `export { foo } from './other'`\n- Dynamic exports: `module.exports = ...`\n- Multi-line declarations where export keyword and identifier span separate lines\n### reporter.ts\n**Purpose:** reporter.ts aggregates inconsistency issues into structured InconsistencyReport objects with summary counts and forma...\n\n**reporter.ts aggregates inconsistency issues into structured InconsistencyReport objects with summary counts and formats them as plain-text CLI output without color dependencies.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nConstructs InconsistencyReport by counting issues across three dimensions: type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). Returns object with `metadata` (timestamp ISO 8601, projectRoot, filesChecked, durationMs), `issues[]` array, and `summary` object containing `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info` integer counts. Iterates issues once using discriminated union pattern on `type` and `severity` fields.\n\n### formatReportForCli\n\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms InconsistencyReport into newline-delimited plain text with no ANSI color codes (caller applies picocolors if needed). Output format:\n\n```\n=== Inconsistency Report ===\nChecked {filesChecked} files in {durationMs}ms\nFound {total} issue(s)\n\n[ERROR|WARN|INFO] {description}\n  File: {filePath}              (for code-vs-doc)\n  Doc: {agentsMdPath}           (for phantom-path)\n  Path: {referencedPath}        (for phantom-path)\n  Files: {files.join(', ')}     (for code-vs-code)\n```\n\nMaps severity to tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`. Appends blank line after each issue block. Uses ternary operator chain for tag selection.\n\n## Type Dependencies\n\nImports `Inconsistency` and `InconsistencyReport` from `../types.js`. Inconsistency is discriminated union handling three issue types with distinct `filePath`, `files[]`, or `agentsMdPath` + `details.referencedPath` properties depending on type field.\n\n## Design Rationale\n\nModule designed for testability by avoiding picocolors dependency—pure string concatenation only. CLI layer applies color formatting to returned string. Summary aggregation uses simple counter variables instead of reduce/groupBy to minimize allocations during validation phase when thousands of inconsistencies may exist.\n\n## Import Map (verified — use these exact paths)\n\ncode-vs-code.ts:\n  ../types.js → CodeCodeInconsistency (type)\n\ncode-vs-doc.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n  ../types.js → CodeDocInconsistency (type)\n\nreporter.ts:\n  ../types.js → Inconsistency, InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\n**Detects three classes of code-documentation drift: code-vs-doc (exports missing from .sum files), code-vs-code (duplicate symbol exports within directory scope), and phantom-paths (broken file references in AGENTS.md), aggregating findings into structured InconsistencyReport objects with plain-text CLI formatting.**\n\n## Contents\n\n### Detection Modules\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts identifiers from TypeScript/JavaScript source via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` compares extracted symbols against SumFileContent.summary substring presence, returns CodeDocInconsistency with `missingFromDoc[]` when exports absent from documentation.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` aggregates exports via `Map<string, string[]>` across scoped file group, returns CodeCodeInconsistency array flagging symbols exported from multiple files with `pattern: 'duplicate-export'` sentinel, relies on caller to enforce per-directory scoping.\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates Inconsistency discriminated union into InconsistencyReport with summary counts across type/severity dimensions, `formatReportForCli()` renders plain-text output without picocolors dependency for testability.\n\n## File Relationships\n\ncode-vs-code.ts imports `extractExports()` from code-vs-doc.ts for shared regex-based export extraction. Both detection modules return type-specific inconsistency objects (CodeDocInconsistency, CodeCodeInconsistency) consumed by reporter.ts via Inconsistency discriminated union. Reporter aggregates findings from both modules into unified InconsistencyReport structure with metadata (timestamp, projectRoot, filesChecked, durationMs) and typed summary counts.\n\n## Behavioral Contracts\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nCaptures identifiers from `export function`, `export const`, `export default class`, `export type`, `export interface`, `export enum`. Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n### Inconsistency Type Discriminants\n- `code-vs-doc`: requires `filePath`, `sumPath`, `details.missingFromDoc[]`\n- `code-vs-code`: requires `files[]`, `pattern: 'duplicate-export'`\n- `phantom-path`: requires `agentsMdPath`, `details.referencedPath`\n\n### Severity Levels\nMapped to CLI tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`\n\n## Detection Limitations\n\n**code-vs-doc**: Substring matching (`sumText.includes(e)`) yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**code-vs-code**: Name-only comparison without AST analysis cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions. Requires caller-enforced per-directory scoping to prevent false positives across module boundaries.\n\n**Shared**: Regex-based extraction misses complex export patterns (destructured, namespace, dynamic). Both modules operate as pure heuristics without AI service calls.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 929,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12450,
      "latencyMs": 27102,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.107Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/telemetry\" (telemetry)\n\n## File Summaries (3 files)\n\n### cleanup.ts\n**Purpose:** `cleanup.ts` removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent ...\n\n**`cleanup.ts` removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent `run-*.json` files sorted lexicographically by ISO timestamp filenames.**\n\n## Exported Functions\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` reads the logs directory at `path.join(projectRoot, LOGS_DIR)`, filters entries matching the pattern `run-*.json`, sorts lexicographically (newest first via `sort()` then `reverse()`), slices beyond `keepCount`, and unlinks excess files. Returns the number of deleted files. Returns `0` without error if the logs directory does not exist (catches `ENOENT` error code). Throws on other filesystem errors.\n\n## Constants\n\n`LOGS_DIR` equals `'.agents-reverse-engineer/logs'`, defining the relative path from project root to the telemetry log directory.\n\n## Integration Points\n\nCalled by `src/ai/telemetry/logger.ts` after writing a new run log to enforce retention policy specified in `config.ai.telemetry.keepRuns` (default 50). Works in conjunction with `src/ai/telemetry/run-log.ts` which defines the `RunLog` structure and `src/orchestration/runner.ts` which orchestrates telemetry logging during command execution.\n\n## Sorting Behavior\n\nLexicographic sorting works correctly for ISO 8601 timestamps embedded in filenames (e.g., `run-2026-02-09T12-34-56-789Z.json`) because the timestamp format naturally orders chronologically when sorted as strings. The `reverse()` call after `sort()` produces newest-first ordering, so `slice(keepCount)` targets oldest files for deletion.\n\n## Error Handling\n\nSuppresses `ENOENT` errors during `fs.readdir()` to handle missing logs directory gracefully (fresh projects or post-`are clean` state). Propagates all other filesystem errors to the caller without wrapping.\n### logger.ts\n**Purpose:** TelemetryLogger accumulates per-call AI service telemetry entries in memory and computes aggregate RunLog summaries w...\n\n**TelemetryLogger accumulates per-call AI service telemetry entries in memory and computes aggregate RunLog summaries with token counts, latency metrics, file-read statistics, and error tallies.**\n\n## Exported Class\n\n**TelemetryLogger** — In-memory accumulator for TelemetryEntry instances created once per CLI invocation. Constructor accepts `runId: string` (ISO timestamp-based identifier). Exposes read-only `runId: string` and `startTime: string` (ISO 8601 timestamp set at construction).\n\n## Public Methods\n\n**addEntry(entry: TelemetryEntry): void** — Records telemetry entry for a completed AI subprocess call. Appends to internal `entries: TelemetryEntry[]` array.\n\n**getEntries(): readonly TelemetryEntry[]** — Returns immutable view of accumulated entries array.\n\n**setFilesReadOnLastEntry(filesRead: FileRead[]): void** — Updates `filesRead` property of most recent entry. Called by AIService after command runner attaches file metadata post-execution. No-op if entries array is empty.\n\n**getSummary(): RunLog['summary']** — Computes aggregate statistics over all entries on every invocation (no caching). Returns object with:\n- `totalCalls: number` — Length of entries array\n- `totalInputTokens: number` — Sum of `entry.inputTokens` across all entries\n- `totalOutputTokens: number` — Sum of `entry.outputTokens`\n- `totalCacheReadTokens: number` — Sum of `entry.cacheReadTokens`\n- `totalCacheCreationTokens: number` — Sum of `entry.cacheCreationTokens`\n- `totalDurationMs: number` — Sum of `entry.latencyMs`\n- `errorCount: number` — Count of entries where `entry.error !== undefined`\n- `totalFilesRead: number` — Sum of `entry.filesRead.length` across all entries\n- `uniqueFilesRead: number` — Distinct file paths via `Set<string>` deduplication of `file.path` values\n\n**toRunLog(): RunLog** — Assembles final RunLog for serialization. Sets `endTime` to `new Date().toISOString()`, copies entries array, invokes `getSummary()`. Call once when run completes. Returns object with `runId`, `startTime`, `endTime`, `entries`, and `summary` properties.\n\n## Integration Points\n\nConsumes `TelemetryEntry`, `RunLog`, and `FileRead` types from `../types.js`. Used by AIService (`src/ai/service.ts`) to track subprocess call metrics. Output written to `.agents-reverse-engineer/logs/run-<timestamp>.json` via run-log persistence layer (`src/ai/telemetry/run-log.ts`).\n### run-log.ts\n**Purpose:** writeRunLog() serializes completed RunLog objects to pretty-printed JSON files in `.agents-reverse-engineer/logs/` wi...\n\n**writeRunLog() serializes completed RunLog objects to pretty-printed JSON files in `.agents-reverse-engineer/logs/` with ISO timestamp-derived filenames.**\n\n## Exported Functions\n\n**writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>**\n\nWrites a completed RunLog to disk as pretty-printed JSON (2-space indentation). Creates `.agents-reverse-engineer/logs/` directory via `fs.mkdir()` with `recursive: true` if absent. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to ensure cross-platform filesystem compatibility. Returns absolute path to written file. Pattern: `run-${safeTimestamp}.json` where `safeTimestamp` transforms `2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z`.\n\n## Constants\n\n**LOGS_DIR**: `'.agents-reverse-engineer/logs'`\n\nRelative path from project root to telemetry log directory.\n\n## Dependencies\n\n- `node:fs/promises`: Async `mkdir()` and `writeFile()` operations\n- `node:path`: Path joining via `path.join()`\n- `RunLog` type from `../types.js`: Telemetry run log schema with `startTime` (ISO 8601 string), token counts, costs, errors, filesRead metadata\n\n## Integration Points\n\nCalled by `TelemetryLogger.endRun()` in `src/ai/telemetry/logger.ts` after aggregating all AIServiceCall results from a generation/update run. Paired with `cleanupOldLogs()` from `src/ai/telemetry/cleanup.ts` which enforces retention limits by deleting oldest run-*.json files beyond configured threshold.\n\n## Behavioral Contracts\n\nFilename transformation: `runLog.startTime.replace(/[:.]/g, '-')` converts ISO 8601 timestamps with colons and periods into filesystem-safe format. Example: `2026-02-07T12:00:00.000Z` → `run-2026-02-07T12-00-00-000Z.json`.\n\n## Import Map (verified — use these exact paths)\n\nlogger.ts:\n  ../types.js → TelemetryEntry, RunLog, FileRead (type)\n\nrun-log.ts:\n  ../types.js → RunLog (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\n**Accumulates per-subprocess telemetry entries in memory via TelemetryLogger, serializes aggregate RunLog summaries to timestamped NDJSON files in `.agents-reverse-engineer/logs/`, and enforces retention limits via lexicographic filename sorting.**\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes oldest `run-*.json` files beyond retention threshold via lexicographic sort, catches `ENOENT` for missing logs directory.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates `TelemetryEntry` instances in memory, computes aggregate statistics via `getSummary()` (token sums, error counts, unique file deduplication), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` serializes `RunLog` to `.agents-reverse-engineer/logs/run-<safeTimestamp>.json` with ISO timestamp transformation (`2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z.json`).\n\n## Data Flow\n\n1. **AIService** (`src/ai/service.ts`) invokes `TelemetryLogger.addEntry()` after each subprocess call with `TelemetryEntry` containing `inputTokens`, `outputTokens`, `cacheReadTokens`, `latencyMs`, `error`, `filesRead[]`.\n2. **Command orchestrator** (`src/orchestration/runner.ts`) calls `logger.setFilesReadOnLastEntry(filesRead)` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`).\n3. **Run completion**: orchestrator calls `logger.toRunLog()` to produce `RunLog` with aggregated summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`).\n4. **Persistence**: `writeRunLog(projectRoot, runLog)` serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n5. **Retention enforcement**: `cleanupOldLogs(projectRoot, keepCount)` deletes oldest logs beyond `config.ai.telemetry.keepRuns` threshold (default 50).\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService instantiates TelemetryLogger once per run, calls `addEntry()` per subprocess, invokes `toRunLog()` at finalization.\n- `src/orchestration/runner.ts` — Orchestrates telemetry lifecycle: logger creation, entry enrichment with file reads, run log serialization, cleanup invocation.\n\n**Consumes:**\n- `src/ai/types.ts` — `TelemetryEntry`, `RunLog`, `FileRead` type definitions with token count fields, latency metrics, error summaries.\n\n## Behavioral Contracts\n\n**Filename transformation** (run-log.ts):\n```typescript\nrunLog.startTime.replace(/[:.]/g, '-')\n// Input:  \"2026-02-09T12:34:56.789Z\"\n// Output: \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Lexicographic sorting** (cleanup.ts):\n- ISO 8601 timestamps in filenames sort chronologically when treated as strings.\n- `sort()` then `reverse()` produces newest-first order.\n- `slice(keepCount)` targets oldest files for deletion.\n\n**Aggregate statistics** (logger.ts `getSummary()`):\n- `totalCalls` = `entries.length`\n- `totalInputTokens` = `Σ(entry.inputTokens)`\n- `uniqueFilesRead` = distinct count via `Set<string>` deduplication of `entry.filesRead.map(f => f.path)`\n\n## Retention Policy\n\nDefault retention: 50 runs (`config.ai.telemetry.keepRuns`). `cleanupOldLogs()` invoked after every `writeRunLog()` call. Missing logs directory handled gracefully (returns 0 without error). Propagates non-`ENOENT` filesystem errors to caller.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1047,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12159,
      "latencyMs": 28714,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.128Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery/filters\" (filters)\n\n## File Summaries (5 files)\n\n### binary.ts\n**Purpose:** binary.ts implements extension-first binary file detection with fallback content analysis and size-based exclusion fo...\n\n**binary.ts implements extension-first binary file detection with fallback content analysis and size-based exclusion for the file discovery pipeline.**\n\n## Exported Functions\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` constructs a `FileFilter` that returns `true` from `shouldExclude()` for binary files using a two-phase detection strategy: fast-path extension lookup in `BINARY_EXTENSIONS` set, then slow-path content analysis via `isBinaryFile()` from the `isbinaryfile` library for unknown extensions. Files exceeding `maxFileSize` (default 1MB) are excluded regardless of content type. Returns `true` on `fs.stat()` errors to skip inaccessible files.\n\n## Exported Constants\n\n`BINARY_EXTENSIONS: Set<string>` contains 96 lowercase file extensions with leading dots categorized as images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`), executables (`.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`), media (`.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`), documents (`.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`), compiled bytecode (`.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`), databases (`.db`, `.sqlite`, `.sqlite3`, `.mdb`), and other binary formats (`.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`).\n\n## Configuration Interface\n\n`BinaryFilterOptions` accepts optional `maxFileSize: number` (bytes, default 1048576 via `DEFAULT_MAX_FILE_SIZE`) and `additionalExtensions: string[]` merged into the detection set with automatic leading-dot normalization via `ext.startsWith('.') ? ext : \\`.${ext}\\``.\n\n## Detection Algorithm\n\n`shouldExclude()` executes:\n1. Extract extension via `path.extname(absolutePath).toLowerCase()`\n2. Check membership in merged `binaryExtensions` set (returns `true` immediately)\n3. Call `fs.stat()` to retrieve file size\n4. Compare `stats.size > maxFileSize` (returns `true` if exceeded)\n5. Invoke `isBinaryFile(absolutePath)` for content-based binary detection\n6. Catch all exceptions and return `true` to exclude unreadable files\n\n## Integration Point\n\nReturns `FileFilter` interface with `name: 'binary'` property for pipeline composition via `src/discovery/filters/index.ts`.\n### custom.ts\n**Purpose:** createCustomFilter() implements gitignore-style pattern matching for user-defined exclusion rules during file discovery.\n\n**createCustomFilter() implements gitignore-style pattern matching for user-defined exclusion rules during file discovery.**\n\n## Exported Interface\n\n- **`createCustomFilter(patterns: string[], root: string): FileFilter`** — Constructs FileFilter that excludes paths matching gitignore-style patterns relative to root directory. Returns filter with `name: 'custom'` and `shouldExclude()` method. Empty patterns array bypasses all exclusion logic (returns false for all paths).\n\n## Pattern Matching Implementation\n\nUses `ignore` library to parse gitignore syntax. Creates `Ignore` instance via `ignore()`, adds patterns via `ig.add(patterns)`, evaluates paths via `ig.ignores(relativePath)`. Normalizes root via `path.resolve(root)` to ensure consistent absolute path comparison.\n\n## Path Transformation Logic\n\nConverts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)` before pattern matching (ignore library requires relative paths). Bypasses exclusion for paths outside root hierarchy (relative path starts with `..`) or empty relative paths. Pattern evaluation only runs if `patterns.length > 0`.\n\n## Filter Contract Compliance\n\nImplements `FileFilter` interface from `../types.js` with required `name: 'custom'` property and `shouldExclude(absolutePath: string): boolean` method. Integrates with filter chain orchestrated by discovery walker.\n\n## Example Behavior\n\nPatterns `['*.log', 'tmp/**', 'secret.txt']` with root `/project`:\n- `/project/debug.log` → relative `debug.log` → matches `*.log` → returns `true`\n- `/project/src/app.ts` → relative `src/app.ts` → no match → returns `false`\n- `/other/file.log` → relative `../other/file.log` → outside root → returns `false`\n### gitignore.ts\n**Purpose:** createGitignoreFilter() builds a FileFilter that excludes paths matching .gitignore patterns using the `ignore` library.\n\n**createGitignoreFilter() builds a FileFilter that excludes paths matching .gitignore patterns using the `ignore` library.**\n\n## Exported Interface\n\n```typescript\nasync function createGitignoreFilter(root: string): Promise<FileFilter>\n```\n\nReturns a `FileFilter` object with `name: 'gitignore'` and `shouldExclude(absolutePath: string): boolean` method.\n\n## Implementation Details\n\ncreateGitignoreFilter() normalizes `root` via `path.resolve()`, attempts to read `.gitignore` from `path.join(normalizedRoot, '.gitignore')`, passes content to `ignore.add()` if file exists, silently skips if ENOENT occurs (try-catch with empty catch block).\n\nshouldExclude() converts `absolutePath` to relative via `path.relative(normalizedRoot, absolutePath)`, returns `false` immediately if relativePath is empty or starts with `'..'` (path outside root), calls `ig.ignores(relativePath)` otherwise.\n\n## Critical Constraints\n\nThe `ignore` library requires relative paths without leading slashes. shouldExclude() does NOT append trailing slash to paths, assuming walker returns files only (not directories). Directory exclusion would require appending `/` to directory paths before calling `ig.ignores()`.\n\nReturns `false` for paths outside root tree to prevent exclusion of unrelated filesystem locations.\n\n## Dependencies\n\n- `ignore` library (`import ignore, { type Ignore } from 'ignore'`) for .gitignore pattern matching\n- `fs.promises.readFile()` for async .gitignore content loading\n- `path.resolve()`, `path.join()`, `path.relative()` for path normalization\n\n## Integration\n\nUsed in composable filter chain via `src/discovery/filters/index.ts` alongside binary, vendor, and custom pattern filters. Returned `FileFilter` object matches `FileFilter` interface from `src/discovery/types.ts`.\n### index.ts\n**Purpose:** src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, applying multiple FileFilter p...\n\n**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, applying multiple FileFilter predicates sequentially with bounded concurrency and tracking exclusion reasons per filter.**\n\n## Exported Functions\n\n**applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>** processes files through filter chain with short-circuit evaluation. Returns FilterResult containing `included: string[]` and `excluded: ExcludedFile[]` arrays. Each file runs through filters sequentially until one returns true from `shouldExclude()`, at which point remaining filters are skipped. Uses bounded concurrency of 30 workers sharing single iterator to prevent file descriptor exhaustion during I/O-heavy binary content detection.\n\n## Re-Exported Filter Creators\n\nAggregates filter factory functions from sibling modules:\n- **createGitignoreFilter** from `./gitignore.js`\n- **createVendorFilter** and **DEFAULT_VENDOR_DIRS** from `./vendor.js`\n- **createBinaryFilter**, **BINARY_EXTENSIONS**, **BinaryFilterOptions** from `./binary.js`\n- **createCustomFilter** from `./custom.js`\n\n## Concurrency Strategy\n\nUses iterator-based worker pool pattern with `CONCURRENCY = 30` to process files in parallel without exhausting file handles. Workers share single `files.entries()` iterator, each consuming items until exhausted. BinaryFilter calls `isBinaryFile()` which performs synchronous file reads, making concurrency bounds critical to prevent system resource limits (ulimit -n).\n\n## Filter Execution Algorithm\n\n1. Initialize `filterStats` map tracking `matched` and `rejected` counts per filter name\n2. Spawn 30 workers (or `files.length` if fewer) each iterating over shared file entries\n3. For each file, iterate through `filters[]` array sequentially\n4. On first `shouldExclude()` returning true, push `ExcludedFile` with `{ path, reason, filter: filter.name }` and break inner loop (short-circuit)\n5. If no filter excludes file, push path to `included[]` and increment `matched` count for all filters\n6. Flatten worker results, sort by original index to preserve order\n7. Emit `filter:applied` trace events with `filesMatched` and `filesRejected` counts per filter\n\n## Trace Events\n\nEmits `filter:applied` events via `options?.tracer?.emit()` containing:\n- `type: 'filter:applied'`\n- `filterName: string` — filter.name from FileFilter\n- `filesMatched: number` — count of files passing through this filter\n- `filesRejected: number` — count of files excluded by this filter\n\n## Debug Output\n\nWhen `options?.debug` is true and `stats.rejected > 0`, writes `pc.dim()` colored message to stderr: `[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`.\n\n## Return Type\n\nFilterResult contains:\n- `included: string[]` — absolute paths passing all filters\n- `excluded: ExcludedFile[]` — objects with `{ path: string, reason: string, filter: string }` describing which filter excluded each file\n### vendor.ts\n**Purpose:** createVendorFilter() excludes files within third-party dependency directories via single-segment matching and path-pa...\n\n**createVendorFilter() excludes files within third-party dependency directories via single-segment matching and path-pattern substring containment, supporting both flat directory names (e.g., `node_modules`) and hierarchical patterns (e.g., `.agents/skills`).**\n\n## Exported Symbols\n\n**DEFAULT_VENDOR_DIRS**: `readonly string[]` — Array of 10 default vendor directory names to exclude: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**createVendorFilter(vendorDirs: string[]): FileFilter** — Constructs a `FileFilter` instance with `shouldExclude()` method implementing dual-pattern matching logic.\n\n## Matching Algorithms\n\n**createVendorFilter()** partitions input `vendorDirs` into two collections:\n\n1. **Single segments** (no path separators): Stored in `Set<string>` for O(1) lookup. Path split via `path.sep` yields segment array; filter returns `true` if any segment exists in set.\n2. **Path patterns** (containing `path.sep`): Stored in `string[]`. Filter returns `true` if `absolutePath.includes(pattern)` for any pattern.\n\nPath normalization via `dir.replace(/[\\\\/]/g, path.sep)` ensures cross-platform separator handling before classification.\n\n## Filter Interface\n\nReturned `FileFilter` object contains:\n- **name**: `'vendor'` — Filter identifier for debugging/logging.\n- **shouldExclude(absolutePath: string): boolean** — Returns `true` if path matches either single-segment or path-pattern rules.\n\n## Integration Points\n\nImports `FileFilter` from `../types.js`. Consumed by filter chain in `src/discovery/filters/index.ts` alongside `createBinaryFilter()`, `createGitignoreFilter()`, `createCustomFilter()`. Applied during `walk()` in `src/discovery/walker.ts` to exclude vendor directories from file discovery results.\n\n## Pattern Examples\n\nSingle-segment match: `/project/node_modules/lodash/index.js` → splits to `['', 'project', 'node_modules', 'lodash', 'index.js']` → `'node_modules'` in `singleSegments` → returns `true`.\n\nPath-pattern match: `/project/apps/foo/.agents/skills/bar.md` → `includes('.agents/skills')` → returns `true` (assuming `.agents/skills` in `pathPatterns` after normalization).\n\nNo match: `/project/src/utils.js` → no segment in `singleSegments`, no pattern in `pathPatterns` → returns `false`.\n\n## Import Map (verified — use these exact paths)\n\nbinary.ts:\n  ../types.js → FileFilter (type)\n\ncustom.ts:\n  ../types.js → FileFilter (type)\n\ngitignore.ts:\n  ../types.js → FileFilter (type)\n\nindex.ts:\n  ../types.js → FileFilter, FilterResult, ExcludedFile (type)\n  ../../orchestration/trace.js → ITraceWriter (type)\n\nvendor.ts:\n  ../types.js → FileFilter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters for discovery pipeline: gitignore pattern matching, binary file detection, vendor directory exclusion, and custom glob rules with short-circuit evaluation and bounded-concurrency application.\n\n## Contents\n\n### Filter Orchestration\n\n**[index.ts](./index.ts)** — `applyFilters(files, filters, options)` executes filter chain with short-circuit evaluation (stops at first exclusion) using 30-worker bounded concurrency pool to prevent file descriptor exhaustion during `isBinaryFile()` I/O operations. Returns `FilterResult` with `included[]` and `excluded[]` arrays, emits `filter:applied` trace events with per-filter `filesMatched`/`filesRejected` counts. Re-exports all filter factory functions.\n\n### Filter Implementations\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter(root)` async factory reads `.gitignore`, parses via `ignore` library, returns `FileFilter` that converts absolute paths to relative before calling `ig.ignores()`. Returns `false` for paths outside root tree (relative path starts with `..`).\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter(options?)` constructs fast-path extension lookup in `BINARY_EXTENSIONS` set (96 extensions: `.png`, `.exe`, `.mp4`, `.pdf`, `.woff`, `.class`, `.db`, etc.) with fallback to `isBinaryFile()` content analysis. Excludes files exceeding `maxFileSize` (default 1MB via `DEFAULT_MAX_FILE_SIZE`). Returns `true` on `fs.stat()` errors.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter(vendorDirs)` partitions input into single-segment Set (e.g., `node_modules`) for O(1) membership testing and path-pattern array (e.g., `.agents/skills`) for substring matching. `DEFAULT_VENDOR_DIRS` contains `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter(patterns, root)` parses gitignore-style glob patterns via `ignore` library, converts absolute paths to relative before evaluation. Returns `false` immediately for empty patterns array or paths outside root.\n\n## Concurrency Strategy\n\n`applyFilters()` uses iterator-based worker pool with `CONCURRENCY = 30` to share single `files.entries()` iterator across workers. Each file runs through `filters[]` sequentially until `shouldExclude()` returns `true` (short-circuit). BinaryFilter calls synchronous `isBinaryFile()`, making concurrency bounds critical to avoid ulimit violations.\n\n## Filter Contract\n\nAll factories return `FileFilter` interface from `../types.js`:\n- `name: string` — filter identifier for logging\n- `shouldExclude(absolutePath: string): boolean` — exclusion predicate\n\n## Behavioral Contracts\n\n### Binary Extension Set\n`BINARY_EXTENSIONS` (96 entries): `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`, `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`, `.db`, `.sqlite`, `.sqlite3`, `.mdb`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS` (10 entries): `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Path Normalization\n- Gitignore filter: `path.relative(normalizedRoot, absolutePath)` before `ig.ignores()`\n- Vendor filter: `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling\n- Binary filter: `path.extname(absolutePath).toLowerCase()` for extension extraction\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1229,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 13668,
      "latencyMs": 30457,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.103Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/backends\" (backends)\n\n## File Summaries (3 files)\n\n### claude.ts\n**Purpose:** ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), arg...\n\n**ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), argument construction with bypass permissions, and JSON response parsing with Zod schema validation.**\n\n## Exported Symbols\n\n**isCommandOnPath(command: string): Promise<boolean>** — Splits `process.env.PATH` by platform delimiter, checks each directory for file matching command name, on Windows iterates `process.env.PATHEXT` extensions (`.exe`, `.cmd`, `.bat`), uses `fs.stat()` instead of `fs.access()` for cross-platform compatibility without Unix execute bits.\n\n**ClaudeBackend** — Implements `AIBackend` interface with properties `name: 'claude'`, `cliCommand: 'claude'`, methods `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()`.\n\n## ClaudeBackend Methods\n\n**isAvailable(): Promise<boolean>** — Delegates to `isCommandOnPath(this.cliCommand)` to verify `claude` binary exists on system PATH.\n\n**buildArgs(options: AICallOptions): string[]** — Constructs argument array `['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']`, appends `--model` if `options.model` present, appends `--system-prompt` if `options.systemPrompt` present, appends `--max-turns` converted to string if `options.maxTurns` defined, does NOT include prompt text (goes to stdin via `runSubprocess()`).\n\n**parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse** — Finds first `{` character via `stdout.indexOf('{')` to handle non-JSON prefix text (upgrade notices per RESEARCH.md Pitfall 4), slices stdout from `jsonStart`, parses JSON and validates against `ClaudeResponseSchema`, extracts model name from first key of `parsed.modelUsage` object (fallback `'unknown'`), returns normalized `AIResponse` with `text: parsed.result`, `inputTokens: parsed.usage.input_tokens`, `outputTokens: parsed.usage.output_tokens`, `cacheReadTokens: parsed.usage.cache_read_input_tokens`, `cacheCreationTokens: parsed.usage.cache_creation_input_tokens`, `durationMs`, `exitCode`, `raw: parsed`, throws `AIServiceError` with code `PARSE_ERROR` if JSON object missing or schema validation fails.\n\n**getInstallInstructions(): string** — Returns multiline string with npm install command `npm install -g @anthropic-ai/claude-code` and URL `https://code.claude.com`.\n\n## JSON Schema Validation\n\n**ClaudeResponseSchema** — Zod schema validated against Claude CLI v2.1.31 JSON output format, defines object with `type: z.literal('result')`, `subtype: z.enum(['success', 'error'])`, `is_error: z.boolean()`, `duration_ms: z.number()`, `duration_api_ms: z.number()`, `num_turns: z.number()`, `result: z.string()`, `session_id: z.string()`, `total_cost_usd: z.number()`, nested `usage` object containing `input_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens`, `output_tokens` as numbers, `modelUsage: z.record(z.object({...}))` mapping model names to usage statistics with `inputTokens`, `outputTokens`, `cacheReadInputTokens`, `cacheCreationInputTokens`, `costUSD` fields.\n\n## CLI Argument Patterns\n\n**Permission Mode** — `--permission-mode bypassPermissions` flag enables non-interactive execution by skipping permission prompts (documented in PITFALLS.md §8), required for subprocess operation without TTY.\n\n**Session Persistence** — `--no-session-persistence` flag prevents writing session state to disk, appropriate for stateless documentation generation tasks.\n\n**Output Format** — `--output-format json` flag triggers structured JSON output instead of conversational text, enabling machine parsing with schema validation.\n\n## Integration Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js`, imports `AIServiceError` from `../types.js`, uses `runSubprocess()` from subprocess module (referenced in JSDoc but not imported here), follows backend adapter pattern where prompt goes to stdin and CLI arguments configure behavior.\n### gemini.ts\n**Purpose:** GeminiBackend implements AIBackend interface as stub demonstrating extension pattern for Gemini CLI integration, curr...\n\n**GeminiBackend implements AIBackend interface as stub demonstrating extension pattern for Gemini CLI integration, currently throwing \"not implemented\" errors pending stable JSON output format.**\n\n## Exports\n\n**GeminiBackend** class implementing AIBackend interface with properties:\n- `name: 'gemini'` — Backend identifier string\n- `cliCommand: 'gemini'` — CLI executable name\n\n## Public Interface\n\n**isAvailable(): Promise<boolean>** checks if `gemini` CLI exists on PATH via `isCommandOnPath(this.cliCommand)` imported from `claude.ts`.\n\n**buildArgs(_options: AICallOptions): string[]** returns CLI argument array `['-p', '--output-format', 'json']` based on documented Gemini CLI flags from RESEARCH.md. Prompt delivered via stdin through subprocess wrapper.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'` Always fails—parser not implemented.\n\n**getInstallInstructions(): string** returns multiline installation guidance:\n```\nGemini CLI (experimental):\n  npm install -g @anthropic-ai/gemini-cli\n  https://github.com/google-gemini/gemini-cli\n```\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` from `../types.js`. Imports `isCommandOnPath` utility from `./claude.js` for CLI detection.\n\n## Implementation Status\n\nStub backend demonstrating AIBackend extension contract. Full implementation deferred until Gemini CLI JSON output stabilizes (RESEARCH.md Open Question 2). Detection and argument building functional, response parsing unimplemented.\n### opencode.ts\n**Purpose:** OpenCodeBackend implements the AIBackend interface as a stub demonstrating backend extension patterns, detecting CLI ...\n\n**OpenCodeBackend implements the AIBackend interface as a stub demonstrating backend extension patterns, detecting CLI availability and building argument arrays but throwing AIServiceError on parseResponse() until JSONL output parsing is implemented.**\n\n## Public Interface\n\n**OpenCodeBackend** class implements `AIBackend` with properties:\n- `name: 'opencode'` — Backend identifier constant\n- `cliCommand: 'opencode'` — Executable name for PATH detection\n\n**isAvailable(): Promise<boolean>** — Delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.js` to check if `opencode` binary exists on PATH.\n\n**buildArgs(_options: AICallOptions): string[]** — Returns `['run', '--format', 'json']` argument array for OpenCode CLI invocation. Prompt input delivered via stdin by subprocess wrapper in `src/ai/subprocess.ts`.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** — Unconditionally throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'OpenCode backend is not yet implemented. Use Claude backend.'` to prevent runtime execution until JSONL parsing implemented.\n\n**getInstallInstructions(): string** — Returns multiline installation guide:\n```\nOpenCode (experimental):\n  curl -fsSL https://opencode.ai/install | bash\n  https://opencode.ai\n```\n\n## Integration Points\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js` for interface compliance and `AIServiceError` for error construction. Imports `isCommandOnPath` from `./claude.js` for PATH detection reuse.\n\nBackend registered in `src/ai/registry.ts` alongside ClaudeBackend and GeminiBackend for auto-detection via `detectFirstAvailableBackend()`.\n\n## Implementation Status\n\nStub implementation deferred per RESEARCH.md Open Question 3 pending JSONL output format specification. Current `parseResponse()` behavior blocks production use while allowing compilation and registration scaffolding to remain in place.\n\n## Import Map (verified — use these exact paths)\n\nclaude.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\ngemini.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\nopencode.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\nBackend adapters implementing AIBackend interface for three AI CLI tools: ClaudeBackend with full JSON parsing via Zod schema validation, GeminiBackend/OpenCodeBackend as stub implementations throwing AIServiceError until stable output formats arrive.\n\n## Contents\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection splitting process.env.PATH by platform delimiter, buildArgs() constructing ['--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions'] with optional --model/--system-prompt/--max-turns, parseResponse() slicing stdout from first '{' character to strip upgrade notices then validating against ClaudeResponseSchema extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens and model name from modelUsage object keys, getInstallInstructions() returning npm command for @anthropic-ai/claude-code.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub with isAvailable() delegating to isCommandOnPath('gemini'), buildArgs() returning ['-p', '--output-format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning npm command for @anthropic-ai/gemini-cli with GitHub URL (implementation deferred per RESEARCH.md Open Question 2 pending stable JSON format).\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub with isAvailable() delegating to isCommandOnPath('opencode'), buildArgs() returning ['run', '--format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning curl install command for https://opencode.ai (implementation deferred per RESEARCH.md Open Question 3 pending JSONL parsing).\n\n## Architecture\n\n**Backend Adapter Contract**\n\nAIBackend interface defines five methods: isAvailable() checking CLI binary presence on PATH, buildArgs(options) constructing subprocess argument arrays with prompt delivered via stdin, parseResponse(stdout, durationMs, exitCode) extracting AIResponse with normalized token counts, getInstallInstructions() returning user-facing setup guidance, name/cliCommand properties identifying backend.\n\n**PATH Detection Strategy**\n\nisCommandOnPath() splits process.env.PATH by path.delimiter (';' on Windows, ':' elsewhere), iterates directories calling fs.stat() on potential executable paths, on Windows iterates process.env.PATHEXT extensions ['.exe', '.cmd', '.bat', '.com'] to match platform conventions, returns true on first match. Function exported from claude.ts and reused by gemini.ts/opencode.ts for cross-backend consistency.\n\n**CLI Argument Patterns**\n\nAll backends use stdin for prompt delivery (not CLI arguments) to avoid shell escaping issues. ClaudeBackend appends --model/--system-prompt/--max-turns conditionally via buildArgs() inspecting AICallOptions properties. --permission-mode bypassPermissions enables non-interactive subprocess execution per PITFALLS.md §8. --no-session-persistence prevents state file writes.\n\n**JSON Response Parsing**\n\nClaudeBackend.parseResponse() handles stdout prefix content (upgrade notices, warnings) by finding first '{' via indexOf() before JSON.parse(). ClaudeResponseSchema validates against v2.1.31 output format with type: 'result', usage object containing input_tokens/cache_creation_input_tokens/cache_read_input_tokens/output_tokens, modelUsage record mapping model names to detailed statistics. Throws AIServiceError with code PARSE_ERROR on validation failure or missing JSON.\n\n**Stub Backend Pattern**\n\nGeminiBackend and OpenCodeBackend implement full interface surface but throw AIServiceError from parseResponse() to prevent runtime usage until output format research completes. Enables backend registration in src/ai/registry.ts and auto-detection via detectFirstAvailableBackend() while blocking production execution.\n\n## Integration Dependencies\n\nImports AIBackend/AICallOptions/AIResponse/AIServiceError from ../types.js. ClaudeBackend requires zod for ClaudeResponseSchema validation. Consumed by AIService in src/ai/service.ts via BackendRegistry.getBackend(name) lookup. Backend selection happens via config.ai.backend or auto-detection iterating ['claude', 'gemini', 'opencode'] until isAvailable() returns true.\n\n## Behavioral Contracts\n\n**ClaudeBackend Argument Construction**\n```javascript\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n// Appends conditionally:\n['--model', options.model] if options.model present\n['--system-prompt', options.systemPrompt] if options.systemPrompt present\n['--max-turns', String(options.maxTurns)] if options.maxTurns defined\n```\n\n**ClaudeResponseSchema Token Fields**\n```typescript\nusage: z.object({\n  input_tokens: z.number(),\n  cache_creation_input_tokens: z.number(),\n  cache_read_input_tokens: z.number(),\n  output_tokens: z.number()\n})\n```\n\n**GeminiBackend Arguments** — `['-p', '--output-format', 'json']`\n\n**OpenCodeBackend Arguments** — `['run', '--format', 'json']`\n\n**Stub Error Message Pattern** — `'<Backend> backend is not yet implemented. Use Claude backend.'` with AIServiceError code `'SUBPROCESS_ERROR'`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1344,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12543,
      "latencyMs": 36248,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.121Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/prompts\" (prompts)\n\n## File Summaries (4 files)\n\n### builder.ts\n**Purpose:** builder.ts constructs AI prompt pairs (system/user) for the three-phase documentation pipeline: file analysis with im...\n\n**builder.ts constructs AI prompt pairs (system/user) for the three-phase documentation pipeline: file analysis with import maps and context files, directory-level AGENTS.md aggregation with child summaries and manifests, and root CLAUDE.md synthesis from all AGENTS.md corpus.**\n\n## Exported Functions\n\n### buildFilePrompt\n\n```typescript\nbuildFilePrompt(context: PromptContext, debug = false): { system: string; user: string }\n```\n\nConstructs file-level analysis prompts by injecting `context.filePath`, `context.content`, detected language (via `detectLanguage()`), and optional `context.projectPlan` into `FILE_USER_PROMPT` template. Appends `context.contextFiles[]` as fenced code blocks if provided. For incremental updates, appends `context.existingSum` and returns `FILE_UPDATE_SYSTEM_PROMPT` instead of `FILE_SYSTEM_PROMPT`.\n\n### buildDirectoryPrompt\n\n```typescript\nasync buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }>\n```\n\nAggregates directory context for AGENTS.md generation via parallel reads:\n- Reads all `.sum` files in `dirPath` via `getSumPath()` + `readSumFile()`, formats as `### filename\\n**Purpose:** ...\\n\\nsummary`\n- Reads all child `AGENTS.md` files from subdirectories (filtered by `knownDirs` set if provided)\n- Detects user-authored documentation: reads `AGENTS.local.md` or non-generated `AGENTS.md` (checked via `GENERATED_MARKER` absence)\n- Detects manifest files via hardcoded array: `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n- Extracts actual import statements via `extractDirectoryImports()` for source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/`, formats via `formatImportMap()`\n- Scans for `.annex.md` files and lists them in \"Annex Files\" section\n- Injects optional `projectStructure` (full file tree) and `existingAgentsMd` for incremental updates\n- Returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided, else `DIRECTORY_SYSTEM_PROMPT`\n\n### buildRootPrompt\n\n```typescript\nasync buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }>\n```\n\nSynthesizes project-wide CLAUDE.md by:\n- Collecting all `AGENTS.md` files via `collectAgentsDocs(projectRoot)`, formatting as `### relativePath\\n\\ncontent`\n- Reading root `package.json` and extracting metadata: `name`, `version`, `description`, `packageManager`, `scripts` (formatted as bullet list with inline code)\n- Embedding synthesis constraints: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs...\"\n- Specifying output requirements: architecture overview, key directories table, getting started, key technologies\n- Returns `ROOT_SYSTEM_PROMPT` with no update variant (root always regenerated fully)\n\n### detectLanguage\n\n```typescript\ndetectLanguage(filePath: string): string\n```\n\nMaps file extensions to syntax highlighting language identifiers via lookup table with 22 entries (`.ts` → `\"typescript\"`, `.py` → `\"python\"`, `.md` → `\"markdown\"`, etc.), defaulting to `\"text\"` for unknown extensions.\n\n### logTemplate\n\n```typescript\nlogTemplate(debug: boolean, action: string, filePath: string, extra?: string): void\n```\n\nEmits debug logs with picocolors formatting: `[prompt] action → relativePath extra` when `debug` is true. Used by all builder functions to trace prompt construction.\n\n## Template Dependencies\n\nImports six constant templates from `./templates.js`:\n- `FILE_SYSTEM_PROMPT` — system prompt for new file analysis\n- `FILE_USER_PROMPT` — user prompt template with placeholders: `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}`\n- `FILE_UPDATE_SYSTEM_PROMPT` — system prompt for incremental file updates\n- `DIRECTORY_SYSTEM_PROMPT` — system prompt for new directory aggregation\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — system prompt for incremental directory updates\n- `ROOT_SYSTEM_PROMPT` — system prompt for root synthesis (no update variant)\n\n## Context Enrichment Strategies\n\n**File-level:**\n- Language detection for fenced code block syntax\n- Full project structure tree via `context.projectPlan` (optional)\n- Related files via `context.contextFiles[]` with auto-detected language per file\n- Existing summary for diff-based updates\n\n**Directory-level:**\n- Child `.sum` file summaries with purpose frontmatter\n- Subdirectory `AGENTS.md` recursion (one level deep)\n- Import maps with verified path constraints via static analysis\n- Manifest detection for package root hints\n- Annex file listing for reproduction-critical constants\n- User-authored documentation preservation (`AGENTS.local.md` or pre-existing `AGENTS.md`)\n- Full project structure tree (optional)\n\n**Root-level:**\n- Complete `AGENTS.md` corpus via `collectAgentsDocs()`\n- Root `package.json` metadata with script listings\n- Explicit anti-invention constraints in prompt text\n\n## Integration Points\n\n- `readSumFile()` from `../writers/sum.js` — parses YAML frontmatter + summary content\n- `getSumPath()` from `../writers/sum.js` — computes `.sum` file path from source path\n- `GENERATED_MARKER` from `../writers/agents-md.js` — detects ARE-generated vs. user-authored AGENTS.md\n- `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` — static import analysis\n- `collectAgentsDocs()` from `../collector.js` — recursive AGENTS.md tree traversal\n\n## Placeholder Syntax\n\nUser prompts use mustache-style placeholders replaced via `String.replace()` with regex flags `/g`:\n- `{{FILE_PATH}}` — source file relative path\n- `{{CONTENT}}` — file content (unescaped)\n- `{{LANG}}` — detected language identifier\n- `{{PROJECT_PLAN_SECTION}}` — project structure tree or empty string\n\n## Debug Logging Pattern\n\nAll builder functions accept optional `debug` boolean (default `false`). When enabled, calls `logTemplate()` with:\n- `buildFilePrompt`: logs `lang=<detected>`\n- `buildDirectoryPrompt`: logs `files=<count> subdirs=<count> imports=<count>`\n- `buildRootPrompt`: logs `agents=<count>`\n\nLogs written to stderr via `console.error()` with picocolors formatting: `pc.dim('[prompt]')`, `pc.cyan(action)`, `pc.dim('→')`.\n### index.ts\n**Purpose:** Barrel export module re-exporting prompt construction functions (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRoo...\n\n**Barrel export module re-exporting prompt construction functions (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage`) and shared prompt types (`PromptContext`, `SUMMARY_GUIDELINES`) from internal `builder.ts` and `types.ts` modules.**\n\n## Exported Symbols\n\n- **`PromptContext`** (type): Re-exported from `./types.js` — context object passed to prompt builders containing file metadata, import maps, and configuration\n- **`SUMMARY_GUIDELINES`** (constant): Re-exported from `./types.js` — string constant defining documentation density rules, identifier preservation requirements, and behavioral contract specifications\n- **`buildFilePrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 1 prompt for AI-driven file analysis, embedding source content and import relationships\n- **`buildDirectoryPrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 2 prompt for `AGENTS.md` generation from aggregated `.sum` files and child directory docs\n- **`buildRootPrompt(context: PromptContext): string`**: Re-exported from `./builder.js` — constructs Phase 3 prompt for root integration document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`)\n- **`detectLanguage(filePath: string): string`**: Re-exported from `./builder.js` — infers programming language from file extension for syntax-aware prompt customization\n\n## Module Role\n\nCentralizes public API for prompt construction subsystem. Internal implementation split across:\n- `builder.ts`: Implements prompt assembly logic with template interpolation\n- `types.ts`: Defines `PromptContext` interface and `SUMMARY_GUIDELINES` constant (likely contains reproduction-critical prompt text per annex pattern)\n- `templates.ts`: Likely contains multi-line prompt templates (annex candidate based on project structure showing `.annex.md` siblings)\n\nConsumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` to construct AI service call prompts for three-phase pipeline.\n### templates.ts\n**Purpose:** Exports six string constants defining system and user prompts for ARE's three-phase documentation pipeline (file anal...\n\n**Exports six string constants defining system and user prompts for ARE's three-phase documentation pipeline (file analysis, directory aggregation, root synthesis) with density/anchor-preservation rules, incremental update strategies, and behavioral contract reproduction requirements.**\n\n## Exported Constants\n\n- `FILE_SYSTEM_PROMPT` — System prompt for file-level `.sum` generation instructing AI to produce dense, identifier-rich summaries with mandatory behavioral contract preservation (regex patterns verbatim in backticks, format strings, magic constants, environment variables). Defines density rules prohibiting filler phrases (\"this file\", \"provides\", \"responsible for\"), requires every sentence reference specific identifiers, mandates all exported function/class/type/const names appear with exact casing. Specifies annex overflow mechanism for large string constants: write concise summary listing constant names, append `## Annex References` section identifying reproduction-critical constants by name and line count.\n\n- `FILE_USER_PROMPT` — User prompt template for file analysis containing `{{FILE_PATH}}` and `{{CONTENT}}` placeholders. Embeds full project structure tree in `<project-structure>` tags, instructs AI to lead with bold purpose statement `**[FileName] does X.**`, requires minimum sections (purpose + exported symbols with signatures), allows adaptive section selection based on file content.\n\n- `DIRECTORY_SYSTEM_PROMPT` — System prompt for directory-level `AGENTS.md` generation. Mandates first line `<!-- Generated by agents-reverse-engineer -->`, requires `#` heading with directory name, one-paragraph purpose statement. Defines adaptive section strategy (Contents, Subdirectories, Architecture/Data Flow, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants). Enforces path accuracy constraints: use only Import Map paths, exact directory names from Project Directory Structure, no invented/renamed module paths. Requires Behavioral Contracts section when file summaries contain regex/format specs/magic constants (preserve verbatim, do NOT paraphrase). Links to `.annex.md` files for reproduction-critical constants without inlining content. Defines AGENTS.md as \"NAVIGATIONAL INDEX\" focusing on what files do, how files relate, directory patterns.\n\n- `FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates. Instructs AI to preserve structure/headings/phrasing of existing summary where code unchanged, modify only content directly affected by changes, keep unchanged sections VERBATIM without rephrasing. Mandates behavioral contract preservation: regex patterns/format strings/magic constants from existing summary preserved verbatim unless source changed them. Enforces same density/anchor-preservation rules as `FILE_SYSTEM_PROMPT`.\n\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental `AGENTS.md` updates. Instructs AI to preserve structure/descriptions for unaffected content, modify only entries for changed file summaries/subdirectories, add entries for new files, remove entries for deleted files, avoid reorganization unless required by additions/deletions. Mandates first line `<!-- Generated by agents-reverse-engineer -->`, preserves existing purpose statement unless directory role fundamentally changed. Enforces same path accuracy/consistency/density rules as `DIRECTORY_SYSTEM_PROMPT`.\n\n- `ROOT_SYSTEM_PROMPT` — System prompt for root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`). Mandates raw markdown output without conversational text/preamble/meta-commentary. Enforces critical constraint: synthesize ONLY from provided `AGENTS.md` content, do NOT invent/extrapolate/hallucinate features/hooks/APIs/patterns/dependencies not explicitly mentioned, omit missing sections rather than guessing, every claim must be traceable to specific `AGENTS.md` file.\n\n## Behavioral Contracts\n\n**YAML Frontmatter Format:**\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Placeholder Substitution Pattern:**\n- `{{FILE_PATH}}` — Replaced with source file path in `FILE_USER_PROMPT`\n- `{{CONTENT}}` — Replaced with source file content in `FILE_USER_PROMPT`\n\n**Prohibited Filler Phrases:**\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**Output Formatting Constraints:**\n- Start with bold purpose statement: `**Purpose statement here.**`\n- No preamble, thinking, meta-commentary before purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- First line of directory docs must be exactly: `<!-- Generated by agents-reverse-engineer -->`\n\n**Annex Reference Format:**\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n## Integration Points\n\nUsed by `buildFilePrompt()` in `src/generation/prompts/builder.ts` for `.sum` generation (consumes `FILE_SYSTEM_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`). Used by `buildDirectoryPrompt()` for `AGENTS.md` generation (consumes `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`). Used by `buildRootPrompt()` for root document synthesis (consumes `ROOT_SYSTEM_PROMPT`). Template placeholders `{{FILE_PATH}}`, `{{CONTENT}}` replaced by prompt builder string interpolation.\n\n## Annex References\n\n- `FILE_SYSTEM_PROMPT` — System prompt for file-level `.sum` generation (72 lines)\n- `FILE_USER_PROMPT` — User prompt template for file analysis with project structure embedding (109 lines)\n- `DIRECTORY_SYSTEM_PROMPT` — System prompt for directory-level `AGENTS.md` generation (94 lines)\n- `FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates (45 lines)\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental `AGENTS.md` updates (70 lines)\n- `ROOT_SYSTEM_PROMPT` — System prompt for root document synthesis (12 lines)\n### types.ts\n**Purpose:** types.ts defines the PromptContext interface for prompt construction and exports SUMMARY_GUIDELINES constants specify...\n\n**types.ts defines the PromptContext interface for prompt construction and exports SUMMARY_GUIDELINES constants specifying documentation generation rules.**\n\n## Exported Types\n\n**PromptContext** — Interface for prompt builder input with fields:\n- `filePath: string` — Absolute path to the file being analyzed\n- `content: string` — File content to analyze\n- `contextFiles?: Array<{path: string; content: string}>` — Related files for additional context\n- `projectPlan?: string` — Project structure listing for bird's-eye context\n- `existingSSum?: string` — Existing .sum summary text for incremental updates\n\n## Exported Constants\n\n**SUMMARY_GUIDELINES** — Const object defining documentation generation constraints:\n- `targetLength: {min: 300, max: 500}` — Target word count range\n- `include: string[]` — Required content categories (8 items):\n  - Purpose and responsibility\n  - Public interface (exports, key functions)\n  - Key patterns and notable algorithms\n  - Dependencies with usage context\n  - Key function signatures as code snippets\n  - Tightly coupled sibling files\n  - Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables\n  - Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section\n- `exclude: string[]` — Forbidden content categories (3 items):\n  - Control flow minutiae (loop structures, variable naming, temporary state)\n  - Generic TODOs/FIXMEs (keep only security/breaking)\n  - Broad architectural relationships (handled by AGENTS.md)\n\n## Usage Context\n\nPromptContext consumed by `buildPrompt()` in `src/generation/prompts/builder.ts` to construct AI prompts for file analysis. SUMMARY_GUIDELINES referenced in prompt templates (`FILE_SYSTEM_PROMPT` in `src/generation/prompts/templates.ts`) to enforce documentation standards across Phase 1 file analysis and Phase 2 directory aggregation.\n\n## Import Map (verified — use these exact paths)\n\nbuilder.ts:\n  ../writers/sum.js → readSumFile, getSumPath\n  ../writers/agents-md.js → GENERATED_MARKER\n  ../../imports/index.js → extractDirectoryImports, formatImportMap\n  ../collector.js → collectAgentsDocs\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Annex Files (reproduction-critical constants)\n\n- templates.ts.annex.md\n- types.ts.annex.md",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# prompts\n\nExports prompt construction pipeline for ARE's three-phase documentation generation: `buildFilePrompt()` constructs Phase 1 file analysis prompts with import maps and project structure trees, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` for Phase 2 directory synthesis, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus in Phase 3.\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements prompt builders via template interpolation: `buildFilePrompt()` injects `filePath`/`content`/detected language (via `detectLanguage()`) into `FILE_USER_PROMPT`, appending `contextFiles[]` and optional `projectPlan`. `buildDirectoryPrompt()` aggregates `.sum` files via `readSumFile()`, child `AGENTS.md` from subdirectories, import maps via `extractDirectoryImports()`, and manifest files (9 types: package.json, Cargo.toml, go.mod, etc.), returning `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided. `buildRootPrompt()` collects all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json` metadata, embeds synthesis constraints prohibiting invented features. Returns `{system, user}` prompt pairs with incremental update system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when existing content provided.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` for AI service call construction.\n\n### [templates.ts](./templates.ts)\nExports six prompt constants: `FILE_SYSTEM_PROMPT` (density rules, identifier preservation, behavioral contract verbatim reproduction), `FILE_USER_PROMPT` (contains `{{FILE_PATH}}`, `{{CONTENT}}` placeholders), `DIRECTORY_SYSTEM_PROMPT` (adaptive section strategy, path accuracy constraints, annex linking), `FILE_UPDATE_SYSTEM_PROMPT` (preserve unchanged sections verbatim), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (modify only affected entries), `ROOT_SYSTEM_PROMPT` (synthesis-only constraint: no invented features). Defines prohibited filler phrases (\"this file\", \"provides\", \"responsible for\"), annex reference format, YAML frontmatter structure for `.sum` files.\n\n### [types.ts](./types.ts)\nDefines `PromptContext` interface (fields: `filePath`, `content`, `contextFiles?`, `projectPlan?`, `existingSum?`) and `SUMMARY_GUIDELINES` constant specifying target word count (300-500), 8 required content categories (purpose, public interface, patterns, dependencies, signatures, related files, behavioral contracts, annex references), 3 excluded categories (control flow, generic TODOs, broad architecture).\n\n## Behavioral Contracts\n\n**Mustache-Style Placeholder Substitution** (from templates.ts):\n```\n{{FILE_PATH}}  — Replaced with source file relative path\n{{CONTENT}}    — Replaced with source file content (unescaped)\n{{LANG}}       — Replaced with detected language identifier\n{{PROJECT_PLAN_SECTION}} — Replaced with project structure tree or empty string\n```\n\n**Prohibited Filler Phrases** (from templates.ts):\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**YAML Frontmatter Format** (from templates.ts):\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Language Detection Map** (from builder.ts):\n```\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n**Annex Reference Format** (from templates.ts):\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n**Manifest Detection Array** (from builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n## Annex References\n\n- Full prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [types.ts.annex.md](./types.ts.annex.md)\n\n## File Relationships\n\nbuilder.ts depends on templates.ts constants (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, etc.) for interpolation, calls `detectLanguage()` for syntax highlighting identifiers, reads `.sum` files via `readSumFile()` (from `../writers/sum.js`), checks `GENERATED_MARKER` (from `../writers/agents-md.js`) to distinguish user-authored `AGENTS.md`, extracts imports via `extractDirectoryImports()` + `formatImportMap()` (from `../../imports/index.js`), collects root corpus via `collectAgentsDocs()` (from `../collector.js`). types.ts defines `PromptContext` interface consumed by all builder functions. index.ts re-exports public API consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1559,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 14916,
      "latencyMs": 36438,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:03.110Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/writers\" (writers)\n\n## File Summaries (3 files)\n\n### agents-md.ts\n**Purpose:** writeAgentsMd orchestrates preservation of user-authored AGENTS.md files by renaming them to AGENTS.local.md and prep...\n\n**writeAgentsMd orchestrates preservation of user-authored AGENTS.md files by renaming them to AGENTS.local.md and prepending their content above LLM-generated documentation with marker-based identification.**\n\n## Exported Functions\n\n**writeAgentsMd(dirPath: string, _projectRoot: string, content: string): Promise<string>**\nWrites AGENTS.md to dirPath with four-step preservation workflow: (1) detects existing non-generated AGENTS.md via absence of GENERATED_MARKER and renames to AGENTS.local.md, (2) reads AGENTS.local.md from previous runs if no rename occurred, (3) strips GENERATED_MARKER prefix from incoming LLM content, (4) assembles final content as `GENERATED_MARKER` + optional user content block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter + horizontal rule + LLM content. Returns written file path.\n\n**isGeneratedAgentsMd(filePath: string): Promise<boolean>**\nReturns true if file at filePath contains GENERATED_MARKER substring via readFile, false on read errors (treats missing files as non-generated).\n\n## Marker Constant\n\n**GENERATED_MARKER**\nString constant `'<!-- Generated by agents-reverse-engineer -->'` used for detection of tool-generated AGENTS.md files versus user-authored ones. Referenced by isGeneratedAgentsMd for substring matching and writeAgentsMd for marker injection/stripping.\n\n## User Content Preservation Strategy\n\nwriteAgentsMd implements two-path detection: (1) rename existing AGENTS.md to AGENTS.local.md if non-generated, (2) read AGENTS.local.md if already exists from prior runs. User content prepended with comment header `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` followed by content, horizontal rule separator `---`, then LLM content. Ensures AI assistants see user-defined context before generated summaries.\n\n## Content Assembly Logic\n\nLLM content undergoes marker stripping: if content.startsWith(GENERATED_MARKER), slices marker length and removes leading newlines via `/^\\n+/` regex. Final assembly uses parts array joined with `\\n`: marker, empty line, optional user block (comment + content + rule), LLM content. Directory creation via mkdir with `recursive: true` ensures parent paths exist before writeFile.\n\n## Error Handling\n\nBoth readFile operations (existing AGENTS.md, AGENTS.local.md) wrapped in try-catch blocks that silently continue on errors, treating absence as null userContent. No explicit error propagation for missing files—only writeFile failures bubble up to caller.\n### index.ts\n**Purpose:** Barrel module exporting file-level documentation writers for `.sum` files and directory-level `AGENTS.md` files.\n\n**Barrel module exporting file-level documentation writers for `.sum` files and directory-level `AGENTS.md` files.**\n\n## Exported Symbols\n\n### From `sum.js`\n\n- **`writeSumFile`** — writes `.sum` file with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files` fields plus markdown summary content\n- **`readSumFile`** — parses existing `.sum` file, extracts YAML frontmatter and markdown content, returns `SumFileContent` object\n- **`getSumPath`** — computes `.sum` file path from source file path by appending `.sum` extension\n- **`sumFileExists`** — checks existence of `.sum` file for given source path via `existsSync()`\n- **`SumFileContent`** (type) — structured representation of `.sum` file with parsed frontmatter fields and summary text\n\n### From `agents-md.js`\n\n- **`writeAgentsMd`** — generates directory-level `AGENTS.md` by aggregating child `.sum` files and subdirectory `AGENTS.md` content, preserves user-authored `AGENTS.local.md` content above generated sections, writes output with `<!-- Generated by agents-reverse-engineer -->` marker\n\n## Integration Role\n\nProvides unified import point for Phase 1 (file analysis → `.sum` output) and Phase 2 (directory aggregation → `AGENTS.md` output) writers consumed by `src/generation/executor.ts` during three-phase pipeline execution. Separates low-level file I/O and YAML serialization logic from orchestration concerns.\n### sum.ts\n**Purpose:** sum.ts implements YAML frontmatter-based `.sum` file I/O for storing AI-generated file summaries with SHA-256 content...\n\n**sum.ts implements YAML frontmatter-based `.sum` file I/O for storing AI-generated file summaries with SHA-256 content hashes and structured metadata.**\n\n## Exported Types\n\n**SumFileContent** — structured content for a `.sum` file with fields:\n- `summary: string` — main summary text (detailed description)\n- `metadata: SummaryMetadata` — extracted metadata (imported from `../types.js`)\n- `generatedAt: string` — generation timestamp (ISO 8601 format)\n- `contentHash: string` — SHA-256 hash of source file content for change detection\n\n## Core File Operations\n\n**writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>** — writes `.sum` file alongside source file (e.g., `foo.ts` → `foo.ts.sum`), creates parent directory via `mkdir({ recursive: true })`, formats content via `formatSumFile()`, returns written `.sum` path.\n\n**readSumFile(sumPath: string): Promise<SumFileContent | null>** — parses `.sum` file into structured content via `parseSumFile()`, returns `null` if file doesn't exist or parsing fails.\n\n**getSumPath(sourcePath: string): string** — returns `.sum` path by appending `.sum` to source path.\n\n**sumFileExists(sourcePath: string): Promise<boolean>** — checks if `.sum` file exists by calling `readSumFile()` and testing for non-null result.\n\n## Annex File Operations\n\n**writeAnnexFile(sourcePath: string, sourceContent: string): Promise<string>** — writes `.annex.md` file alongside source file (e.g., `foo.ts` → `foo.ts.annex.md`) containing full source content for reproduction-critical files whose verbatim constants cannot fit within `.sum` word limits, wraps source in triple-backtick code fence with markdown header using `GENERATED_MARKER` from `agents-md.js`, returns written annex path.\n\n**getAnnexPath(sourcePath: string): string** — returns `.annex.md` path by appending `.annex.md` to source path.\n\n## YAML Frontmatter Format\n\n**formatSumFile(content: SumFileContent): string** — serializes `SumFileContent` to YAML frontmatter format:\n```\n---\ngenerated_at: <ISO 8601 timestamp>\ncontent_hash: <SHA-256 hex>\npurpose: <single line string>\ncritical_todos: [<array>] or multi-line YAML\nrelated_files: [<array>] or multi-line YAML\n---\n\n<summary text>\n```\n\n**parseSumFile(content: string): SumFileContent | null** — extracts frontmatter via regex `/^---\\n([\\s\\S]*?)\\n---\\n/`, parses fields using line-based regex patterns (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), delegates array parsing to `parseYamlArray()`, returns `null` on parse failure.\n\n## YAML Array Parsing\n\n**parseYamlArray(frontmatter: string, key: string): string[]** — parses YAML array fields from frontmatter supporting two formats:\n1. Inline: `key: [a, b, c]` via regex `/key:\\s*\\[([^\\]]*)\\]/`\n2. Multi-line: `key:\\n  - item1\\n  - item2` via regex `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\nStrips quotes from inline values via `.replace(/^[\"']|[\"']$/g, '')`, trims whitespace, filters empty strings.\n\n**formatYamlArray(key: string, values: string[]): string** — serializes array to YAML frontmatter using inline format for arrays with ≤3 items where all items are <40 chars, otherwise uses multi-line format with `  - ` prefix per item.\n\n## Integration Points\n\nImports `SummaryMetadata` from `../types.js` (contains `purpose`, optional `criticalTodos`, optional `relatedFiles`).\n\nImports `GENERATED_MARKER` from `./agents-md.js` for consistent generated file headers in annex files.\n\nUses Node.js `fs/promises` for async file operations (`writeFile`, `readFile`, `mkdir`).\n\n## Behavioral Contracts\n\n**Frontmatter regex patterns:**\n- Frontmatter block: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar field: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**File naming conventions:**\n- Summary file: `<sourcePath>.sum`\n- Annex file: `<sourcePath>.annex.md`\n\n**Annex file template:**\n```\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<sourceContent>\n```\n\n```\n\n## Import Map (verified — use these exact paths)\n\nsum.ts:\n  ../types.js → SummaryMetadata (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n**YAML frontmatter-based file I/O layer implementing `.sum` file persistence with SHA-256 hashing, `AGENTS.md` generation with user content preservation, and `.annex.md` verbatim source archival for reproduction-critical artifacts.**\n\n## Contents\n\n### Core Writers\n\n**[sum.ts](./sum.ts)** — SHA-256-tracked `.sum` file I/O with YAML frontmatter serialization via `writeSumFile()`/`readSumFile()`/`formatSumFile()`/`parseSumFile()`, regex-based field extraction (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), dual-format YAML array handling (inline `[a,b,c]` vs multi-line `  - item`) via `parseYamlArray()`/`formatYamlArray()`, `.annex.md` verbatim source archival via `writeAnnexFile()` for reproduction-critical files, path resolution via `getSumPath()`/`getAnnexPath()`.\n\n**[agents-md.ts](./agents-md.ts)** — `AGENTS.md` preservation logic via `writeAgentsMd()` four-step workflow: (1) detects user-authored files lacking `GENERATED_MARKER` and renames to `AGENTS.local.md`, (2) reads preserved `AGENTS.local.md` content, (3) strips marker prefix from LLM content, (4) assembles final output with marker header + user content block (`<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter) + horizontal rule + LLM content. Exports `isGeneratedAgentsMd()` marker detection predicate.\n\n**[index.ts](./index.ts)** — Barrel re-exporting `writeSumFile`/`readSumFile`/`getSumPath`/`sumFileExists`/`SumFileContent` from sum.ts and `writeAgentsMd` from agents-md.ts for unified import in `src/generation/executor.ts`.\n\n## File Naming Conventions\n\n- **Summary files**: `<sourcePath>.sum` (e.g., `foo.ts` → `foo.ts.sum`)\n- **Annex files**: `<sourcePath>.annex.md` (e.g., `foo.ts` → `foo.ts.annex.md`)\n- **Preserved user docs**: `AGENTS.md` → `AGENTS.local.md` (renamed on first generation)\n\n## YAML Frontmatter Structure\n\n`.sum` files use YAML frontmatter block delimited by `---\\n...\\n---\\n`:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex digest)\npurpose: One-line file purpose statement\ncritical_todos: [Security issue, Performance bottleneck]\nrelated_files: [../config/schema.ts, ./index.ts]\n---\n\nMarkdown summary content...\n```\n\n**Field serialization rules** (via `formatYamlArray()`):\n- Arrays with ≤3 items where all items <40 chars: inline format `key: [a, b, c]`\n- Otherwise: multi-line format with `  - ` prefix per item\n\n**Parsing patterns** (via `parseSumFile()`):\n- Frontmatter block extraction: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`\n- Arrays: inline `/key:\\s*\\[([^\\]]*)\\]/`, multi-line `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n## User Content Preservation Strategy\n\n`writeAgentsMd()` implements two-path detection for existing `AGENTS.md` files:\n\n1. **First-time generation**: If existing `AGENTS.md` lacks `GENERATED_MARKER` (via `isGeneratedAgentsMd()` substring search), rename to `AGENTS.local.md` to preserve user content\n2. **Subsequent runs**: Read `AGENTS.local.md` if already exists from prior rename operation\n\nFinal assembly concatenates:\n1. `GENERATED_MARKER` constant: `'<!-- Generated by agents-reverse-engineer -->'`\n2. User content block (if present): `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\\n<content>\\n---`\n3. LLM-generated content (with marker prefix stripped if present)\n\nEnsures AI assistants see user-defined directory context before generated summaries during codebase navigation.\n\n## Annex File Pattern\n\n**Purpose**: Archive verbatim source content for reproduction-critical files (e.g., prompt templates with regex patterns, config schemas with magic constants) whose behavioral contracts cannot fit within `.sum` word limits.\n\n**Generated format** (via `writeAnnexFile()`):\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<triple-backtick code fence with full source content>\n```\n```\n\n**Reference pattern**: `.sum` files include `## Annex References` section linking to annex: `[templates.ts.annex.md](./templates.ts.annex.md)`.\n\n## Integration with Generation Pipeline\n\n**Phase 1 (File Analysis)**:\n- `src/generation/executor.ts` calls `writeSumFile()` after LLM subprocess returns `SumFileContent`\n- `writeSumFile()` creates `.sum` alongside source file via `mkdir({ recursive: true })`\n- Optional `writeAnnexFile()` call for files flagged as reproduction-critical\n\n**Phase 2 (Directory Aggregation)**:\n- `src/generation/executor.ts` reads child `.sum` files via `readSumFile()` for aggregation prompts\n- After LLM generates directory summary, calls `writeAgentsMd()` to assemble final `AGENTS.md`\n- `writeAgentsMd()` preserves any existing `AGENTS.local.md` content above generated sections\n\n**Phase 3 (Root Synthesis)**:\n- Root document generators consume all `AGENTS.md` files via `src/generation/collector.ts`\n- No direct writer interaction (root docs written by `src/integration/generate.ts`)\n\n## Change Detection Integration\n\n`readSumFile()` exposes `contentHash` field parsed from YAML frontmatter, consumed by `src/update/orchestrator.ts` for SHA-256-based incremental update detection:\n\n1. Read current file content hash via `src/change-detection/detector.ts`\n2. Read `.sum` file's stored `contentHash` via `readSumFile()`\n3. Hash mismatch → add to regeneration queue\n4. Hash match → skip (file unchanged)\n\n## Error Handling\n\n- **`readSumFile()`**: Returns `null` on missing file or parse failure (no exceptions thrown)\n- **`sumFileExists()`**: Returns `false` if `readSumFile()` returns `null`\n- **`isGeneratedAgentsMd()`**: Returns `false` on read errors (treats missing files as non-generated)\n- **`writeAgentsMd()`**: Silent try-catch blocks for existing `AGENTS.md` / `AGENTS.local.md` reads, treats absence as `null` user content\n- **`writeSumFile()` / `writeAnnexFile()`**: No error handling—writeFile failures bubble to caller\n\n## Behavioral Contracts\n\n### Regex Patterns (sum.ts)\n\n**Frontmatter extraction**:\n- Block delimiter: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**Value normalization**:\n- Quote stripping: `/^[\"']|[\"']$/g` (removes leading/trailing quotes from inline array items)\n- Leading newline removal: `/^\\n+/` (strips marker prefix from LLM content)\n\n### Constants\n\n**GENERATED_MARKER** (agents-md.ts): `'<!-- Generated by agents-reverse-engineer -->'` — marker for tool-generated `AGENTS.md` detection, referenced by `isGeneratedAgentsMd()` substring search and `writeAgentsMd()` marker injection/stripping.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2139,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12783,
      "latencyMs": 49024,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.166Z",
      "prompt": "Generate AGENTS.md for directory: \".github/workflows\" (workflows)\n\n## File Summaries (1 files)\n\n### publish.yml\n**Purpose:** GitHub Actions workflow automating npm package publication with Sigstore-signed provenance attestation on release eve...\n\n**GitHub Actions workflow automating npm package publication with Sigstore-signed provenance attestation on release events.**\n\n## Workflow Triggers\n\nExecutes on `release[published]` GitHub events and manual `workflow_dispatch` invocation.\n\n## Job Configuration\n\nSingle `publish` job runs on `ubuntu-latest` with permissions `contents: read` and `id-token: write` (enables OIDC authentication for provenance signing).\n\n## Build and Publish Steps\n\nExecutes `actions/checkout@v4` to clone repository, then `actions/setup-node@v4` with `node-version: '20'` and `registry-url: 'https://registry.npmjs.org'` to configure npm registry authentication.\n\nRuns `npm ci` for dependency installation (clean install from lockfile), then `npm run build` invoking TypeScript compilation via `tsc` and hook copying via `build:hooks` script (triggered through `prepublishOnly` lifecycle hook).\n\nExecutes `npm publish --provenance --access public` with `NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}` environment variable for registry authentication. The `--provenance` flag generates Sigstore-signed attestation linking published tarball to source commit SHA via GitHub's OIDC provider, creating cryptographic proof of build integrity recorded in Sigstore's public transparency log. The `--access public` flag ensures package visibility on npm registry.\n\n## Integration with Project Build System\n\nDepends on `prepublishOnly` script in `package.json` executing `npm run build && npm run build:hooks`, which compiles `src/` to `dist/` via TypeScript compiler and copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion via `scripts/build-hooks.js`.\n\n## Security Model\n\nUses GitHub Actions secrets (`NPM_TOKEN`) for npm registry write access and GitHub's native OIDC identity provider (`id-token: write` permission) for provenance attestation without requiring long-lived signing keys.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\n**GitHub Actions workflow for automated npm package publishing with cryptographic provenance attestation on release events.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Automates npm publish on `release[published]` events with Sigstore-signed provenance linking published tarball to source commit SHA via GitHub OIDC provider.\n\n## Workflow Architecture\n\n**Trigger Conditions**: Executes on `release[published]` GitHub webhook events and manual `workflow_dispatch` invocation.\n\n**Runtime Environment**: Single `publish` job on `ubuntu-latest` runner with Node.js 20, npm registry authentication via `NODE_AUTH_TOKEN` secret, and `id-token: write` permission enabling OIDC-based attestation signing.\n\n**Build Pipeline**: Sequential steps execute `npm ci` (lockfile-based clean install), `npm run build` (invokes `prepublishOnly` hook running TypeScript compilation via `tsc` and hook file copying via `scripts/build-hooks.js`), then `npm publish --provenance --access public` generating Sigstore transparency log entry cryptographically binding published artifact to GitHub Actions workflow run and source commit.\n\n## Integration Points\n\n**Package Build System**: Depends on `prepublishOnly` script in root `package.json` executing `npm run build && npm run build:hooks`, compiling `src/` to `dist/` and copying `hooks/` to `hooks/dist/` for tarball inclusion.\n\n**Security Model**: Uses GitHub Actions secrets (`NPM_TOKEN`) for registry write access and native GitHub OIDC provider for provenance signing without long-lived cryptographic keys.\n\n**Provenance Chain**: The `--provenance` flag generates SLSA build attestation including workflow file path (`.github/workflows/publish.yml`), runner environment metadata, and commit SHA, recorded in Sigstore's public transparency log for post-publication verification via `npm audit signatures`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 452,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9577,
      "latencyMs": 23594,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.172Z",
      "prompt": "Generate AGENTS.md for directory: \"src/output\" (output)\n\n## File Summaries (1 files)\n\n### logger.ts\n**Purpose:** Terminal logging interface with picocolors-based output formatting for CLI progress reporting, file discovery, and er...\n\n**Terminal logging interface with picocolors-based output formatting for CLI progress reporting, file discovery, and error messages.**\n\n## Exported Types\n\n`Logger` interface defines six output methods: `info(message: string): void` for general messages, `file(path: string): void` for discovered files, `excluded(path: string, reason: string, filter: string): void` for skipped files with justification, `summary(included: number, excluded: number): void` for discovery totals, `warn(message: string): void` for warnings, `error(message: string): void` for errors.\n\n`LoggerOptions` interface specifies `colors: boolean` configuration (defaults to `true` per JSDoc).\n\n`ColorFunctions` internal interface wraps five picocolors methods: `green`, `dim`, `red`, `bold`, `yellow`, each accepting `string` and returning `string`.\n\n## Factory Functions\n\n`createLogger(options: LoggerOptions): Logger` returns configured logger instance. When `options.colors` is `true`, assigns `picocolors` to `ColorFunctions` variable `c`; when `false`, assigns `noColor` identity function wrapper. Implements output format specification:\n- `file`: renders `c.green('  +') + ' ' + path`\n- `excluded`: renders `c.dim('  -') + ' ' + path + c.dim(\\` (\\${reason}: \\${filter})\\`)`\n- `summary`: renders `c.bold(\\`\\\\nDiscovered \\${included} files\\`) + c.dim(\\` (\\${excluded} excluded)\\`)`\n- `warn`: renders `c.yellow('Warning: ') + message` via `console.warn()`\n- `error`: renders `c.red('Error: ') + message` via `console.error()`\n\n`createSilentLogger(): Logger` returns no-op logger for testing. Creates `noop = (): void => {}` and assigns to all six Logger methods.\n\n## Color Mode Abstraction\n\n`noColor` constant implements `ColorFunctions` with `identity` function for all five color methods. `identity` function returns input string unchanged: `(s: string): string => s`.\n\nConditional logic `options.colors ? pc : noColor` enables compile-time type safety for color-enabled/disabled paths without runtime string inspection.\n\n## Output Format Specification\n\nReferences CONTEXT.md for human-readable format rules:\n- File discovery prefixes: `  +` (green) for included, `  -` (dim) for excluded\n- Summary format: bold count + dim excluded count with leading newline\n- Error/warning prefixes: `Error:` (red), `Warning:` (yellow)\n\n## Integration Points\n\nConsumed by CLI commands (discover, generate, update) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging and `src/orchestration/progress.ts` for phase execution updates.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal logging interface providing picocolors-based formatted output for CLI progress reporting, file discovery summaries, warnings, and errors with runtime color toggle support.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (6 methods: `info`, `file`, `excluded`, `summary`, `warn`, `error`), `createLogger(options: LoggerOptions)` factory with conditional color mode (`picocolors` vs identity passthrough), `createSilentLogger()` no-op factory for testing, and `LoggerOptions` configuration interface (`colors: boolean`). Implements format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes.\n\n## Color Mode Abstraction\n\n`ColorFunctions` internal interface wraps `picocolors` methods (`green`, `dim`, `red`, `bold`, `yellow`). Conditional logic `options.colors ? pc : noColor` selects between `picocolors` and `noColor` identity wrapper. `noColor` constant implements all five color methods as `identity: (s: string): string => s` passthrough, enabling compile-time type safety for color-enabled/disabled code paths without runtime string inspection.\n\n## Output Format Specification\n\n- **File discovery**: `c.green('  +') + ' ' + path` (included), `c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`)` (excluded)\n- **Summary**: `c.bold(\\`\\nDiscovered ${included} files\\`) + c.dim(` (${excluded} excluded)`)`\n- **Warnings**: `c.yellow('Warning: ') + message` via `console.warn()`\n- **Errors**: `c.red('Error: ') + message` via `console.error()`\n\n## Integration Points\n\nConsumed by CLI commands (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging (`file()`, `excluded()`, `summary()` calls) and `src/orchestration/progress.ts` for phase execution updates (`info()`, `warn()` calls during pool orchestration).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 555,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9789,
      "latencyMs": 24473,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.187Z",
      "prompt": "Generate AGENTS.md for directory: \"src/imports\" (imports)\n\n## File Summaries (3 files)\n\n### extractor.ts\n**Purpose:** extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files via regex parsing, pro...\n\n**extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files via regex parsing, producing structured import maps for LLM prompt integration with internal/external classification and type-only detection.**\n\n## Exported Functions\n\n**`extractImports(sourceContent: string): ImportEntry[]`**\nParses source text via `IMPORT_REGEX` to extract import statements, returning array of `ImportEntry` objects with `specifier` (module path), `symbols` (imported identifiers), and `typeOnly` (boolean flag for `import type` syntax). Handles named imports (`{ Foo, Bar }`), namespace imports (`* as name`), and default imports. Strips alias syntax (`as alias`) from named imports via `.replace(/\\s+as\\s+\\w+/, '')` to retain only source identifier.\n\n**`extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>`**\nReads first 100 lines of each file in `fileNames` array (optimization assuming imports at top), calls `extractImports()` on sliced content, filters for relative imports (starts with `.` or `..`), excludes `node:` builtins and npm bare specifiers, classifies as `internalImports` (same-directory via `./`) or `externalImports` (parent-directory via `../`), returns `FileImports[]` with `fileName`, `externalImports`, `internalImports` fields. Silently skips unreadable files via empty catch block.\n\n**`formatImportMap(fileImports: FileImports[]): string`**\nTransforms `FileImports[]` into structured text format for LLM prompts. Output pattern: `fileName:\\n  specifier → symbol1, symbol2 (type)\\n`. Type-only imports append ` (type)` suffix via `imp.typeOnly` check. Only includes files with non-empty `externalImports` arrays. Sections separated by double newlines.\n\n## Import Regex Pattern\n\n**`IMPORT_REGEX`** constant: `/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`\n\nCapture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nMatches only lines starting with `import` (anchored with `^`) to avoid matching dynamic imports, comments, or string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset.\n\n## Classification Strategy\n\nRelative imports categorized by specifier prefix:\n- **Internal**: Starts with `./` (same directory)\n- **External**: Starts with `../` (parent directory)\n- **Excluded**: Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nFiltering via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before classification. Used in Phase 2 directory aggregation prompts via `src/generation/prompts/builder.ts` to show dependency relationships within `.sum` generation context.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Full file content read via `readFile(filePath, 'utf-8')` but only import region substring passed to `extractImports()`.\n\n## Integration Points\n\nConsumed by `src/generation/prompts/builder.ts` in `buildFilePrompt()` and `buildDirectoryPrompt()` to include import context in AI analysis prompts. Output of `formatImportMap()` embedded in prompt templates to inform LLM about cross-file dependencies when generating `.sum` summaries and `AGENTS.md` aggregations.\n### index.ts\n**Purpose:** Barrel module exporting static import analysis functions and types for dependency graph construction.\n\n**Barrel module exporting static import analysis functions and types for dependency graph construction.**\n\n## Exported Symbols\n\n- `extractImports` — Function analyzing TypeScript/JavaScript source text to extract import declarations with source module paths\n- `extractDirectoryImports` — Function aggregating imports across all source files in a directory, returning structured import maps\n- `formatImportMap` — Function converting parsed import data structures into human-readable markdown formatted output\n- `ImportEntry` — Type representing single import statement with source module path and imported symbols\n- `FileImports` — Type representing all imports within a single file, mapping file paths to ImportEntry arrays\n\n## Integration Points\n\nRe-exports from `./extractor.js` and `./types.js` provide public API surface for import analysis consumed by:\n- `src/generation/prompts/builder.ts` — Includes import maps in file analysis prompts via `extractDirectoryImports()` calls\n- `src/generation/prompts/templates.ts` — Directory aggregation prompts reference import relationships for understanding module coupling\n\n## Module Organization\n\nFollows barrel pattern: centralizes exports from implementation module (`extractor.js`) and type definitions (`types.js`) for single import point. Consumers import via `import { extractImports } from './imports/index.js'` rather than deep file references.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for representing import statement metadata extracted from source files, distinguishing ...\n\n**Defines TypeScript interfaces for representing import statement metadata extracted from source files, distinguishing between external module imports and internal same-directory imports.**\n\n## Exported Types\n\n### ImportEntry\nRepresents a single import statement extracted from source code.\n\n**Fields:**\n- `specifier: string` — Raw import specifier as written in source (e.g., `'../ai/index.js'`, `'@anthropic-ai/sdk'`)\n- `symbols: string[]` — Array of imported symbol names (e.g., `['AIService', 'AIResponse']` from `import { AIService, AIResponse } from '...'`)\n- `typeOnly: boolean` — True for `import type` statements, false for value imports\n\n### FileImports\nAggregates all imports from a single source file, partitioned by external vs internal origin.\n\n**Fields:**\n- `fileName: string` — Relative file path (e.g., `'runner.ts'`, `'src/ai/service.ts'`)\n- `externalImports: ImportEntry[]` — Imports from other modules (parent directories, npm packages, absolute paths)\n- `internalImports: ImportEntry[]` — Imports from same directory (e.g., `'./types.js'`, `'./helper.js'`)\n\n## Usage Context\n\nThese types support the import analysis system (`src/imports/extractor.ts`) which feeds dependency information into directory-level `AGENTS.md` prompt construction via `extractDirectoryImports()`. The external/internal partition enables documentation to emphasize cross-module coupling while de-emphasizing sibling file imports within the same directory.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Extracts and classifies TypeScript/JavaScript import statements from source files via regex-based parsing, producing structured import maps with internal/external partitioning for LLM prompt integration during `.sum` generation and directory aggregation phases.**\n\n## Contents\n\n### Core Implementation\n\n**[extractor.ts](./extractor.ts)** — Parses source text via `IMPORT_REGEX` to extract import declarations with `extractImports()` returning `ImportEntry[]` (specifier, symbols, typeOnly), aggregates directory-level imports via `extractDirectoryImports()` reading first 100 lines per file with relative specifier filtering (`.` or `..` prefix) and internal/external classification, formats import maps for LLM prompts via `formatImportMap()` producing structured `fileName:\\n  specifier → symbol1, symbol2 (type)` output.\n\n**[types.ts](./types.ts)** — Defines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` fields representing single import statements, and `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` fields partitioning imports by external (parent directories/npm packages) versus internal (same-directory sibling files) origin.\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` functions and `ImportEntry`, `FileImports` types from implementation modules for single-point import surface consumed by prompt builders.\n\n## Import Extraction Strategy\n\n**Regex Pattern:** `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` with capture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nAnchored with `^` to match only lines starting with `import`, avoiding dynamic imports/comments/string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset for multi-import files.\n\n**Classification Logic:**\n- **Internal:** Specifier starts with `./` (same directory)\n- **External:** Specifier starts with `../` (parent directory)\n- **Excluded:** Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nApplied via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before internal/external partitioning in `extractDirectoryImports()`.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing in `extractDirectoryImports()`, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Silently skips unreadable files via empty catch block.\n\n## Integration Points\n\nConsumed by `../generation/prompts/builder.ts`:\n- `buildFilePrompt()` includes import context from `extractDirectoryImports()` to inform LLM about cross-file dependencies when generating `.sum` summaries\n- `buildDirectoryPrompt()` embeds `formatImportMap()` output in Phase 2 directory aggregation prompts to show module coupling within `AGENTS.md` synthesis\n\nOutput format embeds in prompt templates defined in `../generation/prompts/templates.ts` for structured import relationship documentation.\n\n## Behavioral Contracts\n\n**Import Regex Pattern (IMPORT_REGEX):**\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n**Alias Stripping Pattern:**\n```\n.replace(/\\s+as\\s+\\w+/, '')\n```\n\n**Relative Specifier Filter:**\n```javascript\n.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))\n```\n\n**Import Map Format Template:**\n```\nfileName:\n  specifier → symbol1, symbol2 (type)\n```\n\n**First N Lines Slice:**\n```javascript\ncontent.split('\\n').slice(0, 100).join('\\n')\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1138,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 10843,
      "latencyMs": 32111,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.181Z",
      "prompt": "Generate AGENTS.md for directory: \"src/change-detection\" (change-detection)\n\n## File Summaries (3 files)\n\n### detector.ts\n**Purpose:** detector.ts implements Git-based change detection using simple-git and SHA-256 content hashing for incremental docume...\n\n**detector.ts implements Git-based change detection using simple-git and SHA-256 content hashing for incremental documentation updates.**\n\n## Exported Functions\n\n### isGitRepo\n```typescript\nasync function isGitRepo(projectRoot: string): Promise<boolean>\n```\nChecks if `projectRoot` is inside a git repository by calling `simpleGit(projectRoot).checkIsRepo()`.\n\n### getCurrentCommit\n```typescript\nasync function getCurrentCommit(projectRoot: string): Promise<string>\n```\nReturns the current HEAD commit hash via `git.revparse(['HEAD'])` with trimmed whitespace.\n\n### getChangedFiles\n```typescript\nasync function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult>\n```\nDetects files changed between `baseCommit` and `HEAD` using `git diff --name-status -M` for rename detection with 50% similarity threshold. Parses diff output line-by-line splitting on tab characters to extract status codes and file paths. Maps git status codes to `FileChange` objects:\n- `A` → `{ status: 'added' }`\n- `M` → `{ status: 'modified' }`\n- `D` → `{ status: 'deleted' }`\n- `R<percentage>` → `{ status: 'renamed', oldPath }` (parts[1] is old path, parts[parts.length-1] is new path)\n\nWhen `options.includeUncommitted` is true, merges uncommitted changes from `git.status()`:\n- `status.modified` → modified files not staged\n- `status.deleted` → staged deletions\n- `status.not_added` → untracked files\n- `status.staged` → staged files\n\nDeduplicates changes using `changes.some(c => c.path === file)` check before appending uncommitted entries. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]`, and `includesUncommitted` flag.\n\n### computeContentHash\n```typescript\nasync function computeContentHash(filePath: string): Promise<string>\n```\nComputes SHA-256 hash of file content at `filePath` (absolute path) by reading via `readFile()` and returning hex-encoded digest from `createHash('sha256').update(content).digest('hex')`.\n\n### computeContentHashFromString\n```typescript\nfunction computeContentHashFromString(content: string): string\n```\nComputes SHA-256 hash from in-memory `content` string using `createHash('sha256').update(content).digest('hex')`. Avoids redundant disk reads when file content is already loaded.\n\n## Dependencies\n\nUses `simpleGit` from `simple-git` library for all git operations (`checkIsRepo`, `revparse`, `diff`, `status`). Uses `createHash` from `node:crypto` for SHA-256 digest computation. Uses `readFile` from `node:fs/promises` for async file content loading.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` for incremental update workflow: reads `.sum` file `content_hash` from YAML frontmatter, compares against `computeContentHash()` result to determine if file needs regeneration. Supports non-git workflows via SHA-256 fallback (no git calls when `isGitRepo()` returns false).\n\n## Behavioral Contracts\n\n**Diff output parsing format:** `STATUS\\tFILE` for add/modify/delete, `STATUS\\tOLD\\tNEW` for renames where STATUS is `R<percentage>` (e.g., `R100\\told/path.ts\\tnew/path.ts`).\n\n**Rename detection flag:** `-M` uses 50% similarity threshold (git default).\n\n**Deduplication strategy:** Linear `Array.some()` scan prevents duplicate entries when uncommitted changes overlap with committed changes.\n### index.ts\n**Purpose:** Re-exports git-based change detection functions and types from `detector.ts` and `types.ts` for incremental update wo...\n\n**Re-exports git-based change detection functions and types from `detector.ts` and `types.ts` for incremental update workflows.**\n\n## Exported Functions\n\n- `isGitRepo(): Promise<boolean>` — checks if current working directory is a git repository\n- `getCurrentCommit(): Promise<string>` — retrieves current HEAD commit SHA\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — detects added/modified/deleted/renamed files via git diff, returns `ChangeDetectionResult` with `changes: FileChange[]` array\n- `computeContentHash(filePath: string): Promise<string>` — computes SHA-256 hash of file content (reads file from disk)\n- `computeContentHashFromString(content: string): string` — computes SHA-256 hash of string content (synchronous, no I/O)\n\n## Exported Types\n\n- `ChangeType` — discriminated union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — object with `path: string`, `status: ChangeType`, optional `oldPath?: string` (for renames)\n- `ChangeDetectionResult` — object with `changes: FileChange[]`, `baseCommit?: string`, `includesUncommitted: boolean`\n- `ChangeDetectionOptions` — configuration object controlling git diff scope (base commit, uncommitted flag, path filters)\n\n## Module Purpose\n\nServes as public API surface for `src/change-detection/` module, consumed by `src/update/orchestrator.ts` and `src/cli/update.ts` to compute file delta sets for incremental `.sum` regeneration. Hash comparison workflow: read `.sum` frontmatter `content_hash`, call `computeContentHash()` on current file, compare SHA-256 hex strings to detect modifications.\n### types.ts\n**Purpose:** Defines TypeScript interfaces and type aliases for git-based change detection workflows, enabling hash-based incremen...\n\n**Defines TypeScript interfaces and type aliases for git-based change detection workflows, enabling hash-based incremental updates via status discrimination (`added`/`modified`/`deleted`/`renamed`) and uncommitted change inclusion.**\n\n## Exported Types\n\n**`ChangeType`** — String literal union discriminating file change status: `'added' | 'modified' | 'deleted' | 'renamed'`.\n\n**`FileChange`** — Represents single file delta with `path: string` (new path for renames), `status: ChangeType`, and optional `oldPath?: string` (populated only when `status: 'renamed'`).\n\n**`ChangeDetectionResult`** — Aggregates diff output with `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, and `includesUncommitted: boolean` flag indicating working tree inclusion.\n\n**`ChangeDetectionOptions`** — Configuration object with `includeUncommitted?: boolean` controlling whether `git status --porcelain` changes merge with `git diff` output.\n\n## Integration Context\n\nUsed by `src/update/orchestrator.ts` to compute `filesToAnalyze` (changes requiring `.sum` regeneration) vs `filesToSkip` (hash matches). The `oldPath` field enables orphan cleanup via `cleanupOrphans()` when files renamed via `git diff -M` (50% similarity threshold). The `includesUncommitted` flag determines whether session-end hooks (`are-session-end.js`) trigger auto-updates.\n\n## Discriminated Union Pattern\n\n`FileChange` uses `status` as discriminant where `oldPath` presence correlates with `status: 'renamed'`. Consumers check `change.status === 'renamed'` before accessing `change.oldPath` to avoid undefined errors.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\n**Git-based change detection with SHA-256 content hashing for incremental documentation updates, supporting rename tracking via `git diff -M`, uncommitted change merging via `git status --porcelain`, and hash-only fallback for non-git workflows.**\n\n## Contents\n\n### [detector.ts](./detector.ts)\nImplements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, and `computeContentHashFromString()` for delta computation. Parses `git diff --name-status -M` output with tab-split line parsing to extract status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, maps to `FileChange` objects with optional `oldPath` for renames. Optionally merges uncommitted changes from `git.status()` (`modified`, `deleted`, `not_added`, `staged`) when `includeUncommitted: true`, using linear `Array.some()` deduplication to prevent duplicate entries. SHA-256 hashing via `node:crypto` `createHash('sha256')` with hex digest output.\n\n### [types.ts](./types.ts)\nDefines `ChangeType` (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` (discriminated by `status` with optional `oldPath` for `'renamed'`), `ChangeDetectionResult` (`currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`), and `ChangeDetectionOptions` (`includeUncommitted?: boolean`). Discriminated union pattern enables type-safe `oldPath` access after `status === 'renamed'` check.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting all functions (`isGitRepo`, `getCurrentCommit`, `getChangedFiles`, `computeContentHash`, `computeContentHashFromString`) and types (`ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions`) from `detector.ts` and `types.ts`. Serves as public API consumed by `../update/orchestrator.ts` and `../cli/update.ts`.\n\n## Integration Points\n\n**Incremental Update Workflow:**\n1. `src/update/orchestrator.ts` calls `getChangedFiles(projectRoot, baseCommit, options)` to compute `changes[]` array\n2. For each changed file, reads `.sum` YAML frontmatter `content_hash` via `src/generation/writers/sum.ts`\n3. Calls `computeContentHash(filePath)` to compute current file SHA-256 hash\n4. Hash mismatch → add to `filesToAnalyze` as `FileChange`, hash match → add to `filesToSkip`\n5. Detects orphans via `changes.filter(c => c.status === 'renamed')` and calls `cleanupOrphans()` to delete `.sum` files for `oldPath` entries\n6. Session-end hooks (`hooks/are-session-end.js`) check `git status --porcelain` and spawn `npx agents-reverse-engineer@latest update --quiet` if changes detected\n\n**Non-Git Fallback:**\nWhen `isGitRepo(projectRoot)` returns `false`, consumers skip git operations and rely on SHA-256 hash comparison alone for change detection (reads all `.sum` files, compares hashes).\n\n## Behavioral Contracts\n\n**Diff output format:** `git diff --name-status -M` produces tab-delimited lines where:\n- Add/Modify/Delete: `STATUS\\tFILE` (e.g., `M\\tsrc/foo.ts`)\n- Rename: `R<percentage>\\tOLDPATH\\tNEWPATH` (e.g., `R100\\tsrc/old.ts\\tsrc/new.ts`)\n\n**Rename similarity threshold:** `-M` flag uses git default 50% content similarity for rename detection.\n\n**Status code mapping:**\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n**Uncommitted change sources (when `includeUncommitted: true`):**\n- `status.modified` — modified files not staged for commit\n- `status.deleted` — staged deletions\n- `status.not_added` — untracked files\n- `status.staged` — files added to git index\n\n**Deduplication strategy:** Linear `changes.some(c => c.path === file)` scan before appending uncommitted entries prevents duplicate `FileChange` objects when working tree changes overlap with committed diff.\n\n**SHA-256 output format:** 64-character hex string from `createHash('sha256').update(content).digest('hex')`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1175,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 10987,
      "latencyMs": 34466,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.195Z",
      "prompt": "Generate AGENTS.md for directory: \"src/specify\" (specify)\n\n## File Summaries (3 files)\n\n### index.ts\n**Purpose:** Barrel module exporting specification synthesis API comprising prompt builder (`buildSpecPrompt`), filesystem writer ...\n\n**Barrel module exporting specification synthesis API comprising prompt builder (`buildSpecPrompt`), filesystem writer (`writeSpec`, `SpecExistsError`), and associated types (`SpecPrompt`, `WriteSpecOptions`).**\n\n## Exported Symbols\n\n- **`buildSpecPrompt`** — Function from `./prompts.js` constructing prompts for AI-driven project specification synthesis from `AGENTS.md` corpus\n- **`SpecPrompt`** — Type from `./prompts.js` defining prompt structure for specification generation\n- **`writeSpec`** — Function from `./writer.js` writing synthesized specification to filesystem (single-file or multi-file output)\n- **`SpecExistsError`** — Error class from `./writer.js` thrown when target specification files already exist without `force` flag\n- **`WriteSpecOptions`** — Type from `./writer.js` defining configuration for `writeSpec` (output mode, force overwrite, target directory)\n\n## Module Purpose\n\nConsolidates specification synthesis functionality into single import point for `src/cli/specify.ts` command implementation. Specification synthesis consumes all `AGENTS.md` directory documentation files and produces `specs/SPEC.md` (single-file) or `specs/<dirname>.md` (multi-file) project specifications via AI-driven aggregation. Prompt builder likely includes constraints enforcing synthesis-only behavior (no invention of features not present in source documents).\n### prompts.ts\n**Purpose:** prompts.ts defines the system prompt template and prompt construction logic for AI-driven specification synthesis fro...\n\n**prompts.ts defines the system prompt template and prompt construction logic for AI-driven specification synthesis from AGENTS.md documentation corpus.**\n\n## Exported Interface\n\n`SpecPrompt` interface encapsulates prompt pair with `system: string` and `user: string` properties for AI service consumption.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` constructs complete prompt pair by:\n- Mapping `docs` array to markdown sections with `### ${doc.relativePath}` headers and embedded `doc.content`\n- Conditionally appending annex sections when `annexFiles` provided (reproduction-critical source files like templates)\n- Injecting 11-point section requirement list into user prompt: Project Overview, Architecture, Public API Surface, Data Structures & State, Configuration, Dependencies, Behavioral Contracts (split into Runtime Behavior and Implementation Contracts with verbatim regex/format/constant requirements), Test Contracts, Build Plan (with \"Defines:\" and \"Consumes:\" interface contract lists), Prompt Templates & System Instructions (verbatim reproduction), IDE Integration & Installer (verbatim templates)\n- Returns `SpecPrompt` with `SPEC_SYSTEM_PROMPT` as system and constructed markdown sections as user\n\n## System Prompt Template\n\n`SPEC_SYSTEM_PROMPT` constant defines 600+ character system instruction enforcing:\n- **Audience**: AI agents (LLMs) requiring actionable, instruction-oriented language\n- **Organization mandate**: Group by concern (not directory structure), use 11 conceptual sections in prescribed order\n- **Anti-patterns**: Prohibits folder-mirroring, exact file path prescription, section headings derived from directory names\n- **Module boundary focus**: Describe interfaces and exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for all external dependencies\n- **Build Plan requirements**: Phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface section, dependency ordering\n- **Behavioral contract mandates**: Exact error types/codes with throw conditions, verbatim regex patterns in backticks, format strings and output templates with exact structure, magic constants and sentinel values with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical content handling**: Sections 10-11 must reproduce annex file content verbatim (prompt templates, IDE templates, installer configs) without summarization or paraphrasing\n- **Output format**: Raw markdown with no preamble, meta-commentary, or conversational prefix\n\n## Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompt via:\n1. Header: `\"Generate a comprehensive project specification from the following documentation.\"`\n2. Section delimiter: `\"## AGENTS.md Files (${docs.length} directories)\"`\n3. Per-document subsections: `agentsSections` array mapping each doc to `### ${relativePath}\\n\\n${content}`\n4. Conditional annex block: When `annexFiles` non-empty, appends `\"## Annex Files (${annexFiles.length} reproduction-critical source files)\"` followed by annex subsections\n5. Requirement checklist: `\"## Output Requirements\"` followed by 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n6. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim. Do NOT summarize prompt templates or IDE templates into prose descriptions.\"`\n7. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\n## Integration Points\n\nConsumed by `src/specify/index.ts` specification synthesis command via `AIService.call()` invocation. Receives `AgentsDocs` from `collectAgentsDocs()` in `src/generation/collector.ts` (recursive AGENTS.md traversal). Optional `annexFiles` parameter populated from `.annex.md` companion files containing verbatim source code for reproduction-critical modules (prompt templates, IDE command templates, installer configs).\n\n## Behavioral Contracts\n\nUser prompt sections joined with `'\\n'` separator producing multiline string. Section 7 split into subsections: `\"7a. Runtime Behavior\"` and `\"7b. Implementation Contracts\"` targeting distinct documentation domains (execution semantics vs. reproduction-critical constants).\n### writer.ts\n**Purpose:** writeSpec() orchestrates single-file or multi-file specification output with overwrite protection, slugified heading-...\n\n**writeSpec() orchestrates single-file or multi-file specification output with overwrite protection, slugified heading-based splitting, and existence-check guardrails for the `/are-specify` command.**\n\n## Exported Interface\n\n**WriteSpecOptions** configures output behavior:\n- `outputPath: string` — absolute path to target file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — bypass existence checks to overwrite files\n- `multiFile: boolean` — split AI output on top-level `# ` headings into separate files\n\n**SpecExistsError** extends `Error` with `paths: string[]` property listing conflicting file paths. Constructor message formats paths as bullet list with `\"Use --force to overwrite.\"` suffix.\n\n**writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>** writes specification markdown to disk:\n- Single-file mode: writes `content` directly to `outputPath` after `mkdir(dirname(outputPath), { recursive: true })`\n- Multi-file mode: calls `splitByHeadings(content)` to partition on `/^# /m` regex, writes each section to `dirname(outputPath)/<slug>.md`\n- Pre-write existence check: aggregates conflicts via `fileExists()` and throws `SpecExistsError` unless `force=true`\n- Returns array of absolute paths for all written files\n\n## Heading-Based Splitting\n\n**splitByHeadings(content: string): Array<{ filename: string; content: string }>** partitions markdown on top-level headings:\n- Splits via `/^(?=# )/m` regex (positive lookahead for lines starting with `# `)\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify()`\n- Pre-heading content assigned to `00-preamble.md` filename\n- Each section object contains `{ filename, content }` with trailing newline appended\n\n**slugify(heading: string): string** sanitizes heading text into filename-safe slug:\n- Lowercases via `.toLowerCase()`\n- Replaces whitespace with hyphens via `/\\s+/g` → `'-'`\n- Strips non-alphanumeric characters (except hyphens) via `/[^a-z0-9-]/g` → `''`\n- Collapses consecutive hyphens via `/-+/g` → `'-'`\n- Trims leading/trailing hyphens via `/^-|-$/g` → `''`\n\n## File System Operations\n\n**fileExists(filePath: string): Promise<boolean>** checks file existence:\n- Invokes `access(filePath, constants.F_OK)` from `node:fs/promises`\n- Returns `true` on success, `false` on rejection (file not found)\n\n**Multi-file conflict detection:** Before writing any files in multi-file mode, iterates all sections to check existence via `fileExists()`. Aggregates conflicts into array and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write operation (all-or-nothing).\n\n## Integration Context\n\nConsumed by `src/cli/specify.ts` after AI synthesis completes. The `outputPath` parameter receives value from `config.output.specPath` (default: `specs/SPEC.md`). The `--force` CLI flag maps to `WriteSpecOptions.force`. The `--multi-file` CLI flag maps to `WriteSpecOptions.multiFile`.\n\n## Import Map (verified — use these exact paths)\n\nprompts.ts:\n  ../generation/collector.js → AgentsDocs (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/specify\n\n**Project specification synthesis from AGENTS.md documentation corpus: prompt engineering, heading-based file splitting, and overwrite-protected filesystem operations for single-file or multi-file spec generation.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export for specification synthesis API: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions`\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs, annexFiles?)` constructs `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration) and verbatim annex reproduction mandates\n\n**[writer.ts](./writer.ts)** — `writeSpec(content, options)` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings`), throws `SpecExistsError` on conflicts unless `force=true`, returns written paths array\n\n## Architecture\n\n### Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompts by concatenating:\n1. Header directive: `\"Generate a comprehensive project specification...\"`\n2. Documentation sections: `### ${doc.relativePath}` with embedded content from `AgentsDocs` array\n3. Optional annex block: `## Annex Files` with verbatim source code for reproduction-critical modules (prompt templates, IDE configs)\n4. Requirement checklist: 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n5. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim\"`\n6. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\nSystem prompt enforces:\n- **Audience**: AI agents requiring instruction-oriented language (not human documentation)\n- **Anti-patterns**: Prohibits folder-mirroring, file path prescription, directory-derived headings\n- **Module boundary focus**: Describe interfaces/exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for external dependencies\n- **Build Plan**: Phased implementation with explicit \"Defines:\"/\"Consumes:\" interface contract cross-references\n- **Behavioral contracts**: Exact error types/codes, verbatim regex patterns in backticks, format strings with structure, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical sections**: Sections 10-11 reproduce annex content without summarization\n\n### Multi-File Splitting\n\n`splitByHeadings` partitions markdown on `/^(?=# )/m` regex (positive lookahead for top-level headings):\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify` (lowercase + whitespace→hyphens + strip non-alphanumeric + collapse consecutive hyphens + trim)\n- Pre-heading content assigned to `00-preamble.md`\n- Each section object contains `{ filename, content }` with trailing newline\n\nConflict detection: Before writing files, iterates sections to check existence via `fileExists` (wraps `fs.access(..., F_OK)`). Aggregates conflicts and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write (all-or-nothing).\n\n## Integration Points\n\n### Consumed By\n\n`src/cli/specify.ts` command invokes `writeSpec(aiOutput, { outputPath: config.output.specPath, force: cliFlags.force, multiFile: cliFlags.multiFile })` after `AIService.call(buildSpecPrompt(docs, annexFiles))` completes.\n\n### Dependencies\n\n**[../generation/collector.js](../generation/collector.ts)** — `collectAgentsDocs()` provides recursive `AGENTS.md` traversal results via `AgentsDocs` type\n\n**node:fs/promises** — `mkdir`, `writeFile`, `access` for filesystem operations\n\n**node:path** — `dirname`, `join` for path resolution\n\n## Behavioral Contracts\n\n### Runtime Behavior\n\n- `writeSpec` throws `SpecExistsError` when target paths exist and `force=false`\n- Single-file mode creates parent directory recursively via `mkdir(..., { recursive: true })`\n- Multi-file mode writes all sections or none (atomic operation via upfront conflict check)\n\n### Implementation Contracts\n\n**Heading regex**: `/^(?=# )/m` (matches lines starting with `# ` via positive lookahead)\n\n**Heading extraction**: `/^# (.+)/` (captures heading text after `# ` prefix)\n\n**Slugification transforms**:\n- Whitespace→hyphens: `/\\s+/g` → `'-'`\n- Strip non-alphanumeric: `/[^a-z0-9-]/g` → `''`\n- Collapse hyphens: `/-+/g` → `'-'`\n- Trim hyphens: `/^-|-$/g` → `''`\n\n**SpecExistsError message format**: `\"Specification file(s) already exist:\\n\" + paths.map(p => \"  - \" + p).join(\"\\n\") + \"\\n\\nUse --force to overwrite.\"`\n\n**Preamble filename**: `00-preamble.md` (for content before first `# ` heading)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1363,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 11389,
      "latencyMs": 35546,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:32:15.767Z",
      "prompt": "Generate AGENTS.md for directory: \"src/types\" (types)\n\n## File Summaries (1 files)\n\n### index.ts\n**Purpose:** Defines core TypeScript interfaces for file discovery results, exclusion metadata, and discovery statistics shared ac...\n\n**Defines core TypeScript interfaces for file discovery results, exclusion metadata, and discovery statistics shared across discovery and change-detection modules.**\n\n## Exported Interfaces\n\n**ExcludedFile** represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to the excluded file\n- `reason: string` — Exclusion rationale string (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** encapsulates file discovery output consumed by generation orchestrators:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics and reporting)\n\n**DiscoveryStats** aggregates discovery metrics for progress reporting and telemetry:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Map of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n`DiscoveryResult` is returned by `discoverFiles()` in `src/discovery/run.ts` and consumed by `generateDocumentation()` in `src/generation/orchestrator.ts`.\n\n`DiscoveryStats` is computed from `DiscoveryResult` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, then logged by `src/output/logger.ts` and written to `GENERATION-PLAN.md`.\n\n`ExcludedFile` is produced by filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) and aggregated into `DiscoveryResult.excluded`.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/types/\n\n**Core TypeScript interface definitions for file discovery results, exclusion metadata, and discovery statistics shared across discovery, change-detection, generation, and quality validation modules.**\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `ExcludedFile`, `DiscoveryStats` interfaces consumed by orchestrators and reporters.\n\n## Exported Interfaces\n\n**ExcludedFile** — Represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** — Encapsulates file discovery output:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics/reporting)\n\n**DiscoveryStats** — Aggregates discovery metrics:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Histogram of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n**Producers:**\n- `discoverFiles()` in `src/discovery/run.ts` returns `DiscoveryResult`\n- Filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) produce `ExcludedFile` instances aggregated into `DiscoveryResult.excluded`\n\n**Consumers:**\n- `generateDocumentation()` in `src/generation/orchestrator.ts` consumes `DiscoveryResult.files` for Phase 1 pool execution\n- `updateDocumentation()` in `src/update/orchestrator.ts` merges `DiscoveryResult` with `FileChange[]` from change detector\n- Progress reporters and telemetry loggers in `src/output/logger.ts` compute `DiscoveryStats` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, written to `GENERATION-PLAN.md` and `.agents-reverse-engineer/progress.log`\n\n## File Relationships\n\n`DiscoveryResult` flows from discovery filters → orchestrator → worker pool → generation writers. `DiscoveryStats` computed post-discovery via reduction over `excluded[]` array for reporting and plan documentation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 640,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27162,
      "latencyMs": 13422,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.209Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery\" (discovery)\n\n## File Summaries (3 files)\n\n### run.ts\n**Purpose:** High-level file discovery orchestrator that composes directory walking with a four-stage filter chain (gitignore, ven...\n\n**High-level file discovery orchestrator that composes directory walking with a four-stage filter chain (gitignore, vendor, binary, custom) into a single reusable pipeline for all commands.**\n\n## Exported Symbols\n\n### `discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>`\nExecutes the standard discovery pipeline: creates four filters via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, walks directory tree via `walkDirectory()`, applies filters via `applyFilters()`, and returns full `FilterResult` with included/excluded file lists and filter attribution. The `root` parameter must be an absolute path. The `config.exclude.vendorDirs` array feeds `createVendorFilter()`, `config.exclude.binaryExtensions` feeds `createBinaryFilter()` additionalExtensions, `config.exclude.patterns` feeds `createCustomFilter()` with gitignore-style globs, `config.options.maxFileSize` sets binary detection threshold, and `config.options.followSymlinks` controls symlink traversal in `walkDirectory()`.\n\n### `DiscoveryConfig` Interface\nStructural subset of full config schema defining discovery-related settings. Contains `exclude` object with `vendorDirs: string[]` (third-party directory names like `node_modules`), `binaryExtensions: string[]` (file extensions like `.png`, `.zip`), `patterns: string[]` (gitignore-style glob patterns), and `options` object with `maxFileSize: number` (binary detection byte threshold), `followSymlinks: boolean` (symlink traversal toggle). Structurally compatible with full `Config` type from `src/config/schema.ts` per JSDoc comment.\n\n### `DiscoverFilesOptions` Interface\nOptional parameters for discovery pipeline execution. Contains `tracer?: ITraceWriter` for NDJSON trace event emission and `debug?: boolean` for verbose logging. Both fields forwarded to `applyFilters()` third parameter.\n\n## Integration Points\n\nDepends on `walkDirectory()` from `./walker.js` for filesystem traversal and five filter factories from `./filters/index.js`: `createGitignoreFilter()` (parses `.gitignore`), `createVendorFilter()` (excludes vendor directories), `createBinaryFilter()` (detects non-text files), `createCustomFilter()` (applies user-defined glob patterns), `applyFilters()` (chains filter execution). Imports `FilterResult` type from `./types.js` and `ITraceWriter` from `../orchestration/trace.js`. Filter creation order (gitignore → vendor → binary → custom) determines priority when multiple filters match a file.\n\n## Design Pattern\n\nImplements the Facade pattern by hiding filter instantiation complexity behind a single function call. Commands in `src/cli/` (discover, generate, update) invoke `discoverFiles()` rather than manually constructing filter chains. The returned `FilterResult` contains both `included: string[]` for files passing all filters and `excluded: Array<{file: string, reason: string, filter: string}>` for audit trails showing why files were rejected.\n### types.ts\n**Purpose:** `src/discovery/types.ts` defines the core type contracts for the file discovery pipeline's filter chain architecture ...\n\n**`src/discovery/types.ts` defines the core type contracts for the file discovery pipeline's filter chain architecture and walker configuration.**\n\n## Exported Interfaces\n\n**`FileFilter`** — Contract for pluggable filters in the discovery pipeline. Requires `name: string` for logging which filter excluded a file, and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` to determine exclusion synchronously or asynchronously. Concrete implementations include `GitignoreFilter`, `BinaryFilter`, `VendorFilter`, `CustomPatternFilter` (per JSDoc examples).\n\n**`ExcludedFile`** — Record of a file rejected by the filter chain. Contains `path: string` (absolute path), `reason: string` (human-readable explanation), `filter: string` (name of the `FileFilter` that excluded it).\n\n**`FilterResult`** — Output of running the discovery filter chain. Contains `included: string[]` (files passing all filters for analysis) and `excluded: ExcludedFile[]` (rejected files with exclusion metadata).\n\n**`WalkerOptions`** — Configuration for the directory walker. Requires `cwd: string` (absolute root directory). Optional `followSymlinks?: boolean` (default `false` per CONTEXT.md to skip symlinks) and `dot?: boolean` (default `true` to include dotfiles for analysis).\n\n## Filter Chain Pattern\n\nThe `FileFilter` interface enables composable filter chains where each filter independently evaluates files. The `shouldExclude()` method supports both synchronous (direct `boolean` return) and asynchronous (`Promise<boolean>` return) implementations, allowing filters to perform I/O operations like reading file headers for binary detection or parsing `.gitignore` rules.\n\n## Integration Points\n\nThese types are consumed by `src/discovery/walker.ts` (directory traversal), `src/discovery/filters/` implementations (gitignore parsing, binary detection, vendor exclusion, custom glob patterns), and `src/discovery/run.ts` (filter chain orchestration). The `Stats` parameter in `shouldExclude()` enables size-based filtering (e.g., `maxFileSize` threshold from config) without redundant filesystem calls.\n### walker.ts\n**Purpose:** walkDirectory() wraps fast-glob to traverse directories, returning all absolute file paths with performance optimizat...\n\n**walkDirectory() wraps fast-glob to traverse directories, returning all absolute file paths with performance optimizations (`.git` exclusion, symlink control, permission error suppression) but no filtering logic (deferred to filter chain in `src/discovery/filters/`).**\n\n## Exported Function\n\n`walkDirectory(options: WalkerOptions): Promise<string[]>` — Async function returning absolute paths for all files discovered via `fg.glob('**/*', ...)` with configuration from `WalkerOptions`.\n\n## WalkerOptions Interface\n\nExpects `WalkerOptions` type (defined in `src/discovery/types.ts`) with fields:\n- `cwd` — Base directory for traversal (required)\n- `dot` — Include dotfiles (optional, defaults to `true`)\n- `followSymlinks` — Follow symbolic links (optional, defaults to `false`)\n\n## fast-glob Configuration\n\nPasses following parameters to `fg.glob()`:\n- `cwd: options.cwd` — Base directory\n- `absolute: true` — Returns absolute paths instead of relative\n- `onlyFiles: true` — Excludes directories from results\n- `dot: options.dot ?? true` — Includes hidden files by default\n- `followSymbolicLinks: options.followSymlinks ?? false` — Disables symlink traversal by default\n- `suppressErrors: true` — Continues on permission errors (per RESEARCH.md design decision)\n- `ignore: ['**/.git/**']` — Hardcoded exclusion of `.git` internals for performance\n\n## Design Constraint\n\nDoes NOT apply filtering logic for gitignore patterns, binary detection, vendor directories, or custom globs. Filtering happens via separate filter chain modules (`src/discovery/filters/gitignore.ts`, `src/discovery/filters/binary.ts`, `src/discovery/filters/vendor.ts`, `src/discovery/filters/custom.ts`) after `walkDirectory()` returns raw file list.\n\n## Import Map (verified — use these exact paths)\n\nrun.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### filters/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters for discovery pipeline: gitignore pattern matching, binary file detection, vendor directory exclusion, and custom glob rules with short-circuit evaluation and bounded-concurrency application.\n\n## Contents\n\n### Filter Orchestration\n\n**[index.ts](./index.ts)** — `applyFilters(files, filters, options)` executes filter chain with short-circuit evaluation (stops at first exclusion) using 30-worker bounded concurrency pool to prevent file descriptor exhaustion during `isBinaryFile()` I/O operations. Returns `FilterResult` with `included[]` and `excluded[]` arrays, emits `filter:applied` trace events with per-filter `filesMatched`/`filesRejected` counts. Re-exports all filter factory functions.\n\n### Filter Implementations\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter(root)` async factory reads `.gitignore`, parses via `ignore` library, returns `FileFilter` that converts absolute paths to relative before calling `ig.ignores()`. Returns `false` for paths outside root tree (relative path starts with `..`).\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter(options?)` constructs fast-path extension lookup in `BINARY_EXTENSIONS` set (96 extensions: `.png`, `.exe`, `.mp4`, `.pdf`, `.woff`, `.class`, `.db`, etc.) with fallback to `isBinaryFile()` content analysis. Excludes files exceeding `maxFileSize` (default 1MB via `DEFAULT_MAX_FILE_SIZE`). Returns `true` on `fs.stat()` errors.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter(vendorDirs)` partitions input into single-segment Set (e.g., `node_modules`) for O(1) membership testing and path-pattern array (e.g., `.agents/skills`) for substring matching. `DEFAULT_VENDOR_DIRS` contains `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter(patterns, root)` parses gitignore-style glob patterns via `ignore` library, converts absolute paths to relative before evaluation. Returns `false` immediately for empty patterns array or paths outside root.\n\n## Concurrency Strategy\n\n`applyFilters()` uses iterator-based worker pool with `CONCURRENCY = 30` to share single `files.entries()` iterator across workers. Each file runs through `filters[]` sequentially until `shouldExclude()` returns `true` (short-circuit). BinaryFilter calls synchronous `isBinaryFile()`, making concurrency bounds critical to avoid ulimit violations.\n\n## Filter Contract\n\nAll factories return `FileFilter` interface from `../types.js`:\n- `name: string` — filter identifier for logging\n- `shouldExclude(absolutePath: string): boolean` — exclusion predicate\n\n## Behavioral Contracts\n\n### Binary Extension Set\n`BINARY_EXTENSIONS` (96 entries): `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`, `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`, `.db`, `.sqlite`, `.sqlite3`, `.mdb`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS` (10 entries): `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Path Normalization\n- Gitignore filter: `path.relative(normalizedRoot, absolutePath)` before `ig.ignores()`\n- Vendor filter: `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling\n- Binary filter: `path.extname(absolutePath).toLowerCase()` for extension extraction\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery pipeline using fast-glob directory traversal with composable four-stage filter chain (gitignore, vendor, binary, custom) and bounded-concurrency short-circuit evaluation, exposing `discoverFiles()` facade for all CLI commands.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[run.ts](./run.ts)** — `discoverFiles(root, config, options?)` facade composes `walkDirectory()` with four-stage filter chain: `createGitignoreFilter()` parses `.gitignore`, `createVendorFilter()` excludes `node_modules`/`.git`/`dist` via `config.exclude.vendorDirs`, `createBinaryFilter()` applies 96-extension allowlist + `isBinaryFile()` fallback with `config.options.maxFileSize` threshold, `createCustomFilter()` processes user glob patterns from `config.exclude.patterns`. Returns `FilterResult` with `included: string[]` and `excluded: Array<{file, reason, filter}>`. Exports `DiscoveryConfig` interface (structural subset of `Config`) and `DiscoverFilesOptions` (`tracer?: ITraceWriter`, `debug?: boolean`).\n\n**[walker.ts](./walker.ts)** — `walkDirectory(options)` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `suppressErrors: true`, hardcoded `ignore: ['**/.git/**']`, and `followSymbolicLinks: options.followSymlinks ?? false`. Requires `WalkerOptions` with `cwd: string` root, optional `dot?: boolean` (default `true` for dotfiles), `followSymlinks?: boolean` (default `false`). Returns raw `string[]` file paths without filtering logic (deferred to filter chain).\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` (`included[]`, `excluded[]`), `WalkerOptions` (`cwd`, `followSymlinks?`, `dot?`).\n\n## Subdirectory\n\n**[filters/](./filters/)** — Five modules implementing `FileFilter` contract: `gitignore.ts` (`createGitignoreFilter()` async factory with `ignore` library), `binary.ts` (96-extension set + `isBinaryFile()` content analysis), `vendor.ts` (single-segment Set + path-pattern array matching), `custom.ts` (gitignore-style glob parsing), `index.ts` (`applyFilters()` with 30-worker bounded concurrency, short-circuit evaluation, `filter:applied` trace emission).\n\n## Filter Chain Architecture\n\n`discoverFiles()` instantiates filters in priority order (gitignore → vendor → binary → custom), passes to `applyFilters()` which runs filters sequentially per file until `shouldExclude()` returns `true`. Concurrency pool prevents file descriptor exhaustion during `isBinaryFile()` I/O. Filters receive absolute paths; gitignore/custom filters convert to relative via `path.relative()` before pattern matching.\n\n## Configuration Surface\n\n- `config.exclude.vendorDirs: string[]` — directory names like `node_modules` (default 10 entries)\n- `config.exclude.binaryExtensions: string[]` — additional extensions beyond 96-entry `BINARY_EXTENSIONS` set\n- `config.exclude.patterns: string[]` — gitignore-style globs processed by `ignore` library\n- `config.options.maxFileSize: number` — binary detection threshold (default 1MB)\n- `config.options.followSymlinks: boolean` — symlink traversal toggle (default `false`)\n\n## Integration Points\n\nConsumed by `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` via `discoverFiles()` facade. Imports `ITraceWriter` from `../orchestration/trace.js` for `filter:applied` event emission. Filter factories depend on `ignore` library (gitignore parsing), `isbinaryfile` library (content analysis), `fast-glob` (directory traversal), `node:fs` (Stats objects).\n\n## Behavioral Contracts\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS`: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Binary Extension Set\n96 extensions including: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm` (full list in `filters/binary.ts`)\n\n### Concurrency Limit\n`applyFilters()` uses `CONCURRENCY = 30` workers via iterator-based pool pattern from `src/orchestration/pool.ts`\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1336,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 29746,
      "latencyMs": 42693,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.191Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality\" (quality)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** `src/quality/index.ts` re-exports quality analysis types and functions from inconsistency detection, phantom path val...\n\n**`src/quality/index.ts` re-exports quality analysis types and functions from inconsistency detection, phantom path validation, and density validation submodules.**\n\n## Exported Types\n\nRe-exports from `./types.js`:\n- `InconsistencySeverity` — severity level enumeration for quality issues\n- `CodeDocInconsistency` — reports exports missing from `.sum` documentation\n- `CodeCodeInconsistency` — reports duplicate symbol definitions across files\n- `PhantomPathInconsistency` — reports unresolvable path references in `AGENTS.md`\n- `Inconsistency` — discriminated union of all inconsistency types\n- `InconsistencyReport` — aggregated quality report with metadata, issues array, and summary counts\n\n## Code-vs-Doc Validation\n\nRe-exports from `./inconsistency/code-vs-doc.js`:\n- `extractExports(sourceCode: string): string[]` — parses exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — verifies all extracted exports appear in `.sum` summary text via substring search\n\n## Code-vs-Code Validation\n\nRe-exports from `./inconsistency/code-vs-code.js`:\n- `checkCodeVsCode(directoryPath: string, exports: Map<string, string[]>): CodeCodeInconsistency[]` — detects duplicate symbols across multiple files within a directory, returns issues with `pattern: 'duplicate-export'`\n\n## Inconsistency Reporting\n\nRe-exports from `./inconsistency/reporter.js`:\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — constructs report with timestamp, projectRoot, filesChecked, durationMs, and summary counts by type/severity\n- `formatReportForCli(report: InconsistencyReport): string` — renders human-readable CLI output with picocolors formatting\n\n## Phantom Path Detection\n\nRe-exports from `./phantom-paths/index.js`:\n- `checkPhantomPaths(agentsMdPath: string, projectRoot: string): PhantomPathInconsistency[]` — extracts path-like strings from `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback\n\n## Density Validation\n\nRe-exports from `./density/validator.js`:\n- `validateFindability(sumContent: string): FindabilityResult` — validates identifier density in summaries (currently disabled after structured `publicInterface` removal from schema)\n- `FindabilityResult` — type representing density validation results\n\n## Module Purpose\n\nServes as barrel export aggregating all quality validation capabilities: code-documentation consistency checks (`extractExports`, `checkCodeVsDoc`), duplicate symbol detection (`checkCodeVsCode`), phantom path resolution (`checkPhantomPaths`), report synthesis (`buildInconsistencyReport`, `formatReportForCli`), and density validation (`validateFindability`). Called by Phase 1 post-processing to emit `InconsistencyReport` with counts logged to `.agents-reverse-engineer/progress.log` and included in telemetry run logs.\n### types.ts\n**Purpose:** Defines discriminated union types for quality analysis inconsistency detection and reporting across code-vs-doc, code...\n\n**Defines discriminated union types for quality analysis inconsistency detection and reporting across code-vs-doc, code-vs-code, and phantom-path validation domains.**\n\n## Exported Types\n\n**InconsistencySeverity**: String literal union `'info' | 'warning' | 'error'` categorizing inconsistency impact levels.\n\n**CodeDocInconsistency**: Interface with `type: 'code-vs-doc'` discriminant representing mismatches between source code exports and `.sum` documentation content. Contains:\n- `severity: InconsistencySeverity`\n- `filePath: string` — source file path\n- `sumPath: string` — corresponding `.sum` file path\n- `description: string`\n- `details.missingFromDoc: string[]` — exported symbols absent from `.sum` summary\n- `details.missingFromCode: string[]` — symbols mentioned in `.sum` but not found in source\n- `details.purposeMismatch?: string` — purpose statement contradicting observable behavior\n\n**CodeCodeInconsistency**: Interface with `type: 'code-vs-code'` discriminant representing conflicts across multiple source files (e.g., duplicate exports). Contains:\n- `severity: InconsistencySeverity`\n- `files: string[]` — paths to conflicting files\n- `description: string`\n- `pattern: string` — detected pattern identifier (e.g., `'duplicate-export'`)\n\n**PhantomPathInconsistency**: Interface with `type: 'phantom-path'` discriminant representing unresolvable path references in `AGENTS.md` files. Contains:\n- `severity: InconsistencySeverity`\n- `agentsMdPath: string` — path to `AGENTS.md` containing phantom reference\n- `description: string`\n- `details.referencedPath: string` — phantom path as written in document\n- `details.resolvedTo: string` — resolution attempt context (project root or `AGENTS.md` location)\n- `details.context: string` — line of text containing phantom reference\n\n**Inconsistency**: Discriminated union type `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` enabling exhaustive pattern matching via `type` discriminant.\n\n**InconsistencyReport**: Structured analysis output interface containing:\n- `metadata.timestamp: string` — ISO 8601 execution timestamp\n- `metadata.projectRoot: string` — absolute project root path\n- `metadata.filesChecked: number` — count of analyzed files\n- `metadata.durationMs: number` — analysis duration in milliseconds\n- `issues: Inconsistency[]` — array of detected inconsistencies\n- `summary.total: number` — total inconsistency count\n- `summary.codeVsDoc: number` — code-vs-doc inconsistency count\n- `summary.codeVsCode: number` — code-vs-code inconsistency count\n- `summary.phantomPaths: number` — phantom path inconsistency count\n- `summary.errors: number` — error-severity count\n- `summary.warnings: number` — warning-severity count\n- `summary.info: number` — info-severity count\n\n## Integration Points\n\nConsumed by validators in `src/quality/inconsistency/` (`code-vs-doc.ts`, `code-vs-code.ts`) and `src/quality/phantom-paths/validator.ts` for constructing typed inconsistency objects. `InconsistencyReport` returned by `src/quality/inconsistency/reporter.ts` aggregation logic and logged via `src/orchestration/progress.ts` streaming reporter.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### density/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub findability validation module, currently disabled after removal of structured `publicInterface` metadata from `.sum` file schema. Preserved for future re-implementation via post-processing symbol extraction.\n\n## Contents\n\n### [validator.ts](./validator.ts)\nExports `validateFindability()` stub returning empty `FindabilityResult[]` array and `FindabilityResult` interface defining validation outcome structure with `filePath`, `symbolsTested`, `symbolsFound`, `symbolsMissing`, `score` fields.\n\n## Architecture\n\n**Validation Approach (Disabled):** Originally performed string-based substring matching to verify exported symbols from `.sum` files appeared in parent `AGENTS.md` content without LLM calls. Logic removed when `SumFileContent.metadata.publicInterface` field was deleted from schema.\n\n**Current State:** `validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>)` unconditionally returns `[]` with parameters prefixed by underscores indicating unused status. Function signature retained for future structured extraction support via post-processing passes.\n\n## Integration Points\n\nConsumed by `src/quality/index.ts` which aggregates quality validators (`validateCodeDocConsistency`, `validateCodeCodeConsistency`, `validatePhantomPaths`, `validateFindability`). Produces no findings in current implementation but return type `FindabilityResult[]` preserved in reporting pipeline.\n\n**Type Dependencies:**\n- `SumFileContent` imported from `../../generation/writers/sum.js` (verified path)\n- `FindabilityResult` consumed by quality reporting aggregator\n\n## Behavioral Contracts\n\n**Return Value:** Always `[]` (empty array) indicating zero validation findings.\n\n**Score Calculation (Historical):** Ratio of `symbolsFound.length / symbolsTested.length` yielding `0.0` (no symbols) to `1.0` (all symbols present) when validation was active.\n### inconsistency/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\n**Detects three classes of code-documentation drift: code-vs-doc (exports missing from .sum files), code-vs-code (duplicate symbol exports within directory scope), and phantom-paths (broken file references in AGENTS.md), aggregating findings into structured InconsistencyReport objects with plain-text CLI formatting.**\n\n## Contents\n\n### Detection Modules\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts identifiers from TypeScript/JavaScript source via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` compares extracted symbols against SumFileContent.summary substring presence, returns CodeDocInconsistency with `missingFromDoc[]` when exports absent from documentation.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` aggregates exports via `Map<string, string[]>` across scoped file group, returns CodeCodeInconsistency array flagging symbols exported from multiple files with `pattern: 'duplicate-export'` sentinel, relies on caller to enforce per-directory scoping.\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates Inconsistency discriminated union into InconsistencyReport with summary counts across type/severity dimensions, `formatReportForCli()` renders plain-text output without picocolors dependency for testability.\n\n## File Relationships\n\ncode-vs-code.ts imports `extractExports()` from code-vs-doc.ts for shared regex-based export extraction. Both detection modules return type-specific inconsistency objects (CodeDocInconsistency, CodeCodeInconsistency) consumed by reporter.ts via Inconsistency discriminated union. Reporter aggregates findings from both modules into unified InconsistencyReport structure with metadata (timestamp, projectRoot, filesChecked, durationMs) and typed summary counts.\n\n## Behavioral Contracts\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nCaptures identifiers from `export function`, `export const`, `export default class`, `export type`, `export interface`, `export enum`. Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n### Inconsistency Type Discriminants\n- `code-vs-doc`: requires `filePath`, `sumPath`, `details.missingFromDoc[]`\n- `code-vs-code`: requires `files[]`, `pattern: 'duplicate-export'`\n- `phantom-path`: requires `agentsMdPath`, `details.referencedPath`\n\n### Severity Levels\nMapped to CLI tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`\n\n## Detection Limitations\n\n**code-vs-doc**: Substring matching (`sumText.includes(e)`) yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**code-vs-code**: Name-only comparison without AST analysis cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions. Requires caller-enforced per-directory scoping to prevent false positives across module boundaries.\n\n**Shared**: Regex-based extraction misses complex export patterns (destructured, namespace, dynamic). Both modules operate as pure heuristics without AI service calls.\n### phantom-paths/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable file path references in generated `AGENTS.md` documentation through regex extraction, multi-strategy filesystem resolution with TypeScript/JavaScript extension fallback, and `PhantomPathInconsistency` issue reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Re-exports `checkPhantomPaths` from `validator.js` as barrel export for phantom path detection subsystem within parent `src/quality/` pipeline.\n\n**[validator.ts](./validator.ts)** — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` scanning AGENTS.md text via `PATH_PATTERNS` regex array (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location resolution (agentsMdDir-relative, projectRoot-relative, .ts fallback for both), filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), extracts 120-char contextLine from content on resolution failure, deduplicates via Set, returns `PhantomPathInconsistency[]` with severity='warning', type='phantom-path', referencedPath, resolvedTo, contextLine details.\n\n## Path Extraction Strategy\n\n`PATH_PATTERNS` contains three RegExp patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link syntax\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures code-formatted paths starting with src/, ./, ../\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures src/ paths following trigger keywords\n\n`SKIP_PATTERNS` excludes seven non-file reference types: node_modules, .git, HTTP(S) URLs, template placeholders (`{{`, `${`), glob wildcards, brace expansion syntax.\n\n## Resolution Protocol\n\nFor each extracted path, `checkPhantomPaths` attempts `existsSync()` validation at four filesystem locations in sequence:\n1. `path.resolve(agentsMdDir, rawPath)` — relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — relative to project root (handles absolute-style src/ paths)\n3. agentsMdDir-relative path with .js → .ts extension substitution\n4. projectRoot-relative path with .js → .ts extension substitution\n\nStops at first successful resolution; reports unresolved path as `PhantomPathInconsistency` with contextLine extracted via `lines.find((l) => l.includes(rawPath))`, trimmed and sliced to 120-char maximum.\n\n## Behavioral Contracts\n\n**Path extraction patterns (validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation suite detecting three classes of documentation drift: code-vs-doc (exported symbols missing from `.sum` files), code-vs-code (duplicate exports within directory scope), and phantom-paths (unresolvable file references in `AGENTS.md`). Aggregates findings into `InconsistencyReport` objects with severity stratification and plain-text CLI formatting.\n\n## Contents\n\n### Barrel Exports\n\n**[index.ts](./index.ts)** — Re-exports `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` from types.ts; `extractExports()`, `checkCodeVsDoc()` from inconsistency/code-vs-doc.js; `checkCodeVsCode()` from inconsistency/code-vs-code.js; `buildInconsistencyReport()`, `formatReportForCli()` from inconsistency/reporter.js; `checkPhantomPaths()` from phantom-paths/index.js; `validateFindability()`, `FindabilityResult` from density/validator.js.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency` (code-vs-doc | code-vs-code | phantom-path) with shared `severity: 'info' | 'warning' | 'error'` field. `CodeDocInconsistency` contains `filePath`, `sumPath`, `details.missingFromDoc[]`. `CodeCodeInconsistency` contains `files[]`, `pattern: 'duplicate-export'`. `PhantomPathInconsistency` contains `agentsMdPath`, `details.referencedPath`, `details.resolvedTo`, `details.context`. `InconsistencyReport` aggregates `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, `summary` counts by type/severity.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — Implements code-vs-doc detection via regex export extraction and substring matching, code-vs-code duplicate symbol detection across per-directory file groups, and report aggregation with plain-text formatting. Exports `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`.\n\n**[phantom-paths/](./phantom-paths/)** — Scans `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location filesystem resolution with `.ts`/`.js` fallback, reports unresolved references as `PhantomPathInconsistency` with 120-char contextLine. Exports `checkPhantomPaths()`.\n\n**[density/](./density/)** — Stub module returning empty `FindabilityResult[]` array, disabled after removal of `SumFileContent.metadata.publicInterface` schema field. Exports `validateFindability()` signature for future symbol extraction support.\n\n## Validation Pipeline\n\n**Code-vs-Doc:** `extractExports()` parses source via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies extracted symbols appear in `SumFileContent.summary` via substring search, returns `CodeDocInconsistency` with `missingFromDoc[]` on detection gaps.\n\n**Code-vs-Code:** `checkCodeVsCode()` aggregates exports into `Map<string, string[]>` across scoped file group, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Requires caller-enforced per-directory scoping to prevent cross-module false positives.\n\n**Phantom Paths:** `checkPhantomPaths()` extracts paths from `AGENTS.md` via three regex patterns, attempts `existsSync()` resolution at agentsMdDir-relative, projectRoot-relative, and `.ts` fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), reports unresolved references with severity='warning'.\n\n**Report Synthesis:** `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with metadata (timestamp, projectRoot, filesChecked, durationMs) and summary counts (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info). `formatReportForCli()` renders plain-text output with `[ERROR]`/`[WARN]`/`[INFO]` severity tags.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` Phase 1 post-processing: constructs `InconsistencyReport` via `buildInconsistencyReport()` after `.sum` generation completes, logs summary counts to `.agents-reverse-engineer/progress.log` via `src/orchestration/progress.ts` streaming reporter, includes report metadata in telemetry run logs via `src/ai/telemetry/run-log.ts`.\n\n**Upstream Inputs:**\n- Source files for `extractExports()` regex parsing\n- `.sum` files via `SumFileContent` schema from `src/generation/writers/sum.ts`\n- `AGENTS.md` files for `checkPhantomPaths()` scanning\n- Export maps from `src/imports/extractor.ts` for `checkCodeVsCode()` aggregation\n\n**Downstream Consumers:**\n- `src/orchestration/progress.ts` logs summary counts via `formatReportForCli()`\n- `src/ai/telemetry/run-log.ts` persists report metadata to NDJSON run logs\n- CLI output displays validation results after Phase 1 completion\n\n## Behavioral Contracts\n\n**Export extraction regex (inconsistency/code-vs-doc.ts):**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path extraction patterns (phantom-paths/validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (phantom-paths/validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n\n**Severity levels:** `'info' | 'warning' | 'error'` mapped to CLI tags `[INFO]`, `[WARN]`, `[ERROR]`.\n\n**Inconsistency type discriminants:** `code-vs-doc`, `code-vs-code`, `phantom-path` enable exhaustive pattern matching via `type` field.\n\n## Detection Limitations\n\n**Regex-based export extraction:** Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n**Substring matching:** `sumText.includes(e)` yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**Code-vs-code name-only comparison:** Cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions without AST context.\n\n**Phantom path resolution:** Four-location strategy (`agentsMdDir`, `projectRoot`, .ts fallback) does not resolve symlinks, aliased imports, or paths requiring module resolution algorithms.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1958,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 13105,
      "latencyMs": 42885,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.185Z",
      "prompt": "Generate AGENTS.md for directory: \"src/config\" (config)\n\n## File Summaries (3 files)\n\n### defaults.ts\n**Purpose:** defaults.ts exports configuration constants and dynamic concurrency calculation for agents-reverse-engineer's resourc...\n\n**defaults.ts exports configuration constants and dynamic concurrency calculation for agents-reverse-engineer's resource-adaptive worker pool.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** — Computes default worker pool concurrency by multiplying `os.availableParallelism()` (or `os.cpus().length`) by `CONCURRENCY_MULTIPLIER` (5), then clamping the result between `MIN_CONCURRENCY` (2) and the minimum of `MAX_CONCURRENCY` (20) and a memory-derived cap. Memory cap formula: `Math.floor((os.totalmem() * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)` where `MEMORY_FRACTION` is 0.5 and `SUBPROCESS_HEAP_GB` is 0.512, ensuring subprocesses consume at most 50% of total system RAM given 512MB heap per subprocess (matching `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts).\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS: readonly string[]** — Array of 18 directory names excluded from file discovery: `'node_modules'`, `'vendor'`, `'.git'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`, `'.agents-reverse-engineer'`, `'.agents'`, `'.planning'`, `'.claude'`, `'.opencode'`, `'.gemini'`. Last 6 entries are AI assistant tooling directories.\n\n**DEFAULT_EXCLUDE_PATTERNS: readonly string[]** — Gitignore-style glob patterns excluding 26 file types: AI documentation files (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` with `**/` recursive variants), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), dotfiles (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`), and generated artifacts (`*.log`, `*.sum`, `**/*.sum`, `**/SKILL.md`). Comment notes that dotfiles require glob pattern matching since `path.extname()` returns empty string.\n\n**DEFAULT_BINARY_EXTENSIONS: readonly string[]** — Array of 26 binary file extensions categorized as images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled files (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE: number** — File size threshold of `1024 * 1024` bytes (1MB) triggering binary detection warnings.\n\n**DEFAULT_CONFIG: object** — Nested configuration object with structure matching Zod schema in schema.ts: `exclude` property containing `patterns`, `vendorDirs`, `binaryExtensions` arrays (spread from corresponding DEFAULT_ constants); `options` property with `followSymlinks: false` and `maxFileSize: DEFAULT_MAX_FILE_SIZE`; `output` property with `colors: true`.\n\n## Internal Constants\n\n**CONCURRENCY_MULTIPLIER: number** — Value `5` multiplied by CPU core count in getDefaultConcurrency formula.\n\n**MIN_CONCURRENCY: number** — Value `2` defining lower bound for worker pool size.\n\n**MAX_CONCURRENCY: number** — Value `20` defining upper bound matching Zod schema `.max(20)` constraint in schema.ts.\n\n**SUBPROCESS_HEAP_GB: number** — Value `0.512` representing 512MB heap limit per subprocess, matching `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in subprocess.ts.\n\n**MEMORY_FRACTION: number** — Value `0.5` representing 50% memory allocation threshold for subprocess pool in memory cap calculation.\n\n## Resource Adaptive Strategy\n\nThe concurrency formula `clamp(cores * 5, MIN, min(memCap, MAX))` prevents RAM exhaustion in constrained environments (WSL, containers) by computing `memCap = floor(totalMemGB * 0.5 / 0.512)`. On systems with 4GB RAM: `memCap = floor(4 * 0.5 / 0.512) = 3`, limiting concurrency to 3 even if CPU count suggests higher value. On systems with < 1GB RAM, `memCap = Infinity` delegates to MAX_CONCURRENCY cap. The 0.512GB denominator synchronizes with subprocess heap limit enforced in subprocess.ts via `NODE_OPTIONS` environment variable.\n### loader.ts\n**Purpose:** loadConfig() orchestrates YAML configuration parsing from `.agents-reverse-engineer/config.yaml` with Zod schema vali...\n\n**loadConfig() orchestrates YAML configuration parsing from `.agents-reverse-engineer/config.yaml` with Zod schema validation, ConfigError-wrapped failure reporting, default fallback, and optional trace emission via ITraceWriter.**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nLoads configuration from `{root}/.agents-reverse-engineer/config.yaml` by reading file content, parsing YAML via `parse()`, validating against ConfigSchema via `ConfigSchema.parse(raw)`, and returning validated Config object. On ENOENT error, returns default Config from `ConfigSchema.parse({})`. On ZodError, extracts `issue.path` and `issue.message` fields to construct multi-line error report wrapped in ConfigError. Emits `'config:loaded'` trace event with `configPath`, `model`, `concurrency` fields when tracer provided. Logs debug output with `pc.dim()` formatting when `options.debug` enabled.\n\n**configExists(root: string): Promise<boolean>**\nChecks existence of configuration file at `{root}/.agents-reverse-engineer/config.yaml` via `access(configPath, constants.F_OK)`. Returns true if file accessible, false if ENOENT caught.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `.agents-reverse-engineer/` directory via `mkdir(configDir, { recursive: true })` and writes `config.yaml` with commented YAML template. Content includes sections for `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions`, `options.followSymlinks`, `options.maxFileSize`, `output.colors`, `ai.backend`, `ai.model`, `ai.timeoutMs`, `ai.maxRetries`, `ai.concurrency`, `ai.telemetry.keepRuns`. Applies DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE constants from defaults module. Calls `yamlScalar()` on pattern values to escape YAML special characters.\n\n**yamlScalar(value: string): string**\nQuotes string values containing YAML special characters matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/`. Escapes backslashes (`\\\\`) and double-quotes (`\\\"`) when quoting applied. Returns unmodified value when no special characters detected.\n\n## ConfigError Class\n\n**ConfigError extends Error** with additional properties `filePath: string` and `cause?: Error`. Constructor accepts `message`, `filePath`, optional `cause`. Sets `name` property to `'ConfigError'`.\n\n## Constants\n\n**CONFIG_DIR = '.agents-reverse-engineer'** — Directory name for configuration storage.\n\n**CONFIG_FILE = 'config.yaml'** — Configuration filename.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises` for file I/O. Imports `constants` from `node:fs` for F_OK flag. Imports `parse` and `stringify` from `yaml` library. Imports ZodError from `zod` for validation error handling. Imports `pc` from `picocolors` for terminal formatting. Imports ConfigSchema and Config type from `./schema.js`. Imports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from `./defaults.js`. Imports ITraceWriter from `../orchestration/trace.js`.\n\n## Error Handling Strategy\n\nloadConfig() distinguishes three error cases via instanceof checks and `(err as NodeJS.ErrnoException).code` inspection:\n1. ENOENT errors trigger default config return via `ConfigSchema.parse({})`\n2. ZodError instances map `err.issues[]` to formatted validation report and wrap in ConfigError\n3. Existing ConfigError instances re-thrown without modification\n4. All other errors wrapped in ConfigError with YAML parse failure message\n\n## Trace Event Contract\n\nEmits trace event with type `'config:loaded'` containing fields:\n- `configPath: string` — Relative path from root or literal `'(defaults)'`\n- `model: string` — Value from `config.ai.model`\n- `concurrency: number` — Value from `config.ai.concurrency`\n\n## Template Generation Pattern\n\nwriteDefaultConfig() constructs YAML content via template literals with embedded array mapping:\n```typescript\nvendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n```\nProduces indented YAML list items with 4-space prefix. Uses `yamlScalar()` wrapper for patterns array to escape glob special characters like `*`.\n### schema.ts\n**Purpose:** ConfigSchema defines Zod validation schemas for `.agents-reverse-engineer/config.yaml` with nested object structures ...\n\n**ConfigSchema defines Zod validation schemas for `.agents-reverse-engineer/config.yaml` with nested object structures for exclusion rules, discovery options, output formatting, and AI service configuration, exporting ConfigSchema, Config, and section-specific types (ExcludeConfig, OptionsConfig, OutputConfig, AIConfig).**\n\n## Schema Hierarchy\n\nConfigSchema composes four nested schemas via z.object():\n- ExcludeSchema validates `exclude` section with `patterns: string[]`, `vendorDirs: string[]`, `binaryExtensions: string[]`\n- OptionsSchema validates `options` section with `followSymlinks: boolean`, `maxFileSize: number`\n- OutputSchema validates `output` section with `colors: boolean`\n- AISchema validates `ai` section with `backend: 'claude' | 'gemini' | 'opencode' | 'auto'`, `model: string`, `timeoutMs: number`, `maxRetries: number`, `concurrency: number`, nested `telemetry: { keepRuns: number }`\n\nEach schema calls `.default({})` to enable partial overrides while preserving unspecified fields.\n\n## Exported Types\n\n```typescript\nexport const ConfigSchema: z.ZodObject<{\n  exclude: typeof ExcludeSchema,\n  options: typeof OptionsSchema,\n  output: typeof OutputSchema,\n  ai: typeof AISchema\n}>\n\nexport type Config = z.infer<typeof ConfigSchema>\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>\nexport type OptionsConfig = z.infer<typeof OptionsSchema>\nexport type OutputConfig = z.infer<typeof OutputSchema>\nexport type AIConfig = z.infer<typeof AISchema>\n```\n\nConfig serves as the inferred TypeScript type for validated YAML config objects consumed by `src/config/loader.ts`.\n\n## Default Value Integration\n\nExcludeSchema.patterns defaults to spread of `DEFAULT_EXCLUDE_PATTERNS` from `./defaults.js`. ExcludeSchema.vendorDirs defaults to spread of `DEFAULT_VENDOR_DIRS`. ExcludeSchema.binaryExtensions defaults to spread of `DEFAULT_BINARY_EXTENSIONS`. OptionsSchema.maxFileSize defaults to `DEFAULT_MAX_FILE_SIZE`. AISchema.concurrency defaults to `getDefaultConcurrency` function result (dynamic CPU/memory detection).\n\nSpread operators (`[...DEFAULT_VENDOR_DIRS]`) create shallow copies preventing shared array mutation across config instances.\n\n## Validation Constraints\n\nAISchema.backend constrains to z.enum(['claude', 'gemini', 'opencode', 'auto']) rejecting invalid backend strings. AISchema.timeoutMs applies z.number().positive() requiring >0 milliseconds. AISchema.maxRetries applies z.number().min(0) requiring non-negative integers. AISchema.concurrency applies z.number().min(1).max(20) clamping parallelism to 1-20 workers. OptionsSchema.maxFileSize applies z.number().positive() requiring >0 byte limit. AISchema.telemetry.keepRuns applies z.number().min(0) requiring non-negative run retention count.\n\n## Usage Pattern\n\n```typescript\n// Parse with defaults\nconst config = ConfigSchema.parse({});\n\n// Parse with partial overrides\nconst config = ConfigSchema.parse({\n  exclude: { patterns: ['*.log'] },\n  ai: { backend: 'claude', model: 'opus' },\n});\n```\n\nZod parse() throws ZodError on validation failure. Loader applies this schema to YAML-parsed config objects before returning validated Config instances.\n\n## Import Map (verified — use these exact paths)\n\nloader.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# config\n\nExports YAML-based configuration loading with Zod validation, resource-adaptive concurrency calculation, and default constant definitions for gitignore patterns, vendor directories, binary extensions, and AI service parameters.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Defines ConfigSchema (Zod) with nested ExcludeSchema (patterns/vendorDirs/binaryExtensions arrays), OptionsSchema (followSymlinks boolean, maxFileSize number), OutputSchema (colors boolean), AISchema (backend enum, model string, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Exports Config, ExcludeConfig, OptionsConfig, OutputConfig, AIConfig types. Applies validation constraints: backend restricted to `'claude' | 'gemini' | 'opencode' | 'auto'`, timeoutMs requires positive integer, concurrency clamped to 1-20, maxFileSize requires positive integer, keepRuns requires non-negative integer.\n\n**[loader.ts](./loader.ts)** — Exports loadConfig(root, options?) reading `.agents-reverse-engineer/config.yaml`, parsing YAML, validating via ConfigSchema.parse(), emitting `config:loaded` trace event (configPath/model/concurrency fields), returning validated Config or defaults on ENOENT, wrapping ZodError/parse failures in ConfigError. Exports configExists(root) checking file accessibility via fs.access(). Exports writeDefaultConfig(root) generating commented YAML template with DEFAULT_EXCLUDE_PATTERNS/DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS arrays, yamlScalar() escaping special characters.\n\n**[defaults.ts](./defaults.ts)** — Exports getDefaultConcurrency() computing worker pool size via `clamp(os.availableParallelism() * CONCURRENCY_MULTIPLIER, MIN_CONCURRENCY, min(MAX_CONCURRENCY, memCap))` where memCap = `floor(os.totalmem() * MEMORY_FRACTION / SUBPROCESS_HEAP_GB)` with MEMORY_FRACTION=0.5, SUBPROCESS_HEAP_GB=0.512, CONCURRENCY_MULTIPLIER=5, MIN_CONCURRENCY=2, MAX_CONCURRENCY=20. Exports DEFAULT_VENDOR_DIRS (18 directories: node_modules/vendor/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini), DEFAULT_EXCLUDE_PATTERNS (26 patterns: AGENTS.md/CLAUDE.md/OPENCODE.md/GEMINI.md/lock files/dotfiles/logs/sum files), DEFAULT_BINARY_EXTENSIONS (26 extensions: images/archives/executables/media/documents/fonts/compiled files), DEFAULT_MAX_FILE_SIZE (1MB), DEFAULT_CONFIG (composite object spreading constants).\n\n## Data Flow\n\n1. CLI commands invoke loadConfig(root, { tracer, debug }) from loader.ts\n2. loader.ts reads `.agents-reverse-engineer/config.yaml`, parses YAML, validates via ConfigSchema.parse() from schema.ts\n3. schema.ts applies defaults from defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_EXCLUDE_PATTERNS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency())\n4. On validation failure, loader.ts extracts ZodError.issues[] and wraps in ConfigError with formatted message\n5. On success, loader.ts emits `config:loaded` trace event and returns Config object to CLI orchestrators\n6. writeDefaultConfig() generates YAML template by embedding DEFAULT_ constants and calling yamlScalar() for pattern escaping\n\n## Resource Adaptive Concurrency\n\ngetDefaultConcurrency() prevents RAM exhaustion by computing memory cap: on 4GB systems, `memCap = floor(4 * 0.5 / 0.512) = 3` limits concurrency to 3 workers despite CPU count * 5 formula suggesting higher values. SUBPROCESS_HEAP_GB synchronizes with `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for subprocess resource limiting. Formula `cores * CONCURRENCY_MULTIPLIER` optimized for I/O-bound AI subprocess workloads with high wait-to-compute ratios.\n\n## Error Handling Strategy\n\nloader.ts distinguishes three error cases: ENOENT errors return `ConfigSchema.parse({})` defaults without throwing, ZodError instances map issues[] to multi-line validation report wrapped in ConfigError, existing ConfigError instances re-thrown without modification. ConfigError extends Error adding filePath: string and optional cause: Error properties for stack trace preservation.\n\n## File System Contracts\n\n- Configuration directory: `.agents-reverse-engineer/`\n- Configuration file: `config.yaml` (YAML format with commented sections)\n- Telemetry logs: `.agents-reverse-engineer/logs/` (referenced by AISchema.telemetry.keepRuns)\n- Trace output: `.agents-reverse-engineer/traces/` (referenced by ITraceWriter interface)\n\n## Behavioral Contracts\n\n**YAML Special Character Escaping:**\nyamlScalar() quotes values matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` and escapes `\\\\` → `\\\\\\\\`, `\\\"` → `\\\\\\\"` within quoted strings.\n\n**Default Exclusion Patterns:**\n```typescript\nDEFAULT_EXCLUDE_PATTERNS = [\n  'AGENTS.md', 'CLAUDE.md', 'OPENCODE.md', 'GEMINI.md',\n  '**/AGENTS.md', '**/CLAUDE.md', '**/OPENCODE.md', '**/GEMINI.md',\n  '*.lock', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\n  'bun.lock', 'bun.lockb', 'Gemfile.lock', 'Cargo.lock',\n  'poetry.lock', 'composer.lock', 'go.sum',\n  '.gitignore', '.gitattributes', '.gitkeep', '.env', '**/.env', '**/.env.*',\n  '*.log', '*.sum', '**/*.sum', '**/SKILL.md'\n]\n```\n\n**Vendor Directory Exclusions:**\n```typescript\nDEFAULT_VENDOR_DIRS = [\n  'node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__',\n  '.next', 'venv', '.venv', 'target', '.cargo', '.gradle',\n  '.agents-reverse-engineer', '.agents', '.planning',\n  '.claude', '.opencode', '.gemini'\n]\n```\n\n**Binary Extension Detection:**\n```typescript\nDEFAULT_BINARY_EXTENSIONS = [\n  '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp',\n  '.zip', '.tar', '.gz', '.rar', '.7z',\n  '.exe', '.dll', '.so', '.dylib',\n  '.mp3', '.mp4', '.wav',\n  '.pdf',\n  '.woff', '.woff2', '.ttf', '.eot',\n  '.class', '.pyc'\n]\n```\n\n**Concurrency Formula:**\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n## Cross-Module Dependencies\n\n- **schema.ts** imports defaults.ts (DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency)\n- **loader.ts** imports schema.ts (ConfigSchema, Config type), defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency), `../orchestration/trace.js` (ITraceWriter type)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1942,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12416,
      "latencyMs": 48739,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.234Z",
      "prompt": "Generate AGENTS.md for directory: \"src/rebuild\" (rebuild)\n\n## File Summaries (7 files)\n\n### checkpoint.ts\n**Purpose:** CheckpointManager provides session continuity for rebuild operations via serialized writes of per-module completion s...\n\n**CheckpointManager provides session continuity for rebuild operations via serialized writes of per-module completion status and spec file hash-based drift detection.**\n\n## Exported Symbols\n\n**CheckpointManager** — Main class managing rebuild checkpoint persistence with promise-chain write serialization to prevent corruption from concurrent worker updates.\n\n### Static Factory Methods\n\n- `static async load(outputDir: string, specFiles: Array<{ relativePath: string; content: string }>, unitNames: string[]): Promise<{ manager: CheckpointManager; isResume: boolean }>` — Loads existing checkpoint from `<outputDir>/.rebuild-checkpoint`, validates schema via `RebuildCheckpointSchema.safeParse()`, compares stored spec hashes against current hashes via `computeContentHashFromString()`, returns `isResume: true` only if checkpoint exists and no spec drift detected, otherwise creates fresh checkpoint.\n\n- `static createFresh(outputDir: string, specFiles: Array<{ relativePath: string; content: string }>, unitNames: string[]): CheckpointManager` — Constructs new checkpoint with all modules set to `status: 'pending'`, computes spec content hashes via `computeContentHashFromString()`, initializes `RebuildCheckpoint.modules` record from `unitNames[]`, sets `createdAt`/`updatedAt` to current ISO timestamp.\n\n### Instance Methods\n\n- `markDone(unitName: string, filesWritten: string[]): void` — Updates `modules[unitName]` to `{ status: 'done', completedAt: ISO timestamp, filesWritten }`, updates `data.updatedAt`, queues serialized write via `queueWrite()`.\n\n- `markFailed(unitName: string, error: string): void` — Updates `modules[unitName]` to `{ status: 'failed', error }`, updates `data.updatedAt`, queues serialized write via `queueWrite()`.\n\n- `getPendingUnits(): string[]` — Returns array of module names with `status: 'pending'` or `'failed'` (eligible for execution).\n\n- `isDone(unitName: string): boolean` — Returns true if `modules[unitName].status === 'done'`.\n\n- `async flush(): Promise<void>` — Waits for `writeQueue` promise chain to complete, ensuring all pending writes finish before returning.\n\n- `async initialize(): Promise<void>` — Creates output directory via `mkdir()` with `recursive: true`, writes initial checkpoint JSON to `<outputDir>/.rebuild-checkpoint`, swallows errors (non-critical failure).\n\n- `getData(): RebuildCheckpoint` — Returns current checkpoint data for inspection or dry-run display.\n\n### Private Implementation\n\n- `private writeQueue: Promise<void>` — Promise chain for serializing concurrent writes from pool workers (same pattern as `PlanTracker` and `TraceWriter`).\n\n- `private queueWrite(): void` — Appends serialized write to `writeQueue` via `.then()` chain, writes `JSON.stringify(this.data, null, 2)` to `checkpointPath`, swallows errors with empty catch block (non-critical).\n\n## Data Structure\n\n**RebuildCheckpoint** (from `./types.js`) — Validated via `RebuildCheckpointSchema` Zod schema, contains:\n- `version: string` — ARE version via `getVersion()`\n- `createdAt: string` / `updatedAt: string` — ISO 8601 timestamps\n- `outputDir: string` — Absolute path to rebuild output directory\n- `specHashes: Record<string, string>` — Map of spec file relative paths to SHA-256 content hashes (drift detection)\n- `modules: Record<string, ModuleStatus>` — Per-unit completion tracking\n\n**ModuleStatus** discriminated union:\n- `{ status: 'pending' }` — Not yet processed\n- `{ status: 'done', completedAt: string, filesWritten: string[] }` — Successfully completed with output file list\n- `{ status: 'failed', error: string }` — Failed with error message\n\n## Drift Detection Algorithm\n\n1. `load()` reads checkpoint JSON from `.rebuild-checkpoint`\n2. Parses and validates via `RebuildCheckpointSchema.safeParse()`, returns fresh checkpoint on failure\n3. Computes `currentHashes` map via `computeContentHashFromString()` for each spec file\n4. Compares hash count: if `Object.keys(checkpoint.specHashes).length !== Object.keys(currentHashes).length`, returns fresh checkpoint\n5. Compares individual hashes: if any `checkpoint.specHashes[path] !== currentHashes[path]`, returns fresh checkpoint\n6. Returns `isResume: true` only if all checks pass\n\n## Concurrency Safety\n\nFollows promise-chain serialization pattern from `PlanTracker` and `TraceWriter` modules to handle concurrent `markDone()`/`markFailed()` calls from worker pool:\n- All writes queued via `this.writeQueue = this.writeQueue.then(...)` pattern\n- Each `queueWrite()` appends to chain, ensuring FIFO execution order\n- `flush()` awaits `writeQueue` to guarantee completion before process exit\n- Non-critical failures swallowed with empty catch blocks (checkpoint corruption does not halt rebuild)\n\n## Dependencies\n\n- `node:fs/promises` — `writeFile()`, `readFile()`, `mkdir()`, `rm()`\n- `node:path` — Path joining for `.rebuild-checkpoint` resolution\n- `../change-detection/index.js` — `computeContentHashFromString()` for spec drift detection\n- `./types.js` — `RebuildCheckpointSchema`, `RebuildCheckpoint` type\n- `../version.js` — `getVersion()` for checkpoint version stamping\n\n## Checkpoint File Format\n\n**Location:** `<outputDir>/.rebuild-checkpoint`\n\n**Content:** Pretty-printed JSON (`JSON.stringify(data, null, 2)`) with schema:\n```typescript\n{\n  version: string,           // ARE version\n  createdAt: string,        // ISO 8601 timestamp\n  updatedAt: string,        // ISO 8601 timestamp\n  outputDir: string,        // Absolute path\n  specHashes: {             // Drift detection\n    \"specs/SPEC.md\": \"a3f5d8e9...\",\n    \"specs/module.md\": \"7b2c1a4f...\"\n  },\n  modules: {                // Per-unit tracking\n    \"ModuleName\": { status: 'pending' | 'done' | 'failed', ... }\n  }\n}\n```\n\n## Usage Pattern\n\nConstructor is private — clients must use `load()` or `createFresh()` factory methods:\n\n```typescript\nconst { manager, isResume } = await CheckpointManager.load(outputDir, specFiles, unitNames);\nif (!isResume) {\n  await manager.initialize();\n}\n\n// In worker pool callbacks:\nmanager.markDone(unitName, filesWritten);\n// or\nmanager.markFailed(unitName, errorMsg);\n\n// Before exit:\nawait manager.flush();\n```\n### index.ts\n**Purpose:** `src/rebuild/index.ts` re-exports all public types, schemas, functions, and classes for the rebuild pipeline through ...\n\n**`src/rebuild/index.ts` re-exports all public types, schemas, functions, and classes for the rebuild pipeline through a barrel export pattern.**\n\n## Exported Types\n\n- **`RebuildCheckpoint`** — Type representing a rebuild checkpoint state from `./types.js`\n- **`RebuildUnit`** — Type representing a unit of work in the rebuild process from `./types.js`\n- **`RebuildPlan`** — Type representing a complete rebuild execution plan from `./types.js`\n- **`RebuildResult`** — Type representing the outcome of a rebuild operation from `./types.js`\n\n## Exported Schemas\n\n- **`RebuildCheckpointSchema`** — Zod schema for validating `RebuildCheckpoint` structures from `./types.js`\n\n## Exported Functions\n\n- **`readSpecFiles(specPath: string): Promise<...>`** — Reads specification files from the given path from `./spec-reader.js`\n- **`partitionSpec(...): ...`** — Partitions a specification into rebuild units from `./spec-reader.js`\n- **`parseModuleOutput(...): ...`** — Parses AI-generated module output into structured data from `./output-parser.js`\n- **`buildRebuildPrompt(...): string`** — Constructs prompts for the rebuild AI subprocess from `./prompts.js`\n- **`executeRebuild(options: RebuildExecutionOptions): Promise<RebuildResult>`** — Orchestrates the complete rebuild pipeline execution from `./orchestrator.js`\n\n## Exported Classes\n\n- **`CheckpointManager`** — Manages checkpoint persistence and recovery for incremental rebuild progress from `./checkpoint.js`\n\n## Exported Constants\n\n- **`REBUILD_SYSTEM_PROMPT`** — System prompt string template for rebuild AI interactions from `./prompts.js`\n\n## Exported Type Aliases\n\n- **`RebuildExecutionOptions`** — Type alias for options passed to `executeRebuild()` from `./orchestrator.js`\n\n## Module Organization\n\nThis barrel export aggregates the rebuild pipeline's public interface across six internal modules: type definitions (`types.js`), specification parsing (`spec-reader.js`), AI output parsing (`output-parser.js`), checkpoint management (`checkpoint.js`), prompt engineering (`prompts.js`), and orchestration logic (`orchestrator.js`). Consumers import from this index file rather than individual submodules to access rebuild functionality.\n### orchestrator.ts\n**Purpose:** orchestrator.ts orchestrates the rebuild pipeline by partitioning spec files into ordered groups (RebuildUnit[]), exe...\n\n**orchestrator.ts orchestrates the rebuild pipeline by partitioning spec files into ordered groups (RebuildUnit[]), executing them sequentially by order value with concurrent AI-driven file generation within each group via runPool, accumulating export signatures as built context for subsequent groups, and managing checkpoint persistence for resumability.**\n\n## Exported Interface\n\n**executeRebuild(aiService: AIService, projectRoot: string, options: RebuildExecutionOptions): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }>**\nMain pipeline executor that reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads/creates checkpoint via `CheckpointManager.load()`, groups units by `order` field, processes each order group sequentially while running units within each group concurrently via `runPool()`, accumulates built context from generated files, and returns summary counts.\n\n**RebuildExecutionOptions**\nConfiguration interface with fields:\n- `outputDir: string` — absolute path to output directory\n- `concurrency: number` — max concurrent AI calls within each order group\n- `failFast?: boolean` — abort on first failure\n- `force?: boolean` — wipe output directory and start fresh\n- `debug?: boolean` — enable verbose debug logging\n- `tracer?: ITraceWriter` — trace writer for concurrency debugging\n- `progressLog?: ProgressLog` — progress log for tail -f monitoring\n\n## Rebuild Pipeline Stages\n\n**Stage 1: Spec Loading and Partitioning**\nCalls `readSpecFiles(projectRoot)` to load `.md` spec files from `specs/` directory, then `partitionSpec(specFiles)` to extract RebuildUnit objects with `name`, `order`, `outPath`, `dependencies`, `specContent` fields.\n\n**Stage 2: Checkpoint Management**\nInvokes `CheckpointManager.load(outputDir, specFiles, unitNames)` returning `{ manager, isResume }`. If `isResume === true`, logs count of already-complete modules via `checkpoint.getPendingUnits()`. Calls `checkpoint.initialize()` to write `.rebuild-checkpoint.json` to disk. Filters units via `checkpoint.isDone(unit.name)` to build `pendingUnits[]` array, increments `modulesSkipped` for completed units.\n\n**Stage 3: Order Group Execution**\nGroups pendingUnits by `unit.order` value into `Map<number, RebuildUnit[]>`, sorts keys ascending via `[...orderGroups.keys()].sort((a, b) => a - b)`. For each orderValue sequentially:\n1. Emits `tracer.emit({ type: 'phase:start', phase: 'rebuild-order-${orderValue}', taskCount, concurrency })`\n2. Creates pool tasks mapping each unit to async function calling `buildRebuildPrompt(unit, fullSpec, builtContext)` → `aiService.call()` → `parseModuleOutput(response.text)` → `writeFile()` for each parsed file → `checkpoint.markDone(unit.name, filesWritten)` → returns RebuildResult\n3. Calls `runPool(groupTasks, { concurrency, failFast, tracer, phaseLabel, taskLabels }, onCompleteCallback)`\n4. onComplete callback increments `modulesProcessed`/`modulesFailed`, calls `reporter.onFileDone()` or `reporter.onFileError()`, invokes `checkpoint.markFailed(unitName, errorMsg)` on failure\n5. Emits `tracer.emit({ type: 'phase:end', phase, durationMs, tasksCompleted, tasksFailed })`\n\n**Stage 4: Context Accumulation**\nAfter each order group completes, reads all `filesWrittenInGroup` via `readFile(path.join(outputDir, filePath))`, skips non-source files (`.md`, `.json`, `.yml`), appends content to `builtContext` string with `// === ${filePath} ===` delimiter. If `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits by `\\n// === ` delimiter, keeps recent sections in full, truncates older sections to first `TRUNCATED_HEAD_LINES` (20) lines with `// ... (truncated)` marker.\n\n**Stage 5: Finalization**\nCalls `checkpoint.flush()` to persist final state, returns summary object with `modulesProcessed`, `modulesFailed`, `modulesSkipped` counts.\n\n## Progress Reporting\n\nCreates `ProgressReporter(pendingUnits.length, 0, progressLog)` for streaming output. Calls `reporter.onFileStart(unit.name)` before AI call, `reporter.onFileDone(unitName, durationMs, tokensIn, tokensOut, model, cacheReadTokens, cacheCreationTokens)` on success, `reporter.onFileError(unitName, errorMsg)` on failure. All progress events mirrored to `progressLog.write()` for `tail -f` monitoring.\n\n## Dependencies\n\nImports `runPool` and `ProgressReporter` from `../orchestration/index.js`, `CheckpointManager` from `./checkpoint.js`, `readSpecFiles` and `partitionSpec` from `./spec-reader.js`, `parseModuleOutput` from `./output-parser.js`, `buildRebuildPrompt` from `./prompts.js`, `AIService` and `AIResponse` from `../ai/` types. Uses `writeFile`, `mkdir`, `readFile`, `rm` from `node:fs/promises` for disk I/O.\n\n## Error Handling\n\nThrows if `parseModuleOutput()` returns empty Map: `\"AI produced no files for unit \"${unit.name}\". Response may have used unexpected format.\"` Pool failures captured in runPool onComplete callback with `result.error?.message ?? 'Unknown error'`, logged via `checkpoint.markFailed()` and `reporter.onFileError()`. Non-critical errors during context accumulation (unreadable files) silently skipped via try-catch.\n\n## Configuration Constants\n\n**BUILT_CONTEXT_LIMIT = 100_000**\nCharacter threshold before truncating older group context to prevent unbounded memory growth.\n\n**TRUNCATED_HEAD_LINES = 20**\nNumber of lines kept from truncated files (typically imports + type declarations) when context exceeds limit.\n### output-parser.ts\n**Purpose:** Parses AI-generated multi-file rebuild responses using delimiter-based and markdown fence fallback strategies to extr...\n\n**Parses AI-generated multi-file rebuild responses using delimiter-based and markdown fence fallback strategies to extract file paths and contents into a `Map<string, string>`.**\n\n## Exported Interface\n\n**`parseModuleOutput(responseText: string): Map<string, string>`** — Primary entry point that attempts delimiter-based parsing via `parseDelimiterFormat()`, falling back to `parseFencedBlockFormat()` if no matches found, returning empty Map on total failure (caller handles error case).\n\n## Parsing Strategies\n\n**Primary Format (Delimiter-based):**\n`parseDelimiterFormat()` uses regex `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g` to extract files between `===FILE: path===` and `===END_FILE===` markers. File paths undergo `.trim()`, content preserves whitespace verbatim (no trimming).\n\n**Fallback Format (Markdown Fenced Blocks):**\n`parseFencedBlockFormat()` matches code blocks with file path annotations using `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g` pattern. Expects format:\n````\n```language:path/to/file\ncontent\n```\n````\n\n## Behavioral Contracts\n\n**Delimiter pattern (primary):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block pattern (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Content preservation:** File content captured without trimming to preserve indentation and trailing whitespace. Only file paths undergo `.trim()` normalization.\n\n**Empty Map semantics:** Returns `new Map<string, string>()` when neither format matches, requiring caller to validate non-empty result.\n\n## Integration Points\n\nUsed by rebuild orchestrator (`src/rebuild/orchestrator.ts`) to parse AI responses during multi-file project reconstruction from specification documents. Output Map keys are relative file paths, values are raw file contents for filesystem writes.\n### prompts.ts\n**Purpose:** prompts.ts defines system and user prompt construction for AI-driven project reconstruction, emitting source files wi...\n\n**prompts.ts defines system and user prompt construction for AI-driven project reconstruction, emitting source files with `===FILE:===` / `===END_FILE===` delimiters.**\n\n## Exported Constants\n\n`REBUILD_SYSTEM_PROMPT` is a string constant containing the system prompt that instructs the AI model to reconstruct source code from a project specification. The prompt enforces:\n\n- **Output format:** Each file must be wrapped in `===FILE: relative/path.ext===` and `===END_FILE===` delimiters with no markdown fencing or external commentary\n- **Quality requirements:** Code must compile, use exact type names/function signatures/constants from spec, follow described architecture, import only real modules from spec, generate production code only (no tests/stubs/placeholders), avoid inventing features or adding explanatory comments\n- **Strict compliance:** When spec defines exact names for functions/methods/types/classes/constants, those exact names MUST be used (no synonyms like `reportSuccess()` for spec's `done()`), \"Interfaces for This Phase\" section signatures must be implemented precisely, exported symbols from \"Already Built\" context must be imported and used without redefinition\n- **Context awareness:** Import from previously built modules shown in \"Already Built\" context, use their exported types/functions, match API signatures exactly\n\n## Exported Functions\n\n`buildRebuildPrompt(unit: RebuildUnit, fullSpec: string, builtContext: string | undefined): { system: string; user: string }` constructs the prompt pair for a single `RebuildUnit`. Returns object with `system` field set to `REBUILD_SYSTEM_PROMPT` and `user` field containing concatenated sections:\n\n1. **Full Specification** section with `fullSpec` parameter content\n2. **Current Phase** section with `unit.specContent` describing module to build\n3. **Already Built** section (conditionally included when `builtContext` provided) listing exported type signatures from previously built modules for import resolution\n4. **Output Format** section reminding of `===FILE:===` / `===END_FILE===` delimiter syntax and relative path requirements\n\n## Integration Points\n\nImports `RebuildUnit` type from `./types.js` which defines the structure for individual rebuild tasks. `buildRebuildPrompt` consumes `unit.specContent` field containing phase-specific specification content to isolate current build target within larger project context.\n### spec-reader.ts\n**Purpose:** spec-reader.ts reads specification markdown files from `specs/` directory and partitions them into ordered `RebuildUn...\n\n**spec-reader.ts reads specification markdown files from `specs/` directory and partitions them into ordered `RebuildUnit[]` arrays for incremental reconstruction workflows, extracting phase-level content from Build Plan sections with targeted context injection or falling back to top-level heading splits.**\n\n## Exported Functions\n\n### readSpecFiles\n\n```typescript\nasync function readSpecFiles(\n  projectRoot: string\n): Promise<Array<{ relativePath: string; content: string }>>\n```\n\nReads all `.md` files from `specs/` subdirectory, returning sorted array of `{ relativePath: 'specs/filename.md', content: string }` objects. Throws error with message `\"No spec files found in specs/. Run \\\"are specify\\\" first.\"` if directory missing or empty. Uses `readdir()` for discovery, `readFile(..., 'utf-8')` for content loading, sorts filenames alphabetically via `.sort()`.\n\n### partitionSpec\n\n```typescript\nfunction partitionSpec(\n  specFiles: Array<{ relativePath: string; content: string }>\n): RebuildUnit[]\n```\n\nConverts spec file array into ordered `RebuildUnit[]` by concatenating contents and applying two extraction strategies: (1) `extractFromBuildPlan()` seeking `## Build Plan` or `## 9. Build Plan` with `### Phase N:` subsections, (2) `extractFromTopLevelHeadings()` splitting on `^## (.+)$` regex as fallback. Validates non-empty units, logs warnings via `console.error()` for empty sections, throws error if zero units extracted: `\"Could not extract rebuild units from spec files. Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings. Check your spec file format.\"`. Sorts output by `RebuildUnit.order` ascending.\n\n## Internal Extraction Functions\n\n### extractFromBuildPlan\n\n```typescript\nfunction extractFromBuildPlan(fullContent: string): RebuildUnit[]\n```\n\nMatches Build Plan section via regex `/^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m`, extracts `### Phase (\\d+):\\s*(.+)$` subsections, detects **Change 2 format** presence via `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern to enable targeted API injection. Calls `extractSection()` to retrieve Architecture, Public API Surface, Data Structures, Behavioral Contracts sections. Calls `extractSubsections()` to parse `### ` headings within API/Data/Behavioral content. For phases in Change 2 format (with `Defines:`/`Consumes:` lists), invokes `findRelevantSubsections()` to filter API/Data/Behavioral subsections matching phase keywords, injecting only relevant content under headings `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase`. Falls back to full `## Public API Surface` inclusion for older specs without Defines/Consumes lists (graceful degradation). Prepends Architecture and filtered API content to each phase's `RebuildUnit.specContent`. Returns empty array if no `## Build Plan` heading or no phases found.\n\n### extractFromTopLevelHeadings\n\n```typescript\nfunction extractFromTopLevelHeadings(fullContent: string): RebuildUnit[]\n```\n\nFallback strategy matching all `^## (.+)$/gm` patterns, slicing content between consecutive headings. Assigns `RebuildUnit.order` as sequential position (1-indexed). Returns empty array if no headings found.\n\n### extractSubsections\n\n```typescript\nfunction extractSubsections(sectionContent: string): Map<string, string>\n```\n\nParses section content into `Map<subsectionHeading, content>` by matching `^### (.+)$/gm` and slicing text between consecutive `### ` headings. Returns map with subsection heading text (trimmed) as keys and full content (including heading line) as values.\n\n### findRelevantSubsections\n\n```typescript\nfunction findRelevantSubsections(\n  phaseContent: string,\n  subsections: Map<string, string>\n): string | null\n```\n\nExtracts keywords from phase text via three sources: (1) `Defines:` list matching `/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m`, (2) `Consumes:` list matching `/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m`, (3) file path references via `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`. Splits Defines/Consumes on `[,;]` delimiters. Performs case-insensitive fuzzy matching: checks if subsection heading contains keyword, keyword contains heading, or individual words (length >3) from keyword appear in heading. Returns concatenated matched subsections joined with `\\n\\n`, or all subsections if no keywords found (fallback), or `null` if subsection map empty.\n\n### extractSection\n\n```typescript\nfunction extractSection(\n  fullContent: string,\n  sectionName: string\n): string | null\n```\n\nExtracts named section content between `## N. SectionName` or `## SectionName` heading and next `^## ` heading via regex `/^## (?:\\\\d+\\\\.\\\\s*)?${sectionName}\\s*$/m`. Returns trimmed content string or `null` if heading not found or content empty.\n\n## Data Flow Integration\n\nCalled by `orchestrator.ts` in `src/rebuild/` module during reconstruction workflow. `readSpecFiles()` locates spec markdown files generated by `are specify` command. `partitionSpec()` transforms concatenated spec content into executable units consumed by rebuild orchestrator's sequential phase execution. Each `RebuildUnit` contains `name` (e.g., `\"Phase 3: Core Data Structures\"`), `specContent` (phase description + context sections), `order` (numeric sequencing).\n\n## Behavioral Contracts\n\n**Build Plan phase extraction regex**: `/^### Phase (\\d+):\\s*(.+)$/gm` matches phase headings with capture groups for number and name.\n\n**Change 2 format detection pattern**: `/^\\*\\*Defines:\\*\\*|^Defines:/m` identifies specs using Defines/Consumes lists for targeted API injection.\n\n**Section heading pattern**: `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m` matches numbered or unnumbered section headings.\n\n**Subsection heading pattern**: `/^### (.+)$/gm` extracts all third-level headings for subsection parsing.\n\n**Keyword extraction from lists**: Split on `/[,;]/` delimiters, trim whitespace.\n\n**File path reference extraction**: `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g` captures source file paths and module names from phase text.\n\n**Error messages**:\n- `\"No spec files found in specs/. Run \\\"are specify\\\" first.\"` (directory missing or no .md files)\n- `\"Could not extract rebuild units from spec files. Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings. Check your spec file format.\"` (zero units after extraction)\n\n**Warning format**: `[warn] Skipping empty spec section: \"${unit.name}\"` logged via `console.error()` for units with empty trimmed content.\n\n## Context Injection Strategy\n\n**Change 2 format** (with Defines/Consumes lists): Injects only subsections matching phase keywords under section headings:\n- `## Interfaces for This Phase`\n- `## Data Structures for This Phase`\n- `## Behavioral Contracts for This Phase`\n\n**Legacy format** (no Defines/Consumes): Injects full `## Public API Surface` content for all phases (graceful degradation).\n\n**Always injected**: `## Architecture` section prepended to all phases regardless of format.\n### types.ts\n**Purpose:** Defines Zod-validated checkpoint schema and TypeScript interfaces for the rebuild module's state persistence, unit pl...\n\n**Defines Zod-validated checkpoint schema and TypeScript interfaces for the rebuild module's state persistence, unit planning, and execution results.**\n\n## Exported Types and Schema\n\n`RebuildCheckpointSchema` validates checkpoint JSON structure with fields:\n- `version` (string) — checkpoint format version\n- `createdAt` (string) — ISO 8601 creation timestamp\n- `updatedAt` (string) — ISO 8601 last update timestamp\n- `outputDir` (string) — absolute path to rebuild target directory\n- `specHashes` (Record<string, string>) — SHA-256 hashes of spec files for drift detection\n- `modules` (Record<string, ModuleStatus>) — per-module completion state with nested object schema:\n  - `status` (enum: `'pending' | 'done' | 'failed'`)\n  - `completedAt` (optional string) — ISO 8601 completion timestamp\n  - `error` (optional string) — error message for failed modules\n  - `filesWritten` (optional string[]) — relative paths of generated files\n\n`RebuildCheckpoint` type inferred from `RebuildCheckpointSchema` via `z.infer<>`, represents state persisted to `.rebuild-checkpoint` file inside output directory.\n\n`RebuildUnit` interface defines single rebuild work unit:\n- `name` (string) — derived from spec section heading\n- `specContent` (string) — spec section markdown content for AI prompt\n- `order` (number) — execution sequence from Build Plan phase numbering\n\n`RebuildPlan` interface defines full rebuild plan computed before execution:\n- `specFiles` (Array<{relativePath: string, content: string}>) — spec files read from `specs/` directory\n- `units` (RebuildUnit[]) — ordered rebuild units extracted from spec content\n- `outputDir` (string) — target directory for rebuilt project\n\n`RebuildResult` interface captures single unit execution outcome:\n- `unitName` (string) — name of rebuild unit\n- `success` (boolean) — whether rebuild succeeded\n- `filesWritten` (string[]) — relative paths within output directory\n- `tokensIn` (number) — input tokens consumed\n- `tokensOut` (number) — output tokens produced\n- `cacheReadTokens` (number) — cache read tokens\n- `cacheCreationTokens` (number) — cache creation tokens\n- `durationMs` (number) — execution duration in milliseconds\n- `model` (string) — AI model used for generation\n- `error` (optional string) — error message if rebuild failed\n\n## Checkpoint Persistence Pattern\n\n`RebuildCheckpoint` serializes to `.rebuild-checkpoint` JSON file enabling resume-on-failure workflow. `specHashes` field enables detection of spec file changes requiring re-execution. `modules` record maps unit names to status enums tracking completion state across interrupted runs.\n\n## Import Map (verified — use these exact paths)\n\ncheckpoint.ts:\n  ../change-detection/index.js → computeContentHashFromString\n  ../version.js → getVersion\n\norchestrator.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../orchestration/index.js → runPool, ProgressReporter, type ProgressLog, type ITraceWriter\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# rebuild\n\nImplements AI-driven project reconstruction from specification documents via ordered phase execution with checkpoint-based resumability, concurrent file generation within dependency groups, and progressive context accumulation from previously built modules.\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — Executes three-stage rebuild workflow: spec partitioning into `RebuildUnit[]` via `partitionSpec()`, sequential order group processing with concurrent AI calls per group via `runPool()`, and progressive context accumulation from generated file exports with LRU truncation at 100k chars.\n\n**[checkpoint.ts](./checkpoint.ts)** — Provides session continuity via `CheckpointManager` class with SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), and promise-chain write serialization to `.rebuild-checkpoint` JSON file.\n\n### Specification Processing\n\n**[spec-reader.ts](./spec-reader.ts)** — Reads markdown files from `specs/` directory via `readSpecFiles()`, partitions content into `RebuildUnit[]` via Build Plan phase extraction (`### Phase N:`) or top-level heading fallback, injects targeted context (Architecture, filtered API subsections, Data Structures, Behavioral Contracts) based on Defines/Consumes keyword matching.\n\n**[output-parser.ts](./output-parser.ts)** — Extracts file paths and contents from AI responses using delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback (```` ```language:path ````), returning `Map<string, string>` for filesystem writes.\n\n### Prompt Engineering\n\n**[prompts.ts](./prompts.ts)** — Defines `REBUILD_SYSTEM_PROMPT` enforcing delimiter format, exact spec compliance (no synonym substitution), and production code constraints (no tests/stubs/placeholders), plus `buildRebuildPrompt()` constructing per-unit prompts with full spec, phase-specific content, and accumulated built context.\n\n### Type System\n\n**[types.ts](./types.ts)** — Exports `RebuildCheckpointSchema` Zod validator with `specHashes`/`modules` fields, `RebuildUnit` interface with `name`/`specContent`/`order`, `RebuildPlan` with `units[]`/`outputDir`, and `RebuildResult` with token counts and `filesWritten[]`.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting all public types, schemas (`RebuildCheckpointSchema`), functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`), classes (`CheckpointManager`), and constants (`REBUILD_SYSTEM_PROMPT`).\n\n## Architecture\n\n### Sequential Order Groups with Concurrent Execution\n\n`executeRebuild()` groups `RebuildUnit[]` by `order` field into `Map<number, RebuildUnit[]>`, processes order values sequentially (ascending), executes units within each group concurrently via `runPool()` with configurable concurrency. Accumulates exported symbols from completed groups into `builtContext` string for injection into subsequent groups as \"Already Built\" context, enabling correct import resolution across phases.\n\n### Checkpoint-Based Resume Workflow\n\n`CheckpointManager.load()` compares SHA-256 hashes of current spec files against `checkpoint.specHashes`, returns `isResume: false` if hash count differs or any individual hash mismatches (drift detected). Filters `pendingUnits[]` via `checkpoint.isDone(unitName)` predicate, increments `modulesSkipped` counter for already-complete work. Workers call `checkpoint.markDone(unitName, filesWritten)` or `checkpoint.markFailed(unitName, errorMsg)` on completion, serializing writes via promise chain to prevent corruption.\n\n### Context Accumulation with LRU Truncation\n\nAfter each order group completes, `orchestrator.ts` reads all `filesWrittenInGroup` via `readFile()`, filters out non-source files (`.md`/`.json`/`.yml`), appends to `builtContext` with `// === ${filePath} ===` delimiters. When `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits on delimiter pattern, preserves recent files in full, truncates older files to first `TRUNCATED_HEAD_LINES` (20 lines) with `// ... (truncated)` marker.\n\n### Targeted API Injection (Change 2 Format)\n\n`extractFromBuildPlan()` detects `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern in phase content, extracts keywords from Defines/Consumes lists and file path references, calls `findRelevantSubsections()` with fuzzy matching (substring, word overlap) to filter API/Data/Behavioral subsections. Injects matched subsections under `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase` headings. Falls back to full `## Public API Surface` inclusion for legacy specs without Defines/Consumes (graceful degradation).\n\n## Behavioral Contracts\n\n**Delimiter format (primary output parsing):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block format (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Build Plan phase extraction:** `/^### Phase (\\d+):\\s*(.+)$/gm`\n\n**Change 2 format detection:** `/^\\*\\*Defines:\\*\\*|^Defines:/m`\n\n**Section heading pattern:** `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m`\n\n**Subsection heading pattern:** `/^### (.+)$/gm`\n\n**File path extraction:** `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`\n\n**Context delimiter format:** `// === ${filePath} ===` (used for splitting truncated context)\n\n## Integration Points\n\nInvoked by `src/cli/rebuild.ts` command handler. Consumes `../ai/index.js` `AIService` for subprocess spawning, `../orchestration/index.js` `runPool`/`ProgressReporter`/`ITraceWriter` for concurrency control, `../change-detection/index.js` `computeContentHashFromString` for drift detection. Writes checkpoint to `<outputDir>/.rebuild-checkpoint` JSON, generated source files to `<outputDir>/<relativePath>` as specified in parsed AI responses.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1616,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 16836,
      "latencyMs": 52838,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.214Z",
      "prompt": "Generate AGENTS.md for directory: \"src/integration\" (integration)\n\n## File Summaries (4 files)\n\n### detect.ts\n**Purpose:** detectEnvironments() discovers AI coding assistant installations by checking for Claude Code (`.claude/` or `CLAUDE.m...\n\n**detectEnvironments() discovers AI coding assistant installations by checking for Claude Code (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), and Aider (`.aider.conf.yml` or `.aider/`) configuration artifacts.**\n\n## Exported Functions\n\n`detectEnvironments(projectRoot: string): DetectedEnvironment[]` returns array of detected AI assistant environments by testing existence of configuration directories and files via `existsSync()`. Checks Claude Code markers (`.claude` directory OR `CLAUDE.md` file), OpenCode marker (`.opencode` directory), and Aider markers (`.aider.conf.yml` file OR `.aider` directory). Each detected environment produces `DetectedEnvironment` object with `type`, `configDir`, and `detected: true` properties.\n\n`hasEnvironment(projectRoot: string, type: EnvironmentType): boolean` returns true if specified environment type exists in project root. Delegates to `detectEnvironments()` and tests result array via `some()` predicate matching `env.type === type`.\n\n## Detection Logic\n\nClaude detection uses dual-path OR logic: `.claude/` directory existence OR `CLAUDE.md` file existence both trigger detection with `configDir: '.claude'` in result. OpenCode requires `.opencode/` directory existence. Aider uses dual-path OR logic: `.aider.conf.yml` file existence OR `.aider/` directory existence both trigger detection with `configDir: '.aider'` in result.\n\n## Path Resolution\n\nAll checks use `path.join(projectRoot, relativePath)` for absolute path construction passed to `existsSync()`. No recursive traversal or parent directory scanning—only direct children of `projectRoot` tested.\n\n## Type Dependencies\n\nImports `DetectedEnvironment` interface and `EnvironmentType` union type from `./types.js`. `EnvironmentType` expected to constrain values `'claude' | 'opencode' | 'aider'` based on detection logic branches.\n### generate.ts\n**Purpose:** generateIntegrationFiles() orchestrates AI assistant integration file creation with environment-specific template sel...\n\n**generateIntegrationFiles() orchestrates AI assistant integration file creation with environment-specific template selection, bundled hook copying, and skip-if-exists behavior.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>**\n- Main entry point for integration file generation\n- Accepts `projectRoot` string and optional `GenerateOptions` with `dryRun`, `force`, and `environment` fields\n- If `options.environment` is specified, bypasses `detectEnvironments()` and targets single environment using `configDirMap: Record<EnvironmentType, string>` mapping (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n- Otherwise calls `detectEnvironments(projectRoot)` from `./detect.js` to get `{ type: EnvironmentType; configDir: string }[]` array\n- For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve template array\n- Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file if not exists or `force=true` via `writeFileSync(fullPath, template.content, 'utf-8')`\n- Tracks created/skipped paths in `IntegrationResult.filesCreated[]` and `IntegrationResult.filesSkipped[]`\n- Special case for `claude` environment: after template processing, generates hook file at `.claude/hooks/are-session-end.js` via `readBundledHook('are-session-end.js')` and `writeFileSync(fullHookPath, hookContent, 'utf-8')`\n- Returns `IntegrationResult[]` array with one entry per environment\n\n**getBundledHookPath(hookName: string): string**\n- Resolves absolute path to bundled hook file in `hooks/dist/` directory\n- Uses `fileURLToPath(import.meta.url)` to get current module path\n- Navigates from `dist/integration/` up two levels to project root, then into `hooks/dist/`\n- Returns `path.join(__dirname, '..', '..', 'hooks', 'dist', hookName)`\n\n**readBundledHook(hookName: string): string**\n- Calls `getBundledHookPath(hookName)` to resolve hook file path\n- Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false\n- Returns hook file content via `readFileSync(hookPath, 'utf-8')`\n\n**ensureDir(filePath: string): void**\n- Creates parent directories for `filePath` if they don't exist\n- Calls `mkdirSync(dir, { recursive: true })` where `dir` is `path.dirname(filePath)`\n\n**getTemplatesForEnvironment(type: EnvironmentType): ReturnType<typeof getClaudeTemplates>**\n- Switch statement dispatches on `type` to call environment-specific template getters from `./templates.js`\n- `'claude'` → `getClaudeTemplates()`\n- `'opencode'` → `getOpenCodeTemplates()`\n- `'gemini'` → `getGeminiTemplates()`\n- `'aider'` → returns empty array `[]` (no templates defined)\n- Default case returns empty array `[]`\n\n## Type Definitions\n\n**GenerateOptions** interface:\n- `dryRun?: boolean` — preview mode without file writes\n- `force?: boolean` — overwrite existing files instead of skipping\n- `environment?: EnvironmentType` — bypass auto-detection and target specific environment\n\n## Integration with Project\n\nImports `IntegrationResult` and `EnvironmentType` types from `./types.js`, `detectEnvironments()` from `./detect.js`, and template getters (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`) from `./templates.js`. Called by installer CLI commands to create `.claude/skills/`, `.opencode/commands/`, `.gemini/commands/` files and session hooks.\n\n## File System Operations\n\nAll file writes gated by `!dryRun` check to support preview mode. Uses `existsSync()` before write to implement skip-if-exists behavior (unless `force=true` overrides). Calls `ensureDir()` before every `writeFileSync()` to guarantee parent directories exist via `mkdirSync()` with `recursive: true`. Hook files read from build artifact location `hooks/dist/` which is populated by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Environment Targeting Behavior\n\nWhen `options.environment` is undefined, calls `detectEnvironments(projectRoot)` to scan for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories and generates files for all detected environments. When `options.environment` is specified, creates single-element environment array via `configDirMap` lookup, bypassing detection. This supports both multi-environment batch generation (`npx agents-reverse-engineer --runtime all`) and targeted single-environment installs (`npx agents-reverse-engineer --runtime claude -g`).\n### templates.ts\n**Purpose:** Generates platform-specific command templates for Claude Code, OpenCode, and Gemini CLI integration with AI-driven do...\n\n**Generates platform-specific command templates for Claude Code, OpenCode, and Gemini CLI integration with AI-driven documentation workflow instructions.**\n\n## Exported Functions\n\n- `getClaudeTemplates()` — Returns `IntegrationTemplate[]` for Claude Code (.claude/skills/are-{command}/SKILL.md format with frontmatter `name:` field)\n- `getOpenCodeTemplates()` — Returns `IntegrationTemplate[]` for OpenCode (.opencode/commands/are-{command}.md format with `agent: build` frontmatter)\n- `getGeminiTemplates()` — Returns `IntegrationTemplate[]` for Gemini CLI (.gemini/commands/are-{command}.toml format with `description` and triple-quoted `prompt` fields)\n\n## Template Generation Architecture\n\n`buildTemplate(platform, commandName, command)` constructs `IntegrationTemplate` objects with platform-specific file paths, frontmatter schemas, and placeholder substitution. `buildFrontmatter(platform, commandName, description)` generates Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure with `description = \"...\"` and `prompt = \"\"\"...\"\"\"` multiline strings per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n`getTemplatesForPlatform(platform)` maps all entries in `COMMANDS` constant to `buildTemplate()` calls, returning array of fully materialized templates. Each template includes `filename`, `path`, and `content` fields populated via `PLATFORM_CONFIGS` lookup and placeholder replacement for `COMMAND_PREFIX` (`/are-` for all platforms) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n## Command Definitions\n\n`COMMANDS` constant defines seven command templates: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. Each entry contains `description`, `argumentHint`, and `content` fields.\n\n`generate` template embeds three-phase pipeline documentation: Discovery → File Analysis (concurrent .sum file generation) → Directory/Root Documents (sequential AGENTS.md and CLAUDE.md synthesis). Includes background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, and completion summarization covering file counts, failures, and inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n\n`update` template describes incremental change detection workflow with `--uncommitted` flag for staged changes, hash-based change comparison, orphan cleanup, and affected directory regeneration. Monitoring pattern identical to `generate`.\n\n`discover` template enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay, reports file count on completion.\n\n`clean` template mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview support.\n\n`specify` template orchestrates AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n\n`rebuild` template describes spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation after each group, ===FILE:=== delimited output parsing, and exit code semantics (0=success, 1=partial failure, 2=total failure). Supports checkpoint-based resumability. Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n\n`help` template outputs command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, and repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n## Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines `PlatformConfig` objects for `claude`, `opencode`, `gemini` keys with fields:\n- `commandPrefix` — Slash command prefix (`/are-` for all platforms)\n- `pathPrefix` — Installation directory (.claude/skills/, .opencode/commands/, .gemini/commands/)\n- `filenameSeparator` — Path component delimiter (`.` for Claude, `-` for OpenCode/Gemini)\n- `extraFrontmatter` — Optional platform-specific metadata (`agent: build` for OpenCode only)\n- `usesName` — Boolean controlling frontmatter `name:` field emission (true for Claude only)\n- `versionFilePath` — Platform-specific version cache path (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION)\n\nClaude uses nested directory structure (.claude/skills/are-{command}/SKILL.md), while OpenCode and Gemini use flat file naming (.opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml).\n\n## Placeholder Substitution\n\nAll `content` strings undergo two replacements before template materialization:\n1. `COMMAND_PREFIX` → platform-specific slash command prefix (`/are-`)\n2. `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`\n\n## Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Type Dependencies\n\nImports `IntegrationTemplate` from `./types.js` with structure:\n```typescript\ninterface IntegrationTemplate {\n  filename: string;\n  path: string;\n  content: string;\n}\n```\n### types.ts\n**Purpose:** Defines TypeScript interfaces for AI coding assistant environment detection and integration template generation acros...\n\n**Defines TypeScript interfaces for AI coding assistant environment detection and integration template generation across platforms (Claude Code, OpenCode, Gemini, Aider).**\n\n## Type Definitions\n\n**`EnvironmentType`** — String literal union type enumerating supported AI assistant platforms: `'claude' | 'opencode' | 'aider' | 'gemini'`.\n\n**`DetectedEnvironment`** — Interface representing environment detection results with fields:\n- `type: EnvironmentType` — Detected AI assistant platform identifier\n- `configDir: string` — Configuration directory path (e.g., `.claude`, `.opencode`)\n- `detected: boolean` — Boolean flag indicating whether environment was found in project\n\n**`IntegrationTemplate`** — Interface representing a single integration file to be generated with fields:\n- `filename: string` — Base filename (e.g., `'generate.md'`)\n- `path: string` — Relative path from project root (e.g., `.claude/commands/ar/generate.md`)\n- `content: string` — Template content to write to filesystem\n\n**`IntegrationResult`** — Interface representing batch integration generation outcome with fields:\n- `environment: EnvironmentType` — Target AI assistant platform\n- `filesCreated: string[]` — Array of successfully written file paths\n- `filesSkipped: string[]` — Array of file paths skipped due to existing files\n\n## Integration Points\n\nConsumed by `src/integration/detect.ts` for environment detection logic and `src/integration/generate.ts` for template generation workflows. Used by `src/installer/operations.ts` to orchestrate IDE-specific command/hook installation. Referenced in `src/cli/init.ts` for initialization flows requiring platform-specific configuration.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific integration file generation for AI coding assistants (Claude Code, OpenCode, Gemini CLI, Aider) via environment detection, template materialization with frontmatter variants, and bundled hook deployment.**\n\n## Contents\n\n**[detect.ts](./detect.ts)** — `detectEnvironments(projectRoot)` scans for `.claude/`, `CLAUDE.md`, `.opencode/`, `.aider.conf.yml`, `.aider/`, `.gemini/` artifacts via `existsSync()` and returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment(projectRoot, type)` predicate tests for specific platform presence.\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles(projectRoot, options)` orchestrates template generation with skip-if-exists behavior (unless `force=true`) and optional `dryRun` preview. Dispatches to platform-specific template getters via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` after `ensureDir()` parent directory creation, copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays.\n\n**[templates.ts](./templates.ts)** — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generate command file arrays via `buildTemplate(platform, commandName, command)` with platform-specific paths (.claude/skills/are-{command}/SKILL.md, .opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml). `COMMANDS` constant defines seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. `PLATFORM_CONFIGS` maps frontmatter schemas (Claude `name:` field, OpenCode `agent: build` directive, Gemini `description`/`prompt` TOML structure). `buildFrontmatter()` handles Markdown variants, `buildGeminiToml()` formats triple-quoted prompts. Placeholder substitution replaces `COMMAND_PREFIX` (`/are-`) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Architecture\n\n### Environment Detection\n\n`detectEnvironments()` applies existence checks via `path.join(projectRoot, relativePath)` → `existsSync()` for platform-specific markers:\n- Claude: `.claude/` directory OR `CLAUDE.md` file → `{ type: 'claude', configDir: '.claude', detected: true }`\n- OpenCode: `.opencode/` directory → `{ type: 'opencode', configDir: '.opencode', detected: true }`\n- Aider: `.aider.conf.yml` file OR `.aider/` directory → `{ type: 'aider', configDir: '.aider', detected: true }`\n- Gemini: `.gemini/` directory → `{ type: 'gemini', configDir: '.gemini', detected: true }`\n\nNo recursive parent directory scanning—only direct children of `projectRoot` tested.\n\n### Template Generation Flow\n\n1. `generateIntegrationFiles()` receives `projectRoot` and optional `GenerateOptions` (`dryRun`, `force`, `environment`)\n2. If `options.environment` specified, constructs single-element array via `configDirMap: Record<EnvironmentType, string>` lookup (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n3. Otherwise calls `detectEnvironments(projectRoot)` to get auto-detected platforms\n4. For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve `IntegrationTemplate[]` from `templates.ts`\n5. Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file via `writeFileSync(fullPath, template.content, 'utf-8')` if not exists or `force=true`\n6. Special case for `claude` environment: after template processing, reads bundled hook via `getBundledHookPath('are-session-end.js')` → `readFileSync(hookPath, 'utf-8')`, writes to `.claude/hooks/are-session-end.js`\n7. Tracks created/skipped paths in `IntegrationResult` per environment\n\n### Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines variants for frontmatter generation and path construction:\n- **Claude**: Nested directory structure (.claude/skills/are-{command}/SKILL.md), frontmatter with `name: /are-{command}`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: Flat file structure (.opencode/commands/are-{command}.md), frontmatter with `agent: build`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: TOML format (.gemini/commands/are-{command}.toml), `description = \"...\"` and `prompt = \"\"\"...\"\"\"` fields, `versionFilePath: '.gemini/ARE-VERSION'`\n- **Aider**: Returns empty array (no templates defined)\n\n`buildFrontmatter(platform, commandName, description)` emits Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n### Command Template Structure\n\n`COMMANDS` constant defines command metadata with `description`, `argumentHint`, `content` fields for seven commands:\n\n- **`generate`**: Three-phase pipeline documentation (Discovery → File Analysis → Directory/Root Documents). Embeds background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, completion summarization covering file counts, failures, inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n- **`update`**: Incremental change detection workflow with `--uncommitted` flag, hash-based comparison, orphan cleanup, affected directory regeneration. Monitoring pattern identical to `generate`.\n- **`discover`**: Enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay.\n- **`clean`**: Mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview.\n- **`specify`**: AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n- **`rebuild`**: Spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation, ===FILE:=== delimited output parsing, exit code semantics (0=success, 1=partial failure, 2=total failure). Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n- **`help`**: Command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n`buildTemplate(platform, commandName, command)` materializes templates via placeholder substitution: `COMMAND_PREFIX` → `/are-`, `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`.\n\n### Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Bundled Hook Deployment\n\n`generateIntegrationFiles()` includes special-case logic for Claude environment: after template generation loop completes, calls `readBundledHook('are-session-end.js')` which resolves path via `getBundledHookPath(hookName)` using `fileURLToPath(import.meta.url)` to navigate from `dist/integration/` up two levels to project root, then into `hooks/dist/`. Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false. Writes hook content to `.claude/hooks/are-session-end.js` via `writeFileSync(fullHookPath, hookContent, 'utf-8')`. Hook files populated in `hooks/dist/` by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Integration with Project\n\nCalled by `src/installer/operations.ts` installer CLI commands to create IDE-specific command/hook files. Consumed by `src/cli/init.ts` initialization workflows requiring platform-specific configuration. Environment detection (`detectEnvironments()`) supports auto-discovery mode when `options.environment` undefined, enabling multi-platform batch generation (`npx agents-reverse-engineer --runtime all`). Single-environment targeting via `options.environment` parameter supports focused installs (`npx agents-reverse-engineer --runtime claude -g`).\n\n## Import Map\n\n- **Local**: `./detect.js` (`detectEnvironments`, `hasEnvironment`), `./templates.js` (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`), `./types.js` (`EnvironmentType`, `DetectedEnvironment`, `IntegrationTemplate`, `IntegrationResult`)\n- **External**: `path` (Node.js), `fs` (`existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`), `url` (`fileURLToPath`), `../installer/paths.js` (referenced in integration context)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2745,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12931,
      "latencyMs": 56070,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:32:16.653Z",
      "prompt": "Generate AGENTS.md for directory: \"src/update\" (update)\n\n## File Summaries (4 files)\n\n### index.ts\n**Purpose:** Barrel export module exposing UpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, ...\n\n**Barrel export module exposing UpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, and TypeScript types for incremental documentation updates.**\n\n## Exported Symbols\n\n**UpdateOrchestrator** — class exported from `./orchestrator.js` coordinating incremental update workflow with change detection, orphan cleanup, and selective regeneration\n\n**createUpdateOrchestrator** — factory function exported from `./orchestrator.js` constructing UpdateOrchestrator instances with dependency injection\n\n**UpdatePlan** — type exported from `./orchestrator.js` representing planned update operations (files to analyze, skip, delete; affected directories)\n\n**cleanupOrphans** — function exported from `./orphan-cleaner.js` removing stale `.sum` files for deleted source files or renamed oldPaths\n\n**cleanupEmptyDirectoryDocs** — function exported from `./orphan-cleaner.js` deleting `AGENTS.md` files from directories with no remaining source files after cleanup\n\n**getAffectedDirectories** — function exported from `./orphan-cleaner.js` computing directories requiring `AGENTS.md` regeneration by walking parent paths of changed files\n\n**UpdateOptions** — type exported from `./types.js` parameterizing update behavior (includeUncommitted flag, baseCommit, concurrency, timeout, retries)\n\n**UpdateResult** — type exported from `./types.js` representing update outcome with counts for analyzed/skipped/deleted files, affected directories, success status\n\n**UpdateProgress** — type exported from `./types.js` describing update progress events emitted during execution (phase transitions, file completions, error reports)\n\n**CleanupResult** — type exported from `./types.js` representing orphan cleanup outcome with arrays of deleted `.sum` files and removed directory docs\n\n## Module Purpose\n\nCentralizes incremental update API surface as public interface for `src/cli/update.ts` command entry point. Separates orchestration logic (UpdateOrchestrator in `./orchestrator.js`) from cleanup utilities (orphan-cleaner functions) and type definitions (`./types.js`). Enables consumers to import only required symbols without loading implementation details from sibling modules.\n\n## Integration Context\n\nInvoked by `src/cli/update.ts` command handler which constructs UpdateOrchestrator via createUpdateOrchestrator factory, passes CommandRunOptions with tracer/progress reporters, and executes UpdateOrchestrator.update() method. UpdateOrchestrator internally calls `src/change-detection/detector.ts` for SHA-256 hash comparison, cleanupOrphans for `.sum` deletion, getAffectedDirectories for directory scope calculation, and `src/generation/executor.ts` for Phase 1 file regeneration and Phase 2 directory aggregation (skips Phase 3 root synthesis).\n### orchestrator.ts\n**Purpose:** UpdateOrchestrator implements frontmatter-based incremental documentation updates by comparing SHA-256 content hashes...\n\n**UpdateOrchestrator implements frontmatter-based incremental documentation updates by comparing SHA-256 content hashes stored in `.sum` file YAML frontmatter against current file content, generating UpdatePlan objects identifying files requiring re-analysis, orphaned `.sum` cleanup targets, and affected directories needing `AGENTS.md` regeneration.**\n\n## Exported Interface\n\n**UpdateOrchestrator** class with constructor signature `(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })`. Provides instance methods:\n\n- `checkPrerequisites(): Promise<void>` — Validates git repository via `isGitRepo()`, throws Error if not a git repo\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — Discovers files, compares content hashes, returns plan with `filesToAnalyze`, `filesToSkip`, `cleanup`, `affectedDirs`, `baseCommit`, `currentCommit`, `isFirstRun`\n- `close(): void` — No-op for API compatibility (no database in frontmatter mode)\n- `recordFileAnalyzed(relativePath: string, contentHash: string, currentCommit: string): Promise<void>` — No-op, kept for compatibility\n- `removeFileState(relativePath: string): Promise<void>` — No-op, kept for compatibility\n- `recordRun(commitHash: string, filesAnalyzed: number, filesSkipped: number): Promise<number>` — No-op, returns 0\n- `getLastRun(): Promise<undefined>` — No-op, returns undefined (no run history)\n- `isFirstRun(): Promise<boolean>` — Checks if any `.sum` files exist via `preparePlan({ dryRun: true })`\n\n**createUpdateOrchestrator** factory function with signature `(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean }): UpdateOrchestrator`.\n\n**UpdatePlan** interface with fields:\n- `filesToAnalyze: FileChange[]` — Added or modified files requiring analysis\n- `filesToSkip: string[]` — Unchanged files based on hash comparison\n- `cleanup: CleanupResult` — Orphaned `.sum` files to delete\n- `affectedDirs: string[]` — Directories needing `AGENTS.md` regeneration (sorted deepest-first)\n- `baseCommit: string` — Not used in frontmatter mode, kept for compatibility\n- `currentCommit: string` — Current git commit SHA\n- `isFirstRun: boolean` — True when `filesToSkip.length === 0 && filesToAnalyze.length > 0`\n\n## Change Detection Algorithm\n\n`preparePlan()` orchestrates hash-based change detection:\n\n1. Calls `checkPrerequisites()` ensuring git repository\n2. Calls `getCurrentCommit()` to retrieve current commit SHA\n3. Invokes `discoverFiles()` → `runDiscovery()` returning absolute paths, converts to relative via `path.relative()`\n4. Iterates discovered files, for each:\n   - Constructs `.sum` path via `getSumPath(filePath)`\n   - Adds to `seenSumFiles` Set for orphan detection\n   - Calls `readSumFile(sumPath)` to extract `SumFileContent` with `contentHash` from frontmatter\n   - If no `.sum` exists: pushes `{ path, status: 'added' }` to `filesToAnalyze`\n   - If `.sum` exists: calls `computeContentHash(filePath)` for current SHA-256 hash\n   - Hash mismatch or missing: pushes `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: pushes path to `filesToSkip`\n5. Calls `cleanupOrphans(projectRoot, deletedOrRenamed, dryRun)` returning `CleanupResult` with `deletedSumFiles`\n6. Calls `getAffectedDirectories(filesToAnalyze)` returning Set of directory paths\n7. Sorts `affectedDirs` by depth descending: `depthB - depthA` where depth = `path.split(path.sep).length`\n8. Emits `plan:created` trace event with `fileCount` and `taskCount`\n\n## Trace Emission Points\n\nEmits trace events via optional `ITraceWriter`:\n\n- `phase:start` at plan creation start with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after plan completion with `phase: 'update-plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n\nDuration computed via `process.hrtime.bigint()` delta converted to milliseconds: `Number(endTime - startTime) / 1_000_000`.\n\n## Dependencies\n\nImports from `../change-detection/index.js`: `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type.\n\nImports from `./orphan-cleaner.js`: `cleanupOrphans()`, `getAffectedDirectories()`.\n\nImports from `../generation/writers/sum.js`: `readSumFile()`, `getSumPath()`.\n\nImports from `../discovery/run.js`: `discoverFiles` as `runDiscovery()`.\n\nImports from `../config/schema.js`: `Config` type.\n\nImports from `../orchestration/trace.js`: `ITraceWriter` interface.\n\nImports from `./types.js`: `UpdateOptions`, `CleanupResult` types.\n\n## Debug Logging\n\nWhen `debug: true`, emits picocolors-formatted stderr messages:\n\n- `[debug] Creating update plan with change detection...` at plan start\n- `[debug] Git commit: ${currentCommit.slice(0, 7)}` after commit retrieval\n- `[debug] Discovering files...` before discovery\n- `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned` after hash comparison\n- `[debug] Affected directories: ${affectedDirs.length}` after directory computation\n\n## Frontmatter Mode Design\n\nNo-op methods (`recordFileAnalyzed`, `removeFileState`, `recordRun`, `getLastRun`) retain signatures for API compatibility with database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by `writeSumFile()` from `../generation/writers/sum.js`. Run history tracking unavailable in frontmatter mode (`getLastRun()` returns `undefined`, `recordRun()` returns `0`).\n\n## Error Handling\n\n`checkPrerequisites()` throws Error with message `Not a git repository: ${projectRoot}\\nThe update command requires a git repository for change detection.` when `isGitRepo()` returns false.\n\nHash comparison wrapped in try-catch: file read errors cause path addition to `filesToSkip` (silent failure for inaccessible files).\n### orphan-cleaner.ts\n**Purpose:** orphan-cleaner.ts deletes stale `.sum` files, `.annex.md` files, and `AGENTS.md` files when source files are deleted ...\n\n**orphan-cleaner.ts deletes stale `.sum` files, `.annex.md` files, and `AGENTS.md` files when source files are deleted or renamed, ensuring documentation artifacts match the current codebase structure.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun: boolean = false): Promise<CleanupResult>`** — Deletes `.sum` and `.annex.md` files for deleted or renamed source files, then removes `AGENTS.md` from directories that no longer contain source files. Processes `FileChange` entries with `status === 'deleted'` or `status === 'renamed'`, adding their paths (or `oldPath` for renames) to cleanup queue. Returns `CleanupResult` containing `deletedSumFiles[]` and `deletedAgentsMd[]` arrays with relative paths of removed files.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun: boolean = false): Promise<boolean>`** — Checks if directory contains any source files by reading entries via `readdir()`, filtering out hidden files (starting with `.`), `.sum` files, `.annex.md` files, and entries in `GENERATED_FILES` set. Returns `true` if `AGENTS.md` was deleted, `false` otherwise. Used to remove directory-level documentation when no source files remain.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** — Extracts parent directory paths from non-deleted `FileChange` entries by walking up via `path.dirname()` until reaching root (`.`). Skips changes with `status === 'deleted'` since deleted files don't require directory doc regeneration. Returns `Set<string>` of relative directory paths that need `AGENTS.md` regeneration during incremental updates.\n\n## Private Implementation\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** — Calls `stat()` to check file existence, then `unlink()` if not in dry-run mode. Returns `true` if file existed (regardless of actual deletion), `false` if `stat()` throws. Swallows all errors via empty catch block.\n\n**`GENERATED_FILES`** — `Set` containing `'AGENTS.md'` and `'CLAUDE.md'`, used by `cleanupEmptyDirectoryDocs()` to distinguish generated documentation from source files during directory emptiness checks.\n\n## Cleanup Logic\n\n**Deletion triggers:** For each `FileChange` with `status === 'deleted'`, constructs `.sum` and `.annex.md` paths via `path.join(projectRoot, \\`${relativePath}.sum\\`)`. For `status === 'renamed'` changes, targets `oldPath` instead of `path` to remove artifacts at previous location.\n\n**Directory cleanup strategy:** Collects affected directories via `path.dirname()` of each cleaned path, skipping root directory (`.`). For each affected directory, calls `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` if no source files remain. Source file detection excludes hidden files (`entry.startsWith('.')`) + `.sum` + `.annex.md` + `GENERATED_FILES` entries.\n\n**Dry-run support:** When `dryRun === true`, skips `unlink()` calls but still executes `stat()` checks and returns `true` for would-be deletions. Used for preview mode in update workflow.\n\n## Integration Context\n\nInvoked by `src/update/orchestrator.ts` after change detection to remove stale artifacts before regenerating modified files. Paired with `getAffectedDirectories()` to compute which directories need `AGENTS.md` regeneration during Phase 2 of incremental update workflow. Return value `CleanupResult` logged to `.agents-reverse-engineer/progress.log` for user visibility into cleanup operations.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for incremental update workflow results, options, and progress callbacks.\n\n**Defines TypeScript interfaces for incremental update workflow results, options, and progress callbacks.**\n\n## Exported Types\n\n**CleanupResult** — Result of orphan cleanup operation with `deletedSumFiles: string[]` (removed `.sum` files) and `deletedAgentsMd: string[]` (removed `AGENTS.md` files from empty directories).\n\n**UpdateOptions** — Configuration for update command with `includeUncommitted?: boolean` (include staged + working directory changes) and `dryRun?: boolean` (preview mode without file writes).\n\n**UpdateResult** — Comprehensive update execution result containing `analyzedFiles: string[]` (added/modified files), `skippedFiles: string[]` (unchanged files), `cleanup: CleanupResult` (orphan deletion summary), `regeneratedDirs: string[]` (directories with rebuilt `AGENTS.md`), `baseCommit: string` (git SHA at start), `currentCommit: string` (git SHA at end), and `dryRun: boolean` (execution mode flag).\n\n**UpdateProgress** — Progress callback interface with optional handlers: `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` (file processing start), `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` (file processing completion), `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` (orphan file deletion), `onDirRegenerate?: (path: string) => void` (directory aggregation).\n\n## Dependencies\n\nImports `FileChange` from `../change-detection/types.js` to represent git-based file modifications (used in orchestrator implementations, not directly exposed in this type module).\n\n## Design Pattern\n\nStatus discriminators use string literal unions (`'analyzing' | 'skipping'`, `'analyzed' | 'skipped' | 'error'`, `'sum' | 'agents-md'`) for type-safe progress event routing and UI rendering in streaming progress reporter.\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../change-detection/index.js → isGitRepo, getCurrentCommit, computeContentHash, type FileChange\n  ../generation/writers/sum.js → readSumFile, getSumPath\n  ../discovery/run.js → discoverFiles\n  ../orchestration/trace.js → ITraceWriter (type)\n\norphan-cleaner.ts:\n  ../change-detection/types.js → FileChange (type)\n\ntypes.ts:\n  ../change-detection/types.js → FileChange (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nIncremental documentation update workflow coordinating SHA-256 hash-based change detection, orphaned artifact cleanup, and selective `.sum` + `AGENTS.md` regeneration for modified files and affected directories.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export exposing UpdateOrchestrator, createUpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, UpdatePlan, UpdateOptions, UpdateResult, UpdateProgress, CleanupResult types.\n\n**[orchestrator.ts](./orchestrator.ts)** — UpdateOrchestrator compares YAML frontmatter `content_hash` fields from existing `.sum` files against SHA-256 hashes of current file content via computeContentHash(), generates UpdatePlan segregating files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), and `cleanup` (orphaned), computes `affectedDirs` requiring `AGENTS.md` regeneration sorted deepest-first. Factory createUpdateOrchestrator() constructs instances with optional ITraceWriter injection.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — cleanupOrphans() deletes `.sum` and `.annex.md` files for deleted/renamed source files (targeting `oldPath` for renames), invokes cleanupEmptyDirectoryDocs() removing `AGENTS.md` from directories with no remaining source files (excludes hidden files, `GENERATED_FILES` set, `.sum`/`.annex.md` from emptiness check). getAffectedDirectories() walks parent directories of non-deleted FileChange entries via path.dirname() to project root.\n\n**[types.ts](./types.ts)** — Defines CleanupResult (`deletedSumFiles[]`, `deletedAgentsMd[]`), UpdateOptions (`includeUncommitted`, `dryRun` flags), UpdateResult (execution summary with `analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`), UpdateProgress (event callbacks: `onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Workflow\n\n1. **Prerequisites** — UpdateOrchestrator.checkPrerequisites() validates git repository via isGitRepo(), throws Error if not a git repo\n2. **Plan Creation** — preparePlan() calls discoverFiles(), iterates results:\n   - Constructs `.sum` path via getSumPath()\n   - Calls readSumFile() extracting `contentHash` from frontmatter\n   - Missing `.sum`: adds `{ path, status: 'added' }` to `filesToAnalyze`\n   - Existing `.sum`: calls computeContentHash(), compares hashes\n   - Hash mismatch: adds `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: adds path to `filesToSkip`\n3. **Orphan Cleanup** — cleanupOrphans() deletes `.sum`/`.annex.md` for deleted/renamed files, cleanupEmptyDirectoryDocs() removes `AGENTS.md` from directories with no source files\n4. **Affected Directories** — getAffectedDirectories() computes parent paths of changed files, sorts by depth descending (deepest-first processing)\n5. **Regeneration** — External caller (src/cli/update.ts) invokes Phase 1 pool execution for `filesToAnalyze`, Phase 2 sequential directory aggregation for `affectedDirs` (skips Phase 3 root synthesis)\n\n## Integration\n\n**Upstream callers**: src/cli/update.ts constructs UpdateOrchestrator via createUpdateOrchestrator(), calls preparePlan() returning UpdatePlan, passes `filesToAnalyze` to src/generation/executor.ts Phase 1 file analysis pool, passes `affectedDirs` to Phase 2 directory aggregation.\n\n**Dependencies**: src/change-detection/detector.ts (computeContentHash, isGitRepo, getCurrentCommit), src/generation/writers/sum.ts (readSumFile, getSumPath), src/discovery/run.ts (discoverFiles), src/orchestration/trace.ts (ITraceWriter).\n\n## Trace Events\n\nUpdateOrchestrator emits via optional ITraceWriter:\n\n- `phase:start` at plan creation with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after completion with `durationMs` computed via process.hrtime.bigint() delta\n\n## Frontmatter Mode\n\nNo-op methods (recordFileAnalyzed, removeFileState, recordRun, getLastRun) retain signatures for API compatibility with hypothetical database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by writeSumFile(). Run history tracking unavailable (getLastRun() returns undefined, recordRun() returns 0).\n\n## Dry-Run Support\n\nWhen `UpdateOptions.dryRun === true`, orphan-cleaner skips unlink() calls but executes stat() checks, returning would-be deletions in CleanupResult. Used for preview mode in src/cli/update.ts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1322,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 14458,
      "latencyMs": 36097,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.226Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation\" (generation)\n\n## File Summaries (5 files)\n\n### collector.ts\n**Purpose:** collector.ts provides recursive directory traversal functions to collect all `AGENTS.md` and `.annex.md` files from a...\n\n**collector.ts provides recursive directory traversal functions to collect all `AGENTS.md` and `.annex.md` files from a project tree, returning sorted arrays of relative paths and content while skipping vendor/build/meta directories.**\n\n## Exported Functions\n\n`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>` recursively walks the directory tree starting from `projectRoot`, collects all files named exactly `AGENTS.md`, reads their content via `readFile(filePath, 'utf-8')`, and returns an array of `{ relativePath, content }` objects sorted alphabetically by `relativePath` using `localeCompare()`. Skips directories in `SKIP_DIRS` set and gracefully handles permission errors by catching and ignoring exceptions from `readdir()` and `readFile()`.\n\n`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>` uses identical traversal logic to `collectAgentsDocs()` but collects files ending with `.annex.md` instead of exact-match `AGENTS.md`. Tests file names with `entry.name.endsWith('.annex.md')` and returns the same `AgentsDocs` structure sorted alphabetically.\n\n## Type Definitions\n\n`AgentsDocs` is an exported type alias for `Array<{ relativePath: string; content: string }>` representing collected documentation files with their project-relative paths and text content.\n\n## Directory Exclusion\n\n`SKIP_DIRS` is a `Set<string>` constant containing 13 directory names to exclude during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Both collection functions check `SKIP_DIRS.has(entry.name)` before recursing into subdirectories.\n\n## Traversal Pattern\n\nBoth functions use internal async `walk(currentDir: string): Promise<void>` closures that call `readdir(currentDir, { withFileTypes: true })` to obtain `Dirent` objects, iterate over entries, recurse on directories not in `SKIP_DIRS` via `walk(path.join(currentDir, entry.name))`, and collect matching files by pushing `{ relativePath: path.relative(projectRoot, filePath), content }` into the `results` array. Final sorting happens after traversal completes via `results.sort((a, b) => a.relativePath.localeCompare(b.relativePath))`.\n\n## Error Handling\n\nBoth functions wrap `readdir()` calls in try-catch blocks that return early on exception (permission denied or inaccessible directories). File read operations wrap `readFile()` in try-catch blocks that skip unreadable files silently without logging. No error propagation occurs—all failures are suppressed to ensure partial collection succeeds.\n### complexity.ts\n**Purpose:** `complexity.ts` computes codebase complexity metrics from discovered file lists via directory depth calculation and u...\n\n**`complexity.ts` computes codebase complexity metrics from discovered file lists via directory depth calculation and unique directory extraction.**\n\n## Exported Interface\n\n`ComplexityMetrics` describes structural complexity with:\n- `fileCount: number` — total source files discovered\n- `directoryDepth: number` — maximum depth from project root (file itself excluded from count)\n- `files: string[]` — array of absolute file paths\n- `directories: Set<string>` — unique directory paths extracted via upward traversal\n\n## Exported Function\n\n`analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` orchestrates metric computation by calling `calculateDirectoryDepth()` and `extractDirectories()`, returning populated `ComplexityMetrics` object.\n\n## Internal Functions\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` computes maximum nesting depth via `path.relative()` followed by `split(path.sep).length - 1` (subtracting 1 to exclude filename component). Iterates all files tracking `Math.max()` depth.\n\n`extractDirectories(files: string[]): Set<string>` walks each file path upward via `path.dirname()` loop until reaching root (detected when `parent === dir`), accumulating unique directory paths into `Set<string>`. Stops at `'.'` or root equivalence to prevent infinite loops.\n\n## Usage Context\n\nCalled by Phase 1 orchestration (`src/generation/orchestrator.ts`) after file discovery to compute project structure metrics. Results inform concurrency tuning decisions and progress estimation. The `directories` set drives Phase 2 post-order traversal by identifying all directories requiring `AGENTS.md` aggregation.\n### executor.ts\n**Purpose:** executor.ts transforms GenerationPlan into ExecutionPlan with post-order directory traversal, dependency tracking, an...\n\n**executor.ts transforms GenerationPlan into ExecutionPlan with post-order directory traversal, dependency tracking, and directory completion predicates.**\n\n## Exported Types\n\n**ExecutionTask** represents a single AI processing unit with fields:\n- `id: string` — unique identifier (`file:path`, `dir:path`, `root:docname`)\n- `type: 'file' | 'directory' | 'root-doc'` — discriminator for task category\n- `path: string` — relative path to file or directory\n- `absolutePath: string` — resolved absolute path\n- `systemPrompt: string` — AI system message (placeholder for dir/root tasks)\n- `userPrompt: string` — AI user message (placeholder for dir/root tasks)\n- `dependencies: string[]` — array of task IDs that must complete first\n- `outputPath: string` — where to write generated content (`.sum` or `AGENTS.md` or `CLAUDE.md`)\n- `metadata: { directoryFiles?: string[], depth?: number, packageRoot?: string }` — tracking data\n\n**ExecutionPlan** contains dependency graph with fields:\n- `projectRoot: string` — absolute path to project root\n- `tasks: ExecutionTask[]` — all tasks in execution order\n- `fileTasks: ExecutionTask[]` — Phase 1 file analysis tasks (parallel eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 AGENTS.md generation tasks (post-order)\n- `rootTasks: ExecutionTask[]` — Phase 3 root document synthesis tasks (sequential)\n- `directoryFileMap: Record<string, string[]>` — maps directory paths to contained file relative paths\n- `projectStructure?: string` — compact project listing for prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** constructs three-phase task graph:\n1. Populates `directoryFileMap` by grouping files via `path.dirname()`\n2. Creates file tasks with `id: 'file:${filePath}'`, `dependencies: []`, `outputPath: '${absolutePath}.sum'`\n3. Sorts file tasks by directory depth descending via `getDirectoryDepth(path.dirname(path))`\n4. Creates directory tasks sorted by depth descending for post-order traversal, each depending on all file tasks in that directory (dependencies populated from `directoryFileMap`)\n5. Creates root tasks (`id: 'root:CLAUDE.md'`) depending on all directory task IDs\n6. Directory and root task prompts are placeholders (`'Built at runtime by buildRootPrompt()'`) — actual prompts constructed by `runner.ts` Phase 2/3 logic\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** checks readiness by calling `sumFileExists()` for each file in `expectedFiles`, returns `{ complete: true, missing: [] }` if all `.sum` files exist.\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** filters `directoryFileMap` entries by calling `isDirectoryComplete()` for each directory, returns array of directory paths ready for AGENTS.md generation.\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** generates GENERATION-PLAN.md content:\n- Header: `# Documentation Generation Plan` with ISO date and `plan.projectRoot`\n- Summary: counts for `tasks.length`, `fileTasks.length`, `directoryTasks.length`, `rootTasks.length`\n- Phase 1: groups files by directory using `directoryTasks` order (already post-order), outputs `### Depth N: dir/ (M files)` sections with `- [ ] \\`file\\`` checklist items\n- Phase 2: groups directories by `metadata.depth`, outputs `### Depth N` sections with `- [ ] \\`dir/AGENTS.md\\`` checklist items in descending depth order\n- Phase 3: outputs `- [ ] \\`CLAUDE.md\\`` checklist item\n\n## Helper Functions\n\n**getDirectoryDepth(dir: string): number** calculates depth by splitting on `path.sep` and counting segments:\n- `'.'` → `0` (root special case)\n- `'src'` → `1`\n- `'src/cli'` → `2`\n\nUsed for post-order sorting: deeper directories (higher depth) must complete before shallower parents.\n\n## Integration Points\n\nConsumes `GenerationPlan` from `orchestrator.ts` (contains `tasks[]`, `files[]`, `projectStructure` fields).\n\nConsumed by `runner.ts`:\n- `buildExecutionPlan()` called in Phase 1 initialization\n- `isDirectoryComplete()` called in Phase 2 to wait for child `.sum` files\n- `formatExecutionPlanAsMarkdown()` output written to `GENERATION-PLAN.md` via `PlanTracker`\n\nCalls `sumFileExists()` from `writers/sum.ts` to check `.sum` file presence without parsing YAML frontmatter.\n\n## Post-Order Traversal Strategy\n\nDirectory tasks sorted by depth descending (deepest first) ensures child `AGENTS.md` files exist before parent directories attempt aggregation. Sort key: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` (descending). File tasks similarly sorted to process leaf directories first. Root tasks depend on all directory task IDs, enforcing three-phase waterfall: files → directories → root.\n### orchestrator.ts\n**Purpose:** GenerationOrchestrator coordinates three-phase documentation pipeline planning by preparing files, analyzing complexi...\n\n**GenerationOrchestrator coordinates three-phase documentation pipeline planning by preparing files, analyzing complexity, building file/directory task queues with embedded prompts, and emitting trace events for plan creation.**\n\n## Exported Types\n\n**PreparedFile** represents a file ready for analysis with `filePath: string` (absolute), `relativePath: string` (from project root), and `content: string` (file contents loaded via `readFile()`).\n\n**AnalysisTask** discriminated union with `type: 'file' | 'directory'`, `filePath: string`, optional `systemPrompt?: string`, optional `userPrompt?: string` (both set for file tasks, built at runtime for directory tasks), and optional `directoryInfo?: { sumFiles: string[], fileCount: number }` for directory tasks.\n\n**GenerationPlan** aggregates `files: PreparedFile[]`, `tasks: AnalysisTask[]`, `complexity: ComplexityMetrics` (from `analyzeComplexity()`), and optional `projectStructure?: string` (compact directory tree listing).\n\n**GenerationOrchestrator** class constructs via `constructor(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })` storing config, projectRoot, tracer, and debug flag.\n\n**createOrchestrator** factory function wraps `new GenerationOrchestrator(config, projectRoot, options)`.\n\n## Core Methods\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads file content via `readFile(filePath, 'utf-8')`, computes `relativePath` via `path.relative(projectRoot, filePath)`, returns array of `PreparedFile` objects, silently skips unreadable files (permission errors).\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** builds file analysis tasks by invoking `buildFilePrompt({ filePath, content, projectPlan: projectStructure }, debug)` for each `PreparedFile`, returns `AnalysisTask[]` with `type: 'file'`, `filePath` set to `relativePath`, `systemPrompt` and `userPrompt` populated from prompt builder.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files by directory via `path.dirname(relativePath)`, aggregates into `Map<string, PreparedFile[]>`, generates one `AnalysisTask` per directory with `type: 'directory'`, `filePath` set to directory path (or `'.'` for root), `directoryInfo.sumFiles` set to array of `.sum` file paths (`${relativePath}.sum`), `directoryInfo.fileCount` set to file count. Prompts built at execution time via `buildDirectoryPrompt()`.\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates plan creation by emitting `phase:start` trace event with `phase: 'plan-creation'`, calling `prepareFiles()`, invoking `analyzeComplexity(filePaths, projectRoot)`, calling `buildProjectStructure()`, invoking `createFileTasks()` and `createDirectoryTasks()`, concatenating tasks, clearing `PreparedFile.content` to free memory (content already embedded in prompts), emitting `plan:created` trace event with `fileCount`, `taskCount + 1` (+1 for root CLAUDE.md task added later by `buildExecutionPlan()`), emitting `phase:end` trace event with `durationMs` computed via `process.hrtime.bigint()` delta, returning `GenerationPlan`.\n\n## Project Structure Builder\n\n**buildProjectStructure(files: PreparedFile[]): string** private method groups files by directory via `path.dirname(relativePath) || '.'`, creates `Map<string, string[]>` of directory to basenames, sorts directories lexicographically, formats as multi-line string with pattern `${dir}/\\n  ${basename}\\n  ${basename}...`, provides bird's-eye context for AI prompts.\n\n## Memory Management\n\nMemory release pattern: `createPlan()` calls `createFileTasks()` which embeds file content into `systemPrompt` and `userPrompt` strings via `buildFilePrompt()`. After task creation, the loop `for (const file of files) { (file as { content: string }).content = '' }` clears `PreparedFile.content` fields to free heap memory. Runner re-reads files from disk when executing tasks.\n\n## Trace Events\n\nEmits `phase:start` event with `type: 'phase:start'`, `phase: 'plan-creation'`, `taskCount: discoveryResult.files.length`, `concurrency: 1`.\n\nEmits `plan:created` event with `type: 'plan:created'`, `planType: 'generate'`, `fileCount: files.length`, `taskCount: tasks.length + 1` (includes future root CLAUDE.md task).\n\nEmits `phase:end` event with `type: 'phase:end'`, `phase: 'plan-creation'`, `durationMs: Number(planEndTime - planStartTime) / 1_000_000`, `tasksCompleted: 1`, `tasksFailed: 0`.\n\n## Dependencies\n\nImports `readFile` from `node:fs/promises` for file content loading, `path` from `node:path` for path operations, `pc` (picocolors) for debug logging, `Config` from `../config/schema.js`, `DiscoveryResult` from `../types/index.js`, `buildFilePrompt` from `./prompts/index.js`, `analyzeComplexity` and `ComplexityMetrics` from `./complexity.js`, `ITraceWriter` from `../orchestration/trace.js`.\n\n## Debug Logging\n\nDebug mode controlled via `this.debug` flag. Logs messages via `console.error(pc.dim(...))` at key milestones: preparing files, analyzing complexity, complexity metrics (`depth=${complexity.directoryDepth}`), final plan stats (`${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)`).\n### types.ts\n**Purpose:** Defines TypeScript interfaces for the three-phase documentation generation pipeline: `AnalysisResult` for Phase 1 fil...\n\n**Defines TypeScript interfaces for the three-phase documentation generation pipeline: `AnalysisResult` for Phase 1 file analysis outputs, `SummaryMetadata` for extracted file metadata, and `SummaryOptions` for summary generation configuration.**\n\n## Exported Types\n\n**`AnalysisResult`** — Return type for Phase 1 file analysis operations populated by LLM subprocess calls via `AIService.call()`. Contains two fields:\n- `summary: string` — Generated markdown summary text written to `.sum` file body\n- `metadata: SummaryMetadata` — Structured metadata written to YAML frontmatter\n\n**`SummaryMetadata`** — Structured metadata extracted during file analysis and serialized as YAML frontmatter in `.sum` files. Fields:\n- `purpose: string` — Single-line file purpose statement (mandatory)\n- `criticalTodos?: string[]` — Optional array of security/breaking issues only (excludes routine TODOs)\n- `relatedFiles?: string[]` — Optional array of tightly coupled sibling file paths\n\n**`SummaryOptions`** — Configuration for summary generation behavior passed to prompt builders. Fields:\n- `targetLength: 'short' | 'standard' | 'detailed'` — Controls summary verbosity level\n- `includeCodeSnippets: boolean` — Toggles inclusion of code examples in generated summaries\n\n## Integration Points\n\nReferenced by:\n- `src/generation/prompts/builder.ts` — Consumes `SummaryOptions` when constructing file analysis prompts\n- `src/generation/executor.ts` — Produces `AnalysisResult` from AI subprocess responses, validates against schema\n- `src/generation/writers/sum.ts` — Serializes `AnalysisResult.metadata` as YAML frontmatter via `yaml.stringify()`, writes `AnalysisResult.summary` as markdown body\n\n## Schema Constraints\n\nNo runtime validation types exported (Zod schemas located elsewhere). `SummaryMetadata.purpose` is mandatory while `criticalTodos` and `relatedFiles` are optional arrays. Empty arrays should be omitted (undefined) rather than serialized as `[]` in YAML frontmatter.\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### prompts/\n<!-- Generated by agents-reverse-engineer -->\n\n# prompts\n\nExports prompt construction pipeline for ARE's three-phase documentation generation: `buildFilePrompt()` constructs Phase 1 file analysis prompts with import maps and project structure trees, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` for Phase 2 directory synthesis, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus in Phase 3.\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements prompt builders via template interpolation: `buildFilePrompt()` injects `filePath`/`content`/detected language (via `detectLanguage()`) into `FILE_USER_PROMPT`, appending `contextFiles[]` and optional `projectPlan`. `buildDirectoryPrompt()` aggregates `.sum` files via `readSumFile()`, child `AGENTS.md` from subdirectories, import maps via `extractDirectoryImports()`, and manifest files (9 types: package.json, Cargo.toml, go.mod, etc.), returning `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided. `buildRootPrompt()` collects all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json` metadata, embeds synthesis constraints prohibiting invented features. Returns `{system, user}` prompt pairs with incremental update system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when existing content provided.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` for AI service call construction.\n\n### [templates.ts](./templates.ts)\nExports six prompt constants: `FILE_SYSTEM_PROMPT` (density rules, identifier preservation, behavioral contract verbatim reproduction), `FILE_USER_PROMPT` (contains `{{FILE_PATH}}`, `{{CONTENT}}` placeholders), `DIRECTORY_SYSTEM_PROMPT` (adaptive section strategy, path accuracy constraints, annex linking), `FILE_UPDATE_SYSTEM_PROMPT` (preserve unchanged sections verbatim), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (modify only affected entries), `ROOT_SYSTEM_PROMPT` (synthesis-only constraint: no invented features). Defines prohibited filler phrases (\"this file\", \"provides\", \"responsible for\"), annex reference format, YAML frontmatter structure for `.sum` files.\n\n### [types.ts](./types.ts)\nDefines `PromptContext` interface (fields: `filePath`, `content`, `contextFiles?`, `projectPlan?`, `existingSum?`) and `SUMMARY_GUIDELINES` constant specifying target word count (300-500), 8 required content categories (purpose, public interface, patterns, dependencies, signatures, related files, behavioral contracts, annex references), 3 excluded categories (control flow, generic TODOs, broad architecture).\n\n## Behavioral Contracts\n\n**Mustache-Style Placeholder Substitution** (from templates.ts):\n```\n{{FILE_PATH}}  — Replaced with source file relative path\n{{CONTENT}}    — Replaced with source file content (unescaped)\n{{LANG}}       — Replaced with detected language identifier\n{{PROJECT_PLAN_SECTION}} — Replaced with project structure tree or empty string\n```\n\n**Prohibited Filler Phrases** (from templates.ts):\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**YAML Frontmatter Format** (from templates.ts):\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Language Detection Map** (from builder.ts):\n```\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n**Annex Reference Format** (from templates.ts):\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n**Manifest Detection Array** (from builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n## Annex References\n\n- Full prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [types.ts.annex.md](./types.ts.annex.md)\n\n## File Relationships\n\nbuilder.ts depends on templates.ts constants (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, etc.) for interpolation, calls `detectLanguage()` for syntax highlighting identifiers, reads `.sum` files via `readSumFile()` (from `../writers/sum.js`), checks `GENERATED_MARKER` (from `../writers/agents-md.js`) to distinguish user-authored `AGENTS.md`, extracts imports via `extractDirectoryImports()` + `formatImportMap()` (from `../../imports/index.js`), collects root corpus via `collectAgentsDocs()` (from `../collector.js`). types.ts defines `PromptContext` interface consumed by all builder functions. index.ts re-exports public API consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts`.\n### writers/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n**YAML frontmatter-based file I/O layer implementing `.sum` file persistence with SHA-256 hashing, `AGENTS.md` generation with user content preservation, and `.annex.md` verbatim source archival for reproduction-critical artifacts.**\n\n## Contents\n\n### Core Writers\n\n**[sum.ts](./sum.ts)** — SHA-256-tracked `.sum` file I/O with YAML frontmatter serialization via `writeSumFile()`/`readSumFile()`/`formatSumFile()`/`parseSumFile()`, regex-based field extraction (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), dual-format YAML array handling (inline `[a,b,c]` vs multi-line `  - item`) via `parseYamlArray()`/`formatYamlArray()`, `.annex.md` verbatim source archival via `writeAnnexFile()` for reproduction-critical files, path resolution via `getSumPath()`/`getAnnexPath()`.\n\n**[agents-md.ts](./agents-md.ts)** — `AGENTS.md` preservation logic via `writeAgentsMd()` four-step workflow: (1) detects user-authored files lacking `GENERATED_MARKER` and renames to `AGENTS.local.md`, (2) reads preserved `AGENTS.local.md` content, (3) strips marker prefix from LLM content, (4) assembles final output with marker header + user content block (`<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter) + horizontal rule + LLM content. Exports `isGeneratedAgentsMd()` marker detection predicate.\n\n**[index.ts](./index.ts)** — Barrel re-exporting `writeSumFile`/`readSumFile`/`getSumPath`/`sumFileExists`/`SumFileContent` from sum.ts and `writeAgentsMd` from agents-md.ts for unified import in `src/generation/executor.ts`.\n\n## File Naming Conventions\n\n- **Summary files**: `<sourcePath>.sum` (e.g., `foo.ts` → `foo.ts.sum`)\n- **Annex files**: `<sourcePath>.annex.md` (e.g., `foo.ts` → `foo.ts.annex.md`)\n- **Preserved user docs**: `AGENTS.md` → `AGENTS.local.md` (renamed on first generation)\n\n## YAML Frontmatter Structure\n\n`.sum` files use YAML frontmatter block delimited by `---\\n...\\n---\\n`:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex digest)\npurpose: One-line file purpose statement\ncritical_todos: [Security issue, Performance bottleneck]\nrelated_files: [../config/schema.ts, ./index.ts]\n---\n\nMarkdown summary content...\n```\n\n**Field serialization rules** (via `formatYamlArray()`):\n- Arrays with ≤3 items where all items <40 chars: inline format `key: [a, b, c]`\n- Otherwise: multi-line format with `  - ` prefix per item\n\n**Parsing patterns** (via `parseSumFile()`):\n- Frontmatter block extraction: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`\n- Arrays: inline `/key:\\s*\\[([^\\]]*)\\]/`, multi-line `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n## User Content Preservation Strategy\n\n`writeAgentsMd()` implements two-path detection for existing `AGENTS.md` files:\n\n1. **First-time generation**: If existing `AGENTS.md` lacks `GENERATED_MARKER` (via `isGeneratedAgentsMd()` substring search), rename to `AGENTS.local.md` to preserve user content\n2. **Subsequent runs**: Read `AGENTS.local.md` if already exists from prior rename operation\n\nFinal assembly concatenates:\n1. `GENERATED_MARKER` constant: `'<!-- Generated by agents-reverse-engineer -->'`\n2. User content block (if present): `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\\n<content>\\n---`\n3. LLM-generated content (with marker prefix stripped if present)\n\nEnsures AI assistants see user-defined directory context before generated summaries during codebase navigation.\n\n## Annex File Pattern\n\n**Purpose**: Archive verbatim source content for reproduction-critical files (e.g., prompt templates with regex patterns, config schemas with magic constants) whose behavioral contracts cannot fit within `.sum` word limits.\n\n**Generated format** (via `writeAnnexFile()`):\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<triple-backtick code fence with full source content>\n```\n```\n\n**Reference pattern**: `.sum` files include `## Annex References` section linking to annex: `[templates.ts.annex.md](./templates.ts.annex.md)`.\n\n## Integration with Generation Pipeline\n\n**Phase 1 (File Analysis)**:\n- `src/generation/executor.ts` calls `writeSumFile()` after LLM subprocess returns `SumFileContent`\n- `writeSumFile()` creates `.sum` alongside source file via `mkdir({ recursive: true })`\n- Optional `writeAnnexFile()` call for files flagged as reproduction-critical\n\n**Phase 2 (Directory Aggregation)**:\n- `src/generation/executor.ts` reads child `.sum` files via `readSumFile()` for aggregation prompts\n- After LLM generates directory summary, calls `writeAgentsMd()` to assemble final `AGENTS.md`\n- `writeAgentsMd()` preserves any existing `AGENTS.local.md` content above generated sections\n\n**Phase 3 (Root Synthesis)**:\n- Root document generators consume all `AGENTS.md` files via `src/generation/collector.ts`\n- No direct writer interaction (root docs written by `src/integration/generate.ts`)\n\n## Change Detection Integration\n\n`readSumFile()` exposes `contentHash` field parsed from YAML frontmatter, consumed by `src/update/orchestrator.ts` for SHA-256-based incremental update detection:\n\n1. Read current file content hash via `src/change-detection/detector.ts`\n2. Read `.sum` file's stored `contentHash` via `readSumFile()`\n3. Hash mismatch → add to regeneration queue\n4. Hash match → skip (file unchanged)\n\n## Error Handling\n\n- **`readSumFile()`**: Returns `null` on missing file or parse failure (no exceptions thrown)\n- **`sumFileExists()`**: Returns `false` if `readSumFile()` returns `null`\n- **`isGeneratedAgentsMd()`**: Returns `false` on read errors (treats missing files as non-generated)\n- **`writeAgentsMd()`**: Silent try-catch blocks for existing `AGENTS.md` / `AGENTS.local.md` reads, treats absence as `null` user content\n- **`writeSumFile()` / `writeAnnexFile()`**: No error handling—writeFile failures bubble to caller\n\n## Behavioral Contracts\n\n### Regex Patterns (sum.ts)\n\n**Frontmatter extraction**:\n- Block delimiter: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**Value normalization**:\n- Quote stripping: `/^[\"']|[\"']$/g` (removes leading/trailing quotes from inline array items)\n- Leading newline removal: `/^\\n+/` (strips marker prefix from LLM content)\n\n### Constants\n\n**GENERATED_MARKER** (agents-md.ts): `'<!-- Generated by agents-reverse-engineer -->'` — marker for tool-generated `AGENTS.md` detection, referenced by `isGeneratedAgentsMd()` substring search and `writeAgentsMd()` marker injection/stripping.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates ARE's three-phase documentation generation pipeline: Phase 1 concurrent file analysis via worker pools generating `.sum` files with SHA-256 hashes, Phase 2 post-order directory aggregation synthesizing `AGENTS.md` from child summaries, Phase 3 sequential root document synthesis producing `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` from complete corpus.**\n\n## Contents\n\n### Pipeline Coordination\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` class exports `createPlan()` method constructing `GenerationPlan` via four-step workflow: `prepareFiles()` loads file content into `PreparedFile[]`, `analyzeComplexity()` computes `directoryDepth` and unique `directories` set, `buildProjectStructure()` formats compact directory tree, `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()`, `createDirectoryTasks()` groups files by directory returning directory-level tasks. Emits trace events (`phase:start`, `plan:created`, `phase:end`) via injected `ITraceWriter`. Clears `PreparedFile.content` after prompt embedding to free heap memory. Returns `{ files, tasks, complexity, projectStructure }`.\n\n**[executor.ts](./executor.ts)** — Transforms `GenerationPlan` into dependency-aware `ExecutionPlan` via `buildExecutionPlan()`: groups files by directory into `directoryFileMap`, creates `fileTasks[]` with `id: 'file:${path}'` and empty `dependencies[]`, sorts by depth descending for leaf-first processing, creates `directoryTasks[]` depending on child file task IDs for post-order traversal, creates `rootTasks[]` depending on all directory task IDs. Exports `isDirectoryComplete()` predicate checking child `.sum` file existence via `sumFileExists()` and `getReadyDirectories()` async filter. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with three-phase checklist grouped by directory depth.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` via `calculateDirectoryDepth()` (max depth via `split(sep).length - 1`) and `extractDirectories()` (unique directories via upward `dirname()` traversal). Returns `{ fileCount, directoryDepth, files, directories }` consumed by orchestrator for concurrency tuning and Phase 2 directory queue construction.\n\n**[collector.ts](./collector.ts)** — Exports `collectAgentsDocs()` recursively walking project tree collecting `AGENTS.md` files as `AgentsDocs` array of `{ relativePath, content }` sorted alphabetically, and `collectAnnexFiles()` similarly collecting `.annex.md` files. Both skip `SKIP_DIRS` set (13 entries: node_modules, .git, vendor, dist, build, etc.) and silently suppress permission errors.\n\n**[types.ts](./types.ts)** — Defines `AnalysisResult` containing `summary: string` and `metadata: SummaryMetadata` returned by Phase 1 AI subprocess calls, `SummaryMetadata` YAML frontmatter schema with `purpose`, `criticalTodos?`, `relatedFiles?` fields, and `SummaryOptions` for summary verbosity configuration.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Template-based prompt construction pipeline: `buildFilePrompt()` injects file path/content/imports into `FILE_USER_PROMPT` with density rules and identifier preservation constraints, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` with manifest detection (9 types) and import maps via `extractDirectoryImports()`, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus with synthesis-only constraints prohibiting invented features. Exports six prompt constants with mustache-style placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`), prohibited filler phrases, YAML frontmatter format, and annex reference format.\n\n**[writers/](./writers/)** — YAML frontmatter-based file I/O layer: `writeSumFile()`/`readSumFile()` implement `.sum` persistence with SHA-256 `content_hash` via regex-based field extraction and dual-format YAML array handling (inline `[a,b,c]` vs multi-line), `writeAgentsMd()` preserves user-authored `AGENTS.md` by renaming to `AGENTS.local.md` and prepending above generated content with `GENERATED_MARKER` injection/stripping, `writeAnnexFile()` archives verbatim source for reproduction-critical files (prompt templates, config schemas). Exports `sumFileExists()` predicate for change detection and `isGeneratedAgentsMd()` marker detection.\n\n## Three-Phase Execution Strategy\n\n**Phase 1: Concurrent File Analysis**\n- Orchestrator creates `fileTasks[]` with prompts via `buildFilePrompt()` embedding import maps and project structure\n- Runner spawns worker pool (`src/orchestration/pool.ts`) executing tasks concurrently (default concurrency: 2 for WSL, 5 elsewhere)\n- Each worker calls `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`--max-old-space-size=512`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`)\n- Worker writes `AnalysisResult` via `writeSumFile()` with YAML frontmatter containing SHA-256 `content_hash` and markdown summary body\n\n**Phase 2: Post-Order Directory Aggregation**\n- Executor sorts `directoryTasks[]` by depth descending (deepest first) via `getDirectoryDepth()` ensuring child directories complete before parents\n- Runner sequentially processes directories checking readiness via `isDirectoryComplete()` predicate polling for child `.sum` file existence\n- Prompt builder calls `buildDirectoryPrompt()` reading child `.sum` files via `readSumFile()`, aggregating subdirectory `AGENTS.md`, extracting imports via `extractDirectoryImports()`, detecting manifests (9 types: package.json, Cargo.toml, go.mod, etc.)\n- Runner writes `AGENTS.md` via `writeAgentsMd()` preserving any `AGENTS.local.md` user content above generated sections\n\n**Phase 3: Sequential Root Synthesis**\n- Executor creates `rootTasks[]` depending on all directory task IDs enforcing sequential execution (concurrency=1)\n- Prompt builder calls `buildRootPrompt()` consuming all `AGENTS.md` files via `collectAgentsDocs()`, reading root `package.json` metadata, embedding synthesis constraints prohibiting invented features\n- Runner writes platform-specific root documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) via `src/integration/generate.ts`\n\n## Post-Order Traversal Mechanism\n\nExecutor sorts directory tasks by depth descending:\n```typescript\ndirectoryTasks.sort((a, b) => \n  getDirectoryDepth(b.path) - getDirectoryDepth(a.path)\n)\n```\n\nwhere `getDirectoryDepth('.')` returns `0`, `getDirectoryDepth('src')` returns `1`, `getDirectoryDepth('src/cli')` returns `2`. Deepest directories process first ensuring child `AGENTS.md` exist before parent aggregation attempts. Runner polls `isDirectoryComplete()` checking all expected `.sum` files exist via `sumFileExists()` before processing directory task.\n\n## Memory Management Pattern\n\nOrchestrator clears `PreparedFile.content` after prompt construction:\n```typescript\nfor (const file of files) {\n  (file as { content: string }).content = ''\n}\n```\n\nThis frees heap memory since file content already embedded in `AnalysisTask.userPrompt` strings. Runner re-reads files from disk during execution if needed. Prevents memory exhaustion on large codebases (10k+ files).\n\n## Integration Points\n\nConsumes:\n- `DiscoveryResult` from `src/discovery/walker.ts` (file list input)\n- `Config` from `src/config/schema.ts` (concurrency, timeout, model settings)\n- `ITraceWriter` from `src/orchestration/trace.ts` (event emission)\n- `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt` from `./prompts/builder.ts`\n- `writeSumFile`, `writeAgentsMd` from `./writers/`\n- `extractDirectoryImports` from `src/imports/extractor.ts`\n- `collectAgentsDocs` from `./collector.ts`\n\nProduces:\n- `GenerationPlan` consumed by `src/orchestration/runner.ts`\n- `ExecutionPlan` consumed by Phase 1/2/3 execution loops\n- `.sum` files consumed by `src/update/orchestrator.ts` for change detection\n- `AGENTS.md` files consumed by Phase 3 root synthesis and `src/specify/index.ts`\n- `GENERATION-PLAN.md` consumed by progress tracking\n\nReferenced by:\n- `src/cli/generate.ts` (command entry point)\n- `src/cli/update.ts` (incremental update workflow)\n- `src/orchestration/runner.ts` (phase execution orchestrator)\n\n## Behavioral Contracts\n\n### Depth Calculation (executor.ts)\n```typescript\ngetDirectoryDepth('.')          → 0\ngetDirectoryDepth('src')        → 1\ngetDirectoryDepth('src/cli')    → 2\ngetDirectoryDepth('a/b/c/d')    → 4\n```\n\n### File Task Dependencies (executor.ts)\n```typescript\nfileTasks.forEach(task => task.dependencies = [])  // No dependencies, all parallel\n```\n\n### Directory Task Dependencies (executor.ts)\n```typescript\ndirectoryTask.dependencies = directoryFileMap[dirPath].map(f => `file:${f}`)\n```\n\n### Root Task Dependencies (executor.ts)\n```typescript\nrootTask.dependencies = directoryTasks.map(t => t.id)  // All directories\n```\n\n### SKIP_DIRS Set (collector.ts)\n```typescript\n['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', \n 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle']\n```\n\n### Manifest Detection Array (prompts/builder.ts)\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n### Language Detection Map (prompts/builder.ts)\n```typescript\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n## Annex References\n\n- Full prompt template text: [prompts/templates.ts.annex.md](./prompts/templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [prompts/types.ts.annex.md](./prompts/types.ts.annex.md)\n- Phase 2/3 execution workflow details: [../orchestration/runner.ts.annex.md](../orchestration/runner.ts.annex.md)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2924,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 17416,
      "latencyMs": 71950,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.248Z",
      "prompt": "Generate AGENTS.md for directory: \"src/installer\" (installer)\n\n## File Summaries (7 files)\n\n### banner.ts\n**Purpose:** banner.ts provides ASCII art banner display and styled terminal output functions for the ARE installer using picocolors.\n\n**banner.ts provides ASCII art banner display and styled terminal output functions for the ARE installer using picocolors.**\n\n## Exported Functions\n\n`displayBanner()` renders green ASCII art spelling \"ARE\" in block letters, followed by dimmed version string (`v${VERSION}`) and tagline \"AI-friendly codebase documentation\".\n\n`showHelp()` prints usage syntax, option flags (`--runtime`, `-g/--global`, `-l/--local`, `-u/--uninstall`, `--force`, `-q/--quiet`, `-h/--help`), and example invocations for interactive mode, global/local installation, and uninstallation.\n\n`showSuccess(msg: string)` displays message prefixed with green `✓` checkmark via `pc.green('✓') + ' ' + msg`.\n\n`showError(msg: string)` displays message prefixed with red `✗` via `pc.red('✗') + ' ' + msg`.\n\n`showWarning(msg: string)` displays message prefixed with yellow `!` via `pc.yellow('!') + ' ' + msg`.\n\n`showInfo(msg: string)` displays message prefixed with cyan `>` via `pc.cyan('>') + ' ' + msg`.\n\n`showNextSteps(runtime: string, filesCreated: number)` prints installation completion summary with file count, then numbered list of next steps: `/are-help` to verify, `/are-init` to initialize, `/are-discover` for plan, `/are-generate` for docs, `/are-update` for changes, `/are-specify` for spec, `/are-clean` for cleanup, followed by GitHub docs URL.\n\n## Exported Constants\n\n`VERSION` string constant imported from `getVersion()` in `../version.js`, used in banner output.\n\n## Dependencies\n\n`picocolors` (`pc`) provides terminal color functions: `pc.green()`, `pc.red()`, `pc.yellow()`, `pc.cyan()`, `pc.dim()`, `pc.bold()`.\n\n`getVersion()` from `../version.js` reads package version at module load time.\n\n## Terminal Output Patterns\n\nAll output functions write directly to `console.log()` with no return values. Banner includes 7 lines of ASCII art using box-drawing characters (`█`, `╗`, `╔`, `═`, `║`) followed by 3 lines of metadata. Help text uses bold headings (`pc.bold('Usage:')`, `pc.bold('Options:')`) with cyan-highlighted command examples. Next steps uses bold heading, dim file count, bold subheading, then numbered list with cyan-highlighted slash commands and dim GitHub URL footer.\n### index.ts\n**Purpose:** Main installer entry point orchestrating npx installation workflow with interactive prompts, flag parsing, installati...\n\n**Main installer entry point orchestrating npx installation workflow with interactive prompts, flag parsing, installation/uninstallation execution, and result display for ARE command/hook deployment.**\n\n## Exported Functions\n\n**`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>`** — Main entry point executing install/uninstall workflow with interactive prompts or non-interactive flag-based mode, returns array of per-runtime installation results.\n\n**`parseInstallerArgs(args: string[]): InstallerArgs`** — Parses command-line arguments into `InstallerArgs` structure, handles short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--help`, `--quiet`, `--force`, `--runtime <value>`), validates runtime against `['claude', 'opencode', 'gemini', 'all']`.\n\n## Re-Exported Types and Functions\n\nRe-exports from submodules: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`, `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`, `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION`, `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`.\n\n## Installation Workflow\n\n**`runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>`** — Executes installation via `installFiles()`, verifies via `verifyInstallation()` on `allCreatedFiles`, displays results unless quiet mode enabled, returns installation results array.\n\n**`runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]`** — Executes uninstallation via `uninstallFiles()`, deletes `.agents-reverse-engineer` config folder via `deleteConfigFolder(location, false)` for local installs, displays results unless quiet, returns uninstallation results array.\n\n## Argument Resolution\n\n**`determineLocation(args: InstallerArgs): Location | undefined`** — Returns `'global'` if `args.global && !args.local`, `'local'` if `args.local && !args.global`, undefined otherwise (triggers interactive prompt).\n\n**`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>`** — Returns empty array if runtime undefined (needs prompt), calls `getAllRuntimes()` if runtime is `'all'`, returns single-element array `[runtime]` for specific runtimes.\n\n## Display Functions\n\n**`displayInstallResults(results: InstallerResult[]): void`** — Shows per-runtime success/error messages via `showSuccess()`/`showError()`, aggregates `totalCreated`/`totalSkipped`/`hooksRegistered` counts, displays summary with `showSuccess()`/`showWarning()`, calls `showNextSteps(primaryRuntime, totalCreated)`, prints GitHub docs link.\n\n**`displayUninstallResults(results: InstallerResult[], configDeleted?: boolean): void`** — Shows per-runtime uninstall status (repurposes `filesCreated` as deleted file count, `filesSkipped` as not-found count), aggregates `totalDeleted`/`hooksUnregistered`, displays summary with config folder deletion status.\n\n## Non-Interactive Mode Requirements\n\nNon-interactive mode (when `!isInteractive()`) requires `--runtime` flag and either `-g`/`--global` or `-l`/`--local` flag, exits with error via `showError()` + `process.exit(1)` if missing.\n\n## Integration Dependencies\n\nImports `InstallerArgs`, `InstallerResult`, `Runtime`, `Location` from `./types.js`, `getAllRuntimes`, `resolveInstallPath` from `./paths.js`, display functions from `./banner.js`, `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive` from `./prompts.js`, `installFiles`, `verifyInstallation`, `formatInstallResult` from `./operations.js`, `uninstallFiles`, `deleteConfigFolder` from `./uninstall.js`.\n\n## Control Flow Pattern\n\n`runInstaller()` checks `args.help` first (displays help, returns empty array), displays banner unless quiet, determines location/runtime from flags, enforces non-interactive requirements, prompts for missing values in interactive mode, branches to `runUninstall()` if `args.uninstall`, otherwise calls `runInstall()`.\n### operations.ts\n**Purpose:** operations.ts orchestrates file copying, settings.json hook registration, and permission configuration for IDE comman...\n\n**operations.ts orchestrates file copying, settings.json hook registration, and permission configuration for IDE command/hook installation across Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Exported Functions\n\n**installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]** — installs command templates and hooks for one or all runtimes, returning array of InstallerResult objects. When runtime='all', iterates getAllRuntimes() and calls installFilesForRuntime() for each. Otherwise returns single-element array from installFilesForRuntime().\n\n**verifyInstallation(files: string[]): { success: boolean; missing: string[] }** — checks existsSync() for each file path, returns missing array of non-existent files.\n\n**registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean** — updates settings.json with SessionStart/SessionEnd hook registrations, returns true if any hook added. Routes to registerClaudeHooks() or registerGeminiHooks() based on runtime.\n\n**registerPermissions(settingsPath: string, dryRun: boolean): boolean** — adds ARE_PERMISSIONS array entries to settings.json permissions.allow for Claude Code, returns true if any permission added.\n\n**getPackageVersion(): string** — reads package.json from `__dirname/../../package.json` via fileURLToPath(import.meta.url), returns version field or 'unknown'.\n\n**writeVersionFile(basePath: string, dryRun: boolean): void** — writes getPackageVersion() result to `${basePath}/ARE-VERSION`.\n\n**formatInstallResult(result: InstallerResult): string[]** — generates human-readable lines showing created/skipped files, hook registration status, summary counts.\n\n## Exported Interfaces\n\n**InstallOptions** — `{ force: boolean; dryRun: boolean }` controls overwrite behavior and preview mode.\n\n## Internal Architecture\n\n**installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult** — main worker function handling:\n1. Resolves basePath via resolveInstallPath()\n2. Fetches templates via getTemplatesForRuntime()\n3. Iterates templates, writes to `${basePath}/${relativePath}` after extracting path component past runtime directory (e.g., `commands/are/generate.md` from `.claude/commands/are/generate.md`)\n4. For claude/gemini: copies ARE_HOOKS entries from getBundledHookPath() to `${basePath}/hooks/`, then calls registerHooks() and optionally registerPermissions() for Claude\n5. For opencode: copies ARE_PLUGINS entries from srcFilename to `${basePath}/plugins/${destFilename}`, sets hookRegistered=true\n6. Calls writeVersionFile() if filesCreated.length > 0 and not dryRun\n7. Returns InstallerResult with success, filesCreated, filesSkipped, errors, hookRegistered, versionWritten\n\n**getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)** — switches on runtime to return getClaudeTemplates(), getOpenCodeTemplates(), or getGeminiTemplates() from `../integration/templates.js`.\n\n**getBundledHookPath(hookName: string): string** — constructs path from `__dirname/../../hooks/dist/${hookName}` where `__dirname` is derived via fileURLToPath(import.meta.url). Hooks bundled during npm prepublishOnly via build-hooks.js.\n\n**readBundledHook(hookName: string): string** — reads hook content from getBundledHookPath(), throws Error if not found.\n\n**ensureDir(filePath: string): void** — calls mkdirSync(path.dirname(filePath), { recursive: true }) if directory doesn't exist.\n\n## Hook Registration Formats\n\n**registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean** — parses settings.json as SettingsJson schema, ensures settings.hooks[event] exists for each ARE_HOOKS entry, appends HookEvent with nested hooks array containing `{ type: 'command', command: 'node ${runtimeDir}/hooks/${filename}' }`. Writes JSON with 2-space indentation.\n\n**registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean** — parses settings.json as GeminiSettingsJson schema, appends flat GeminiHook objects with name field to settings.hooks[event] arrays. Command format identical to Claude: `node ${runtimeDir}/hooks/${filename}`.\n\n**SettingsJson** — Claude Code schema with `hooks?: { SessionStart?: HookEvent[]; SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[]; deny?: string[] }`. HookEvent contains `hooks: SessionHook[]` where SessionHook is `{ type: 'command'; command: string }`.\n\n**GeminiSettingsJson** — simpler schema with `hooks?: { SessionStart?: GeminiHook[]; SessionEnd?: GeminiHook[] }` where GeminiHook is `{ name: string; type: 'command'; command: string }`.\n\n## Hook and Plugin Definitions\n\n**ARE_HOOKS: HookDefinition[]** — currently empty array (both SessionStart and SessionEnd entries commented out due to \"causing issues\"). HookDefinition schema: `{ event: 'SessionStart' | 'SessionEnd'; filename: string; name: string }`. Intended entries:\n- `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`\n- `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**ARE_PLUGINS: PluginDefinition[]** — OpenCode plugin mappings with one active entry and one commented. PluginDefinition schema: `{ srcFilename: string; destFilename: string }`. Active entry maps `opencode-are-check-update.js` → `are-check-update.js`. Commented entry maps `opencode-are-session-end.js` → `are-session-end.js`.\n\n**ARE_PERMISSIONS: string[]** — array of 7 bash command patterns for Claude Code auto-approval:\n- `Bash(npx agents-reverse-engineer@latest init*)`\n- `Bash(npx agents-reverse-engineer@latest discover*)`\n- `Bash(npx agents-reverse-engineer@latest generate*)`\n- `Bash(npx agents-reverse-engineer@latest update*)`\n- `Bash(npx agents-reverse-engineer@latest clean*)`\n- `Bash(rm -f .agents-reverse-engineer/progress.log*)`\n- `Bash(sleep *)`\n\n## Error Handling\n\nFile write failures append to errors array with format `Failed to write ${fullPath}: ${err}` or `Failed to write hook ${hookPath}: ${err}`. Subsequent filesCreated filtering checks `errors.some((e) => e.includes(hookPath))` to prevent listing failed files as created. JSON parse failures in registerHooks/registerPermissions silently reset settings to empty object `{}`. Missing bundled hooks throw Error in readBundledHook(). Version file write failures non-fatal (caught without adding to errors). InstallerResult.success = errors.length === 0.\n\n## File System Conventions\n\nTemplate paths contain runtime directory prefix (e.g., `.claude/commands/are/generate.md`). Relative path extraction via `template.path.split('/').slice(1).join('/')` removes first segment to yield `commands/are/generate.md`. Full installation path: `${basePath}/${relativePath}` where basePath from resolveInstallPath(runtime, location). Hook paths: `${basePath}/hooks/${filename}`. OpenCode plugin paths: `${basePath}/plugins/${destFilename}`. Settings path: `${basePath}/settings.json`. Version file: `${basePath}/ARE-VERSION`.\n### paths.ts\n**Purpose:** paths.ts resolves cross-platform directory paths for AI runtime installations (Claude, OpenCode, Gemini) with environ...\n\n**paths.ts resolves cross-platform directory paths for AI runtime installations (Claude, OpenCode, Gemini) with environment variable overrides and local/global installation detection.**\n\n## Exported Functions\n\n`getAllRuntimes()` returns `Array<Exclude<Runtime, 'all'>>` containing `['claude', 'opencode', 'gemini']` as the set of concrete runtime identifiers (excludes the meta-runtime `'all'`).\n\n`getRuntimePaths(runtime: Exclude<Runtime, 'all'>)` returns `RuntimePaths` with `global`, `local`, and `settingsFile` string paths for the specified runtime. Uses `os.homedir()` for absolute paths and `path.join()` for cross-platform compatibility. Applies environment variable overrides: `CLAUDE_CONFIG_DIR` for Claude (default `~/.claude`), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME` for OpenCode (default `~/.config/opencode`), `GEMINI_CONFIG_DIR` for Gemini (default `~/.gemini`). Returns `local` paths as relative strings: `.claude`, `.opencode`, `.gemini`.\n\n`resolveInstallPath(runtime: Exclude<Runtime, 'all'>, location: Location, projectRoot?: string)` returns absolute installation path string. For `location === 'global'`, returns `paths.global` from `getRuntimePaths()`. For local installations, joins `paths.local` with `projectRoot` (defaults to `process.cwd()` if not provided).\n\n`getSettingsPath(runtime: Exclude<Runtime, 'all'>)` returns absolute path string to runtime settings file (`settings.json`) used for hook registration. Delegates to `getRuntimePaths(runtime).settingsFile`.\n\n`isRuntimeInstalledLocally(runtime: Exclude<Runtime, 'all'>, projectRoot: string)` returns `Promise<boolean>` indicating whether the runtime's local config directory exists in the project. Uses `stat()` from `node:fs/promises` to check if path is directory, returns `false` on error.\n\n`isRuntimeInstalledGlobally(runtime: Exclude<Runtime, 'all'>)` returns `Promise<boolean>` indicating whether the runtime's global config directory exists. Uses `stat()` to check `paths.global`, returns `false` on error.\n\n`getInstalledRuntimes(projectRoot: string)` returns `Promise<Array<Exclude<Runtime, 'all'>>>` containing runtime identifiers installed locally in the project. Iterates over `getAllRuntimes()` and filters using `isRuntimeInstalledLocally()`.\n\n## Environment Variable Precedence\n\nClaude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`.\n\nOpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` which overrides `~/.config/opencode`.\n\nGemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`.\n\n## Dependencies\n\nImports `os.homedir()` from `node:os`, `path.join()` from `node:path`, `stat()` from `node:fs/promises`. Imports `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.\n### prompts.ts\n**Purpose:** prompts.ts provides interactive selection prompts for the installer with arrow-key navigation in TTY mode and numbere...\n\n**prompts.ts provides interactive selection prompts for the installer with arrow-key navigation in TTY mode and numbered fallback for CI/non-interactive environments, using Node.js readline with raw mode cleanup via try/finally and process exit handlers.**\n\n## Public Interface\n\n### Type Exports\n\n```typescript\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n```\n\n### Exported Functions\n\n**isInteractive(): boolean** — Returns `process.stdin.isTTY === true` to detect interactive terminal vs CI/piped input.\n\n**selectOption<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Generic option selector dispatching to `arrowKeySelect()` for TTY or `numberedSelect()` for non-TTY environments.\n\n**selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>** — Prompts for runtime selection with options: `'claude'`, `'opencode'`, `'gemini'`, `'all'`. Passes mode to customize prompt text (\"Select runtime to install:\" vs \"Select runtime to uninstall:\").\n\n**selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>** — Prompts for installation location with options: `'global'` (for `~/.claude`, `~/.config/opencode`) or `'local'` (for `./.claude`, `./.opencode`).\n\n**confirmAction(message: string): Promise<boolean>** — Displays confirmation prompt with \"Yes\"/\"No\" options returning true/false.\n\n## Interactive Terminal Handling\n\n**arrowKeySelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Private function enabling arrow key navigation (up/down/enter) via `readline.emitKeypressEvents()` and `process.stdin.setRawMode(true)`. Maintains `selectedIndex` state and re-renders option list with ANSI escape sequences: `\\x1b[${n}A` (cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (cursor down). Selected option highlighted with `pc.cyan('> ')` prefix. Keypress handler responds to `key.name === 'up' | 'down' | 'return'` and `key.ctrl && key.name === 'c'` for graceful exit.\n\n**numberedSelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Private fallback for non-TTY environments printing numbered list (1-indexed) and reading selection via `readline.createInterface()`. Validates input range `1 <= num <= options.length`, rejects with `Error('Invalid selection: ${answer}. Expected 1-${options.length}')` on invalid input.\n\n## Raw Mode Lifecycle Management\n\n**rawModeActive: boolean** — Module-level state tracker ensuring cleanup even on error/interrupt.\n\n**cleanupRawMode(): void** — Restores terminal state via `process.stdin.setRawMode(false)` and `process.stdin.pause()`, guarded by try/catch to ignore errors during cleanup. Sets `rawModeActive = false`.\n\n**Global handlers** — Registered via `process.on('exit', cleanupRawMode)` and `process.on('SIGINT', ...)` to ensure raw mode cleanup on process termination.\n\n**arrowKeySelect error handling** — Wraps raw mode setup in try/catch, calls `cleanupRawMode()` before re-throwing errors. Uses try/finally pattern implicitly via Promise resolution path.\n\n## Dependencies\n\nImports `readline` from Node.js stdlib for keypress events and interface creation, `picocolors` as `pc` for ANSI color formatting (`pc.cyan()`, `pc.bold()`), and types `Runtime`, `Location` from `./types.js`.\n\n## Behavioral Contracts\n\n**Prompt text patterns:**\n- Install mode: `\"Select runtime to install:\"`, `\"Select installation location:\"`\n- Uninstall mode: `\"Select runtime to uninstall:\"`, `\"Select uninstallation location:\"`\n\n**Runtime option labels:** `\"Claude Code\"`, `\"OpenCode\"`, `\"Gemini CLI\"`, `\"All runtimes\"` mapping to values `'claude'`, `'opencode'`, `'gemini'`, `'all'`.\n\n**Location option labels:** `\"Global (~/.claude, ~/.config/opencode, etc.)\"`, `\"Local (./.claude, ./.opencode, etc.)\"` mapping to values `'global'`, `'local'`.\n\n**ANSI escape sequences:**\n- Cursor movement: `\\x1b[${n}A` (up n lines), `\\x1b[1B` (down 1 line)\n- Line operations: `\\x1b[2K` (clear entire line)\n\n**Keypress event matching:** Checks `key.name` against strings `'up'`, `'down'`, `'return'` and compound check `key.ctrl && key.name === 'c'` for interrupt handling.\n### types.ts\n**Purpose:** Defines TypeScript interfaces and type aliases for the npx-driven installer workflow that copies ARE commands and hoo...\n\n**Defines TypeScript interfaces and type aliases for the npx-driven installer workflow that copies ARE commands and hooks to AI runtime configuration directories.**\n\n## Exported Types\n\n**`Runtime`** — String literal union `'claude' | 'opencode' | 'gemini' | 'all'` specifying target AI assistant runtime for installation.\n\n**`Location`** — String literal union `'global' | 'local'` distinguishing user-level installations (`~/.claude`, `~/.config/opencode`, `~/.gemini`) from project-level installations (`.claude`, `.opencode`, `.gemini`).\n\n**`InstallerArgs`** — Interface for parsed command-line arguments supporting both interactive prompts and non-interactive flag mode:\n- `runtime?: Runtime` — Target runtime or `'all'` for multi-runtime installation\n- `global: boolean` — Install to user-level configuration directory\n- `local: boolean` — Install to project-level configuration directory\n- `uninstall: boolean` — Uninstall instead of install\n- `force: boolean` — Overwrite existing files without prompting\n- `help: boolean` — Display help message and exit\n- `quiet: boolean` — Suppress banner and informational output\n\n**`InstallerResult`** — Interface capturing outcome of single runtime/location installation operation:\n- `success: boolean` — Overall operation success indicator\n- `runtime: Exclude<Runtime, 'all'>` — Installed runtime (excludes synthetic `'all'` union member)\n- `location: Location` — Installation target location\n- `filesCreated: string[]` — Absolute paths of successfully written files\n- `filesSkipped: string[]` — Paths skipped due to existing files when `force: false`\n- `errors: string[]` — Error messages from failed operations\n- `hookRegistered?: boolean` — Claude-specific flag indicating `settings.json` hook registration success\n- `versionWritten?: boolean` — Flag indicating VERSION file creation for update check system\n\n**`RuntimePaths`** — Interface defining resolved filesystem paths for runtime installation targets:\n- `global: string` — User-level configuration directory (e.g., `~/.claude`)\n- `local: string` — Project-level configuration directory (e.g., `.claude`)\n- `settingsFile: string` — Path to runtime's settings file for hook registration (e.g., `~/.claude/settings.json`)\n\n## Type Constraints\n\n`InstallerResult.runtime` uses `Exclude<Runtime, 'all'>` to ensure result objects reference concrete runtimes (`'claude'`, `'opencode'`, `'gemini'`) rather than the synthetic `'all'` selector used in `InstallerArgs.runtime`.\n\n## Integration Points\n\nConsumed by:\n- `src/installer/prompts.ts` — Interactive CLI prompts for runtime/location selection\n- `src/installer/operations.ts` — File copying, hook registration, VERSION file writing\n- `src/installer/paths.ts` — Runtime path resolution with environment variable overrides\n- `src/installer/uninstall.ts` — Removal of installed files and hook deregistration\n### uninstall.ts\n**Purpose:** uninstall.ts reverses installer operations by removing ARE command templates, hook files, VERSION markers, and settin...\n\n**uninstall.ts reverses installer operations by removing ARE command templates, hook files, VERSION markers, and settings.json registrations for Claude/Gemini/OpenCode runtimes with directory cleanup and legacy artifact removal.**\n\n## Exported Functions\n\n**uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]** removes all ARE artifacts from specified runtime (or all runtimes if `runtime === 'all'`), returns array of `InstallerResult` objects where `filesCreated` repurposed to track deleted files and `hookRegistered` repurposed to indicate hook unregistration success.\n\n**unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean** removes ARE hook entries from `settings.json` at `basePath` by dispatching to `unregisterClaudeHooks()` or `unregisterGeminiHooks()` based on runtime, returns true if any hooks removed.\n\n**unregisterPermissions(basePath: string, dryRun: boolean): boolean** removes all `ARE_PERMISSIONS` entries from Claude Code `settings.json` permissions.allow array, cleans up empty permission structures, returns true if any permissions removed.\n\n**deleteConfigFolder(location: Location, dryRun: boolean): boolean** deletes `.agents-reverse-engineer` directory via `rmSync({ recursive: true, force: true })` only when `location === 'local'`, returns true if folder existed and was deleted.\n\n## Hook and Permission Constants\n\n**ARE_HOOKS: HookDefinition[]** defines two hook registrations: `{ event: 'SessionStart', filename: 'are-check-update.js' }` and `{ event: 'SessionEnd', filename: 'are-session-end.js' }`, must match `operations.ts` definitions.\n\n**ARE_PLUGIN_FILENAMES: string[]** lists OpenCode plugin files: `['are-check-update.js', 'are-session-end.js']`.\n\n**ARE_PERMISSIONS: string[]** contains five Bash permission patterns for ARE commands (init/discover/generate/update/clean) formatted as `'Bash(npx agents-reverse-engineer@latest <command>*)'`.\n\n**CONFIG_DIR: string** equals `'.agents-reverse-engineer'`, matches `config/loader.ts` constant.\n\n## Settings.json Schema Types\n\n**SettingsJson** interface models Claude Code settings structure with `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`, and index signature for other fields.\n\n**HookEvent** wraps `hooks: SessionHook[]` array, where **SessionHook** defines `{ type: 'command', command: string }`.\n\n**GeminiSettingsJson** uses simpler hook format with `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`, where **GeminiHook** includes `name: string` field alongside `type` and `command`.\n\n**HookDefinition** specifies `event: 'SessionStart' | 'SessionEnd'` and `filename: string` for ARE hook configuration.\n\n## Uninstallation Logic\n\n**uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult** orchestrates removal sequence: fetches templates via `getTemplatesForRuntime()`, deletes command files by joining `basePath` with template path stripped of runtime prefix, removes hook files from `basePath/hooks/` for Claude/Gemini or plugin files from `basePath/plugins/` for OpenCode, calls `unregisterHooks()` and `unregisterPermissions()` for Claude global installs, deletes `ARE-VERSION` file, invokes directory cleanup helpers unless `dryRun === true`.\n\n**getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)** switches on runtime to return `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` from `integration/templates.ts`.\n\n**getHookPatterns(runtimeDir: string): string[]** builds array of hook command strings by combining each `ARE_HOOKS` filename with current path format `node ${runtimeDir}/hooks/${filename}` and legacy format `node hooks/${filename}` for backward compatibility.\n\n## Hook Unregistration\n\n**unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean** loads `settings.json` from `basePath`, filters `hooks.SessionStart` and `hooks.SessionEnd` arrays to remove entries where any hook command matches `getHookPatterns('.claude')`, deletes empty event arrays and hooks object, writes updated JSON with 2-space indent via `JSON.stringify(settings, null, 2)` unless dry run, returns true if any hooks removed.\n\n**unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean** mirrors Claude logic but filters `GeminiHook[]` arrays directly by matching `h.command` against `getHookPatterns('.gemini')`, handles simpler schema without nested `event.hooks` structure.\n\n## Directory Cleanup\n\n**cleanupAreSkillDirs(skillsDir: string): void** iterates entries in `skillsDir`, calls `cleanupEmptyDirs()` recursively on any directory name starting with `'are-'`, used for Claude skills format.\n\n**cleanupEmptyDirs(dirPath: string): void** recursively removes empty directories via `rmdirSync()`, walks up parent chain via `path.dirname()` until hitting non-empty directory or runtime root (`.claude`, `.opencode`, `.gemini`, `.config`), ignores errors from missing directories or permission issues.\n\n**cleanupLegacyGeminiFiles(commandsDir: string): void** removes legacy `are-*.md` files from flat commands directory (pre-TOML format), deletes old `.toml` files from nested `commands/are/` subdirectory (pre-flat structure), calls `cleanupEmptyDirs()` on are/ subdirectory after file deletion.\n\n## Integration Points\n\nUses `resolveInstallPath()`, `getAllRuntimes()`, `getRuntimePaths()` from `./paths.js` for path resolution. Fetches command templates via `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `../integration/templates.js`. Returns `InstallerResult` type with runtime/location/filesCreated/filesSkipped/errors/hookRegistered fields defined in `./types.js`. Deletes config folder matching `CONFIG_DIR` constant from `config/loader.ts`. Must maintain symmetry with `operations.ts` hook/permission definitions to ensure complete uninstall.\n\n## Error Handling\n\nAll filesystem operations wrapped in try-catch blocks that push error messages to `errors[]` array on failure. Missing `settings.json` or JSON parse failures return false from unregister functions. Directory cleanup helpers silently ignore errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` options to handle missing paths gracefully. Dry run mode skips all write operations (`unlinkSync`, `writeFileSync`, `rmSync`) while still performing validation and tracking.\n\n## Import Map (verified — use these exact paths)\n\nbanner.ts:\n  ../version.js → getVersion\n\noperations.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\nuninstall.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/installer/\n\n**npx-driven installer orchestrating IDE command/hook deployment across Claude Code, OpenCode, and Gemini CLI with interactive prompts, platform-specific settings.json registration, detached version-check processes, and recursive empty directory cleanup.**\n\n## Contents\n\n### [banner.ts](./banner.ts)\nASCII art rendering and terminal output styling. `displayBanner()` renders 7-line \"ARE\" logo with version string, `showHelp()` prints usage with cyan-highlighted examples, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` output prefixed symbols (`✓`/`✗`/`!`/`>`), `showNextSteps()` displays numbered command list with `/are-*` shortcuts.\n\n### [index.ts](./index.ts)\nMain entry point orchestrating install/uninstall workflow. `runInstaller()` parses CLI args, displays banner, prompts for missing runtime/location in interactive mode, enforces non-interactive requirements (`--runtime` + `-g/-l` flags mandatory when `!isInteractive()`), dispatches to `runInstall()`/`runUninstall()`, aggregates results. `parseInstallerArgs()` handles short/long flags (`-g`/`--global`, `-h`/`--help`).\n\n### [operations.ts](./operations.ts)\nFile copying, hook registration, permission configuration. `installFilesForRuntime()` writes command templates to `${basePath}/${relativePath}`, copies hooks from bundled `hooks/dist/`, calls `registerClaudeHooks()`/`registerGeminiHooks()` with nested `HookEvent.hooks[]` or flat `GeminiHook[]` schemas, adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` file. `getPackageVersion()` reads package.json via `fileURLToPath(import.meta.url)`.\n\n### [paths.ts](./paths.ts)\nCross-platform path resolution with environment overrides. `getRuntimePaths()` returns `{ global, local, settingsFile }` applying `CLAUDE_CONFIG_DIR`/`OPENCODE_CONFIG_DIR`/`GEMINI_CONFIG_DIR` precedence, `resolveInstallPath()` joins project root with local paths (`.claude`/`.opencode`/`.gemini`), `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` check directory existence via `stat()`.\n\n### [prompts.ts](./prompts.ts)\nInteractive selection with arrow-key navigation or numbered fallback. `arrowKeySelect()` enables raw mode (`process.stdin.setRawMode(true)`), listens for `key.name === 'up'|'down'|'return'`, re-renders with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line). `numberedSelect()` prints 1-indexed list for non-TTY. `cleanupRawMode()` registered on `process.on('exit')` and `SIGINT`.\n\n### [types.ts](./types.ts)\nInterface definitions for installer workflow. `InstallerArgs` with `runtime?: Runtime`, `global/local/uninstall/force/help/quiet: boolean`. `InstallerResult` with `success`, `runtime: Exclude<Runtime, 'all'>`, `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered?`, `versionWritten?`. `RuntimePaths` with `global/local/settingsFile` strings.\n\n### [uninstall.ts](./uninstall.ts)\nArtifact removal and hook deregistration. `uninstallFilesForRuntime()` deletes command templates, hooks/plugins, `ARE-VERSION` file, calls `unregisterClaudeHooks()`/`unregisterGeminiHooks()` filtering `settings.json` by `getHookPatterns()` (current + legacy formats), `unregisterPermissions()` removes `ARE_PERMISSIONS` from Claude `permissions.allow[]`. `cleanupEmptyDirs()` recursively removes directories via `rmdirSync()`. `cleanupLegacyGeminiFiles()` deletes pre-TOML `are-*.md` and old TOML subdirectories.\n\n## Installation Workflow\n\n**Interactive mode (TTY):** `runInstaller()` displays banner → prompts for runtime via `selectRuntime()` (options: claude/opencode/gemini/all) → prompts for location via `selectLocation()` (global: `~/.claude`, local: `./.claude`) → confirms action → dispatches.\n\n**Non-interactive mode (CI):** requires `--runtime <value>` and `-g`/`-l` flags, exits with error via `showError()` + `process.exit(1)` if missing.\n\n**Installation:** `runInstall()` calls `installFiles()` → `installFilesForRuntime()` copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}` (path component after runtime prefix), reads bundled hooks from `hooks/dist/${filename}` via `readBundledHook()`, writes to `${basePath}/hooks/` or `${basePath}/plugins/`, updates `settings.json` with hook/permission entries, writes `ARE-VERSION` from `getPackageVersion()`.\n\n**Uninstallation:** `runUninstall()` calls `uninstallFiles()` → `uninstallFilesForRuntime()` deletes files, unregisters hooks by filtering `settings.json` arrays with `getHookPatterns()` (matches current `node ${runtimeDir}/hooks/${filename}` and legacy `node hooks/${filename}` formats), removes permissions, deletes empty directories via `cleanupEmptyDirs()`, calls `deleteConfigFolder()` for local installs.\n\n## Settings.json Hook Registration\n\n**Claude Code format (nested structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"node ~/.claude/hooks/are-check-update.js\" }\n        ]\n      }\n    ]\n  }\n}\n```\nSchema: `SettingsJson.hooks[event]` contains `HookEvent[]` where each `HookEvent` has `hooks: SessionHook[]` array with `{ type: 'command', command: string }` entries.\n\n**Gemini CLI format (flat structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      { \"name\": \"are-check-update\", \"type\": \"command\", \"command\": \"node ~/.gemini/hooks/are-check-update.js\" }\n    ]\n  }\n}\n```\nSchema: `GeminiSettingsJson.hooks[event]` contains flat `GeminiHook[]` array with `{ name: string, type: 'command', command: string }` entries.\n\n**OpenCode (plugin system):** Copies `ARE_PLUGINS` files (`opencode-are-check-update.js` → `are-check-update.js`) to `${basePath}/plugins/`, sets `hookRegistered = true` without settings.json modification.\n\n## Behavioral Contracts\n\n**Hook definitions (operations.ts):**\n- `ARE_HOOKS: HookDefinition[]` currently empty (commented: \"causing issues\")\n- Intended entries: `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**Plugin definitions (operations.ts):**\n- `ARE_PLUGINS: PluginDefinition[]` with one active entry: `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- Commented: `{ srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }`\n\n**Permission patterns (operations.ts):**\n```javascript\nARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)'\n]\n```\n\n**Bundled hook paths:** `__dirname/../../hooks/dist/${hookName}` where `__dirname` from `fileURLToPath(import.meta.url)`, built via `scripts/build-hooks.js` during `npm run build:hooks`.\n\n**Environment variable precedence:**\n- Claude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- OpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` overrides `~/.config/opencode`\n- Gemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n**ANSI escape sequences (prompts.ts):**\n- `\\x1b[${n}A` — cursor up n lines\n- `\\x1b[1B` — cursor down 1 line\n- `\\x1b[2K` — clear entire line\n\n**Keypress matching (prompts.ts):** `key.name === 'up' | 'down' | 'return'`, interrupt via `key.ctrl && key.name === 'c'`.\n\n## File System Conventions\n\n**Template path extraction:** Remove runtime prefix via `template.path.split('/').slice(1).join('/')`, e.g., `.claude/commands/are/generate.md` → `commands/are/generate.md`.\n\n**Installation paths:**\n- Commands: `${basePath}/${relativePath}`\n- Hooks: `${basePath}/hooks/${filename}` (Claude/Gemini)\n- Plugins: `${basePath}/plugins/${destFilename}` (OpenCode)\n- Settings: `${basePath}/settings.json`\n- Version: `${basePath}/ARE-VERSION`\n\n**Config folder:** `.agents-reverse-engineer` deleted only for local uninstalls via `deleteConfigFolder(location === 'local', dryRun)`.\n\n## Error Handling\n\nFile write failures append to `errors[]` with format `\"Failed to write ${fullPath}: ${err}\"`. JSON parse failures in hook registration silently reset settings to `{}`. Missing bundled hooks throw Error in `readBundledHook()`. Directory cleanup ignores errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` for graceful missing path handling. `InstallerResult.success = errors.length === 0`.\n\n## Dependencies\n\n**Core modules:** `node:os` (homedir), `node:path` (join/dirname), `node:fs` (writeFileSync/readFileSync/unlinkSync/mkdirSync/readdirSync/rmdirSync/rmSync/existsSync), `node:fs/promises` (stat), `node:url` (fileURLToPath).\n\n**External libraries:** `picocolors` (ANSI colors: green/red/yellow/cyan/dim/bold), `readline` (keypress events/interface creation).\n\n**Internal imports:**\n- `../version.js` → getVersion (banner.ts)\n- `../integration/templates.js` → getClaudeTemplates/getOpenCodeTemplates/getGeminiTemplates (operations.ts, uninstall.ts)\n- `./paths.js` → getRuntimePaths/resolveInstallPath/getAllRuntimes (operations.ts, uninstall.ts, index.ts)\n- `./types.js` → Runtime/Location/InstallerArgs/InstallerResult/RuntimePaths (all files)\n- `./banner.js` → display functions (index.ts)\n- `./prompts.js` → selectRuntime/selectLocation/confirmAction/isInteractive (index.ts)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2954,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 17463,
      "latencyMs": 72687,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.241Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai\" (ai)\n\n## File Summaries (6 files)\n\n### index.ts\n**Purpose:** Public API barrel export consolidating type definitions, service orchestration, backend registry, retry utilities, su...\n\n**Public API barrel export consolidating type definitions, service orchestration, backend registry, retry utilities, subprocess execution, and backend detection functions for the AI service layer.**\n\n## Exported Types\n\n- `AIBackend` — Interface defining backend implementation contract (name, model, pricing, call/health methods)\n- `AIResponse` — Response envelope containing `content`, `usage` (tokens/costs), and optional `cached` flag\n- `AICallOptions` — Request options with `prompt`, optional `context`, `model`, `disallowedTools`, `maxTokens`\n- `SubprocessResult` — Raw subprocess output with `stdout`, `stderr`, `exitCode`, `signal`, `timedOut`, `duration`, `error`\n- `RetryOptions` — Configuration for exponential backoff: `maxRetries`, `initialDelayMs`, `maxDelayMs`, `backoffMultiplier`, `retryableErrors`\n- `TelemetryEntry` — Single AI call record with `callId`, `timestamp`, `backend`, `model`, `prompt`, `usage`, `duration`, `error`\n- `RunLog` — Aggregated run summary with `runId`, `startedAt`, `totalInputTokens`, `totalCost`, `uniqueFilesRead`, `errorCount`\n- `FileRead` — File metadata structure with `path`, `sizeBytes`, `linesRead` for telemetry tracking\n- `AIServiceError` — Error class with `code` discriminator (\"BACKEND_ERROR\", \"SUBPROCESS_ERROR\", \"RETRY_EXHAUSTED\", etc.)\n\n## Service Orchestration\n\n- `AIService` — Primary orchestrator class managing backend invocation, retry logic, telemetry logging, and trace emission\n- `AIServiceOptions` — Constructor config with `timeoutMs`, `maxRetries`, `telemetry` (enabled/keepRuns/costThresholdUsd), `tracer`, `debug`\n\n## Backend Registry\n\n- `BackendRegistry` — Class managing available backend implementations with `register()`, `get()`, `getAvailable()`, `list()` methods\n- `createBackendRegistry()` — Factory function returning pre-populated registry with Claude, Gemini, OpenCode backends\n- `resolveBackend()` — Async function taking registry and backend name ('auto' | 'claude' | 'gemini' | 'opencode'), returns first healthy backend or throws\n- `detectBackend()` — Async function returning name of first available backend by health check order (Claude → Gemini → OpenCode)\n- `getInstallInstructions()` — Returns Map of backend names to installation command strings\n\n## Retry Utilities\n\n- `withRetry()` — Generic async retry wrapper with exponential backoff, accepts operation function and `RetryOptions`\n- `DEFAULT_RETRY_OPTIONS` — Constant defining baseline retry config: `maxRetries: 3`, `initialDelayMs: 1000`, `maxDelayMs: 32000`, `backoffMultiplier: 2`\n\n## Subprocess Execution\n\n- `runSubprocess()` — Low-level wrapper around `execFile` with resource limits (NODE_OPTIONS heap constraint, UV_THREADPOOL_SIZE=4, CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1), timeout enforcement (SIGTERM → SIGKILL escalation), process group killing (`kill(-pid)`), returns `SubprocessResult`\n\n## Backend Utilities\n\n- `isCommandOnPath()` — Async predicate checking if executable exists in PATH via `which` command, returns boolean\n\n## Module Architecture\n\nEnforces encapsulation boundary: no external module should import from `src/ai/backends/` or `src/ai/telemetry/` directly. All AI service layer dependencies must route through this barrel export. Telemetry implementation (`src/ai/telemetry/logger.ts`, `src/ai/telemetry/run-log.ts`, `src/ai/telemetry/cleanup.ts`) remains internal to service layer, accessed only via `AIService` constructor options.\n### registry.ts\n**Purpose:** BackendRegistry manages AI CLI backend registration, auto-detection via PATH availability, and resolution with action...\n\n**BackendRegistry manages AI CLI backend registration, auto-detection via PATH availability, and resolution with actionable install instructions when no CLI is found.**\n\n## Exported Classes\n\n**BackendRegistry** stores AIBackend instances keyed by `name` with insertion-order priority for auto-detection. `register(backend: AIBackend): void` adds backends, `get(name: string): AIBackend | undefined` retrieves by name, `getAll(): AIBackend[]` returns all registered backends in priority order.\n\n## Factory Function\n\n**createBackendRegistry(): BackendRegistry** pre-populates registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order (Claude first, OpenCode last). Registration order determines auto-detection sequence.\n\n## Auto-Detection\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `backend.isAvailable()` on each, returning first available backend or `null` if none found. Priority follows registration order: Claude > Gemini > OpenCode.\n\n## Install Instructions\n\n**getInstallInstructions(registry: BackendRegistry): string** aggregates `backend.getInstallInstructions()` from all registered backends with `\\n\\n` separators for error message formatting.\n\n## Backend Resolution\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** handles two modes:\n\n- `requested === 'auto'`: calls `detectBackend()`, throws `AIServiceError` with code `CLI_NOT_FOUND` and formatted install instructions via `getInstallInstructions()` if no backend available\n- Explicit name: calls `registry.get(requested)`, validates availability via `backend.isAvailable()`, throws `CLI_NOT_FOUND` with backend-specific install instructions if not found or unavailable\n\nError messages include `backend.cliCommand` and `backend.getInstallInstructions()` for actionable guidance.\n\n## Dependencies\n\nImports `AIBackend` and `AIServiceError` from `./types.js`, backend implementations from `./backends/claude.js`, `./backends/gemini.js`, `./backends/opencode.js`.\n### retry.ts\n**Purpose:** retry.ts implements exponential backoff retry logic for transient AI service failures with jitter-based delay to prev...\n\n**retry.ts implements exponential backoff retry logic for transient AI service failures with jitter-based delay to prevent thundering herd scenarios.**\n\n## Exported Functions\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` executes an async function with exponential backoff retry on failure. Returns result immediately on success. On transient failure where `options.isRetryable(error)` returns true, waits with exponential backoff plus jitter (0-500ms random), then retries up to `options.maxRetries` times. On permanent failure where `isRetryable` returns false, throws immediately without retrying. After exhausting all retries, throws the last error. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`.\n\n## Exported Constants\n\n`DEFAULT_RETRY_OPTIONS` provides default retry configuration with `maxRetries: 3`, `baseDelayMs: 1_000`, `maxDelayMs: 8_000`, `multiplier: 2`. Does NOT include `isRetryable` or `onRetry` predicates since those are caller-specific. Type constraint: `Omit<RetryOptions, 'isRetryable' | 'onRetry'>`. Callers must spread these defaults and provide their own predicates.\n\n## Retry Behavior\n\n`withRetry` loops from `attempt = 0` to `options.maxRetries` inclusive (3 retries = 4 total attempts). On each iteration: attempts `fn()`, catches errors, checks `attempt === options.maxRetries` or `!options.isRetryable(error)` to determine if error is terminal (throws immediately), computes exponential delay as `options.baseDelayMs * Math.pow(options.multiplier, attempt)`, caps delay via `Math.min(exponentialDelay, options.maxDelayMs)`, adds jitter via `Math.random() * 500`, invokes `options.onRetry?.(attempt + 1, error)` notification callback, then awaits `setTimeout` promise before next iteration.\n\n## Integration Points\n\nImports `RetryOptions` from `./types.js`. Used by `AIService` in `src/ai/service.ts` to wrap `runSubprocess()` calls with rate limit handling. Expects caller to provide `isRetryable` predicate matching stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\") and `onRetry` callback for telemetry/logging.\n\n## Behavioral Contracts\n\nJitter range: `Math.random() * 500` adds 0-500ms random delay to computed exponential backoff. Delay cap enforced via `Math.min(exponentialDelay, options.maxDelayMs)` before jitter addition. Default base delay: `1_000`ms (1 second). Default max delay: `8_000`ms (8 seconds). Default multiplier: `2` (delay doubles each attempt).\n### service.ts\n**Purpose:** AIService orchestrates AI CLI invocations with subprocess management, exponential backoff retry, telemetry recording,...\n\n**AIService orchestrates AI CLI invocations with subprocess management, exponential backoff retry, telemetry recording, and trace emission.**\n\n## Exported Class\n\n`AIService` — Main orchestrator for AI calls with retry logic, timeout enforcement, and telemetry aggregation.\n\n**Constructor signature:**\n```typescript\nconstructor(backend: AIBackend, options: AIServiceOptions)\n```\n\n**Public methods:**\n- `async call(options: AICallOptions): Promise<AIResponse>` — Execute AI call with retry and telemetry recording\n- `async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>` — Write run log to disk and cleanup old logs\n- `setTracer(tracer: ITraceWriter): void` — Attach trace writer for subprocess/retry event emission\n- `setDebug(enabled: boolean): void` — Enable verbose subprocess logging to stderr\n- `setSubprocessLogDir(dir: string): void` — Set directory for writing subprocess stdout/stderr log files\n- `addFilesReadToLastEntry(filesRead: FileRead[]): void` — Attach file-read metadata to most recent telemetry entry\n- `getSummary(): RunLog['summary']` — Get current run summary without finalizing\n\n## Configuration Types\n\n`AIServiceOptions` interface with fields:\n- `timeoutMs: number` — Default subprocess timeout in milliseconds\n- `maxRetries: number` — Maximum retry attempts for transient errors\n- `model?: string` — Default model identifier applied to all calls unless overridden per-call\n- `telemetry.keepRuns: number` — Number of most recent run logs to keep on disk\n\n## Call Flow\n\n`call()` method executes:\n1. Merge service-level `model` with per-call `options.model` (per-call wins)\n2. Build CLI args via `backend.buildArgs(effectiveOptions)`\n3. Wrap subprocess invocation in `withRetry()` with `DEFAULT_RETRY_OPTIONS` and `maxRetries` from service options\n4. Increment `activeSubprocesses` counter before spawn, decrement after completion\n5. Invoke `runSubprocess(backend.cliCommand, args, { timeoutMs, input: options.prompt, onSpawn })` with `onSpawn` callback emitting `subprocess:spawn` trace event\n6. Emit `subprocess:exit` trace event after completion with `childPid`, `exitCode`, `signal`, `durationMs`, `timedOut` fields\n7. Enqueue non-critical subprocess log write via `enqueueSubprocessLog()` promise chain\n8. If `result.timedOut`, throw `AIServiceError('TIMEOUT', 'Subprocess timed out')`\n9. If `exitCode !== 0`, check `isRateLimitStderr(result.stderr)` → throw `AIServiceError('RATE_LIMIT', ...)` or `AIServiceError('SUBPROCESS_ERROR', ...)`\n10. Parse response via `backend.parseResponse(result.stdout, result.durationMs, result.exitCode)`, catch errors and wrap in `AIServiceError('PARSE_ERROR', ...)`\n11. On success: record telemetry entry via `logger.addEntry()` with token counts, latency, retry count\n12. On failure: record telemetry entry with error message and latency\n13. Return `AIResponse` or rethrow error\n\n## Retry Strategy\n\n`withRetry()` invoked with custom `isRetryable` predicate:\n- **Retryable:** `AIServiceError` with `code === 'RATE_LIMIT'` only\n- **Non-retryable:** All other errors including `TIMEOUT`, `SUBPROCESS_ERROR`, `PARSE_ERROR`\n\nRationale (from inline comment): \"Timeouts are NOT retried because spawning another heavyweight subprocess on a system that's already struggling (or against an unresponsive API) makes things worse and can exhaust system resources.\"\n\n`onRetry` callback:\n- Increments `retryCount` local variable\n- Logs warning to stderr: `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`\n- Emits `retry` trace event with `attempt`, `taskLabel`, `errorCode` fields\n\n## Rate Limit Detection\n\n`isRateLimitStderr(stderr: string)` checks for patterns in `RATE_LIMIT_PATTERNS`:\n- `\"rate limit\"`\n- `\"429\"`\n- `\"too many requests\"`\n- `\"overloaded\"`\n\nCase-insensitive substring matching via `stderr.toLowerCase().includes(pattern)`.\n\n## Telemetry Recording\n\n`TelemetryLogger` instance created in constructor with `new Date().toISOString()` as run timestamp.\n\n`addEntry()` called on both success and failure paths with `TelemetryEntry` containing:\n- `timestamp`, `prompt`, `systemPrompt`, `response`, `model`\n- `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`\n- `latencyMs`, `exitCode`, `retryCount`, `error` (on failure)\n- `thinking: 'not supported'` (hardcoded constant)\n- `filesRead: []` (populated via `addFilesReadToLastEntry()` by command runner after call completes)\n\n`finalize()` workflow:\n1. Call `logger.toRunLog()` to aggregate entries into `RunLog` structure\n2. Write via `writeRunLog(projectRoot, runLog)` returning log file path\n3. Cleanup via `cleanupOldLogs(projectRoot, options.telemetry.keepRuns)` to enforce retention limit\n4. Return `{ logPath, summary: runLog.summary }`\n\n## Debug Logging\n\nWhen `debug` flag enabled via `setDebug(true)`:\n- Pre-spawn: Log `[debug] Spawning subprocess for \"${taskLabel}\" (active: ${activeSubprocesses}, heapUsed: ${formatBytes(mem.heapUsed)}, rss: ${formatBytes(mem.rss)}, timeout: ${(timeoutMs / 1000).toFixed(0)}s)`\n- Post-completion: Log `[debug] Subprocess exited for \"${taskLabel}\" (PID ${childPid ?? 'unknown'}, exitCode: ${exitCode}, duration: ${(durationMs / 1000).toFixed(1)}s, active: ${activeSubprocesses})`\n\nMemory stats retrieved via `process.memoryUsage()` with `heapUsed` and `rss` fields formatted via `formatBytes()`.\n\n## Subprocess Output Logging\n\n`setSubprocessLogDir(dir)` enables logging of subprocess stdout/stderr to individual `.log` files.\n\n`enqueueSubprocessLog(result, taskLabel)` private method:\n- Sanitizes `taskLabel` via `.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')`\n- Generates filename: `${sanitized}_pid${result.childPid ?? 0}.log`\n- Serializes writes via `logWriteQueue` promise chain to prevent concurrent mkdir races\n- Writes header with `task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out` metadata followed by stdout and stderr sections\n- Failures silently swallowed (fire-and-forget pattern, log loss acceptable)\n\n## State Management\n\nInstance fields:\n- `backend: AIBackend` — Resolved backend adapter from registry\n- `options: AIServiceOptions` — Service configuration\n- `logger: TelemetryLogger` — In-memory telemetry accumulator\n- `callCount: number` — Running count of calls made (initialized to 0)\n- `tracer: ITraceWriter | null` — Trace writer for concurrency debugging (null until `setTracer()` called)\n- `debug: boolean` — Debug mode flag (default false)\n- `activeSubprocesses: number` — Count of currently active subprocesses (incremented before spawn, decremented after completion)\n- `subprocessLogDir: string | null` — Directory for subprocess logs (null = disabled)\n- `logWriteQueue: Promise<void>` — Promise chain for serializing subprocess log writes (initialized to `Promise.resolve()`)\n\n## Helper Functions\n\n`formatBytes(bytes: number): string` — Human-readable byte formatting with thresholds:\n- `< 1024` → `${bytes}B`\n- `< 1024 * 1024` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1024 * 1024` → `${(bytes / (1024 * 1024)).toFixed(1)}MB`\n\n## Dependencies\n\nImports from project modules:\n- `./types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `SubprocessResult`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError`\n- `./subprocess.js` — `runSubprocess`\n- `./retry.js` — `withRetry`, `DEFAULT_RETRY_OPTIONS`\n- `./telemetry/logger.js` — `TelemetryLogger`\n- `./telemetry/run-log.js` — `writeRunLog`\n- `./telemetry/cleanup.js` — `cleanupOldLogs`\n- `../orchestration/trace.js` — `ITraceWriter`\n\nNode.js built-ins:\n- `node:fs/promises` — `writeFile`, `mkdir`\n- `node:path` — Path manipulation for subprocess log filenames\n### subprocess.ts\n**Purpose:** subprocess.ts spawns AI CLI processes via execFile() with timeout enforcement, SIGTERM→SIGKILL escalation, stdin pipi...\n\n**subprocess.ts spawns AI CLI processes via execFile() with timeout enforcement, SIGTERM→SIGKILL escalation, stdin piping, process group termination, and active subprocess tracking.**\n\n## Exported Functions\n\n**runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>**\n\nSpawns a CLI subprocess using Node.js `execFile()` with comprehensive process lifecycle management. Never rejects—always resolves with `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, and `childPid`. Executes the following sequence:\n\n1. Spawns child process with `maxBuffer: 10485760` (10MB), `killSignal: 'SIGTERM'`, `encoding: 'utf-8'`\n2. Tracks spawn in `activeSubprocesses` Map with command string and `spawnedAt` timestamp\n3. Invokes `options.onSpawn?.(child.pid)` synchronously after spawn for trace event emission\n4. Writes `options.input` to `child.stdin` if provided, then calls `.end()` to close stream (prevents EOF blocking)\n5. Sets unref'd SIGKILL escalation timer for `options.timeoutMs + SIGKILL_GRACE_MS` (5000ms)\n6. On callback: clears SIGKILL timer, attempts process group kill via `process.kill(-child.pid, 'SIGKILL')` with single-process fallback, removes PID from `activeSubprocesses`, resolves promise\n7. Detects timeout via `error.killed === true` from execFile timeout mechanism\n8. Extracts exit code from `error.code` (if number), else `child.exitCode`, else defaults to 1 (failure) or 0 (success)\n\n**getActiveSubprocessCount(): number**\n\nReturns current size of `activeSubprocesses` Map for debugging concurrency issues.\n\n**getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>**\n\nReturns array of active subprocess metadata with computed `runningMs` delta from `Date.now() - spawnedAt`.\n\n## Interfaces\n\n**SubprocessOptions**\n\n```typescript\n{\n  timeoutMs: number;        // Maximum execution time before SIGTERM\n  input?: string;           // Optional stdin payload\n  onSpawn?: (pid: number | undefined) => void;  // Synchronous spawn callback\n}\n```\n\n**SubprocessResult** (imported from `./types.js`)\n\nExpected shape: `{ stdout: string, stderr: string, exitCode: number, signal: string | null, durationMs: number, timedOut: boolean, childPid: number | undefined }`\n\n## Process Lifecycle Management\n\n**Timeout Enforcement:**\n\n`execFile()` timeout option sends SIGTERM at `timeoutMs`. Unref'd escalation timer sends SIGKILL at `timeoutMs + 5000ms` via `child.kill('SIGKILL')` to handle hung processes ignoring SIGTERM.\n\n**Process Group Termination:**\n\nOn callback completion, attempts `process.kill(-child.pid, 'SIGKILL')` (negative PID targets process group) to kill entire subprocess tree. Falls back to single-process `process.kill(child.pid, 'SIGKILL')` if process group kill throws. Catch block ignores errors (process already dead).\n\n**Zombie Prevention:**\n\nExplicit SIGKILL cleanup in callback ensures no lingering child processes. Timer unref prevents event loop blocking.\n\n## Active Subprocess Tracking\n\n**activeSubprocesses: Map<number, { command: string; spawnedAt: number }>**\n\nModule-level Map tracking PIDs with command string (`${command} ${args.join(' ')}`) and spawn timestamp. Updated on spawn (`.set()`) and callback (`.delete()`). Enables debugging via `getActiveSubprocessCount()` and `getActiveSubprocesses()`.\n\n## Constants\n\n**SIGKILL_GRACE_MS = 5000**\n\nGrace period (5 seconds) between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nImported by `src/ai/service.ts` (`AIService.call()`) which wraps this with retry logic, telemetry logging, and backend-specific argument construction. The `onSpawn` callback is used by `AIService` to emit `subprocess:spawn` trace events with actual spawn timestamp.\n\n## Environment Passthrough\n\nMerges `process.env` into child environment without modification—caller (backends in `src/ai/backends/`) must set resource constraint variables (`NODE_OPTIONS`, `UV_THREADPOOL_SIZE`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS`) before invocation.\n\n## Error Handling Strategy\n\nNever throws. Captures all failure modes in `SubprocessResult`:\n- Non-zero exit codes → `exitCode` field\n- Timeout expiration → `timedOut: true`\n- Kill signals → `signal` field (e.g., `'SIGTERM'`, `'SIGKILL'`)\n- Execution errors → `stderr` field\n\nCaller decides retry/abort via pattern matching on these fields (see `src/ai/retry.ts`).\n### types.ts\n**Purpose:** Defines the contract for the AI service layer through interfaces for subprocess execution, AI backend adapters, retry...\n\n**Defines the contract for the AI service layer through interfaces for subprocess execution, AI backend adapters, retry configuration, telemetry logging, and typed error handling.**\n\n## Core Interfaces\n\n### SubprocessResult\nCaptures the outcome of a CLI subprocess execution returned by the subprocess wrapper. Contains `stdout: string`, `stderr: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`, and optional `childPid?: number`. Always populated even on error or timeout.\n\n### AICallOptions\nInput specification for an AI call requiring only `prompt: string`. Optional fields: `systemPrompt?: string`, `model?: string` (backend-interpreted identifier like \"sonnet\" or \"opus\"), `timeoutMs?: number` (overrides config default), `maxTurns?: number` (agentic turn limit), `taskLabel?: string` (for tracing).\n\n### AIResponse\nNormalized response structure that all backend adapters must produce. Fields: `text: string` (model response), `model: string` (backend-reported identifier), `inputTokens: number`, `outputTokens: number`, `cacheReadTokens: number`, `cacheCreationTokens: number`, `durationMs: number`, `exitCode: number`, `raw: unknown` (original CLI JSON for debugging).\n\n## Backend Contract\n\n### AIBackend\nInterface implemented by Claude, Gemini, and OpenCode adapters. Readonly properties: `name: string` (human-readable like \"Claude\"), `cliCommand: string` (executable name on PATH like \"claude\"). Methods: `isAvailable(): Promise<boolean>` checks PATH availability, `buildArgs(options: AICallOptions): string[]` constructs CLI argument array, `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` normalizes CLI output, `getInstallInstructions(): string` returns user-facing installation guidance when CLI not found.\n\n## Retry Configuration\n\n### RetryOptions\nControls exponential backoff behavior. Fields: `maxRetries: number` (3 means 4 total attempts), `baseDelayMs: number` (initial wait), `maxDelayMs: number` (cap on exponential growth), `multiplier: number` (exponential factor), `isRetryable: (error: unknown) => boolean` (transient error predicate), `onRetry?: (attempt: number, error: unknown) => void` (optional callback before each retry).\n\n## Telemetry Types\n\n### FileRead\nRecords a file sent as context to an AI call. Fields: `path: string` (relative to project root), `sizeBytes: number` (file size at read time).\n\n### TelemetryEntry\nPer-call log entry capturing everything needed to replay or debug a single AI invocation. Fields: `timestamp: string` (ISO 8601), `prompt: string`, `systemPrompt?: string`, `response: string`, `model: string`, `inputTokens: number`, `outputTokens: number`, `cacheReadTokens: number`, `cacheCreationTokens: number`, `latencyMs: number`, `exitCode: number`, `error?: string`, `retryCount: number`, `thinking: string` (AI reasoning content, \"not supported\" when backend lacks this feature), `filesRead: FileRead[]`.\n\n### RunLog\nAggregates all TelemetryEntry instances for a single CLI run plus computed summary. Top-level fields: `runId: string` (ISO timestamp-based), `startTime: string`, `endTime: string`, `entries: TelemetryEntry[]`, `summary` object containing `totalCalls: number`, `totalInputTokens: number`, `totalOutputTokens: number`, `totalDurationMs: number`, `errorCount: number`, `totalCacheReadTokens: number`, `totalCacheCreationTokens: number`, `totalFilesRead: number` (including duplicates), `uniqueFilesRead: number` (deduped by path).\n\n## Error Handling\n\n### AIServiceErrorCode\nTyped error codes: `'CLI_NOT_FOUND'` (executable missing from PATH), `'TIMEOUT'` (subprocess exceeded timeoutMs), `'PARSE_ERROR'` (malformed CLI output), `'SUBPROCESS_ERROR'` (process failure), `'RATE_LIMIT'` (backend rate limiting detected via stderr patterns like \"rate limit\", \"429\", \"too many requests\", \"overloaded\").\n\n### AIServiceError\nExtends Error with machine-readable error codes. Readonly property: `code: AIServiceErrorCode`. Constructor signature: `constructor(code: AIServiceErrorCode, message: string)`. Sets `name` to `'AIServiceError'`. Enables typed catch blocks branching on `error.code` without string parsing.\n\n## Integration Notes\n\nEvery AI service module imports from this file to ensure consistent contracts across the service layer. Backend adapters in `src/ai/backends/` implement AIBackend, subprocess wrapper in `src/ai/subprocess.ts` returns SubprocessResult, retry logic in `src/ai/retry.ts` consumes RetryOptions, telemetry logger in `src/ai/telemetry/logger.ts` writes TelemetryEntry and RunLog structures, and orchestration components thread AICallOptions through worker pools.\n\n## Import Map (verified — use these exact paths)\n\nservice.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### backends/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\nBackend adapters implementing AIBackend interface for three AI CLI tools: ClaudeBackend with full JSON parsing via Zod schema validation, GeminiBackend/OpenCodeBackend as stub implementations throwing AIServiceError until stable output formats arrive.\n\n## Contents\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection splitting process.env.PATH by platform delimiter, buildArgs() constructing ['--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions'] with optional --model/--system-prompt/--max-turns, parseResponse() slicing stdout from first '{' character to strip upgrade notices then validating against ClaudeResponseSchema extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens and model name from modelUsage object keys, getInstallInstructions() returning npm command for @anthropic-ai/claude-code.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub with isAvailable() delegating to isCommandOnPath('gemini'), buildArgs() returning ['-p', '--output-format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning npm command for @anthropic-ai/gemini-cli with GitHub URL (implementation deferred per RESEARCH.md Open Question 2 pending stable JSON format).\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub with isAvailable() delegating to isCommandOnPath('opencode'), buildArgs() returning ['run', '--format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning curl install command for https://opencode.ai (implementation deferred per RESEARCH.md Open Question 3 pending JSONL parsing).\n\n## Architecture\n\n**Backend Adapter Contract**\n\nAIBackend interface defines five methods: isAvailable() checking CLI binary presence on PATH, buildArgs(options) constructing subprocess argument arrays with prompt delivered via stdin, parseResponse(stdout, durationMs, exitCode) extracting AIResponse with normalized token counts, getInstallInstructions() returning user-facing setup guidance, name/cliCommand properties identifying backend.\n\n**PATH Detection Strategy**\n\nisCommandOnPath() splits process.env.PATH by path.delimiter (';' on Windows, ':' elsewhere), iterates directories calling fs.stat() on potential executable paths, on Windows iterates process.env.PATHEXT extensions ['.exe', '.cmd', '.bat', '.com'] to match platform conventions, returns true on first match. Function exported from claude.ts and reused by gemini.ts/opencode.ts for cross-backend consistency.\n\n**CLI Argument Patterns**\n\nAll backends use stdin for prompt delivery (not CLI arguments) to avoid shell escaping issues. ClaudeBackend appends --model/--system-prompt/--max-turns conditionally via buildArgs() inspecting AICallOptions properties. --permission-mode bypassPermissions enables non-interactive subprocess execution per PITFALLS.md §8. --no-session-persistence prevents state file writes.\n\n**JSON Response Parsing**\n\nClaudeBackend.parseResponse() handles stdout prefix content (upgrade notices, warnings) by finding first '{' via indexOf() before JSON.parse(). ClaudeResponseSchema validates against v2.1.31 output format with type: 'result', usage object containing input_tokens/cache_creation_input_tokens/cache_read_input_tokens/output_tokens, modelUsage record mapping model names to detailed statistics. Throws AIServiceError with code PARSE_ERROR on validation failure or missing JSON.\n\n**Stub Backend Pattern**\n\nGeminiBackend and OpenCodeBackend implement full interface surface but throw AIServiceError from parseResponse() to prevent runtime usage until output format research completes. Enables backend registration in src/ai/registry.ts and auto-detection via detectFirstAvailableBackend() while blocking production execution.\n\n## Integration Dependencies\n\nImports AIBackend/AICallOptions/AIResponse/AIServiceError from ../types.js. ClaudeBackend requires zod for ClaudeResponseSchema validation. Consumed by AIService in src/ai/service.ts via BackendRegistry.getBackend(name) lookup. Backend selection happens via config.ai.backend or auto-detection iterating ['claude', 'gemini', 'opencode'] until isAvailable() returns true.\n\n## Behavioral Contracts\n\n**ClaudeBackend Argument Construction**\n```javascript\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n// Appends conditionally:\n['--model', options.model] if options.model present\n['--system-prompt', options.systemPrompt] if options.systemPrompt present\n['--max-turns', String(options.maxTurns)] if options.maxTurns defined\n```\n\n**ClaudeResponseSchema Token Fields**\n```typescript\nusage: z.object({\n  input_tokens: z.number(),\n  cache_creation_input_tokens: z.number(),\n  cache_read_input_tokens: z.number(),\n  output_tokens: z.number()\n})\n```\n\n**GeminiBackend Arguments** — `['-p', '--output-format', 'json']`\n\n**OpenCodeBackend Arguments** — `['run', '--format', 'json']`\n\n**Stub Error Message Pattern** — `'<Backend> backend is not yet implemented. Use Claude backend.'` with AIServiceError code `'SUBPROCESS_ERROR'`\n### telemetry/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\n**Accumulates per-subprocess telemetry entries in memory via TelemetryLogger, serializes aggregate RunLog summaries to timestamped NDJSON files in `.agents-reverse-engineer/logs/`, and enforces retention limits via lexicographic filename sorting.**\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes oldest `run-*.json` files beyond retention threshold via lexicographic sort, catches `ENOENT` for missing logs directory.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates `TelemetryEntry` instances in memory, computes aggregate statistics via `getSummary()` (token sums, error counts, unique file deduplication), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` serializes `RunLog` to `.agents-reverse-engineer/logs/run-<safeTimestamp>.json` with ISO timestamp transformation (`2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z.json`).\n\n## Data Flow\n\n1. **AIService** (`src/ai/service.ts`) invokes `TelemetryLogger.addEntry()` after each subprocess call with `TelemetryEntry` containing `inputTokens`, `outputTokens`, `cacheReadTokens`, `latencyMs`, `error`, `filesRead[]`.\n2. **Command orchestrator** (`src/orchestration/runner.ts`) calls `logger.setFilesReadOnLastEntry(filesRead)` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`).\n3. **Run completion**: orchestrator calls `logger.toRunLog()` to produce `RunLog` with aggregated summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`).\n4. **Persistence**: `writeRunLog(projectRoot, runLog)` serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n5. **Retention enforcement**: `cleanupOldLogs(projectRoot, keepCount)` deletes oldest logs beyond `config.ai.telemetry.keepRuns` threshold (default 50).\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService instantiates TelemetryLogger once per run, calls `addEntry()` per subprocess, invokes `toRunLog()` at finalization.\n- `src/orchestration/runner.ts` — Orchestrates telemetry lifecycle: logger creation, entry enrichment with file reads, run log serialization, cleanup invocation.\n\n**Consumes:**\n- `src/ai/types.ts` — `TelemetryEntry`, `RunLog`, `FileRead` type definitions with token count fields, latency metrics, error summaries.\n\n## Behavioral Contracts\n\n**Filename transformation** (run-log.ts):\n```typescript\nrunLog.startTime.replace(/[:.]/g, '-')\n// Input:  \"2026-02-09T12:34:56.789Z\"\n// Output: \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Lexicographic sorting** (cleanup.ts):\n- ISO 8601 timestamps in filenames sort chronologically when treated as strings.\n- `sort()` then `reverse()` produces newest-first order.\n- `slice(keepCount)` targets oldest files for deletion.\n\n**Aggregate statistics** (logger.ts `getSummary()`):\n- `totalCalls` = `entries.length`\n- `totalInputTokens` = `Σ(entry.inputTokens)`\n- `uniqueFilesRead` = distinct count via `Set<string>` deduplication of `entry.filesRead.map(f => f.path)`\n\n## Retention Policy\n\nDefault retention: 50 runs (`config.ai.telemetry.keepRuns`). `cleanupOldLogs()` invoked after every `writeRunLog()` call. Missing logs directory handled gracefully (returns 0 without error). Propagates non-`ENOENT` filesystem errors to caller.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\n**Backend-agnostic AI CLI orchestration layer implementing subprocess pooling, exponential backoff retry, token cost telemetry with NDJSON persistence, and trace emission for concurrency debugging.**\n\n## Contents\n\n**[index.ts](./index.ts)** — Public API barrel exporting `AIService`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from types, `BackendRegistry`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()` from registry, `withRetry()`, `DEFAULT_RETRY_OPTIONS` from retry, `runSubprocess()`, `isCommandOnPath()` from subprocess/backends.\n\n**[registry.ts](./registry.ts)** — `BackendRegistry` stores AIBackend instances by name with insertion-order priority (Claude → Gemini → OpenCode), `createBackendRegistry()` pre-populates with backend adapters, `detectBackend()` returns first available via `backend.isAvailable()` iteration, `resolveBackend()` handles explicit names and 'auto' mode throwing `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on miss.\n\n**[retry.ts](./retry.ts)** — `withRetry<T>()` executes async operations with exponential backoff via `baseDelayMs * multiplier^attempt` capped at `maxDelayMs` plus 0-500ms jitter, terminates immediately on non-retryable errors via `isRetryable()` predicate, invokes `onRetry()` callback before each attempt, throws last error after exhausting `maxRetries`, `DEFAULT_RETRY_OPTIONS` defines 3 retries with 1s base delay, 8s max delay, 2x multiplier.\n\n**[service.ts](./service.ts)** — `AIService` orchestrates AI calls via `call()` wrapping `runSubprocess()` with `withRetry()`, detects rate limits via stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\"), refuses to retry timeouts (inline comment: \"spawning another heavyweight subprocess on a struggling system makes things worse\"), emits `subprocess:spawn/exit` and `retry` trace events, accumulates `TelemetryEntry` records via `TelemetryLogger`, serializes writes to subprocess log files via promise-chain queue, exposes `finalize()` for `RunLog` persistence and retention enforcement via `cleanupOldLogs()`, tracks active subprocesses for debug logging with heap/RSS metrics.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess()` spawns CLI via `execFile()` with 10MB `maxBuffer`, SIGTERM timeout via `killSignal`, stdin piping for prompt delivery, unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, process group termination via `kill(-pid, 'SIGKILL')` on completion, module-level `activeSubprocesses` Map tracking PIDs with spawn timestamps, exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency debugging, never throws (always resolves `SubprocessResult`).\n\n**[types.ts](./types.ts)** — Defines `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` methods, `AICallOptions` with `prompt`, optional `systemPrompt/model/timeoutMs/maxTurns/taskLabel`, `AIResponse` with normalized `text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw`, `SubprocessResult` with `stdout/stderr/exitCode/signal/durationMs/timedOut/childPid`, `RetryOptions` with exponential backoff config and predicates, `TelemetryEntry` per-call log with token counts and file reads, `RunLog` aggregate summary, `AIServiceError` with typed codes `CLI_NOT_FOUND/TIMEOUT/PARSE_ERROR/SUBPROCESS_ERROR/RATE_LIMIT`, `FileRead` with path/sizeBytes.\n\n## Architecture\n\n**Three-Layer Abstraction**\n\n1. **Subprocess Layer** (`subprocess.ts`): Raw `execFile()` wrapper with timeout enforcement, process group killing, stdin piping, active subprocess tracking. Returns `SubprocessResult` on all code paths (never throws).\n\n2. **Retry Layer** (`retry.ts`): Generic exponential backoff with jitter, caller-provided `isRetryable()` predicate, optional `onRetry()` callback. Terminates immediately on permanent errors.\n\n3. **Service Layer** (`service.ts`): Integrates subprocess + retry + telemetry + tracing. Detects rate limits via stderr patterns, refuses to retry timeouts, accumulates token counts, emits trace events for concurrency debugging, serializes subprocess log writes via promise chain.\n\n**Backend Registry**\n\n`BackendRegistry` stores `AIBackend` instances with insertion-order priority determining auto-detection sequence. `createBackendRegistry()` registers ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. `resolveBackend()` handles two modes: explicit name lookups with availability validation, 'auto' mode calling `detectBackend()` which iterates backends invoking `isAvailable()` until first match. Throws `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on failure.\n\n**Retry Strategy**\n\nRate limit detection via `isRateLimitStderr()` checking lowercase stderr for patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\". Only rate limit errors are retryable—timeouts are permanent failures (inline rationale: \"spawning another heavyweight subprocess on a system that's already struggling or against an unresponsive API makes things worse and can exhaust system resources\"). `withRetry()` invoked with custom predicate: `error instanceof AIServiceError && error.code === 'RATE_LIMIT'`.\n\n**Telemetry Pipeline**\n\n`TelemetryLogger` accumulates `TelemetryEntry` records in memory via `addEntry()` calls after each subprocess completion. Each entry captures `timestamp`, `prompt`, `systemPrompt`, `response`, `model`, token counts (`inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`), `latencyMs`, `exitCode`, `retryCount`, `filesRead[]` array. Command orchestrator calls `addFilesReadToLastEntry()` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`). `finalize()` invokes `logger.toRunLog()` producing aggregate `RunLog` with summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead` via Set deduplication), then serializes via `writeRunLog()` to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace Emission**\n\n`AIService` accepts optional `ITraceWriter` via `setTracer()` for concurrency debugging. Emits three event types: `subprocess:spawn` with `childPid/taskLabel/timestamp`, `subprocess:exit` with `childPid/exitCode/signal/durationMs/timedOut`, `retry` with `attempt/taskLabel/errorCode`. Trace writer (from `src/orchestration/trace.ts`) serializes events to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via promise-chain serialization ensuring write order matches emission order despite concurrent workers.\n\n**Subprocess Resource Management**\n\n`runSubprocess()` tracks active subprocesses in module-level Map with PID → `{ command, spawnedAt }` mapping. Exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for debugging. `AIService.call()` increments `activeSubprocesses` counter before spawn, decrements after completion, logs pre-spawn debug message with active count, heap usage, RSS via `process.memoryUsage()`, and configured timeout. Post-completion debug log includes PID, exit code, duration, updated active count.\n\n**Process Lifecycle**\n\n`runSubprocess()` sequence: spawn with 10MB `maxBuffer` and SIGTERM `killSignal`, track in `activeSubprocesses` Map, invoke `onSpawn()` callback synchronously for trace emission, write `options.input` to stdin then `.end()` to close stream, set unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, on callback clear timer, attempt process group kill via `kill(-pid, 'SIGKILL')` with single-process fallback, remove from `activeSubprocesses`, resolve with `SubprocessResult`. Timeout detection via `error.killed === true` from execFile. Exit code extracted from `error.code` (if number) else `child.exitCode` else defaults to 1 (failure) or 0 (success).\n\n## Behavioral Contracts\n\n**Rate Limit Patterns** (service.ts `RATE_LIMIT_PATTERNS`):\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n**Exponential Backoff Formula** (retry.ts):\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n**Default Retry Configuration** (retry.ts `DEFAULT_RETRY_OPTIONS`):\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n**Subprocess Timeout** (subprocess.ts):\n- SIGTERM sent at `timeoutMs` via execFile `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n\n**Process Group Termination** (subprocess.ts):\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // Negative PID targets process group\n```\n\n**Debug Memory Formatting** (service.ts `formatBytes()`):\n- `< 1024` → `${bytes}B`\n- `< 1048576` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1048576` → `${(bytes / 1048576).toFixed(1)}MB`\n\n**Telemetry Filename Format** (telemetry/run-log.ts):\n```typescript\n`run-${runLog.startTime.replace(/[:.]/g, '-')}.json`\n// \"2026-02-09T12:34:56.789Z\" → \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Subprocess Log Filename Sanitization** (service.ts):\n```typescript\ntaskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')\n```\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting token counts from usage/modelUsage objects, GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR` until output formats stabilize, shared `isCommandOnPath()` splitting `process.env.PATH` by platform delimiter with Windows `PATHEXT` iteration, CLI arguments with stdin prompt delivery and platform-specific flags (`--output-format json`, `--permission-mode bypassPermissions`).\n\n**[telemetry/](./telemetry/)** — `TelemetryLogger` accumulating `TelemetryEntry` records in memory with `addEntry()` and `getSummary()` computing token sums/error counts/unique file deduplication, `writeRunLog()` serializing `RunLog` to `.agents-reverse-engineer/logs/run-<timestamp>.json` with ISO timestamp transformation, `cleanupOldLogs()` deleting oldest files beyond retention threshold via lexicographic sort.\n\n## Integration Points\n\n**Upstream Consumers:**\n- `src/orchestration/runner.ts` — Instantiates AIService with resolved backend and config options, invokes `call()` per task, attaches file reads via `addFilesReadToLastEntry()`, finalizes telemetry via `finalize()`.\n- `src/generation/orchestrator.ts` — Threads AIService through three-phase pipeline (file analysis, directory aggregation, root synthesis).\n\n**Downstream Dependencies:**\n- `src/orchestration/trace.ts` — `ITraceWriter` interface for trace event emission.\n- `src/config/schema.ts` — `AIConfig` with backend selection, timeout, concurrency, telemetry retention.\n- `node:child_process` — `execFile()` for subprocess spawning.\n- `node:fs/promises` — File I/O for subprocess logs and telemetry persistence.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3108,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 18570,
      "latencyMs": 77060,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.273Z",
      "prompt": "Generate AGENTS.md for directory: \"src/cli\" (cli)\n\n## File Summaries (8 files)\n\n### clean.ts\n**Purpose:** cleanCommand removes all generated documentation artifacts (.sum, .annex.md, AGENTS.md, CLAUDE.md, GENERATION-PLAN.md...\n\n**cleanCommand removes all generated documentation artifacts (.sum, .annex.md, AGENTS.md, CLAUDE.md, GENERATION-PLAN.md) with marker-based filtering to preserve user-authored files and restoration of AGENTS.local.md backups.**\n\n## Exported Interface\n\n**cleanCommand(targetPath: string, options: CleanOptions): Promise<void>**\nExecutes artifact deletion workflow with dry-run support, selective AGENTS.md filtering via `GENERATED_MARKER` substring search, and AGENTS.local.md → AGENTS.md restoration.\n\n**CleanOptions**\n```typescript\ninterface CleanOptions {\n  dryRun: boolean; // Preview deletions without filesystem modifications\n}\n```\n\n## Artifact Discovery Strategy\n\nUses `fast-glob` with parallel Promise.all execution to discover four artifact classes:\n- `**/*.sum` — file-level summaries with YAML frontmatter\n- `**/*.annex.md` — companion files for reproduction-critical constants\n- `**/AGENTS.md` — directory-level aggregated documentation\n- `**/AGENTS.local.md` — user-authored backups created during generation\n\nAll globs exclude `**/node_modules/**` and `**/.git/**`, enable `dot: true` for hidden directories, use `absolute: true` paths for unlink operations.\n\n## AGENTS.md Filtering Protocol\n\nReads each AGENTS.md file via `readFile(file, 'utf-8')`, performs substring search for `GENERATED_MARKER` (imported from `generation/writers/agents-md.ts`). Files containing marker added to `generatedAgentsFiles[]` for deletion, others added to `skippedAgentsFiles[]` and preserved. Prevents deletion of user-authored AGENTS.md (SDK documentation, project guides). Read errors silently skip file (no throw, no log).\n\n## Single-File Cleanup Targets\n\nChecks existence via `access(filePath, constants.F_OK)` for:\n- `CLAUDE.md` at project root (path.join(resolvedPath, 'CLAUDE.md'))\n- `.agents-reverse-engineer/GENERATION-PLAN.md` (path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md'))\n\nNon-existent files skipped without error. All single files added to `singleFiles[]` array merged with glob results.\n\n## Restoration Workflow\n\nFor each AGENTS.local.md file, computes restoration target via `path.join(path.dirname(localFile), 'AGENTS.md')`. In dry-run mode, logs `would be restored` preview. In execution mode, calls `rename(localFile, agentsPath)` to undo the backup rename performed during generation. Tracks success count in `restored` variable, reports errors individually without aborting.\n\n## Deletion Loop\n\nIterates `allFiles` array containing `.sum`, `.annex.md`, generated `AGENTS.md`, and single-file targets. Calls `unlink(file)` for each, increments `deleted` counter on success, logs error message with relative path on failure. Does not throw or exit on individual file deletion errors.\n\n## Output Formatting\n\nUses `createLogger({ colors: true })` for terminal output with picocolors styling. Relative paths computed via `path.relative(resolvedPath, absPath)` for user-friendly display. Summary line formats counts with `pc.bold()` wrappers: `${pc.bold(String(sumFiles.length))} .sum file(s)`. Dry-run mode prints yellow warning via `pc.yellow('Dry run — no files were changed.')`. Success message uses `pc.green()` wrapper.\n\n## Error Handling\n\nVerifies target directory existence/permissions via `access(resolvedPath, constants.R_OK)`. Catches ENOENT (not found), EACCES/EPERM (permission denied), logs error and exits with code 1. Other errors rethrown. Individual file operation failures logged but do not abort command execution.\n\n## Dependencies\n\n- `fast-glob` — parallel artifact discovery with glob patterns\n- `picocolors` — ANSI color formatting for terminal output\n- `createLogger` from `output/logger.ts` — console logging abstraction\n- `GENERATED_MARKER` from `generation/writers/agents-md.ts` — sentinel string for ARE-generated files\n- `node:fs/promises` — async file operations (access, readFile, rename, unlink)\n- `node:fs` constants — permission flags (R_OK, F_OK)\n- `node:path` — path resolution and relative path computation\n### discover.ts\n**Purpose:** Command entry point for `are discover` — walks directory tree with gitignore/vendor/binary/custom filters, writes dis...\n\n**Command entry point for `are discover` — walks directory tree with gitignore/vendor/binary/custom filters, writes discovered file list to console and `.agents-reverse-engineer/GENERATION-PLAN.md` with three-phase execution plan.**\n\n## Exported Interface\n\n**discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>**\nResolves `targetPath` to absolute path (defaults to `process.cwd()`), loads config via `loadConfig()`, verifies path accessibility with `access(R_OK)`, runs `discoverFiles()` discovery pipeline, emits trace events (`discovery:start/end` with `filesIncluded`, `filesExcluded`, `durationMs`), logs included/excluded files to console via `createLogger()` and `ProgressLog.create()`, creates `GenerationPlan` via `createOrchestrator().createPlan()`, builds `ExecutionPlan` via `buildExecutionPlan()` with post-order traversal, formats as markdown via `formatExecutionPlanAsMarkdown()`, writes to `.agents-reverse-engineer/GENERATION-PLAN.md`, exits with code 1 on `ENOENT`/`EACCES`/`EPERM` errors or plan write failure.\n\n**DiscoverOptions**\nInterface with `tracer?: ITraceWriter` for trace emission, `debug?: boolean` for verbose output (defaults false).\n\n## Trace Events\n\nEmits `discovery:start` with `targetPath` at start, `discovery:end` with `filesIncluded`, `filesExcluded`, `durationMs` computed via `process.hrtime.bigint()` delta at completion.\n\n## Progress Logging\n\nCreates `ProgressLog` instance via `ProgressLog.create(resolvedPath)`, writes header `=== ARE Discover (${new Date().toISOString()}) ===`, logs each included file as `+ ${relativePath}`, each excluded file as `- ${relativePath} (${reason}: ${filter})`, summary `Discovered ${count} files (${excluded} excluded)`, plan creation status, finalizes with `await progressLog.finalize()`.\n\n## Output Format\n\nConsole output via `createLogger()`:\n- `logger.file(relativePath)` for included files\n- `logger.excluded(relativePath, reason, filter)` for excluded files with reason/filter context\n- `logger.summary(includedCount, excludedCount)` for aggregate counts\n- `logger.info()` for status messages\n- `logger.error()` for fatal errors before `process.exit(1)`\n\nDebug output via `pc.dim()` when `options.debug` is true, writes `[debug] Discovering files in: ${path}` at start, `[debug] Discovery complete: ${included} included, ${excluded} excluded` at end.\n\n## Plan Generation Flow\n\nConstructs `DiscoveryResult` object with `files: result.included`, `excluded: result.excluded.map(e => ({ path, reason }))`, passes to `createOrchestrator(config, resolvedPath).createPlan()` yielding `GenerationPlan`, transforms via `buildExecutionPlan(generationPlan, resolvedPath)` to `ExecutionPlan`, formats via `formatExecutionPlanAsMarkdown()`, writes to `path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md')` after `mkdir(configDir, { recursive: true })`.\n\n## Error Handling\n\nCatches `access()` errors: `ENOENT` → `logger.error('Directory not found')` + exit 1, `EACCES`/`EPERM` → `logger.error('Permission denied')` + exit 1, re-throws other errors. Catches plan write errors: logs `Failed to write plan: ${msg}`, writes to progress log, finalizes, exits with code 1.\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `discoverFiles` from `../discovery/run.js`, `createLogger` from `../output/logger.js`, `createOrchestrator` from `../generation/orchestrator.js`, `buildExecutionPlan`, `formatExecutionPlanAsMarkdown` from `../generation/executor.js`, `ProgressLog` from `../orchestration/index.js`, `DiscoveryResult` type from `../types/index.js`, `ITraceWriter` type from `../orchestration/trace.js`, `picocolors` as `pc`.\n### generate.ts\n**Purpose:** generateCommand orchestrates the three-phase ARE documentation pipeline by discovering files, resolving an AI backend...\n\n**generateCommand orchestrates the three-phase ARE documentation pipeline by discovering files, resolving an AI backend, building an execution plan, and running concurrent analysis via CommandRunner to produce .sum files, AGENTS.md per directory, and root integration documents.**\n\n## Exported Interface\n\n**generateCommand(targetPath: string, options: GenerateOptions): Promise<void>**\n- Main CLI entry point resolving `targetPath` to absolute path\n- Loads config via `loadConfig()`, discovers files via `discoverFiles()`, creates generation plan via `createOrchestrator().createPlan()`\n- Dry-run mode displays execution plan summary without AI calls\n- Normal mode resolves backend via `resolveBackend()`, creates `AIService`, builds `executionPlan` via `buildExecutionPlan()`, executes via `CommandRunner.executeGenerate()`\n- Finalizes telemetry via `aiService.finalize()`, progress log via `ProgressLog.finalize()`, trace via `tracer.finalize()`, cleans up old traces via `cleanupOldTraces()`\n- Exit codes: 0 (all succeeded), 1 (partial failure), 2 (total failure)\n\n**GenerateOptions interface**\n- `dryRun?: boolean` — display plan without AI calls\n- `concurrency?: number` — override worker pool size (1-10)\n- `failFast?: boolean` — abort on first task failure\n- `debug?: boolean` — enable subprocess logging with heap/RSS metrics\n- `trace?: boolean` — emit NDJSON events to `.agents-reverse-engineer/traces/`\n- `model?: string` — override AI model (e.g., \"sonnet\", \"opus\")\n\n## Execution Flow\n\n1. **Initialization**: Resolves `targetPath` to absolute, creates `createLogger()`, creates `createTraceWriter()` (early to thread through config/discovery)\n2. **Configuration**: Calls `loadConfig(absolutePath, { tracer, debug })` loading `.agents-reverse-engineer/config.yaml` with Zod validation\n3. **Discovery**: Calls `discoverFiles(absolutePath, config, { tracer, debug })` returning `FilterResult` with `included`/`excluded` arrays\n4. **Planning**: Creates `createOrchestrator()`, calls `orchestrator.createPlan(discoveryResult)` producing `GenerationPlan` with `files`, `tasks`, `complexity`\n5. **Dry-run branch**: If `options.dryRun`, calls `buildExecutionPlan()`, logs summary with `fileTasks.length`, `directoryTasks.length`, `rootTasks.length`, exits without AI calls\n6. **Backend resolution**: Calls `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)` throwing `AIServiceError` with code `'CLI_NOT_FOUND'` on failure\n7. **AI service creation**: Instantiates `AIService(backend, { timeoutMs, maxRetries, model, telemetry })`, calls `aiService.setDebug(true)` if debug enabled, calls `aiService.setSubprocessLogDir()` if trace enabled\n8. **Execution**: Builds `executionPlan` via `buildExecutionPlan(plan, absolutePath)`, determines concurrency from `options.concurrency ?? config.ai.concurrency`, creates `ProgressLog.create(absolutePath)`, creates `CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog })`, executes via `runner.executeGenerate(executionPlan)` returning `RunSummary`\n9. **Finalization**: Calls `aiService.finalize(absolutePath)` writing telemetry run log, `progressLog.finalize()`, `tracer.finalize()`, `cleanupOldTraces(absolutePath)` retaining 500 most recent traces\n\n## Error Handling\n\n- `resolveBackend()` throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` when no AI CLI detected, caught and logged via `getInstallInstructions(registry)`, exits with code 2\n- `CommandRunner.executeGenerate()` returns `RunSummary` with `filesProcessed`, `filesFailed` counts determining exit code without throwing\n\n## Progress Reporting\n\n**formatPlan(plan: GenerationPlan): string**\n- Returns formatted string with `plan.files.length`, `plan.tasks.length`, `plan.complexity.fileCount`, `plan.complexity.directoryDepth`\n\n**ProgressLog integration**\n- Creates via `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`\n- Writes header with ISO 8601 timestamp, project path, file/directory counts\n- Passes to `CommandRunner` constructor for streaming updates during execution\n- Enables real-time monitoring via `tail -f .agents-reverse-engineer/progress.log`\n\n## Trace Integration\n\n- Creates `createTraceWriter(absolutePath, options.trace ?? false)` before config loading to thread through all operations\n- Logs trace file path via `pc.dim()` if `options.trace && tracer.filePath`\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner` constructor\n- Calls `cleanupOldTraces(absolutePath)` after finalization retaining 500 most recent trace files\n\n## Subprocess Logging\n\n- When `options.trace` enabled, calls `aiService.setSubprocessLogDir(logDir)` with timestamp-based path under `.agents-reverse-engineer/subprocess-logs/`\n- Logs subprocess output directory via `pc.dim()` for debugging concurrent AI calls\n\n## Model Resolution\n\n- Effective model determined by `options.model ?? config.ai.model` (CLI flag overrides config)\n- Passed to `AIService` constructor via `model` option\n- Debug mode logs backend name, CLI command, effective model via `pc.dim()`\n\n## Dependencies\n\n- **Node.js path**: `path.resolve()` for absolute path conversion\n- **picocolors**: `pc.red()`, `pc.cyan()`, `pc.bold()`, `pc.dim()` for terminal formatting\n- **Config**: `loadConfig()` from `src/config/loader.ts`\n- **Output**: `createLogger()` from `src/output/logger.ts`\n- **Discovery**: `discoverFiles()` from `src/discovery/run.ts`\n- **Generation**: `createOrchestrator()`, `buildExecutionPlan()` from `src/generation/`\n- **AI**: `AIService`, `AIServiceError`, `createBackendRegistry()`, `resolveBackend()`, `getInstallInstructions()` from `src/ai/`\n- **Orchestration**: `CommandRunner`, `ProgressLog`, `createTraceWriter()`, `cleanupOldTraces()` from `src/orchestration/`\n### index.ts\n**Purpose:** CLI entry point dispatching install/uninstall/init/discover/generate/update/specify/rebuild/clean commands with flag ...\n\n**CLI entry point dispatching install/uninstall/init/discover/generate/update/specify/rebuild/clean commands with flag parsing, version display, and error handling.**\n\n## Command Routing\n\n`main()` parses `process.argv.slice(2)` via `parseArgs()` and routes to command handlers:\n\n- `install` → `runInstaller()` with `parseInstallerArgs()`\n- `uninstall` → `runInstaller()` with `uninstall: true`\n- `init` → `initCommand(positional[0] || '.', { force })`\n- `clean` → `cleanCommand(positional[0] || '.', { dryRun })`\n- `discover` → `discoverCommand(positional[0] || '.', {})`\n- `generate` → `generateCommand(positional[0] || '.', GenerateOptions)`\n- `update` → `updateCommand(positional[0] || '.', UpdateCommandOptions)`\n- `specify` → `specifyCommand(positional[0] || '.', SpecifyOptions)`\n- `rebuild` → `rebuildCommand(positional[0] || '.', RebuildOptions)`\n\nNo command with `args.length === 0` → launches interactive installer via `runInstaller({ global: false, local: false, uninstall: false, force: false, help: false, quiet: false })`.\n\nNo command with installer flags (detected via `hasInstallerFlags()`) → direct installer invocation with `parseInstallerArgs(args)`.\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command, positional, flags, values }`:\n\n- **Long flags**: `--flag` or `--key value` patterns\n- **Short flags**: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- **Command**: first non-flag argument\n- **Positional**: subsequent non-flag arguments after command\n- **Values map**: stores `--key value` pairs (e.g., `--concurrency 3`, `--output ./spec.md`, `--model sonnet`)\n- **Flags set**: boolean flags without values (e.g., `--dry-run`, `--debug`, `--trace`, `--force`)\n\n`hasInstallerFlags(flags, values)` detects installer-related flags: `global`, `local`, `force`, or `runtime` value presence.\n\n## Option Types\n\n**GenerateOptions** (from `./generate.js`):\n```typescript\n{\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**UpdateCommandOptions** (from `./update.js`):\n```typescript\n{\n  uncommitted?: boolean;\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (from `./clean.js`):\n```typescript\n{\n  dryRun?: boolean;\n}\n```\n\n**SpecifyOptions** (from `./specify.js`):\n```typescript\n{\n  output?: string;\n  force?: boolean;\n  dryRun?: boolean;\n  multiFile?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (from `./rebuild.js`):\n```typescript\n{\n  output?: string;\n  force?: boolean;\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n## Global Flags\n\n- `--version`, `-V` → `showVersion()` prints `agents-reverse-engineer v${VERSION}` and exits\n- `--help`, `-h` → `showHelp()` prints `USAGE` constant and exits (suppressed if installer flags present)\n- `--debug` → passed to command handlers for verbose subprocess logging\n- `--trace` → enables NDJSON trace output to `.agents-reverse-engineer/traces/`\n- `--dry-run` → preview mode without file writes (generate, update, specify, rebuild, clean)\n- `--fail-fast` → abort on first file analysis failure (generate, update, rebuild)\n- `--concurrency <n>` → worker pool size override (generate, update, rebuild)\n- `--model <name>` → AI model override (generate, update, specify, rebuild)\n- `--force` → overwrite existing files (init, install, specify, rebuild)\n- `--uncommitted` → include working tree changes (update only)\n- `--output <path>` → output destination (specify: spec file path, rebuild: output directory)\n- `--multi-file` → split specification into per-directory files (specify only)\n\n## Version Management\n\n`getVersion()` imported from `../version.js` returns package version string.\n\n`showVersionBanner()` prints `agents-reverse-engineer v${VERSION}\\n` before command execution.\n\n`showVersion()` prints version and exits with code 0.\n\n## Error Handling\n\n`showUnknownCommand(command)` prints error message referencing `are --help` and exits with code 1.\n\n`main().catch((err: Error) => ...)` catches uncaught errors, prints `Error: ${err.message}`, exits with code 1.\n\n## Usage Template\n\n`USAGE` constant defines help text with command summaries, option descriptions, and example invocations. Covers install/uninstall workflow with `--runtime` targeting (claude, opencode, gemini, all), global/local installation modes, and command-specific flags.\n### init.ts\n**Purpose:** initCommand() creates `.agents-reverse-engineer/config.yaml` with documented defaults, warning if configuration alrea...\n\n**initCommand() creates `.agents-reverse-engineer/config.yaml` with documented defaults, warning if configuration already exists unless `--force` overrides.**\n\n## Exported Interface\n\n```typescript\nasync function initCommand(\n  root: string,\n  options?: { force?: boolean }\n): Promise<void>\n```\n\nResolves `root` to absolute path via `path.resolve()`, constructs `configPath` by joining `resolvedRoot`, `CONFIG_DIR`, and `CONFIG_FILE` constants from `../config/loader.js`. Checks existence via `configExists(resolvedRoot)` unless `options?.force` is true. Writes default configuration via `writeDefaultConfig(resolvedRoot)` when config missing or force flag set.\n\n## Logger Integration\n\nCreates logger via `createLogger({ colors: true })` for all output. Emits `logger.warn()` when config exists without force flag, `logger.info()` for creation success with customization instructions, `logger.error()` for failures.\n\n## Error Handling Strategy\n\nCatches `NodeJS.ErrnoException` and matches `error.code`:\n- `'EACCES'` or `'EPERM'` → logs permission denied message, suggests checking write permissions, calls `process.exit(1)`\n- Other codes → logs generic failure with `error.message`, calls `process.exit(1)`\n\n## User Guidance Messages\n\nOn successful creation, emits configuration customization instructions via `logger.info()`:\n- `exclude.patterns` — custom glob patterns to exclude\n- `ai.concurrency` — parallel AI calls (1-20, default: auto)\n- `ai.timeoutMs` — subprocess timeout (default: 300,000ms = 5 min)\n- `ai.backend` — AI backend (claude/gemini/opencode/auto)\n- Final line references `README.md` for full configuration reference\n\n## Dependencies\n\nImports `configExists`, `writeDefaultConfig`, `CONFIG_DIR`, `CONFIG_FILE` from `../config/loader.js` for config file operations. Imports `createLogger` from `../output/logger.js` for terminal output with picocolors formatting.\n### rebuild.ts\n**Purpose:** rebuildCommand reconstructs a project from specification files via AI-driven code generation with checkpoint-based se...\n\n**rebuildCommand reconstructs a project from specification files via AI-driven code generation with checkpoint-based session continuity, partitioning specs into ordered rebuild units, resolving AI backend, and orchestrating parallel module generation with progress tracking.**\n\n## Exported Interface\n\n### rebuildCommand\n```typescript\nasync function rebuildCommand(\n  targetPath: string,\n  options: RebuildOptions,\n): Promise<void>\n```\nEntry point for `are rebuild` CLI command. Reads spec files from `specs/` directory in `targetPath`, partitions them via `partitionSpec()`, resolves AI backend through `createBackendRegistry()` and `resolveBackend()`, creates `AIService` with extended timeout (15min minimum), executes `executeRebuild()` orchestrator, finalizes telemetry/trace/progress log, and exits with status code (0=success, 1=partial failure, 2=total failure).\n\n### RebuildOptions\n```typescript\ninterface RebuildOptions {\n  output?: string;      // Custom output directory (default: rebuild/)\n  force?: boolean;      // Wipe output directory and start fresh\n  dryRun?: boolean;     // Show plan without executing\n  concurrency?: number; // Override worker pool size\n  failFast?: boolean;   // Stop on first failure\n  debug?: boolean;      // Verbose subprocess logging\n  trace?: boolean;      // Enable NDJSON tracing\n  model?: string;       // Override AI model (defaults to \"opus\" for rebuild)\n}\n```\n\n## Workflow Phases\n\n### Dry-Run Mode\nWhen `options.dryRun` is true, displays rebuild plan without AI calls:\n- Logs spec file count, rebuild unit count, and output directory path\n- Iterates over `units` array printing `unit.order` and `unit.name`\n- Calls `CheckpointManager.load()` to detect existing checkpoint state\n- If checkpoint exists, reports completed vs. pending module counts via `checkpoint.getPendingUnits()`\n- Returns early without backend resolution or AI service initialization\n\n### Backend Resolution\n- Calls `createBackendRegistry()` to enumerate available AI CLIs (Claude, Gemini, OpenCode)\n- Calls `resolveBackend(registry, config.ai.backend)` with config backend preference (`'auto'` | `'claude'` | `'gemini'` | `'opencode'`)\n- Catches `AIServiceError` with `code: 'CLI_NOT_FOUND'`, prints `getInstallInstructions(registry)`, exits with code 2\n- Resolves effective model via priority: CLI `--model` flag > config override > opus default (upgrades sonnet to opus for rebuild)\n\n### AIService Configuration\nCreates `AIService` instance with:\n- `timeoutMs: Math.max(config.ai.timeoutMs, 900_000)` — enforces 15-minute minimum for large rebuild modules\n- `maxRetries: config.ai.maxRetries` — exponential backoff retry count\n- `model: effectiveModel` — resolved from CLI flag or config or \"opus\" default\n- `telemetry.keepRuns: config.ai.telemetry.keepRuns` — run log retention limit\n\nEnables optional features:\n- `aiService.setDebug(true)` when `options.debug` is true\n- `aiService.setSubprocessLogDir(logDir)` when `options.trace` is true, creates timestamped directory at `.agents-reverse-engineer/subprocess-logs/<ISO-timestamp>/`\n\n### Progress Tracking\n- Calls `ProgressLog.create(absolutePath)` to initialize `.agents-reverse-engineer/progress.log` writer\n- Logs rebuild header with ISO 8601 timestamp, project path, output directory, and unit count\n- Writes progress during `executeRebuild()` via injected `progressLog` parameter\n- Calls `progressLog.finalize()` after orchestrator completes\n\n### Rebuild Execution\nCalls `executeRebuild(aiService, absolutePath, options)` with:\n- `outputDir: options.output ?? path.join(absolutePath, 'rebuild')` — target directory for generated source files\n- `concurrency: options.concurrency ?? config.ai.concurrency` — worker pool size\n- `failFast: options.failFast` — abort-on-first-failure flag\n- `force: options.force` — wipe-and-restart flag\n- `debug: options.debug` — subprocess verbose logging flag\n- `tracer` — NDJSON trace writer instance from `createTraceWriter(absolutePath, options.trace)`\n- `progressLog` — stream writer for tail-monitored progress output\n\nReturns `RebuildResult` with `modulesProcessed`, `modulesSkipped`, `modulesFailed` counts.\n\n### Finalization\nSequential cleanup:\n1. `aiService.finalize(absolutePath)` — writes run log to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs()`\n2. `progressLog.finalize()` — closes progress log file stream\n3. `tracer.finalize()` — closes NDJSON trace file stream\n4. `cleanupOldTraces(absolutePath)` if `options.trace` is true — enforces 500-trace retention limit\n\n### Exit Code Strategy\n- `process.exit(2)` if `modulesProcessed === 0 && modulesFailed > 0` (total failure, no progress)\n- `process.exit(1)` if `modulesFailed > 0` (partial failure, some modules succeeded)\n- Implicit exit code 0 if `modulesFailed === 0` (success)\n\n## Dependencies\n\n### External Imports\n- `picocolors` (aliased as `pc`) — terminal color formatting for dry-run output, summaries, and error messages\n- `node:path` — path resolution for `absolutePath`, `outputDir`, and subprocess log directory construction\n\n### Internal Imports\n- `src/config/loader.js` — `loadConfig()` for YAML config parsing with Zod validation\n- `src/ai/index.js` — `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` for backend abstraction\n- `src/orchestration/index.js` — `ProgressLog`, `createTraceWriter`, `cleanupOldTraces` for telemetry infrastructure\n- `src/rebuild/index.js` — `readSpecFiles`, `partitionSpec`, `CheckpointManager`, `executeRebuild` for rebuild orchestration\n\n## Design Patterns\n\nUses **command pattern** with options object for CLI flag injection. Applies **early return pattern** for dry-run mode to avoid unnecessary backend resolution and AIService instantiation. Implements **defensive defaults** by upgrading sonnet model to opus for rebuild quality (rebuild benefits from best model unlike incremental updates).\n\n## Error Handling\n\nCatches `AIServiceError` with `code: 'CLI_NOT_FOUND'` during backend resolution, prints human-readable installation instructions via `getInstallInstructions(registry)`, exits with code 2 to distinguish \"missing dependency\" from \"task failure\". Rethrows all other errors to propagate unexpected failures to CLI error handler.\n### specify.ts\n**Purpose:** `specify.ts` implements the `specifyCommand()` CLI entry point that synthesizes project specifications from AGENTS.md...\n\n**`specify.ts` implements the `specifyCommand()` CLI entry point that synthesizes project specifications from AGENTS.md documentation corpus via AI, with auto-generation fallback, dry-run preview, single/multi-file output modes, and forced overwrite control.**\n\n## Exported Interface\n\n**`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>`** — Main command function that orchestrates specification generation: loads configuration via `loadConfig()`, collects AGENTS.md files via `collectAgentsDocs()` and annex files via `collectAnnexFiles()`, resolves AI backend via `resolveBackend()`, builds synthesis prompt via `buildSpecPrompt()`, calls `AIService.call()` with task label `'specify'`, writes output via `writeSpec()`, and finalizes telemetry via `aiService.finalize()`.\n\n**`interface SpecifyOptions`** — Command options controlling behavior:\n- `output?: string` — Custom output path (defaults to `specs/SPEC.md`)\n- `force?: boolean` — Overwrite existing specification files\n- `dryRun?: boolean` — Preview input statistics without AI calls or generation\n- `multiFile?: boolean` — Split output into multiple per-directory files\n- `debug?: boolean` — Enable verbose subprocess logging\n- `trace?: boolean` — Enable NDJSON trace emission\n- `model?: string` — Override AI model selection (defaults to `'opus'` for specification synthesis)\n\n## Model Resolution Strategy\n\n**Model override chain:** CLI flag `options.model` > config file `config.ai.model` > default `'opus'`. Specification generation benefits from highest-quality models, so default `'sonnet'` is upgraded to `'opus'` via conditional: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)`.\n\n## Pre-Flight Conflict Detection\n\n**Early exit for existing specs:** Before loading configuration or collecting documentation, checks for conflicting output files via `access()` with `constants.F_OK`. In `multiFile` mode, scans output directory via `readdir()` for any `.md` files. In single-file mode, checks specified output path. Exits with status code `1` and red-colored error message listing conflicts unless `--force` flag present. Avoids waiting for expensive AI call before detecting overwrite conflicts.\n\n## Dry-Run Mode\n\n**Token estimation:** Sums character counts from `docs[]` and `annexFiles[]`, estimates tokens via `totalChars / 4 / 1000` for K-token display. Prints summary with cyan-colored statistics: AGENTS.md count, annex file count, estimated token count, output path, output mode (`'multi-file'` | `'single-file'`). Emits yellow warnings if no AGENTS.md found (`'Run \\`are generate\\` first'`) or if input exceeds 150K tokens (`'Consider using a model with extended context'`). Returns early without AI backend resolution or `AIService` instantiation.\n\n## Auto-Generation Fallback\n\n**Zero-documentation handling:** After `collectAgentsDocs()` returns empty array, prints yellow warning `'No AGENTS.md files found. Running generate first...'`, invokes `generateCommand(targetPath, { debug, trace })` to execute full three-phase pipeline, re-collects documentation and annex files, verifies non-empty result or exits with status code `1` and red error `'No AGENTS.md files found after generation. Cannot proceed.'`.\n\n## Backend Resolution\n\n**AI CLI detection:** Creates backend registry via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`. Catches `AIServiceError` with `code === 'CLI_NOT_FOUND'`, prints red error message followed by installation instructions via `getInstallInstructions(registry)`, exits with status code `2` (distinct from file conflict exit code `1`).\n\n## Timeout Configuration\n\n**Extended synthesis duration:** Instantiates `AIService` with timeout `Math.max(config.ai.timeoutMs, 900_000)` (15 minutes minimum) to accommodate specification synthesis workload exceeding standard file analysis timeout (default 120 seconds). Passes `maxRetries: config.ai.maxRetries`, `model: effectiveModel`, and `telemetry: { keepRuns: config.ai.telemetry.keepRuns }`.\n\n## Progress Monitoring\n\n**Tail-friendly logging:** Creates `ProgressLog` via `ProgressLog.create(absolutePath)`, writes ISO 8601 timestamp header `'=== ARE Specify (${new Date().toISOString()}) ==='`, project path, AGENTS.md count, annex file count. Logs `'Generating specification...'` before AI call. Writes `'Written: ${file}'` for each output file from `writeSpec()`. Logs summary line with format `'Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${durationSec}s | Output: ${outputPath}'`. Finalizes log via `await progressLog.finalize()`.\n\n## Error Handling\n\n**`SpecExistsError` recovery:** Catches `SpecExistsError` thrown by `writeSpec()`, logs error message to progress log, awaits `progressLog.finalize()`, prints red-colored error via `console.error()`, exits with status code `1`. Re-throws all other errors for upstream handling.\n\n## Debug Output\n\n**Conditional verbosity:** When `options.debug === true`, logs backend name, CLI command, effective model, system prompt character count, user prompt character count prefixed with dim-styled `'[debug]'` via `console.error()`. Enables debug mode on `AIService` instance via `aiService.setDebug(true)`.\n\n## Dependencies\n\n**Core modules:** `node:path` (path resolution), `node:fs/promises` (`access`, `readdir`), `node:fs` (`constants.F_OK`). **Styling:** `picocolors` for ANSI color output (`pc.red`, `pc.green`, `pc.yellow`, `pc.cyan`, `pc.dim`, `pc.bold`). **Configuration:** `loadConfig` from `../config/loader.js`. **Collection:** `collectAgentsDocs`, `collectAnnexFiles` from `../generation/collector.js`. **Specification:** `buildSpecPrompt`, `writeSpec`, `SpecExistsError` from `../specify/index.js`. **AI orchestration:** `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `../ai/index.js`. **Progress:** `ProgressLog` from `../orchestration/index.js`. **Fallback:** `generateCommand` from `./generate.js`.\n\n## Task Label\n\n**Telemetry categorization:** Passes `taskLabel: 'specify'` to `AIService.call()` for run log aggregation and cost tracking distinct from file analysis (`'file-analysis'`) and directory synthesis (`'directory-synthesis'`) tasks.\n### update.ts\n**Purpose:** update.ts implements the incremental documentation update CLI command that detects changed files via hash comparison,...\n\n**update.ts implements the incremental documentation update CLI command that detects changed files via hash comparison, cleans up orphaned artifacts, analyzes modified files concurrently via CommandRunner, and regenerates AGENTS.md for affected directories.**\n\n## Exported Interface\n\n**updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>** — Main entry point for `are update` CLI command. Loads config, creates UpdateOrchestrator, prepares UpdatePlan, resolves AI backend, analyzes changed files via CommandRunner (Phase 1), regenerates AGENTS.md for affectedDirs (Phase 2), finalizes telemetry/tracing, and exits with status codes: 0=success, 1=partial failure, 2=total failure/no CLI.\n\n**UpdateCommandOptions** — Interface with optional fields: `uncommitted?: boolean` (include staged+working changes), `dryRun?: boolean` (show plan without writes), `concurrency?: number` (worker pool size), `failFast?: boolean` (abort on first failure), `debug?: boolean` (show prompts/backend info), `trace?: boolean` (emit NDJSON to `.agents-reverse-engineer/traces/`), `model?: string` (override AI model).\n\n## Workflow Steps\n\n1. **Plan Preparation**: Calls `orchestrator.preparePlan({ includeUncommitted, dryRun })` returning UpdatePlan with `filesToAnalyze[]`, `filesToSkip[]`, `cleanup.deletedSumFiles[]`, `cleanup.deletedAgentsMd[]`, `affectedDirs[]`, `currentCommit`, `isFirstRun`.\n\n2. **First Run Detection**: If `plan.isFirstRun === true`, prints hint to run `are generate` first and exits without processing.\n\n3. **No Changes**: If `filesToAnalyze.length === 0` and cleanup arrays empty, prints \"All files are up to date\" and exits.\n\n4. **Backend Resolution**: Creates `BackendRegistry`, calls `resolveBackend(registry, config.ai.backend)`. On `CLI_NOT_FOUND` error, prints `getInstallInstructions(registry)` and exits with code 2.\n\n5. **AI Service Setup**: Instantiates `AIService(backend, { timeoutMs, maxRetries, model, telemetry })`. If `options.trace`, calls `aiService.setSubprocessLogDir(logDir)` to write subprocess stdout/stderr to timestamped directory.\n\n6. **Phase 1 (File Analysis)**: Creates `CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog })`, calls `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` returning `summary` with `filesProcessed`, `filesFailed`.\n\n7. **Phase 2 (Directory Regeneration)**: Iterates `plan.affectedDirs[]` sequentially (concurrency=1). For each dir, calls `buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd)` passing existing AGENTS.md content if it contains `GENERATED_MARKER`. Calls `aiService.call({ prompt, systemPrompt })` and `writeAgentsMd(dirPath, absolutePath, response.text)`. Emits trace events: `phase:start`, `task:start`, `task:done`, `phase:end` with `phase='update-phase-dir-regen'`.\n\n8. **Finalization**: Calls `aiService.finalize(absolutePath)` for telemetry, `progressLog.finalize()`, `tracer.finalize()`, `cleanupOldTraces(absolutePath)` if tracing enabled, `orchestrator.recordRun(commit, filesProcessed, filesSkipped)`.\n\n## Plan Display\n\n**formatPlan(plan: UpdatePlan): string** — Formats UpdatePlan into multi-line string with sections: header, baseline commit (first 7 chars), summary counts (analyze/skip/cleanup), file list with status markers (`+`=added, `M`=modified, `R`=renamed), cleanup actions, affected directories. Uses picocolors: `pc.cyan()` for file counts, `pc.green('+')`, `pc.yellow('M')`, `pc.blue('R')` for status, `pc.dim()` for unchanged files.\n\n**formatCleanup(plan: UpdatePlan): string[]** — Returns array of formatted lines for `cleanup.deletedSumFiles[]` (yellow \"Cleanup (deleted .sum files):\" header, red `-` prefix) and `cleanup.deletedAgentsMd[]` (yellow \"Cleanup (deleted AGENTS.md from empty dirs):\" header).\n\n## Exit Code Strategy\n\n- **0**: All files succeeded or no files to process\n- **1**: Partial failure (`summary.filesProcessed > 0 && summary.filesFailed > 0`)\n- **2**: Total failure (`summary.filesProcessed === 0 && summary.filesFailed > 0`) or no AI CLI found\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `createUpdateOrchestrator` + `UpdatePlan` from `../update/index.js`, `writeAgentsMd` + `GENERATED_MARKER` from `../generation/writers/agents-md.js`, `buildDirectoryPrompt` from `../generation/prompts/index.js`, `AIService` + `AIServiceError` + `createBackendRegistry` + `resolveBackend` + `getInstallInstructions` from `../ai/index.js`, `CommandRunner` + `ProgressReporter` + `ProgressLog` + `createTraceWriter` + `cleanupOldTraces` from `../orchestration/index.js`.\n\n## Progress Logging\n\nCreates `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`. Writes header with ISO timestamp, project path, counts (files to analyze, directories). Passes to `CommandRunner` and `ProgressReporter.onDirectoryStart/Done()` for streaming ETA-enabled output. Calls `progressLog.finalize()` before exit.\n\n## Trace Integration\n\nIf `options.trace === true`, creates `TraceWriter` via `createTraceWriter(absolutePath, true)`, prints trace file path to stderr with `pc.dim()`, threads `tracer` through `loadConfig()`, `createUpdateOrchestrator()`, `CommandRunner()`. Phase 2 manually emits events: `phase:start` with `taskCount=plan.affectedDirs.length`, `task:start/done` for each directory, `phase:end` with `durationMs`, `tasksCompleted`, `tasksFailed`.\n\n## Incremental Update Context\n\nReads existing AGENTS.md via `readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8')`. If content includes `GENERATED_MARKER`, passes as `existingAgentsMd` parameter to `buildDirectoryPrompt()` for incremental update context (LLM receives previous output to preserve user annotations). Skips if file missing (catch block sets `existingAgentsMd = undefined`).\n\n## Import Map (verified — use these exact paths)\n\nclean.ts:\n  ../output/logger.js → createLogger\n  ../generation/writers/agents-md.js → GENERATED_MARKER\n\ndiscover.ts:\n  ../config/loader.js → loadConfig\n  ../discovery/run.js → discoverFiles\n  ../output/logger.js → createLogger\n  ../generation/orchestrator.js → createOrchestrator\n  ../generation/executor.js → buildExecutionPlan, formatExecutionPlanAsMarkdown\n  ../orchestration/index.js → ProgressLog\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\ngenerate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../discovery/run.js → discoverFiles\n  ../generation/orchestrator.js → createOrchestrator, type GenerationPlan\n  ../generation/executor.js → buildExecutionPlan\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces\n\nindex.ts:\n  ../installer/index.js → runInstaller, parseInstallerArgs\n  ../version.js → getVersion\n\ninit.ts:\n  ../config/loader.js → configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE\n  ../output/logger.js → createLogger\n\nrebuild.ts:\n  ../config/loader.js → loadConfig\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog, createTraceWriter, cleanupOldTraces\n  ../rebuild/index.js → readSpecFiles, partitionSpec, CheckpointManager, executeRebuild\n\nspecify.ts:\n  ../config/loader.js → loadConfig\n  ../generation/collector.js → collectAgentsDocs, collectAnnexFiles\n  ../specify/index.js → buildSpecPrompt, writeSpec, SpecExistsError\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog\n\nupdate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../update/index.js → createUpdateOrchestrator, type UpdatePlan\n  ../generation/writers/agents-md.js → writeAgentsMd, GENERATED_MARKER\n  ../generation/prompts/index.js → buildDirectoryPrompt\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand-line interface layer parsing `process.argv`, routing to command handlers, managing shared flags (--debug, --trace, --dry-run, --concurrency, --model), and integrating ProgressLog/TraceWriter/AIService across init/discover/generate/update/specify/rebuild/clean workflows.\n\n## Command Entry Points\n\n**[index.ts](./index.ts)** — Main router parsing args via `parseArgs()`, dispatching to command handlers, handling `--version`/`--help` flags, launching interactive installer when invoked with no arguments or installer-specific flags (--runtime, -g, -l).\n\n**[init.ts](./init.ts)** — `initCommand(root, { force? })` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, exits with warning if config exists without `--force`, catches `EACCES`/`EPERM` with `process.exit(1)`.\n\n**[discover.ts](./discover.ts)** — `discoverCommand(targetPath, { tracer?, debug? })` runs file discovery via `discoverFiles()`, writes included/excluded files to console and `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`, emits `discovery:start/end` trace events.\n\n**[generate.ts](./generate.ts)** — `generateCommand(targetPath, GenerateOptions)` orchestrates three-phase pipeline: discovers files → creates GenerationPlan via `createOrchestrator().createPlan()` → resolves AI backend → builds ExecutionPlan → executes via `CommandRunner.executeGenerate()` → finalizes telemetry/trace/progress, exits with codes 0 (success), 1 (partial failure), 2 (total failure/no CLI).\n\n**[update.ts](./update.ts)** — `updateCommand(targetPath, UpdateCommandOptions)` prepares UpdatePlan via hash comparison, cleans orphaned `.sum`/`AGENTS.md` artifacts, analyzes changed files (Phase 1), regenerates `AGENTS.md` for `affectedDirs` (Phase 2), finalizes telemetry, exits with codes 0/1/2.\n\n**[specify.ts](./specify.ts)** — `specifyCommand(targetPath, SpecifyOptions)` synthesizes project spec from `AGENTS.md` corpus via AI, auto-invokes `generateCommand()` if docs missing, supports single/multi-file output (`--multi-file`), enforces 15min timeout and opus model default, exits with code 1 on `SpecExistsError` (conflicts), 2 on `CLI_NOT_FOUND`.\n\n**[rebuild.ts](./rebuild.ts)** — `rebuildCommand(targetPath, RebuildOptions)` reconstructs project from specs via `partitionSpec()` + `executeRebuild()`, enforces 15min timeout and opus default, supports checkpoint-based resumption via `CheckpointManager.load()`, writes generated code to `rebuild/` (or custom `--output`), exits with codes 0/1/2.\n\n**[clean.ts](./clean.ts)** — `cleanCommand(targetPath, { dryRun })` removes `.sum`, `.annex.md`, generated `AGENTS.md` (via `GENERATED_MARKER` detection), `CLAUDE.md`, `GENERATION-PLAN.md`, restores `AGENTS.local.md` → `AGENTS.md`, logs deletion counts with picocolors formatting.\n\n## Shared Option Types\n\n**GenerateOptions** (`generate.ts`, `rebuild.ts` reuses subset):\n```typescript\n{\n  dryRun?: boolean;       // Show plan without AI calls\n  concurrency?: number;   // Worker pool size (1-10)\n  failFast?: boolean;     // Abort on first failure\n  debug?: boolean;        // Verbose subprocess logging\n  trace?: boolean;        // NDJSON trace emission\n  model?: string;         // Override AI model\n}\n```\n\n**UpdateCommandOptions** (`update.ts`):\n```typescript\n{\n  uncommitted?: boolean;  // Include staged+working changes\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**SpecifyOptions** (`specify.ts`):\n```typescript\n{\n  output?: string;        // Custom spec file path (default: specs/SPEC.md)\n  force?: boolean;        // Overwrite existing files\n  dryRun?: boolean;\n  multiFile?: boolean;    // Split into per-directory specs\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (`rebuild.ts`):\n```typescript\n{\n  output?: string;        // Custom output directory (default: rebuild/)\n  force?: boolean;        // Wipe output dir and restart\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (`clean.ts`):\n```typescript\n{\n  dryRun?: boolean;       // Preview deletions without filesystem writes\n}\n```\n\n## Argument Parsing Protocol\n\n`parseArgs(args: string[])` in `index.ts` returns `{ command, positional, flags, values }`:\n- **Long flags**: `--key value` → `values['key'] = 'value'`, `--flag` → `flags.add('flag')`\n- **Short flags**: `-V` → `flags.add('version')`, `-h` → `flags.add('help')`, `-g` → `flags.add('global')`, `-l` → `flags.add('local')`\n- **Command**: First non-flag argument\n- **Positional**: Subsequent non-flag arguments after command\n- **Values map**: `--concurrency 3`, `--output ./spec.md`, `--model sonnet`, `--runtime claude`\n- **Installer detection**: `hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` value\n\n## Model Resolution Strategy\n\n**generate/update**: `options.model ?? config.ai.model` — CLI flag overrides config, no hardcoded default.\n\n**specify/rebuild**: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)` — upgrades sonnet → opus for quality-critical synthesis, respects explicit opus/haiku config.\n\n## Progress Tracking Infrastructure\n\nAll commands (except `init`, `clean`) create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`:\n- **Header format**: `=== ARE <Command> (${ISO-8601}) ===\\nProject: ${absolutePath}\\n...`\n- **Real-time monitoring**: `tail -f .agents-reverse-engineer/progress.log`\n- **Phase boundaries**: `=== Phase 1: File Analysis ===`, `=== Phase 2: Directory AGENTS.md ===`\n- **Task progress**: `[worker-0] Analyzing src/foo.ts (ETA: 2m 15s)`\n- **Summary line**: `Tokens: 12345 in / 6789 out | Duration: 45s | Exit: 0`\n- **Finalization**: `await progressLog.finalize()` before exit\n\n## Trace Integration\n\nWhen `--trace` flag present:\n- `createTraceWriter(absolutePath, true)` creates `.agents-reverse-engineer/traces/trace-<ISO-timestamp>.ndjson` writer\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner()`, `AIService()`\n- Emits events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs`\n- Finalization: `await tracer.finalize()` + `cleanupOldTraces(absolutePath)` keeps 500 most recent traces\n\n## Backend Resolution Flow\n\nAll commands except `init`, `discover`, `clean`:\n1. `createBackendRegistry()` → enumerates installed CLIs (Claude, Gemini, OpenCode)\n2. `resolveBackend(registry, config.ai.backend)` → throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found\n3. Catch block → `getInstallInstructions(registry)` prints installation commands, `process.exit(2)` (distinct from task failure code 1)\n\n## AIService Configuration\n\nInstantiation pattern (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min min for specify/rebuild\n  maxRetries: config.ai.maxRetries,\n  model: effectiveModel,\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }\n});\n\nif (options.debug) {\n  aiService.setDebug(true);\n  console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n  console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n}\n\nif (options.trace) {\n  const logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', timestamp);\n  aiService.setSubprocessLogDir(logDir);\n  console.error(pc.dim(`[debug] Subprocess logs: ${logDir}`));\n}\n```\n\n## Finalization Sequence\n\nStandard cleanup before exit (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nawait aiService.finalize(absolutePath);  // Write run log, enforce retention\nawait progressLog.finalize();            // Close progress.log stream\nawait tracer.finalize();                 // Close trace NDJSON stream\nif (options.trace) {\n  cleanupOldTraces(absolutePath);        // Keep 500 most recent traces\n}\n```\n\n## Exit Code Conventions\n\n- **0**: Success (all tasks completed or no tasks to process)\n- **1**: Partial failure (some tasks succeeded, some failed) OR file conflict (specify/clean) OR first-run detection (update)\n- **2**: Total failure (no tasks succeeded, only failures) OR AI CLI not found\n\n## Dry-Run Behavior\n\n**generate.ts**: Builds `ExecutionPlan` via `buildExecutionPlan()`, logs file/directory/root task counts, returns without AI backend resolution.\n\n**update.ts**: Prepares `UpdatePlan` via `orchestrator.preparePlan({ includeUncommitted, dryRun: true })`, logs changed files with status markers (M=modified, +=added, R=renamed), cleanup actions (deleted `.sum`, empty dir `AGENTS.md`), affected directories, returns without AI calls.\n\n**specify.ts**: Collects `AGENTS.md` + annex files, estimates tokens via `totalChars / 4 / 1000`, logs counts with cyan styling, warns if input exceeds 150K tokens or docs missing, returns without backend resolution.\n\n**rebuild.ts**: Reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads checkpoint via `CheckpointManager.load()`, logs unit count and checkpoint status (completed vs. pending modules), returns without AIService instantiation.\n\n**clean.ts**: Discovers artifacts via `fast-glob` (`.sum`, `.annex.md`, `AGENTS.md`, `AGENTS.local.md`), filters via `GENERATED_MARKER` substring search, logs deletion preview with picocolors formatting (yellow warning \"Dry run — no files were changed\"), returns without `unlink()` calls.\n\n## Error Handling Patterns\n\n**Directory access errors** (all commands):\n```typescript\nawait access(resolvedPath, constants.R_OK);\n// Catches ENOENT → 'Directory not found: ${path}', exit 1\n// Catches EACCES/EPERM → 'Permission denied: ${path}', exit 1\n```\n\n**Config load errors** (all commands except `init`):\n```typescript\nconst config = await loadConfig(absolutePath, { tracer, debug });\n// Throws on invalid YAML or schema validation failure\n// Caught by top-level try/catch in index.ts main()\n```\n\n**Backend resolution errors** (generate/update/specify/rebuild):\n```typescript\ntry {\n  backend = resolveBackend(registry, config.ai.backend);\n} catch (err) {\n  if (err instanceof AIServiceError && err.code === 'CLI_NOT_FOUND') {\n    console.error(pc.red('No AI CLI found...'));\n    console.error(getInstallInstructions(registry));\n    process.exit(2);\n  }\n  throw err;\n}\n```\n\n**File conflict errors** (specify):\n```typescript\ntry {\n  await writeSpec(...);\n} catch (err) {\n  if (err instanceof SpecExistsError) {\n    await progressLog.finalize();\n    console.error(pc.red(err.message));\n    process.exit(1);\n  }\n  throw err;\n}\n```\n\n## Dependencies\n\n**External**: `picocolors` (as `pc`), `node:path`, `node:fs/promises` (`access`, `readFile`, `rename`, `unlink`, `mkdir`, `readdir`), `node:fs` (`constants.F_OK/R_OK`).\n\n**Internal**:\n- **Config**: `src/config/loader.ts` (loadConfig, configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE)\n- **Discovery**: `src/discovery/run.ts` (discoverFiles)\n- **Generation**: `src/generation/orchestrator.ts` (createOrchestrator, GenerationPlan), `src/generation/executor.ts` (buildExecutionPlan, formatExecutionPlanAsMarkdown), `src/generation/collector.ts` (collectAgentsDocs, collectAnnexFiles), `src/generation/writers/agents-md.ts` (writeAgentsMd, GENERATED_MARKER), `src/generation/prompts/index.ts` (buildDirectoryPrompt)\n- **AI**: `src/ai/index.ts` (AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions)\n- **Orchestration**: `src/orchestration/index.ts` (CommandRunner, ProgressLog, ProgressReporter, createTraceWriter, cleanupOldTraces), `src/orchestration/trace.ts` (ITraceWriter)\n- **Update**: `src/update/index.ts` (createUpdateOrchestrator, UpdatePlan)\n- **Specify**: `src/specify/index.ts` (buildSpecPrompt, writeSpec, SpecExistsError)\n- **Rebuild**: `src/rebuild/index.ts` (readSpecFiles, partitionSpec, CheckpointManager, executeRebuild)\n- **Installer**: `src/installer/index.ts` (runInstaller, parseInstallerArgs)\n- **Output**: `src/output/logger.ts` (createLogger)\n- **Version**: `src/version.ts` (getVersion)\n- **Types**: `src/types/index.ts` (DiscoveryResult)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3765,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 20751,
      "latencyMs": 89029,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:31:52.264Z",
      "prompt": "Generate AGENTS.md for directory: \"src/orchestration\" (orchestration)\n\n## File Summaries (7 files)\n\n### index.ts\n**Purpose:** Barrel export module aggregating orchestration subsystem's public API for concurrent task execution, progress trackin...\n\n**Barrel export module aggregating orchestration subsystem's public API for concurrent task execution, progress tracking, plan persistence, trace emission, and command workflow coordination.**\n\n## Exported Types\n\n**From `types.js`:**\n- `FileTaskResult` — Result envelope for individual file analysis tasks with status, path, tokens, duration, error fields\n- `RunSummary` — Aggregated execution metrics (total files, success/error counts, duration, token totals, cost estimates)\n- `ProgressEvent` — Event payload for progress updates with discriminated type field\n- `CommandRunOptions` — Configuration object threading through runner → pool → AIService (concurrency, timeout, retries, failFast, tracer, etc.)\n\n**From `pool.js`:**\n- `PoolOptions` — Worker pool configuration subset (concurrency, onProgress callback)\n- `TaskResult` — Generic result wrapper for pool tasks with `ok: boolean`, `value?`, `error?` discriminated union\n\n**From `trace.js`:**\n- `ITraceWriter` — Interface defining `emit(event, payload)` and `close()` for pluggable trace backends\n- `TraceEvent` — Union of event type strings: `'phase:start' | 'phase:end' | 'worker:start' | 'worker:end' | 'task:pickup' | 'task:done' | 'subprocess:spawn' | 'subprocess:exit' | 'retry'`\n- `TraceEventPayload` — Discriminated union of payload shapes keyed by event type\n\n## Exported Functions\n\n- `runPool<T, R>(tasks, worker, options)` — Iterator-based concurrency pool sharing single task iterator across N workers, returns `Promise<TaskResult<R>[]>`\n- `createTraceWriter(outputDir)` — Factory returning `ITraceWriter` (either `TraceWriter` writing NDJSON to `traces/` or `NullTraceWriter` stub when disabled)\n- `cleanupOldTraces(traceDir, keepCount)` — Retention enforcement deleting oldest trace files beyond `keepCount` limit\n\n## Exported Classes\n\n- `ProgressReporter` — Streaming progress tracker with ETA calculation via moving average of last 10 task durations, emits console output and writes to `progress.log`\n- `ProgressLog` — Thin wrapper appending human-readable messages to `.agents-reverse-engineer/progress.log` with timestamp prefixes\n- `PlanTracker` — Serialized file writer for `GENERATION-PLAN.md` using promise-chain pattern to prevent corruption from concurrent pool workers\n- `CommandRunner` — High-level workflow orchestrator exposing `executeGenerate(plan)`, `executeUpdate(changes)`, `executeSpecify()` methods coordinating pool execution + progress reporting + plan tracking + trace emission\n\n## Module Organization\n\nThis barrel export consolidates six submodules:\n1. **types.js** — Shared interfaces for results, summaries, events, options\n2. **pool.js** — Concurrency pool implementation with iterator-based task distribution\n3. **progress.js** — Progress reporting with ETA and file logging\n4. **plan-tracker.js** — GENERATION-PLAN.md serialized writer\n5. **trace.js** — NDJSON trace event emission with pluggable backend\n6. **runner.js** — Command workflow coordination integrating all subsystems\n\n## Integration Pattern\n\nConsumers import from `./orchestration/index.js` to access:\n- `CommandRunner` for high-level command execution (`executeGenerate`, `executeUpdate`)\n- `runPool` for direct concurrency pool usage in custom workflows\n- `ProgressReporter` for custom progress tracking\n- `createTraceWriter` for enabling trace emission via `--trace` flag\n- Type definitions for strongly-typed result handling and configuration\n\nExample from `src/cli/generate.ts`:\n```typescript\nimport { CommandRunner } from '../orchestration/index.js';\nconst runner = new CommandRunner(aiService, runOptions);\nconst summary = await runner.executeGenerate(plan);\n```\n### plan-tracker.ts\n**Purpose:** PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via pr...\n\n**PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via promise-chain queuing to prevent file corruption during parallel Phase 1 worker completions.**\n\n## Exported Class\n\n`PlanTracker` manages generation progress by updating markdown checkboxes in `.agents-reverse-engineer/GENERATION-PLAN.md`. Constructor signature: `constructor(projectRoot: string, initialMarkdown: string)`. Three public methods: `initialize()`, `markDone(itemPath: string)`, `flush()`.\n\n## State Management\n\n`content: string` holds current markdown in memory for fast string replacement operations. `planPath: string` stores absolute file path computed as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` where `CONFIG_DIR` imported from `../config/loader.js`. `writeQueue: Promise<void>` implements serialized write queue initialized to `Promise.resolve()`.\n\n## Initialization\n\n`initialize()` creates parent directory via `mkdir(path.dirname(this.planPath), { recursive: true })` then writes initial `this.content` to disk. Catches exceptions silently since tracking is non-critical to generation pipeline success.\n\n## Checkbox Update Pattern\n\n`markDone(itemPath: string)` replaces markdown checkbox pattern `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``. Expects exact paths as they appear in markdown: file paths like `src/cli/init.ts`, directory aggregation paths with `/AGENTS.md` suffix appended by caller (e.g., `src/cli/AGENTS.md`), root documents like `CLAUDE.md`. Returns early if string replacement yields no change (no match found).\n\n## Write Serialization\n\nAfter successful string replacement, chains write operation onto `this.writeQueue` via `this.writeQueue = this.writeQueue.then(() => writeFile(this.planPath, this.content, 'utf8')).catch(() => {})`. Catch clause swallows errors silently (non-critical operation). Pattern ensures concurrent `markDone()` calls from Phase 1 worker pool execute disk writes sequentially despite parallel in-memory updates, preventing TOCTOU corruption.\n\n## Flush Mechanism\n\n`flush()` awaits `this.writeQueue` to drain pending writes before `executeGenerate()` returns. Guarantees final plan state persists to disk despite async worker completions.\n\n## Usage Protocol\n\nCreate single `PlanTracker` instance at start of `executeGenerate()` in `src/generation/executor.ts`. Call `initialize()` once after construction. Workers invoke `markDone(itemPath)` as tasks complete. Orchestrator calls `flush()` before returning to ensure all queued writes finish.\n### pool.ts\n**Purpose:** Iterator-based concurrency pool implementing shared-iterator worker pattern for zero-dependency concurrent task execu...\n\n**Iterator-based concurrency pool implementing shared-iterator worker pattern for zero-dependency concurrent task execution with N-bounded parallelism, fail-fast abort semantics, and NDJSON trace emission.**\n\n## Exported Interface\n\n`runPool<T>(tasks, options, onComplete?)` executes array of async task factories through concurrency-limited pool. Returns `Promise<TaskResult<T>[]>` where results indexed by original task position. Parameters:\n- `tasks: Array<() => Promise<T>>` — zero-argument async factory functions\n- `options: PoolOptions` — pool configuration with required `concurrency: number`, optional `failFast?: boolean`, `tracer?: ITraceWriter`, `phaseLabel?: string`, `taskLabels?: string[]`\n- `onComplete?: (result: TaskResult<T>) => void` — callback invoked after each task settles\n\n`TaskResult<T>` discriminated by `success: boolean`, contains `index: number`, optional `value?: T` (when success true), optional `error?: Error` (when success false).\n\n`PoolOptions` configures maximum `concurrency: number`, optional `failFast?: boolean` stops pulling new tasks on first error, optional `tracer?: ITraceWriter` for concurrency debugging (imported from `'./trace.js'`), optional `phaseLabel?: string` for trace events (example: `'phase-1-files'`), optional `taskLabels?: string[]` indexed by task position for trace event labels (example: file paths).\n\n## Shared-Iterator Worker Pattern\n\nAll workers iterate over same `tasks.entries()` iterator. Each `worker()` consumes `[index, task]` pairs via `for...of` loop over shared iterator. JavaScript iterator protocol ensures each task picked up by exactly one worker (atomic `.next()` calls). When worker finishes task, immediately pulls next from iterator, keeping all worker slots busy without batch idle periods.\n\nEffective concurrency capped via `Math.min(options.concurrency, tasks.length)` to prevent spawning more workers than tasks. Workers spawned via `Array.from({ length: effectiveConcurrency }, (_, workerId) => worker(iterator, workerId))` and awaited via `Promise.allSettled(workers)`.\n\n## Abort Semantics\n\nShared `aborted` flag checked before pulling next task via `if (aborted) break` at loop start. Flag set when `options.failFast` true and task throws error via `aborted = true; break`. Workers in-flight during abort complete their current task before checking flag.\n\n## Trace Event Emission\n\nEmits trace events via `tracer?.emit()` optional chaining when `options.tracer` provided:\n- `{ type: 'worker:start', workerId, phase }` at worker spawn\n- `{ type: 'worker:end', workerId, phase, tasksExecuted }` at worker termination\n- `{ type: 'task:pickup', workerId, taskIndex, taskLabel, activeTasks }` before task execution\n- `{ type: 'task:done', workerId, taskIndex, taskLabel, durationMs, success, activeTasks, error? }` after task settles\n\nTask labels resolved via `taskLabels?.[index] ?? 'task-${index}'`. Duration computed via `Date.now() - taskStart`. Global `activeTasks` counter incremented before execution, decremented after completion for snapshot visibility.\n\n## Result Collection\n\nResults array sparse-populated via `results[index] = result` to preserve task position. Returns `TaskResult<T>[]` where index correlates back to input task array position. Sparse array may contain holes when abort occurs before all tasks execute.\n\nError coercion via `err instanceof Error ? err : new Error(String(err))` normalizes non-Error throws.\n### progress.ts\n**Purpose:** ProgressReporter provides streaming build-log progress output with ETA calculation via moving averages, colored termi...\n\n**ProgressReporter provides streaming build-log progress output with ETA calculation via moving averages, colored terminal formatting, and optional plain-text file mirroring for real-time monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog** — Plain-text progress log file writer mirroring console output to `.agents-reverse-engineer/progress.log` without ANSI escape codes, enabling `tail -f` monitoring when CLI runs inside buffered environments like Claude Code's Bash tool.\n\n- `constructor(filePath: string)` — Creates writer with promise-chain serialization for concurrent-safe writes\n- `static create(projectRoot: string): ProgressLog` — Factory creating instance with path `<projectRoot>/.agents-reverse-engineer/progress.log`\n- `write(line: string): void` — Appends line to log file, creating parent directory and opening file handle ('w' truncate mode) on first call, silently swallowing write failures\n- `finalize(): Promise<void>` — Flushes pending writes and closes file handle\n\nPrivate fields: `writeQueue: Promise<void>` (serialization chain), `fd: FileHandle | null` (open file handle)\n\n**ProgressReporter** — Streaming build-log reporter tracking file/directory task progress with ETA calculation from sliding window of last 10 completion times, outputting colored console logs and optional plain-text file mirroring.\n\n- `constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)` — Initializes reporter with task totals and optional file-based log\n- `onFileStart(filePath: string): void` — Logs file analysis start with format `[X/Y] ANALYZING path`\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — Logs file completion with format `[X/Y] DONE path Xs in/out tok model ~Ns remaining`, records duration for ETA calculation, computes total input tokens as `tokensIn + cacheReadTokens + cacheCreationTokens`\n- `onFileError(filePath: string, error: string): void` — Logs file failure with format `[X/Y] FAIL path error`\n- `onDirectoryStart(dirPath: string): void` — Logs directory AGENTS.md generation start with format `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — Logs directory completion with format `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`, records duration for directory ETA calculation\n- `onRootDone(docPath: string): void` — Logs root document completion with format `[root] DONE docPath`\n- `printSummary(summary: RunSummary): void` — Outputs end-of-run summary with version, files processed/failed/skipped, total calls, token counts (input/output/cache read/cache created), files read (total and unique count), elapsed time, error count, retry count\n\nPrivate fields: `totalFiles: number`, `started: number`, `completed: number`, `failed: number`, `completionTimes: number[]` (sliding window for file ETA), `windowSize: number = 10` (max window size), `startTime: number` (timestamp for elapsed calculation), `totalDirectories: number`, `dirStarted: number`, `dirCompleted: number`, `dirCompletionTimes: number[]` (sliding window for directory ETA), `progressLog: ProgressLog | null`\n\nPrivate methods:\n- `formatETA(): string` — Computes file ETA from moving average of `completionTimes`, returns empty string if fewer than 2 completions, formats as `~12s remaining` or `~2m 30s remaining`\n- `formatDirectoryETA(): string` — Computes directory ETA from moving average of `dirCompletionTimes`, returns empty string if fewer than 2 completions\n\n## Exported Functions\n\n**stripAnsi(str: string): string** — Strips ANSI escape sequences from string for plain-text output via regex pattern `/\\x1b\\[[0-9;]*m/g` (matches SGR, cursor, erase codes).\n\n## Dependencies\n\n**picocolors (`pc`)** — Used for terminal color formatting: `pc.dim()`, `pc.cyan()`, `pc.green()`, `pc.red()`, `pc.blue()`, `pc.bold()`, `pc.yellow()`\n\n**node:fs/promises** — `open()` for file handle creation, `mkdir()` for directory creation, `FileHandle` type\n\n**./types.js** — Imports `RunSummary` type containing aggregated run statistics (version, filesProcessed, filesFailed, filesSkipped, totalCalls, token counts, files read, elapsed time, error/retry counts)\n\n## Output Format Specifications\n\n**File progress lines:**\n- Start: `[X/Y] ANALYZING path`\n- Done: `[X/Y] DONE path Xs in/out tok model ~Ns remaining`\n- Fail: `[X/Y] FAIL path error`\n\n**Directory progress lines:**\n- Start: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- Done: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n\n**Root progress lines:**\n- Done: `[root] DONE docPath`\n\n**Summary format:**\n```\n=== Run Summary ===\n  ARE version:     <version>\n  Files processed: <count>\n  Files failed:    <count>  (if > 0)\n  Files skipped:   <count>  (if > 0)\n  Total calls:     <count>\n  Tokens:          <totalIn> in / <totalOut> out\n  Cache:           <cacheRead> read / <cacheCreated> created  (if cacheRead > 0)\n  Files read:      <total> (<unique> unique)  (if totalFilesRead > 0)\n  Total time:      <elapsed>s\n  Errors:          <count>\n  Retries:         <count>  (if > 0)\n```\n\n## Constants\n\n**PROGRESS_LOG_FILENAME** — `'progress.log'` (relative path for log file)\n\n## Implementation Patterns\n\n**Promise-chain serialization** — `ProgressLog.writeQueue` uses same pattern as `TraceWriter` to serialize concurrent writes from pool workers: `this.writeQueue = this.writeQueue.then(async () => { ... }).catch(() => {})`, preventing NDJSON corruption\n\n**Moving average ETA** — `completionTimes` and `dirCompletionTimes` arrays maintain sliding window (max size `windowSize = 10`) of recent task durations, compute average via `reduce((a, b) => a + b, 0) / length`, multiply by remaining tasks to estimate time remaining\n\n**Atomic console output** — Uses `console.log()` for each line instead of buffered streams to prevent interleaved output from concurrent pool workers\n\n**ANSI stripping** — `stripAnsi()` removes color codes before writing to `ProgressLog` via regex `/\\x1b\\[[0-9;]*m/g` for plain-text tail monitoring\n\n**Lazy file handle creation** — `ProgressLog.fd` remains null until first write, then opens in truncate mode ('w') and reuses handle for subsequent appends\n\n**Graceful degradation** — Progress log write failures are silently caught (`catch(() => {})`) as non-critical telemetry loss is acceptable\n### runner.ts\n**Purpose:** CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent file analysis, post-order di...\n\n**CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent file analysis, post-order directory aggregation, and sequential root document synthesis with quality validation, trace emission, and progress tracking.**\n\n## Exported Class\n\n`CommandRunner` — main orchestration class holding AIService reference and CommandRunOptions for executing generate/update workflows.\n\n### Constructor\n\n```typescript\nconstructor(aiService: AIService, options: CommandRunOptions)\n```\n\nWires tracer into AIService via `setTracer()` if `options.tracer` provided.\n\n### Primary Methods\n\n`executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` — runs full three-phase pipeline: file analysis (concurrent), directory AGENTS.md (post-order by depth), root documents (sequential). Returns aggregated RunSummary with token counts, inconsistency metrics, duration.\n\n`executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` — runs Phase 1 only for changed files, skips Phase 2/3. Supports incremental updates via hash-based change detection.\n\n## Three-Phase Execution Pipeline\n\n### Pre-Phase 1: Cache Existing .sum Content\n\nThrottled parallel read (concurrency=20) of existing `.sum` files via `readSumFile()` into `oldSumCache: Map<string, SumFileContent>`. Used for stale documentation detection in quality validation. Emits `phase:start` and `phase:end` trace events with `phase: 'pre-phase-1-cache'`.\n\n### Phase 1: File Analysis (Concurrent)\n\nMaps `plan.fileTasks` to async functions calling:\n1. `readFile(task.absolutePath)` for source content\n2. `aiService.call()` with `buildFilePrompt()` user/system prompts\n3. `computeContentHashFromString()` for SHA-256 hash\n4. `stripPreamble()` on response text\n5. `extractPurpose()` for one-line metadata\n6. `writeSumFile()` with SumFileContent (summary, generatedAt, contentHash)\n7. `writeAnnexFile()` if response contains `## Annex References` marker\n\nCaches source content in `sourceContentCache: Map<string, string>` for reuse in quality validation. Executes via `runPool()` with `options.concurrency` workers. Updates `PlanTracker.markDone()` and `ProgressReporter.onFileDone()` per completion. Emits `phase:start`, `phase:end` traces with `phase: 'phase-1-files'`.\n\n### Post-Phase 1: Quality Validation\n\nGroups files by directory via `path.dirname()`, runs throttled parallel checks (concurrency=10):\n\n**Code-vs-doc**: `checkCodeVsDoc()` twice per file — once against `oldSumCache` (stale detection), once against freshly written `.sum` (omission detection). Regex-extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Code-vs-code**: `checkCodeVsCode()` aggregates exports per directory group into `Map<symbol, string[]>`, detects duplicates.\n\n**Reporting**: Builds InconsistencyReport via `buildInconsistencyReport()`, prints via `formatReportForCli()`, assigns counts to `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`. Non-throwing — logs errors to stderr on failure. Clears `sourceContentCache` after validation to free memory.\n\nEmits `phase:start`, `phase:end` traces with `phase: 'post-phase-1-quality'`.\n\n### Phase 2: Directory Aggregation (Post-Order by Depth)\n\nGroups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`. Processes depth levels descending (`sort((a,b) => b-a)`) to ensure children precede parents.\n\nPer depth level:\n1. Computes `dirConcurrency = Math.min(options.concurrency, dirsAtDepth.length)`\n2. Calls `buildDirectoryPrompt()` with knownDirs filter (`Set` from `plan.directoryTasks.map(t => t.path)`)\n3. Invokes `aiService.call()` with directory prompt\n4. Writes via `writeAgentsMd()` which merges user-authored `AGENTS.local.md` if present\n5. Updates `PlanTracker.markDone()` and `ProgressReporter.onDirectoryDone()`\n\nEmits `phase:start`, `phase:end` traces per depth with `phase: 'phase-2-dirs-depth-${depth}'`.\n\n### Post-Phase 2: Phantom Path Validation\n\nReads each `AGENTS.md` via `readFile()`, calls `checkPhantomPaths()` to extract path-like strings via three regex patterns:\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\nResolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`. Builds PhantomPathInconsistency report, assigns `phantomPathCount`. Non-throwing — logs errors on failure.\n\n### Phase 3: Root Documents (Sequential)\n\nIterates `plan.rootTasks` with concurrency=1:\n1. Calls `buildRootPrompt()` which injects all `AGENTS.md` via `collectAgentsDocs()`\n2. Invokes `aiService.call()` with `maxTurns: 1` (no tool use)\n3. Strips conversational preamble via markdown start detection (`indexOf('# ')`)\n4. Writes to `rootTask.outputPath` via `writeFile()`\n5. Updates `PlanTracker.markDone()` and `ProgressReporter.onRootDone()`\n\nEmits `task:start`, `task:done` (with success/error), `phase:start`, `phase:end` traces with `phase: 'phase-3-root'`. Re-throws errors to maintain existing error handling.\n\n## Progress Tracking Infrastructure\n\n`PlanTracker` — writes `GENERATION-PLAN.md` with checkbox syntax via `formatExecutionPlanAsMarkdown()`, updates via `markDone()`, flushes via promise chain.\n\n`ProgressReporter` — streams human-readable progress to console and `options.progressLog` file via `onFileStart()`, `onFileDone()`, `onFileError()`, `onDirectoryStart()`, `onDirectoryDone()`, `onRootDone()`, `printSummary()`. Calculates ETA via moving average.\n\nTrace emission via `this.tracer?.emit()` for events: `phase:start`, `phase:end`, `task:start`, `task:done`, `worker:start`, `worker:end`. Tracer threaded through `runPool()` options and `aiService.setTracer()`.\n\n## Update Workflow (Incremental)\n\n`executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`:\n1. Reads existing project plan from `.agents-reverse-engineer/GENERATION-PLAN.md` for context (optional)\n2. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode\n3. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code)\n4. Skips Phase 2/3 — caller (`src/update/orchestrator.ts`) handles `AGENTS.md` regeneration for `affectedDirs`\n\nReturns RunSummary with `filesSkipped: 0` (update mode doesn't skip).\n\n## Helper Functions\n\n`stripPreamble(responseText: string): string` — removes LLM conversational preamble via two patterns:\n1. Content after `\\n---\\n` separator within first 500 chars\n2. Content starting with bold purpose `**[A-Z]` if preceding text is <300 chars and lacks `##`\n\n`extractPurpose(responseText: string): string` — scans lines skipping:\n- Empty, `#` headers, `---` separators\n- Lines starting with `PREAMBLE_PREFIXES` (case-insensitive): `['now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`\n\nStrips bold wrapper `**...**`, truncates to 120 chars with `...` suffix.\n\n## Dependencies\n\n`AIService` — `call()`, `setTracer()`, `addFilesReadToLastEntry()`, `getSummary()` methods.\n\n`ExecutionPlan` — fileTasks, directoryTasks, rootTasks arrays with metadata (depth, absolutePath, path, userPrompt, systemPrompt, outputPath).\n\n`runPool()` — iterator-based concurrency pool from `src/orchestration/pool.ts` with `onResult` callback for progress updates.\n\n`buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` — prompt construction from `src/generation/prompts/`.\n\n`writeSumFile()`, `readSumFile()`, `writeAnnexFile()` — YAML frontmatter `.sum` file I/O from `src/generation/writers/sum.ts`.\n\n`writeAgentsMd()` — directory doc writer from `src/generation/writers/agents-md.ts`.\n\n`checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()` — quality validators from `src/quality/`.\n\n`computeContentHashFromString()` — SHA-256 hashing from `src/change-detection/`.\n\n`PlanTracker`, `ProgressReporter`, `ITraceWriter` — progress tracking from `src/orchestration/`.\n\n`getVersion()` — package version from `src/version.ts`.\n\n## Trace Events Emitted\n\nAll events auto-populated with `seq`, `ts`, `pid`, `elapsedMs` by TraceWriter.\n\n- `phase:start` — taskCount, concurrency, phase label\n- `phase:end` — durationMs, tasksCompleted, tasksFailed, phase label\n- `task:start` — taskLabel, phase (Phase 3 only)\n- `task:done` — workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks (Phase 3 only)\n\n## RunSummary Schema\n\n```typescript\n{\n  version: string,\n  filesProcessed: number,\n  filesFailed: number,\n  filesSkipped: number,\n  totalCalls: number,\n  totalInputTokens: number,\n  totalOutputTokens: number,\n  totalCacheReadTokens: number,\n  totalCacheCreationTokens: number,\n  totalDurationMs: number,\n  errorCount: number,\n  retryCount: number,\n  totalFilesRead: number,\n  uniqueFilesRead: number,\n  inconsistenciesCodeVsDoc: number,\n  inconsistenciesCodeVsCode: number,\n  phantomPaths?: number,\n  inconsistencyReport?: InconsistencyReport\n}\n```\n\n## Error Handling\n\nQuality validation (code-vs-doc, code-vs-code, phantom-paths) wrapped in try-catch with `console.error()` — failures logged but don't abort pipeline.\n\nPhase 3 re-throws errors after emitting `task:done` with `success: false` to preserve existing error propagation.\n\nPool failures handled via `onResult` callback checking `result.success` — updates `filesFailed` counter, logs via `reporter.onFileError()`.\n### trace.ts\n**Purpose:** trace.ts implements append-only NDJSON trace event emission for debugging task/subprocess lifecycle with promise-chai...\n\n**trace.ts implements append-only NDJSON trace event emission for debugging task/subprocess lifecycle with promise-chain serialization to handle concurrent writes from pool workers.**\n\n## Exported Interface\n\n`ITraceWriter` — Public interface for trace event emission with three methods:\n- `emit(event: TraceEventPayload): void` — Emits trace event with auto-populated base fields (seq, ts, pid, elapsedMs)\n- `finalize(): Promise<void>` — Flushes pending writes and closes file handle\n- `filePath: string` — Absolute path to trace file (empty string for NullTraceWriter)\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — Factory returning NullTraceWriter when `enabled` is false, otherwise TraceWriter writing to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — Removes old trace files keeping only most recent ones, returns count deleted\n\n## Event Schema\n\n`TraceEvent` — Discriminated union of 15 event types:\n- `PhaseStartEvent` — type `'phase:start'`, fields: `phase`, `taskCount`, `concurrency`\n- `PhaseEndEvent` — type `'phase:end'`, fields: `phase`, `durationMs`, `tasksCompleted`, `tasksFailed`\n- `WorkerStartEvent` — type `'worker:start'`, fields: `workerId`, `phase`\n- `WorkerEndEvent` — type `'worker:end'`, fields: `workerId`, `phase`, `tasksExecuted`\n- `TaskPickupEvent` — type `'task:pickup'`, fields: `workerId`, `taskIndex`, `taskLabel`, `activeTasks`\n- `TaskDoneEvent` — type `'task:done'`, fields: `workerId`, `taskIndex`, `taskLabel`, `durationMs`, `success`, `error?`, `activeTasks`\n- `TaskStartEvent` — type `'task:start'`, fields: `taskLabel`, `phase`\n- `SubprocessSpawnEvent` — type `'subprocess:spawn'`, fields: `childPid`, `command`, `taskLabel`\n- `SubprocessExitEvent` — type `'subprocess:exit'`, fields: `childPid`, `command`, `taskLabel`, `exitCode`, `signal`, `durationMs`, `timedOut`\n- `RetryEvent` — type `'retry'`, fields: `attempt`, `taskLabel`, `errorCode`\n- `DiscoveryStartEvent` — type `'discovery:start'`, fields: `targetPath`\n- `DiscoveryEndEvent` — type `'discovery:end'`, fields: `filesIncluded`, `filesExcluded`, `durationMs`\n- `FilterAppliedEvent` — type `'filter:applied'`, fields: `filterName`, `filesMatched`, `filesRejected`\n- `PlanCreatedEvent` — type `'plan:created'`, fields: `planType`, `fileCount`, `taskCount`\n- `ConfigLoadedEvent` — type `'config:loaded'`, fields: `configPath`, `model`, `concurrency`\n\n`TraceEventBase` — Common fields auto-populated by TraceWriter: `seq` (monotonic number), `ts` (ISO 8601 string), `pid` (process.pid), `elapsedMs` (high-resolution fractional milliseconds since run start via `process.hrtime.bigint()`)\n\n`TraceEventPayload` — Type alias using `DistributiveOmit<TraceEvent, BaseKeys>` to strip auto-populated fields from event payloads\n\n`DistributiveOmit<T, K extends PropertyKey>` — Distributive conditional type `T extends unknown ? Omit<T, K> : never` ensuring Omit correctly distributes across discriminated union members\n\n## Implementation Classes\n\n`NullTraceWriter implements ITraceWriter` — No-op implementation with empty methods and empty `filePath`, ensures zero overhead when `--trace` flag absent\n\n`TraceWriter implements ITraceWriter` — Real implementation using promise-chain serialization pattern:\n- Private fields: `seq` (counter), `nodePid` (process.pid), `startHr` (process.hrtime.bigint() at construction), `writeQueue` (Promise<void> chain), `fd` (FileHandle | null)\n- `emit()` enriches payload with base fields, serializes to JSON line, chains write via `this.writeQueue = this.writeQueue.then(async () => {...})`\n- Lazy file handle opening: first write creates parent directory via `mkdir(path.dirname(this.filePath), { recursive: true })` and opens file in append mode\n- Write errors caught and silently ignored (trace loss acceptable, non-critical)\n- `finalize()` awaits `writeQueue` then closes `fd` if open\n\n## Trace File Management\n\n`TRACES_DIR` — Constant `'.agents-reverse-engineer/traces'` defines trace directory relative to project root\n\n`createTraceWriter()` generates timestamp via `new Date().toISOString().replace(/[:.]/g, '-')` for filename `trace-{timestamp}.ndjson`\n\n`cleanupOldTraces()` mirrors `src/ai/telemetry/cleanup.ts` pattern: filters `trace-*.ndjson` files, sorts lexicographically (ISO timestamps), reverses to newest-first, deletes beyond `keepCount` (default 500)\n\n## Integration Points\n\nThreaded via `CommandRunOptions.tracer` field to pool workers (`src/orchestration/pool.ts`), AIService (`src/ai/service.ts`), phase runners (`src/orchestration/runner.ts`)\n\nPhase orchestration emits `phase:start`/`phase:end`, worker pool emits `worker:start`/`worker:end`/`task:pickup`/`task:done`, subprocess manager (`src/ai/subprocess.ts`) emits `subprocess:spawn`/`subprocess:exit`, retry logic (`src/ai/retry.ts`) emits `retry`\n\nDiscovery (`src/discovery/run.ts`) emits `discovery:start`/`discovery:end`/`filter:applied`, plan creation emits `plan:created`, config loading emits `config:loaded`\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces for orchestration module communication: task results, run summaries, progress ...\n\n**types.ts defines TypeScript interfaces for orchestration module communication: task results, run summaries, progress events, and command options that flow between worker pools, progress reporters, and command runners.**\n\n## Exported Interfaces\n\n**FileTaskResult** represents the outcome of processing a single file through AI analysis:\n- `path: string` — relative path to source file\n- `success: boolean` — whether AI call succeeded\n- `tokensIn: number` — input tokens consumed (non-cached)\n- `tokensOut: number` — output tokens generated\n- `cacheReadTokens: number` — cache read input tokens\n- `cacheCreationTokens: number` — cache creation input tokens\n- `durationMs: number` — wall-clock duration in milliseconds\n- `model: string` — model identifier used for this call\n- `error?: string` — error message if call failed\n\n**RunSummary** aggregates command execution metrics for telemetry and display:\n- `version: string` — agents-reverse-engineer version\n- `filesProcessed: number` — successfully processed file count\n- `filesFailed: number` — failed file count\n- `filesSkipped: number` — skipped file count (e.g., dry-run)\n- `totalCalls: number` — AI call count\n- `totalInputTokens: number` — sum of input tokens across calls\n- `totalOutputTokens: number` — sum of output tokens across calls\n- `totalCacheReadTokens: number` — sum of cache read tokens\n- `totalCacheCreationTokens: number` — sum of cache creation tokens\n- `totalDurationMs: number` — total wall-clock duration\n- `errorCount: number` — error count\n- `retryCount: number` — retry count\n- `totalFilesRead: number` — total file reads across calls\n- `uniqueFilesRead: number` — unique files read (deduped by path)\n- `inconsistenciesCodeVsDoc?: number` — code-vs-doc inconsistency count\n- `inconsistenciesCodeVsCode?: number` — code-vs-code inconsistency count\n- `phantomPaths?: number` — phantom path reference count in AGENTS.md files\n- `inconsistencyReport?: InconsistencyReport` — full report (undefined if checks didn't run)\n\n**ProgressEvent** emitted by command runners to progress reporters with discriminated `type` field:\n- `type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done'` — event type\n- `filePath: string` — file or directory path\n- `index: number` — zero-based task index in current phase\n- `total: number` — total tasks in current phase\n- `durationMs?: number` — wall-clock duration (present on `'done'` events)\n- `tokensIn?: number` — input tokens consumed (present on `'done'` events)\n- `tokensOut?: number` — output tokens generated (present on `'done'` events)\n- `model?: string` — model identifier (present on `'done'` events)\n- `error?: string` — error message (present on `'error'` events)\n\n**CommandRunOptions** controls command execution behavior populated from config defaults and CLI flags:\n- `concurrency: number` — maximum concurrent AI calls\n- `failFast?: boolean` — stop pulling tasks on first error\n- `debug?: boolean` — show exact prompts sent\n- `dryRun?: boolean` — list files without executing\n- `tracer?: ITraceWriter` — trace writer for NDJSON event emission (no-op when tracing disabled)\n- `progressLog?: ProgressLog` — file-based output mirror for `tail -f` monitoring\n\n## Import Dependencies\n\nImports `InconsistencyReport` from `../quality/index.js` for quality validation results embedded in RunSummary.\n\nImports `ProgressLog` from `./progress.js` for file-based progress logging.\n\nImports `ITraceWriter` from `./trace.js` for NDJSON trace event serialization.\n\n## Event Type Semantics\n\nProgressEvent carries phase-specific optional fields:\n- `'start'` events populate `filePath`, `index`, `total`\n- `'done'` events populate `filePath`, `index`, `total`, `durationMs`, `tokensIn`, `tokensOut`, `model`\n- `'error'` events populate `filePath`, `index`, `total`, `error`\n- `'dir-done'` events populate `filePath` with directory path\n- `'root-done'` events populate `filePath` with root document path\n\n## Import Map (verified — use these exact paths)\n\nplan-tracker.ts:\n  ../config/loader.js → CONFIG_DIR\n\nrunner.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../generation/executor.js → ExecutionPlan, ExecutionTask (type)\n  ../generation/writers/sum.js → writeSumFile, readSumFile, writeAnnexFile\n  ../generation/writers/sum.js → SumFileContent (type)\n  ../generation/writers/agents-md.js → writeAgentsMd\n  ../change-detection/index.js → computeContentHashFromString\n  ../change-detection/types.js → FileChange (type)\n  ../generation/prompts/index.js → buildFilePrompt, buildDirectoryPrompt, buildRootPrompt\n  ../config/schema.js → Config (type)\n  ../config/loader.js → CONFIG_DIR\n  ../quality/index.js → checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli\n  ../quality/index.js → Inconsistency (type)\n  ../generation/executor.js → formatExecutionPlanAsMarkdown\n  ../version.js → getVersion\n\ntypes.ts:\n  ../quality/index.js → InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Annex Files (reproduction-critical constants)\n\n- runner.ts.annex.md",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker-pool concurrency control with iterator-based task distribution, progress telemetry via ETA calculation, serialized plan/log/trace writers for concurrent-safe output, and high-level command workflows integrating three-phase AI-driven documentation pipelines.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export aggregating `runPool`, `createTraceWriter`, `cleanupOldTraces`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, plus types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`)\n\n**[pool.ts](./pool.ts)** — `runPool<T>(tasks, options, onComplete?)` iterator-based worker pool sharing single `tasks.entries()` iterator across N workers, returns `Promise<TaskResult<T>[]>` preserving task index, enforces `options.concurrency` cap via `Math.min(options.concurrency, tasks.length)`, supports fail-fast abort via shared `aborted` flag, emits trace events (`worker:start/end`, `task:pickup/done`) with `activeTasks` counter\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates `executeGenerate(plan)` via three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis) with pre/post validation phases (pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks, post-phase-2 phantom path resolution), `executeUpdate(filesToAnalyze)` runs Phase 1 only for incremental workflows, helpers `stripPreamble()` remove LLM conversational prefix via `\\n---\\n` or bold purpose detection, `extractPurpose()` scans lines skipping `PREAMBLE_PREFIXES` (`['now i', 'perfect', 'based on', ...]`)\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams build-log output with format `[X/Y] ANALYZING/DONE/FAIL path` for files, `[dir X/Y] ANALYZING/DONE dirPath/AGENTS.md` for directories, `[root] DONE docPath` for roots, calculates ETA via moving average of last 10 completion times in `completionTimes[]`/`dirCompletionTimes[]` arrays, `printSummary(summary)` outputs end-of-run aggregates; `ProgressLog` mirrors console output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with lazy file handle creation, `stripAnsi(str)` removes color codes via regex `/\\x1b\\[[0-9;]*m/g`\n\n**[trace.ts](./trace.ts)** — `ITraceWriter` interface with `emit(event)`, `finalize()`, `filePath`, `createTraceWriter(projectRoot, enabled)` factory returns `TraceWriter` writing NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` or `NullTraceWriter` no-op stub, `TraceWriter` auto-populates `TraceEventBase` fields (`seq`, `ts`, `pid`, `elapsedMs` via `process.hrtime.bigint()`), serializes via promise-chain pattern, `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces keeping most recent, event types: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via `markDone(itemPath)` replacing `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``, maintains `content: string` in-memory, serializes writes via `writeQueue: Promise<void>` chain pattern, `initialize()` creates parent directory and writes initial markdown, `flush()` drains pending writes before command completion\n\n**[types.ts](./types.ts)** — `FileTaskResult` (path, success, tokensIn/Out, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed/Failed/Skipped, totalCalls, token totals, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc/CodeVsCode, phantomPaths?, inconsistencyReport?), `ProgressEvent` discriminated by type (`start|done|error|dir-done|root-done`), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?, progressLog?)\n\n## Architecture Patterns\n\n### Iterator-Based Worker Pool\n\n`pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag on error, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern. Prevents NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Three-Phase Execution Pipeline\n\n`runner.ts` executes:\n1. **Pre-Phase-1**: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection\n2. **Phase 1**: Concurrent file analysis via `runPool()` calling `aiService.call()` with `buildFilePrompt()`, writes `.sum` with YAML frontmatter (generatedAt, contentHash, purpose), caches source in `sourceContentCache`\n3. **Post-Phase-1**: Quality validation via `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions), `checkCodeVsCode()` for duplicate exports\n4. **Phase 2**: Post-order directory aggregation sorted by depth descending via `sort((a,b) => b-a)`, waits for all child `.sum` files via implicit dependency, calls `aiService.call()` with `buildDirectoryPrompt()`, writes `AGENTS.md` via `writeAgentsMd()` merging `AGENTS.local.md`\n5. **Post-Phase-2**: Phantom path validation via `checkPhantomPaths()` extracting path-like strings with three regex patterns (markdown links, backtick paths, prose-embedded), resolves via `existsSync()` with `.ts`/`.js` fallback\n6. **Phase 3**: Sequential root synthesis (concurrency=1) via `runPool()` calling `aiService.call()` with `buildRootPrompt()` injecting all `AGENTS.md` via `collectAgentsDocs()`, strips conversational preamble via markdown start detection (`indexOf('# ')`), writes to `rootTask.outputPath`\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes[]` and `dirCompletionTimes[]` with max size `windowSize=10`. Records task duration on completion, computes moving average via `reduce((a,b)=>a+b, 0)/length`, multiplies by remaining tasks. Formats via `formatETA()` as `~12s remaining` or `~2m 30s remaining`. Returns empty string if fewer than 2 completions (insufficient sample).\n\n### Trace Event Emission\n\nTracer threaded via `CommandRunOptions.tracer` → pool options → AIService. `TraceWriter` auto-populates base fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (process.pid), `elapsedMs` (high-resolution fractional via `process.hrtime.bigint()` delta). Events: `phase:start` (taskCount, concurrency, phase), `phase:end` (durationMs, tasksCompleted, tasksFailed, phase), `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks), `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode).\n\n## Incremental Update Strategy\n\n`runner.executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code duplicate detection). Returns `RunSummary` with `filesSkipped: 0`. Caller (`src/update/orchestrator.ts`) handles Phase 2 `AGENTS.md` regeneration for `affectedDirs`.\n\n## Quality Validation Phases\n\n**Pre-Phase-1 Cache**: Reads existing `.sum` files into `oldSumCache: Map<string, SumFileContent>` for stale documentation detection in post-phase-1 code-vs-doc checks.\n\n**Post-Phase-1 Code-vs-Doc**: Calls `checkCodeVsDoc()` twice per file — once against `oldSumCache` (detect stale exports), once against freshly written `.sum` (detect omissions). Extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Post-Phase-1 Code-vs-Code**: Groups files by directory via `path.dirname()`, calls `checkCodeVsCode()` aggregating exports per directory into `Map<symbol, string[]>`, detects duplicates.\n\n**Post-Phase-2 Phantom Paths**: Reads each `AGENTS.md`, calls `checkPhantomPaths()` extracting path-like strings via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`.\n\n## Behavioral Contracts\n\n### PlanTracker Checkbox Update Pattern\n```typescript\nmarkDone(itemPath: string) {\n  // Replaces:  - [ ] `${itemPath}`\n  // With:      - [x] `${itemPath}`\n}\n```\n\n### ProgressReporter Output Formats\n```\nFile progress:\n  [X/Y] ANALYZING path\n  [X/Y] DONE path Xs in/out tok model ~Ns remaining\n  [X/Y] FAIL path error\n\nDirectory progress:\n  [dir X/Y] ANALYZING dirPath/AGENTS.md\n  [dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA\n\nRoot progress:\n  [root] DONE docPath\n\nSummary format:\n  === Run Summary ===\n    ARE version:     <version>\n    Files processed: <count>\n    Files failed:    <count>\n    Files skipped:   <count>\n    Total calls:     <count>\n    Tokens:          <totalIn> in / <totalOut> out\n    Cache:           <cacheRead> read / <cacheCreated> created\n    Files read:      <total> (<unique> unique)\n    Total time:      <elapsed>s\n    Errors:          <count>\n    Retries:         <count>\n```\n\n### Preamble Stripping Patterns\n```typescript\nstripPreamble(responseText: string): string {\n  // Pattern 1: Content after \\n---\\n separator within first 500 chars\n  // Pattern 2: Content starting with bold purpose **[A-Z] if preceding text <300 chars and lacks ##\n}\n\nextractPurpose(responseText: string): string {\n  // Skip lines: empty, #headers, ---separators\n  // Skip prefixes (case-insensitive): 'now i', 'perfect', 'based on', 'let me', \n  //   'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'\n  // Strip bold wrapper **, truncate to 120 chars with ... suffix\n}\n```\n\n### TraceEvent Base Field Population\n```typescript\nemit(event: TraceEventPayload): void {\n  // Auto-populate:\n  //   seq: monotonic counter\n  //   ts: new Date().toISOString()\n  //   pid: process.pid\n  //   elapsedMs: Number(process.hrtime.bigint() - startHr) / 1e6\n}\n```\n\n### Pool Worker Task Distribution\n```typescript\nrunPool<T>(tasks, options, onComplete?) {\n  const iterator = tasks.entries(); // Shared across workers\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) => \n    worker(iterator, workerId)\n  );\n  // Each worker pulls [index, task] via for...of over shared iterator\n  // Iterator protocol guarantees atomic .next() calls → no duplicate pickups\n}\n```\n\n## Reproduction-Critical Constants\n\nFull prompt template texts referenced in `runner.ts`:\n- [runner.ts.annex.md](./runner.ts.annex.md)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3572,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 19567,
      "latencyMs": 94414,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:33:26.692Z",
      "prompt": "Generate AGENTS.md for directory: \"docs\" (docs)\n\n## File Summaries (1 files)\n\n### INPUT.md\n**Purpose:** INPUT.md defines the original vision and requirements specification for the agents-reverse-engineer project, establis...\n\n**INPUT.md defines the original vision and requirements specification for the agents-reverse-engineer project, establishing the Recursive Language Model (RLM) algorithm for brownfield documentation generation via post-order traversal of project trees.**\n\n## Vision Statement\n\nThe document establishes agents-reverse-engineer as a lightweight tool to reverse engineer brownfield projects and generate `AGENTS.md`, `CLAUDE.md`, and platform-specific documentation compatible with Claude Code, OpenCode, and other agent tools. Born from the need to streamline brownfield project documentation, providing agents with clear codebase overviews for long-term maintenance.\n\n## Recursive Language Model Algorithm\n\nThe RLM algorithm executes as follows:\n\n1. Build complete project structure tree via directory traversal\n2. Start execution at first leaf node (file level)\n3. Analyze each file and generate `{filename}.sum` summary files\n4. When all leaf files in a directory are summarized, analyze the directory and generate `AGENTS.md` plus supplementary files if needed\n5. Continue recursively backward until reaching project root\n\nThis post-order traversal ensures child summaries exist before parent directory analysis, enabling bottom-up documentation synthesis where directory docs can reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nEach `AGENTS.md` file describes current directory content and substructure. May reference supplementary documentation:\n\n- `ARCHITECTURE.md` — System design and component relationships\n- `STRUCTURE.md` — Directory organization rationale\n- `STACK.md` — Technology stack and dependencies\n- `INTEGRATIONS.md` — External service integrations\n- `INFRASTRUCTURE.md` — Deployment and runtime configuration\n- `CONVENTIONS.md` — Coding standards and naming patterns\n- `TESTING.md` — Test strategy and frameworks\n- `PATTERNS.md` — Design patterns and architectural idioms\n- `CONCERNS.md` — Known issues and technical debt\n\n## Required Features\n\n- Command execution using Recursive Language Model with Claude Code or alternative LLM agent tools (commands for Claude, available alternatives for others)\n- Session-end hook for incremental updates of impacted files\n- Generation of `AGENTS.md` in every project directory\n- Multi-platform support: Claude Code, OpenCode, and other agent tools\n\n## Usage Commands\n\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references two methodologies for analysis:\n\n- **GSD (Get Shit Done)**: https://github.com/glittercowboy/get-shit-done\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD\n\nRecommends analyzing repository structure (primarily GSD, partially BMAD), brownfield project approaches, special commands, and codebase implementation details.\n\n## Complementary Tooling\n\nDesigned for workflow integration with:\n\n- **SpecKit**: https://github.com/github/spec-kit\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD  \n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\n**Original vision document defining the Recursive Language Model (RLM) algorithm for agents-reverse-engineer, establishing post-order traversal methodology, directory documentation schema, multi-platform requirements, and integration references.**\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nDefines RLM algorithm executing post-order traversal: build project tree, analyze leaf files to generate `{filename}.sum` summaries, synthesize directory `AGENTS.md` when all children complete, recurse to root producing `CLAUDE.md` and platform-specific docs.\n\n## Recursive Language Model Algorithm\n\n**Core traversal strategy:**\n1. Build complete project structure tree via directory walk\n2. Execute at first leaf node (file level)\n3. Generate `{filename}.sum` per file via AI analysis\n4. When directory leaves complete, synthesize `AGENTS.md` consuming child `.sum` files\n5. Recurse backward to root producing final integration docs\n\n**Guarantees:** Child summaries exist before parent directory analysis, enabling bottom-up synthesis where directory docs reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nINPUT.md defines nine supplementary document types referenceable from `AGENTS.md`:\n\n- `ARCHITECTURE.md` — system design and component relationships\n- `STRUCTURE.md` — directory organization rationale\n- `STACK.md` — technology stack and dependencies\n- `INTEGRATIONS.md` — external service integrations\n- `INFRASTRUCTURE.md` — deployment and runtime configuration\n- `CONVENTIONS.md` — coding standards and naming patterns\n- `TESTING.md` — test strategy and frameworks\n- `PATTERNS.md` — design patterns and architectural idioms\n- `CONCERNS.md` — known issues and technical debt\n\n## Multi-Platform Requirements\n\n**Target runtimes:** Claude Code, OpenCode, and alternative LLM agent tools requiring brownfield project context.\n\n**Core features:**\n- Command execution using RLM with Claude Code or alternative backends\n- Session-end hook for incremental updates of impacted files\n- `AGENTS.md` generation in every project directory\n- Platform-agnostic documentation format\n\n**Usage commands:**\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references three methodologies for implementation analysis:\n\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — repository structure and brownfield approaches (primary)\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — partial analysis of special commands and codebase patterns\n- **SpecKit**: https://github.com/github/spec-kit — complementary tooling for specification synthesis",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 655,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9858,
      "latencyMs": 16172,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:33:26.695Z",
      "prompt": "Generate AGENTS.md for directory: \"scripts\" (scripts)\n\n## File Summaries (1 files)\n\n### build-hooks.js\n**Purpose:** build-hooks.js copies session lifecycle hook files from `hooks/` to `hooks/dist/` for npm package distribution.\n\n**build-hooks.js copies session lifecycle hook files from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Execution Context\n\nRuns as Node.js script via shebang `#!/usr/bin/env node`, invoked by `npm run build:hooks` and automatically during `npm run prepublishOnly` (see package.json).\n\n## Build Algorithm\n\n1. Resolves `projectRoot` via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..')`\n2. Defines `HOOKS_SRC = join(projectRoot, 'hooks')` and `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')`\n3. Creates `HOOKS_DIST` directory via `mkdirSync(HOOKS_DIST, { recursive: true })` if not `existsSync(HOOKS_DIST)`\n4. Filters `readdirSync(HOOKS_SRC)` for `.js` files excluding `'dist'` directory itself\n5. Iterates `hookFiles` array, calling `copyFileSync(src, dest)` for each file where `src = join(HOOKS_SRC, file)` and `dest = join(HOOKS_DIST, file)`\n6. Logs each copy operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n7. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Copied Hook Files\n\nBased on project structure, copies four session lifecycle hooks:\n- `are-check-update.js` — SessionStart version check hook for Claude/Gemini\n- `are-session-end.js` — SessionEnd auto-update hook for Claude/Gemini\n- `opencode-are-check-update.js` — OpenCode plugin for version checking\n- `opencode-are-session-end.js` — OpenCode plugin for session-end updates\n\n## Integration with Package Distribution\n\nEnsures `hooks/dist/` contains all hook files before `npm publish` executes, allowing `package.json` `files` field to include `\"hooks/dist\"` for tarball inclusion. The `hooks/dist/` directory is the source for installer operations in `src/installer/operations.ts` which copies hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\n**Build automation directory containing pre-publish hook file preparation script that copies session lifecycle hooks from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Contents\n\n### Build Scripts\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` files from `hooks/` to `hooks/dist/`, invoked by `npm run prepublishOnly` to prepare session lifecycle hooks for npm tarball inclusion.\n\n## Build Pipeline Integration\n\n`build-hooks.js` executes during npm publish workflow via `prepublishOnly` lifecycle hook defined in root `package.json`. The script ensures `hooks/dist/` contains all four session lifecycle hooks before `npm publish` bundles the package tarball:\n\n1. **are-check-update.js** — Claude/Gemini SessionStart hook for version checking\n2. **are-session-end.js** — Claude/Gemini SessionEnd hook for auto-update\n3. **opencode-are-check-update.js** — OpenCode plugin for version checking\n4. **opencode-are-session-end.js** — OpenCode plugin for session-end updates\n\nThe copied files become the source for installer operations (`src/installer/operations.ts`) which deploy hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).\n\n## Execution Flow\n\n1. Resolves project root via `fileURLToPath(import.meta.url)` → `path.dirname()` → `path.join(__dirname, '..')`\n2. Creates `hooks/dist/` via `mkdirSync(HOOKS_DIST, { recursive: true })` if directory missing\n3. Filters `readdirSync('hooks/')` for `.js` files, excluding `'dist'` directory itself\n4. Copies each file via `copyFileSync(src, dest)` where `src = 'hooks/<file>'` and `dest = 'hooks/dist/<file>'`\n5. Logs each operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n6. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Package.json Integration\n\nThe root `package.json` declares:\n- **files**: `[\"hooks/dist\"]` — includes `hooks/dist/` in npm tarball\n- **scripts.build:hooks**: `\"node scripts/build-hooks.js\"` — manual build command\n- **scripts.prepublishOnly**: `\"npm run build && npm run build:hooks\"` — automatic execution before `npm publish`\n\nCI/CD workflow (`.github/workflows/publish.yml`) runs `npm ci` which triggers `prepublishOnly`, ensuring `hooks/dist/` exists before registry upload with Sigstore-signed provenance attestation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 663,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9689,
      "latencyMs": 16050,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:33:26.703Z",
      "prompt": "Generate AGENTS.md for directory: \"hooks\" (hooks)\n\n## File Summaries (4 files)\n\n### are-check-update.js\n**Purpose:** are-check-update.js spawns a detached background process that queries npm for the latest agents-reverse-engineer vers...\n\n**are-check-update.js spawns a detached background process that queries npm for the latest agents-reverse-engineer version, compares it against installed version from ARE-VERSION files, and writes update availability status to ~/.claude/cache/are-update-check.json.**\n\n## Execution Context\n\nInvoked by SessionStart hook in Claude Code runtime. Runs once per session as non-blocking background check. Executable via shebang `#!/usr/bin/env node`.\n\n## Cache Management\n\nWrites JSON to `~/.claude/cache/are-update-check.json` with schema:\n```javascript\n{\n  update_available: boolean,  // true if installed !== latest\n  installed: string,          // version from ARE-VERSION file or '0.0.0'\n  latest: string,             // npm registry version or 'unknown'\n  checked: number             // Unix timestamp (seconds)\n}\n```\n\nCreates cache directory via `mkdirSync(cacheDir, { recursive: true })` if missing at `~/.claude/cache/`.\n\n## Version Resolution\n\nPrioritizes project-local ARE-VERSION over global:\n1. `.claude/ARE-VERSION` in current working directory (`process.cwd()`)\n2. `~/.claude/ARE-VERSION` in home directory (`homedir()`)\n\nFalls back to `'0.0.0'` if neither file exists.\n\n## Background Execution Pattern\n\nUses detached spawn to prevent blocking:\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n}).unref()\n```\n\nChild process executes inline script via `-e` flag performing npm query and cache write. Parent process exits immediately after spawn without waiting for completion.\n\n## npm Registry Query\n\nExecutes `npm view agents-reverse-engineer version` via `execSync()` with:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n- Catches errors silently, sets `latest: 'unknown'` on failure\n\n## File System Operations\n\n**Imports:** `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync` from `fs`; `homedir` from `os`; `join` from `path`; `spawn`, `execSync` from `child_process`.\n\n**Constants:**\n- `cacheDir` = `~/.claude/cache`\n- `cacheFile` = `~/.claude/cache/are-update-check.json`\n- `projectVersionFile` = `<cwd>/.claude/ARE-VERSION`\n- `globalVersionFile` = `~/.claude/ARE-VERSION`\n\nNo exports — script executes immediately on load.\n### are-session-end.js\n**Purpose:** are-session-end.js executes `npx agents-reverse-engineer@latest update --quiet` as a detached background process when...\n\n**are-session-end.js executes `npx agents-reverse-engineer@latest update --quiet` as a detached background process when session ends if uncommitted changes are detected via `git status --porcelain`.**\n\n## Disable Mechanisms\n\nExits immediately (status 0) if:\n- `process.env.ARE_DISABLE_HOOK === '1'` environment variable is set\n- `.agents-reverse-engineer.yaml` file exists and contains substring `'hook_enabled: false'` (no YAML parser used)\n\n## Change Detection\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exits silently if:\n- `status.trim()` returns empty string (no changes)\n- `execSync()` throws error (not a git repo or git unavailable)\n\n## Background Execution Pattern\n\nSpawns detached process to avoid blocking session close:\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n}).unref();\n```\n\n## Module Type\n\nES module using `import` syntax: `import { execSync, spawn } from 'child_process'` and `import { existsSync, readFileSync } from 'fs'`.\n\n## Shebang\n\n`#!/usr/bin/env node` enables direct execution as CLI script.\n\n## Integration Context\n\nRegistered as SessionEnd hook for Claude Code (`~/.claude/hooks/`) and Gemini CLI (`~/.gemini/hooks/`) via installer (`src/installer/operations.ts`). OpenCode integration uses wrapper plugin (`opencode-are-session-end.js`) that imports this script's logic.\n### opencode-are-check-update.js\n**Purpose:** OpenCode plugin factory exporting `AreCheckUpdate()` async function that returns `session.created` event handler for ...\n\n**OpenCode plugin factory exporting `AreCheckUpdate()` async function that returns `session.created` event handler for background version check spawning detached Node.js subprocess writing update availability to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n**`AreCheckUpdate()`** — Async factory function returning plugin object with event handlers. Returns `Promise<{ event: { 'session.created': AsyncFunction } }>`.\n\n## Event Handler Contract\n\nThe `session.created` handler executes synchronously at OpenCode session start without blocking. Implements detached background spawn pattern via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true }).unref()`.\n\n## Version File Resolution\n\nChecks two locations in priority order:\n1. Project-local: `<cwd>/.opencode/ARE-VERSION`\n2. Global: `~/.config/opencode/ARE-VERSION`\n\nReads first existing file via `readFileSync()`, defaults to `'0.0.0'` if neither exists.\n\n## Cache File Structure\n\nWrites JSON object to `~/.config/opencode/cache/are-update-check.json`:\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": string,\n  \"latest\": string,\n  \"checked\": number\n}\n```\n\nThe `update_available` field compares `installed !== latest` after trimming whitespace. The `checked` field stores Unix timestamp via `Math.floor(Date.now() / 1000)`. The `latest` field defaults to `'unknown'` on `npm view` failure.\n\n## NPM Registry Query\n\nSpawned subprocess executes `npm view agents-reverse-engineer version` with `execSync()` options:\n- `encoding: 'utf8'`\n- `timeout: 10000` (10s)\n- `windowsHide: true`\n\nWraps in try-catch; network/timeout failures set `latest = null` → `'unknown'` in cache output.\n\n## Cache Directory Creation\n\nEnsures `~/.config/opencode/cache/` exists via `mkdirSync(cacheDir, { recursive: true })` before spawning background process. Parent directories created automatically.\n\n## Subprocess Detachment Pattern\n\nBackground script injected as inline Node.js string via `-e` flag. Uses `JSON.stringify()` to pass file path constants (`cacheFile`, `projectVersionFile`, `globalVersionFile`) from parent scope into subprocess. The `child.unref()` call allows parent process to exit without waiting for background check completion.\n### opencode-are-session-end.js\n**Purpose:** opencode-are-session-end.js exports `AreSessionEnd` async factory returning OpenCode plugin that triggers `npx agents...\n\n**opencode-are-session-end.js exports `AreSessionEnd` async factory returning OpenCode plugin that triggers `npx agents-reverse-engineer@latest update --quiet` as detached background process when `session.deleted` event fires if git working tree has uncommitted changes.**\n\n## Exported Interface\n\n`AreSessionEnd` — Async function returning plugin object with shape `{ event: { 'session.deleted': AsyncFunction } }`. No parameters. Plugin conforms to OpenCode event handler contract.\n\n## Event Hook Behavior\n\n`event['session.deleted']` handler executes four-stage gate sequence:\n\n1. Environment gate: Returns early if `process.env.ARE_DISABLE_HOOK === '1'`\n2. Config file gate: Reads `.agents-reverse-engineer.yaml` via `existsSync()` + `readFileSync()`, returns early if content `includes('hook_enabled: false')` (substring search, no YAML parser)\n3. Git status gate: Executes `execSync('git status --porcelain', { encoding: 'utf-8' })`, returns early if output `trim()` is empty string, catches and returns on error (non-git repo)\n4. Background spawn: Launches `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })`, calls `child.unref()` to prevent blocking Node.js exit\n\n## Process Lifecycle Pattern\n\nDetached spawn pattern prevents session close blocking. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion.\n\n## Disable Mechanisms\n\n- **Environment**: `ARE_DISABLE_HOOK=1` environment variable\n- **Config**: `hook_enabled: false` anywhere in `.agents-reverse-engineer.yaml` content (not YAML-parsed, simple string inclusion check)\n\n## Platform Context\n\nOpenCode plugin system equivalent to Claude/Gemini SessionEnd hooks (`are-session-end.js`). Event name `session.deleted` maps to OpenCode lifecycle, whereas Claude/Gemini use `SessionEnd` trigger. Identical git change detection and background spawn logic across all three platform variants.\n\n## Installation Path\n\nInstalled to `~/.config/opencode/plugins/` (or `$OPENCODE_CONFIG_DIR/plugins/`) via `src/installer/operations.ts` when user runs `npx agents-reverse-engineer --runtime opencode -g`.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**Session lifecycle automation scripts for IDE integration: background version checking at session start and automatic documentation updates at session end across Claude Code, Gemini CLI, and OpenCode platforms via detached subprocess spawning with git-based change detection.**\n\n## Contents\n\n### Platform-Agnostic Scripts\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached `npm view` query comparing registry version against local ARE-VERSION files, writing availability status to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`. Prioritizes `.claude/ARE-VERSION` over `~/.claude/ARE-VERSION`, defaults to `'0.0.0'` if missing. Executes via shebang `#!/usr/bin/env node`.\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached process when `git status --porcelain` detects uncommitted changes. Exits early if `ARE_DISABLE_HOOK=1` environment variable set or `.agents-reverse-engineer.yaml` contains `hook_enabled: false` substring. ES module using `import` syntax.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — Exports `AreCheckUpdate()` async factory returning `{ event: { 'session.created': AsyncFunction } }` plugin object. Handler spawns detached Node.js subprocess writing to `~/.config/opencode/cache/are-update-check.json`. Reads version from `.opencode/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`, queries npm registry with 10s timeout, creates cache directory via `mkdirSync(cacheDir, { recursive: true })`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — Exports `AreSessionEnd()` async factory returning `{ event: { 'session.deleted': AsyncFunction } }` plugin object. Implements four-stage gate: environment check (`ARE_DISABLE_HOOK`), config substring search (`hook_enabled: false`), git status parsing, detached `npx` spawn with `stdio: 'ignore'` and `child.unref()`.\n\n## Architecture\n\n### Detached Background Execution Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\nInline script injection via `-e` flag avoids filesystem dependencies. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion. `windowsHide: true` prevents console window flash on Windows.\n\n### Version File Resolution Priority\n\nCheck hooks prioritize project-local over global version files:\n\n1. **Claude/Gemini**: `.claude/ARE-VERSION` → `~/.claude/ARE-VERSION`\n2. **OpenCode**: `.opencode/ARE-VERSION` → `~/.config/opencode/ARE-VERSION`\n\nResolution via `existsSync()` + `readFileSync()` with `'0.0.0'` fallback when neither exists.\n\n### Cache File Schema\n\nUpdate check hooks write JSON with four-field schema:\n\n```json\n{\n  \"update_available\": boolean,  // installed !== latest (whitespace-trimmed)\n  \"installed\": string,          // version from ARE-VERSION or '0.0.0'\n  \"latest\": string,             // npm registry response or 'unknown' on error\n  \"checked\": number             // Unix timestamp via Math.floor(Date.now() / 1000)\n}\n```\n\nCache locations:\n- **Claude/Gemini**: `~/.claude/cache/are-update-check.json`\n- **OpenCode**: `~/.config/opencode/cache/are-update-check.json`\n\nDirectory creation via `mkdirSync(cacheDir, { recursive: true })` before write.\n\n### Disable Mechanisms\n\nSession-end hooks implement two gates:\n\n1. **Environment**: Exit if `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file**: Exit if `.agents-reverse-engineer.yaml` exists and `readFileSync().includes('hook_enabled: false')` (substring search, no YAML parser)\n\nBoth gates return silently without error signaling.\n\n## Platform Integration\n\n### Claude Code / Gemini CLI\n\nPlatform-agnostic scripts installed via `src/installer/operations.ts`:\n- Check hook: `~/.claude/hooks/are-check-update.js` or `~/.gemini/hooks/are-check-update.js`\n- End hook: `~/.claude/hooks/are-session-end.js` or `~/.gemini/hooks/are-session-end.js`\n\nRuntime invocation via SessionStart/SessionEnd trigger names.\n\n### OpenCode\n\nPlugin factories installed to `~/.config/opencode/plugins/` or `$OPENCODE_CONFIG_DIR/plugins/`. Event handler contracts:\n- `AreCheckUpdate()` → `event['session.created']`\n- `AreSessionEnd()` → `event['session.deleted']`\n\nPlugin objects returned from async factories conform to OpenCode event system shape `{ event: { [eventName]: AsyncFunction } }`.\n\n## npm Registry Query\n\nAll check hooks execute `npm view agents-reverse-engineer version` via `execSync()` with options:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n\nTry-catch wrapper sets `latest = 'unknown'` on network/timeout errors without propagating exceptions.\n\n## Git Change Detection\n\nSession-end hooks invoke `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exit early if:\n- `status.trim()` returns empty string (clean working tree)\n- `execSync()` throws error (non-git repo or git unavailable)\n\nNo error logging — silent exit on all failure modes.\n\n## File System Operations\n\n**Shared imports across all hooks:**\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `child_process`: `spawn`, `execSync`\n- `path`: `join`\n- `os`: `homedir`\n\n**Module types:**\n- `are-session-end.js`: ES module (`import` syntax)\n- `are-check-update.js`, `opencode-*`: CommonJS-compatible (`#!/usr/bin/env node` shebang)\n\n## Build Integration\n\nHooks copied to distribution via `scripts/build-hooks.js` during `npm run build:hooks` (invoked by `prepublishOnly`). No TypeScript compilation — plain JavaScript for direct execution in IDE runtime environments.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1699,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12769,
      "latencyMs": 34043,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:33:26.699Z",
      "prompt": "Generate AGENTS.md for directory: \"src\" (src)\n\n## File Summaries (1 files)\n\n### version.ts\n**Purpose:** Exports `getVersion()` to extract package version from package.json via filesystem path resolution from ES module URL.\n\n**Exports `getVersion()` to extract package version from package.json via filesystem path resolution from ES module URL.**\n\n## Public Interface\n\n```typescript\nfunction getVersion(): string\n```\n\nReturns the `version` field from `package.json` located one directory above the compiled module location, or `'unknown'` string literal if read fails or version field missing.\n\n## Implementation Details\n\n`getVersion()` resolves the package.json path via:\n1. `import.meta.url` → `fileURLToPath()` converts ES module URL to filesystem path\n2. `dirname()` extracts directory containing the compiled `.js` file\n3. `join(__dirname, '..', 'package.json')` navigates up one level to project root\n\nUses synchronous `readFileSync(packagePath, 'utf-8')` followed by `JSON.parse()` to extract `packageJson.version`. Try-catch block returns `'unknown'` on any error (file not found, invalid JSON, missing version field).\n\n## Runtime Constraints\n\nAssumes compiled output in `dist/` with `package.json` at `dist/../package.json` (project root). Build process must preserve this relative path structure. Path resolution works for both development (`tsx src/cli/index.ts`) and production (`node dist/cli/index.js`) via `import.meta.url` dynamic resolution.\n\n## Integration Points\n\nVersion string consumed by:\n- CLI `--version` flag output (see `src/cli/index.ts`)\n- Session lifecycle hooks for update checking (`hooks/are-check-update.js` compares against `npm view agents-reverse-engineer version`)\n- Telemetry run logs metadata (`src/ai/telemetry/run-log.ts` includes version in run context)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### ai/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\n**Backend-agnostic AI CLI orchestration layer implementing subprocess pooling, exponential backoff retry, token cost telemetry with NDJSON persistence, and trace emission for concurrency debugging.**\n\n## Contents\n\n**[index.ts](./index.ts)** — Public API barrel exporting `AIService`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from types, `BackendRegistry`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()` from registry, `withRetry()`, `DEFAULT_RETRY_OPTIONS` from retry, `runSubprocess()`, `isCommandOnPath()` from subprocess/backends.\n\n**[registry.ts](./registry.ts)** — `BackendRegistry` stores AIBackend instances by name with insertion-order priority (Claude → Gemini → OpenCode), `createBackendRegistry()` pre-populates with backend adapters, `detectBackend()` returns first available via `backend.isAvailable()` iteration, `resolveBackend()` handles explicit names and 'auto' mode throwing `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on miss.\n\n**[retry.ts](./retry.ts)** — `withRetry<T>()` executes async operations with exponential backoff via `baseDelayMs * multiplier^attempt` capped at `maxDelayMs` plus 0-500ms jitter, terminates immediately on non-retryable errors via `isRetryable()` predicate, invokes `onRetry()` callback before each attempt, throws last error after exhausting `maxRetries`, `DEFAULT_RETRY_OPTIONS` defines 3 retries with 1s base delay, 8s max delay, 2x multiplier.\n\n**[service.ts](./service.ts)** — `AIService` orchestrates AI calls via `call()` wrapping `runSubprocess()` with `withRetry()`, detects rate limits via stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\"), refuses to retry timeouts (inline comment: \"spawning another heavyweight subprocess on a struggling system makes things worse\"), emits `subprocess:spawn/exit` and `retry` trace events, accumulates `TelemetryEntry` records via `TelemetryLogger`, serializes writes to subprocess log files via promise-chain queue, exposes `finalize()` for `RunLog` persistence and retention enforcement via `cleanupOldLogs()`, tracks active subprocesses for debug logging with heap/RSS metrics.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess()` spawns CLI via `execFile()` with 10MB `maxBuffer`, SIGTERM timeout via `killSignal`, stdin piping for prompt delivery, unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, process group termination via `kill(-pid, 'SIGKILL')` on completion, module-level `activeSubprocesses` Map tracking PIDs with spawn timestamps, exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency debugging, never throws (always resolves `SubprocessResult`).\n\n**[types.ts](./types.ts)** — Defines `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` methods, `AICallOptions` with `prompt`, optional `systemPrompt/model/timeoutMs/maxTurns/taskLabel`, `AIResponse` with normalized `text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw`, `SubprocessResult` with `stdout/stderr/exitCode/signal/durationMs/timedOut/childPid`, `RetryOptions` with exponential backoff config and predicates, `TelemetryEntry` per-call log with token counts and file reads, `RunLog` aggregate summary, `AIServiceError` with typed codes `CLI_NOT_FOUND/TIMEOUT/PARSE_ERROR/SUBPROCESS_ERROR/RATE_LIMIT`, `FileRead` with path/sizeBytes.\n\n## Architecture\n\n**Three-Layer Abstraction**\n\n1. **Subprocess Layer** (`subprocess.ts`): Raw `execFile()` wrapper with timeout enforcement, process group killing, stdin piping, active subprocess tracking. Returns `SubprocessResult` on all code paths (never throws).\n\n2. **Retry Layer** (`retry.ts`): Generic exponential backoff with jitter, caller-provided `isRetryable()` predicate, optional `onRetry()` callback. Terminates immediately on permanent errors.\n\n3. **Service Layer** (`service.ts`): Integrates subprocess + retry + telemetry + tracing. Detects rate limits via stderr patterns, refuses to retry timeouts, accumulates token counts, emits trace events for concurrency debugging, serializes subprocess log writes via promise chain.\n\n**Backend Registry**\n\n`BackendRegistry` stores `AIBackend` instances with insertion-order priority determining auto-detection sequence. `createBackendRegistry()` registers ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. `resolveBackend()` handles two modes: explicit name lookups with availability validation, 'auto' mode calling `detectBackend()` which iterates backends invoking `isAvailable()` until first match. Throws `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on failure.\n\n**Retry Strategy**\n\nRate limit detection via `isRateLimitStderr()` checking lowercase stderr for patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\". Only rate limit errors are retryable—timeouts are permanent failures (inline rationale: \"spawning another heavyweight subprocess on a system that's already struggling or against an unresponsive API makes things worse and can exhaust system resources\"). `withRetry()` invoked with custom predicate: `error instanceof AIServiceError && error.code === 'RATE_LIMIT'`.\n\n**Telemetry Pipeline**\n\n`TelemetryLogger` accumulates `TelemetryEntry` records in memory via `addEntry()` calls after each subprocess completion. Each entry captures `timestamp`, `prompt`, `systemPrompt`, `response`, `model`, token counts (`inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`), `latencyMs`, `exitCode`, `retryCount`, `filesRead[]` array. Command orchestrator calls `addFilesReadToLastEntry()` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`). `finalize()` invokes `logger.toRunLog()` producing aggregate `RunLog` with summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead` via Set deduplication), then serializes via `writeRunLog()` to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace Emission**\n\n`AIService` accepts optional `ITraceWriter` via `setTracer()` for concurrency debugging. Emits three event types: `subprocess:spawn` with `childPid/taskLabel/timestamp`, `subprocess:exit` with `childPid/exitCode/signal/durationMs/timedOut`, `retry` with `attempt/taskLabel/errorCode`. Trace writer (from `src/orchestration/trace.ts`) serializes events to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via promise-chain serialization ensuring write order matches emission order despite concurrent workers.\n\n**Subprocess Resource Management**\n\n`runSubprocess()` tracks active subprocesses in module-level Map with PID → `{ command, spawnedAt }` mapping. Exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for debugging. `AIService.call()` increments `activeSubprocesses` counter before spawn, decrements after completion, logs pre-spawn debug message with active count, heap usage, RSS via `process.memoryUsage()`, and configured timeout. Post-completion debug log includes PID, exit code, duration, updated active count.\n\n**Process Lifecycle**\n\n`runSubprocess()` sequence: spawn with 10MB `maxBuffer` and SIGTERM `killSignal`, track in `activeSubprocesses` Map, invoke `onSpawn()` callback synchronously for trace emission, write `options.input` to stdin then `.end()` to close stream, set unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, on callback clear timer, attempt process group kill via `kill(-pid, 'SIGKILL')` with single-process fallback, remove from `activeSubprocesses`, resolve with `SubprocessResult`. Timeout detection via `error.killed === true` from execFile. Exit code extracted from `error.code` (if number) else `child.exitCode` else defaults to 1 (failure) or 0 (success).\n\n## Behavioral Contracts\n\n**Rate Limit Patterns** (service.ts `RATE_LIMIT_PATTERNS`):\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n**Exponential Backoff Formula** (retry.ts):\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n**Default Retry Configuration** (retry.ts `DEFAULT_RETRY_OPTIONS`):\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n**Subprocess Timeout** (subprocess.ts):\n- SIGTERM sent at `timeoutMs` via execFile `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n\n**Process Group Termination** (subprocess.ts):\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // Negative PID targets process group\n```\n\n**Debug Memory Formatting** (service.ts `formatBytes()`):\n- `< 1024` → `${bytes}B`\n- `< 1048576` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1048576` → `${(bytes / 1048576).toFixed(1)}MB`\n\n**Telemetry Filename Format** (telemetry/run-log.ts):\n```typescript\n`run-${runLog.startTime.replace(/[:.]/g, '-')}.json`\n// \"2026-02-09T12:34:56.789Z\" → \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Subprocess Log Filename Sanitization** (service.ts):\n```typescript\ntaskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')\n```\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting token counts from usage/modelUsage objects, GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR` until output formats stabilize, shared `isCommandOnPath()` splitting `process.env.PATH` by platform delimiter with Windows `PATHEXT` iteration, CLI arguments with stdin prompt delivery and platform-specific flags (`--output-format json`, `--permission-mode bypassPermissions`).\n\n**[telemetry/](./telemetry/)** — `TelemetryLogger` accumulating `TelemetryEntry` records in memory with `addEntry()` and `getSummary()` computing token sums/error counts/unique file deduplication, `writeRunLog()` serializing `RunLog` to `.agents-reverse-engineer/logs/run-<timestamp>.json` with ISO timestamp transformation, `cleanupOldLogs()` deleting oldest files beyond retention threshold via lexicographic sort.\n\n## Integration Points\n\n**Upstream Consumers:**\n- `src/orchestration/runner.ts` — Instantiates AIService with resolved backend and config options, invokes `call()` per task, attaches file reads via `addFilesReadToLastEntry()`, finalizes telemetry via `finalize()`.\n- `src/generation/orchestrator.ts` — Threads AIService through three-phase pipeline (file analysis, directory aggregation, root synthesis).\n\n**Downstream Dependencies:**\n- `src/orchestration/trace.ts` — `ITraceWriter` interface for trace event emission.\n- `src/config/schema.ts` — `AIConfig` with backend selection, timeout, concurrency, telemetry retention.\n- `node:child_process` — `execFile()` for subprocess spawning.\n- `node:fs/promises` — File I/O for subprocess logs and telemetry persistence.\n### change-detection/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\n**Git-based change detection with SHA-256 content hashing for incremental documentation updates, supporting rename tracking via `git diff -M`, uncommitted change merging via `git status --porcelain`, and hash-only fallback for non-git workflows.**\n\n## Contents\n\n### [detector.ts](./detector.ts)\nImplements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, and `computeContentHashFromString()` for delta computation. Parses `git diff --name-status -M` output with tab-split line parsing to extract status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, maps to `FileChange` objects with optional `oldPath` for renames. Optionally merges uncommitted changes from `git.status()` (`modified`, `deleted`, `not_added`, `staged`) when `includeUncommitted: true`, using linear `Array.some()` deduplication to prevent duplicate entries. SHA-256 hashing via `node:crypto` `createHash('sha256')` with hex digest output.\n\n### [types.ts](./types.ts)\nDefines `ChangeType` (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` (discriminated by `status` with optional `oldPath` for `'renamed'`), `ChangeDetectionResult` (`currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`), and `ChangeDetectionOptions` (`includeUncommitted?: boolean`). Discriminated union pattern enables type-safe `oldPath` access after `status === 'renamed'` check.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting all functions (`isGitRepo`, `getCurrentCommit`, `getChangedFiles`, `computeContentHash`, `computeContentHashFromString`) and types (`ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions`) from `detector.ts` and `types.ts`. Serves as public API consumed by `../update/orchestrator.ts` and `../cli/update.ts`.\n\n## Integration Points\n\n**Incremental Update Workflow:**\n1. `src/update/orchestrator.ts` calls `getChangedFiles(projectRoot, baseCommit, options)` to compute `changes[]` array\n2. For each changed file, reads `.sum` YAML frontmatter `content_hash` via `src/generation/writers/sum.ts`\n3. Calls `computeContentHash(filePath)` to compute current file SHA-256 hash\n4. Hash mismatch → add to `filesToAnalyze` as `FileChange`, hash match → add to `filesToSkip`\n5. Detects orphans via `changes.filter(c => c.status === 'renamed')` and calls `cleanupOrphans()` to delete `.sum` files for `oldPath` entries\n6. Session-end hooks (`hooks/are-session-end.js`) check `git status --porcelain` and spawn `npx agents-reverse-engineer@latest update --quiet` if changes detected\n\n**Non-Git Fallback:**\nWhen `isGitRepo(projectRoot)` returns `false`, consumers skip git operations and rely on SHA-256 hash comparison alone for change detection (reads all `.sum` files, compares hashes).\n\n## Behavioral Contracts\n\n**Diff output format:** `git diff --name-status -M` produces tab-delimited lines where:\n- Add/Modify/Delete: `STATUS\\tFILE` (e.g., `M\\tsrc/foo.ts`)\n- Rename: `R<percentage>\\tOLDPATH\\tNEWPATH` (e.g., `R100\\tsrc/old.ts\\tsrc/new.ts`)\n\n**Rename similarity threshold:** `-M` flag uses git default 50% content similarity for rename detection.\n\n**Status code mapping:**\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n**Uncommitted change sources (when `includeUncommitted: true`):**\n- `status.modified` — modified files not staged for commit\n- `status.deleted` — staged deletions\n- `status.not_added` — untracked files\n- `status.staged` — files added to git index\n\n**Deduplication strategy:** Linear `changes.some(c => c.path === file)` scan before appending uncommitted entries prevents duplicate `FileChange` objects when working tree changes overlap with committed diff.\n\n**SHA-256 output format:** 64-character hex string from `createHash('sha256').update(content).digest('hex')`.\n### cli/\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand-line interface layer parsing `process.argv`, routing to command handlers, managing shared flags (--debug, --trace, --dry-run, --concurrency, --model), and integrating ProgressLog/TraceWriter/AIService across init/discover/generate/update/specify/rebuild/clean workflows.\n\n## Command Entry Points\n\n**[index.ts](./index.ts)** — Main router parsing args via `parseArgs()`, dispatching to command handlers, handling `--version`/`--help` flags, launching interactive installer when invoked with no arguments or installer-specific flags (--runtime, -g, -l).\n\n**[init.ts](./init.ts)** — `initCommand(root, { force? })` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, exits with warning if config exists without `--force`, catches `EACCES`/`EPERM` with `process.exit(1)`.\n\n**[discover.ts](./discover.ts)** — `discoverCommand(targetPath, { tracer?, debug? })` runs file discovery via `discoverFiles()`, writes included/excluded files to console and `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`, emits `discovery:start/end` trace events.\n\n**[generate.ts](./generate.ts)** — `generateCommand(targetPath, GenerateOptions)` orchestrates three-phase pipeline: discovers files → creates GenerationPlan via `createOrchestrator().createPlan()` → resolves AI backend → builds ExecutionPlan → executes via `CommandRunner.executeGenerate()` → finalizes telemetry/trace/progress, exits with codes 0 (success), 1 (partial failure), 2 (total failure/no CLI).\n\n**[update.ts](./update.ts)** — `updateCommand(targetPath, UpdateCommandOptions)` prepares UpdatePlan via hash comparison, cleans orphaned `.sum`/`AGENTS.md` artifacts, analyzes changed files (Phase 1), regenerates `AGENTS.md` for `affectedDirs` (Phase 2), finalizes telemetry, exits with codes 0/1/2.\n\n**[specify.ts](./specify.ts)** — `specifyCommand(targetPath, SpecifyOptions)` synthesizes project spec from `AGENTS.md` corpus via AI, auto-invokes `generateCommand()` if docs missing, supports single/multi-file output (`--multi-file`), enforces 15min timeout and opus model default, exits with code 1 on `SpecExistsError` (conflicts), 2 on `CLI_NOT_FOUND`.\n\n**[rebuild.ts](./rebuild.ts)** — `rebuildCommand(targetPath, RebuildOptions)` reconstructs project from specs via `partitionSpec()` + `executeRebuild()`, enforces 15min timeout and opus default, supports checkpoint-based resumption via `CheckpointManager.load()`, writes generated code to `rebuild/` (or custom `--output`), exits with codes 0/1/2.\n\n**[clean.ts](./clean.ts)** — `cleanCommand(targetPath, { dryRun })` removes `.sum`, `.annex.md`, generated `AGENTS.md` (via `GENERATED_MARKER` detection), `CLAUDE.md`, `GENERATION-PLAN.md`, restores `AGENTS.local.md` → `AGENTS.md`, logs deletion counts with picocolors formatting.\n\n## Shared Option Types\n\n**GenerateOptions** (`generate.ts`, `rebuild.ts` reuses subset):\n```typescript\n{\n  dryRun?: boolean;       // Show plan without AI calls\n  concurrency?: number;   // Worker pool size (1-10)\n  failFast?: boolean;     // Abort on first failure\n  debug?: boolean;        // Verbose subprocess logging\n  trace?: boolean;        // NDJSON trace emission\n  model?: string;         // Override AI model\n}\n```\n\n**UpdateCommandOptions** (`update.ts`):\n```typescript\n{\n  uncommitted?: boolean;  // Include staged+working changes\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**SpecifyOptions** (`specify.ts`):\n```typescript\n{\n  output?: string;        // Custom spec file path (default: specs/SPEC.md)\n  force?: boolean;        // Overwrite existing files\n  dryRun?: boolean;\n  multiFile?: boolean;    // Split into per-directory specs\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (`rebuild.ts`):\n```typescript\n{\n  output?: string;        // Custom output directory (default: rebuild/)\n  force?: boolean;        // Wipe output dir and restart\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (`clean.ts`):\n```typescript\n{\n  dryRun?: boolean;       // Preview deletions without filesystem writes\n}\n```\n\n## Argument Parsing Protocol\n\n`parseArgs(args: string[])` in `index.ts` returns `{ command, positional, flags, values }`:\n- **Long flags**: `--key value` → `values['key'] = 'value'`, `--flag` → `flags.add('flag')`\n- **Short flags**: `-V` → `flags.add('version')`, `-h` → `flags.add('help')`, `-g` → `flags.add('global')`, `-l` → `flags.add('local')`\n- **Command**: First non-flag argument\n- **Positional**: Subsequent non-flag arguments after command\n- **Values map**: `--concurrency 3`, `--output ./spec.md`, `--model sonnet`, `--runtime claude`\n- **Installer detection**: `hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` value\n\n## Model Resolution Strategy\n\n**generate/update**: `options.model ?? config.ai.model` — CLI flag overrides config, no hardcoded default.\n\n**specify/rebuild**: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)` — upgrades sonnet → opus for quality-critical synthesis, respects explicit opus/haiku config.\n\n## Progress Tracking Infrastructure\n\nAll commands (except `init`, `clean`) create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`:\n- **Header format**: `=== ARE <Command> (${ISO-8601}) ===\\nProject: ${absolutePath}\\n...`\n- **Real-time monitoring**: `tail -f .agents-reverse-engineer/progress.log`\n- **Phase boundaries**: `=== Phase 1: File Analysis ===`, `=== Phase 2: Directory AGENTS.md ===`\n- **Task progress**: `[worker-0] Analyzing src/foo.ts (ETA: 2m 15s)`\n- **Summary line**: `Tokens: 12345 in / 6789 out | Duration: 45s | Exit: 0`\n- **Finalization**: `await progressLog.finalize()` before exit\n\n## Trace Integration\n\nWhen `--trace` flag present:\n- `createTraceWriter(absolutePath, true)` creates `.agents-reverse-engineer/traces/trace-<ISO-timestamp>.ndjson` writer\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner()`, `AIService()`\n- Emits events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs`\n- Finalization: `await tracer.finalize()` + `cleanupOldTraces(absolutePath)` keeps 500 most recent traces\n\n## Backend Resolution Flow\n\nAll commands except `init`, `discover`, `clean`:\n1. `createBackendRegistry()` → enumerates installed CLIs (Claude, Gemini, OpenCode)\n2. `resolveBackend(registry, config.ai.backend)` → throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found\n3. Catch block → `getInstallInstructions(registry)` prints installation commands, `process.exit(2)` (distinct from task failure code 1)\n\n## AIService Configuration\n\nInstantiation pattern (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min min for specify/rebuild\n  maxRetries: config.ai.maxRetries,\n  model: effectiveModel,\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }\n});\n\nif (options.debug) {\n  aiService.setDebug(true);\n  console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n  console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n}\n\nif (options.trace) {\n  const logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', timestamp);\n  aiService.setSubprocessLogDir(logDir);\n  console.error(pc.dim(`[debug] Subprocess logs: ${logDir}`));\n}\n```\n\n## Finalization Sequence\n\nStandard cleanup before exit (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nawait aiService.finalize(absolutePath);  // Write run log, enforce retention\nawait progressLog.finalize();            // Close progress.log stream\nawait tracer.finalize();                 // Close trace NDJSON stream\nif (options.trace) {\n  cleanupOldTraces(absolutePath);        // Keep 500 most recent traces\n}\n```\n\n## Exit Code Conventions\n\n- **0**: Success (all tasks completed or no tasks to process)\n- **1**: Partial failure (some tasks succeeded, some failed) OR file conflict (specify/clean) OR first-run detection (update)\n- **2**: Total failure (no tasks succeeded, only failures) OR AI CLI not found\n\n## Dry-Run Behavior\n\n**generate.ts**: Builds `ExecutionPlan` via `buildExecutionPlan()`, logs file/directory/root task counts, returns without AI backend resolution.\n\n**update.ts**: Prepares `UpdatePlan` via `orchestrator.preparePlan({ includeUncommitted, dryRun: true })`, logs changed files with status markers (M=modified, +=added, R=renamed), cleanup actions (deleted `.sum`, empty dir `AGENTS.md`), affected directories, returns without AI calls.\n\n**specify.ts**: Collects `AGENTS.md` + annex files, estimates tokens via `totalChars / 4 / 1000`, logs counts with cyan styling, warns if input exceeds 150K tokens or docs missing, returns without backend resolution.\n\n**rebuild.ts**: Reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads checkpoint via `CheckpointManager.load()`, logs unit count and checkpoint status (completed vs. pending modules), returns without AIService instantiation.\n\n**clean.ts**: Discovers artifacts via `fast-glob` (`.sum`, `.annex.md`, `AGENTS.md`, `AGENTS.local.md`), filters via `GENERATED_MARKER` substring search, logs deletion preview with picocolors formatting (yellow warning \"Dry run — no files were changed\"), returns without `unlink()` calls.\n\n## Error Handling Patterns\n\n**Directory access errors** (all commands):\n```typescript\nawait access(resolvedPath, constants.R_OK);\n// Catches ENOENT → 'Directory not found: ${path}', exit 1\n// Catches EACCES/EPERM → 'Permission denied: ${path}', exit 1\n```\n\n**Config load errors** (all commands except `init`):\n```typescript\nconst config = await loadConfig(absolutePath, { tracer, debug });\n// Throws on invalid YAML or schema validation failure\n// Caught by top-level try/catch in index.ts main()\n```\n\n**Backend resolution errors** (generate/update/specify/rebuild):\n```typescript\ntry {\n  backend = resolveBackend(registry, config.ai.backend);\n} catch (err) {\n  if (err instanceof AIServiceError && err.code === 'CLI_NOT_FOUND') {\n    console.error(pc.red('No AI CLI found...'));\n    console.error(getInstallInstructions(registry));\n    process.exit(2);\n  }\n  throw err;\n}\n```\n\n**File conflict errors** (specify):\n```typescript\ntry {\n  await writeSpec(...);\n} catch (err) {\n  if (err instanceof SpecExistsError) {\n    await progressLog.finalize();\n    console.error(pc.red(err.message));\n    process.exit(1);\n  }\n  throw err;\n}\n```\n\n## Dependencies\n\n**External**: `picocolors` (as `pc`), `node:path`, `node:fs/promises` (`access`, `readFile`, `rename`, `unlink`, `mkdir`, `readdir`), `node:fs` (`constants.F_OK/R_OK`).\n\n**Internal**:\n- **Config**: `src/config/loader.ts` (loadConfig, configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE)\n- **Discovery**: `src/discovery/run.ts` (discoverFiles)\n- **Generation**: `src/generation/orchestrator.ts` (createOrchestrator, GenerationPlan), `src/generation/executor.ts` (buildExecutionPlan, formatExecutionPlanAsMarkdown), `src/generation/collector.ts` (collectAgentsDocs, collectAnnexFiles), `src/generation/writers/agents-md.ts` (writeAgentsMd, GENERATED_MARKER), `src/generation/prompts/index.ts` (buildDirectoryPrompt)\n- **AI**: `src/ai/index.ts` (AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions)\n- **Orchestration**: `src/orchestration/index.ts` (CommandRunner, ProgressLog, ProgressReporter, createTraceWriter, cleanupOldTraces), `src/orchestration/trace.ts` (ITraceWriter)\n- **Update**: `src/update/index.ts` (createUpdateOrchestrator, UpdatePlan)\n- **Specify**: `src/specify/index.ts` (buildSpecPrompt, writeSpec, SpecExistsError)\n- **Rebuild**: `src/rebuild/index.ts` (readSpecFiles, partitionSpec, CheckpointManager, executeRebuild)\n- **Installer**: `src/installer/index.ts` (runInstaller, parseInstallerArgs)\n- **Output**: `src/output/logger.ts` (createLogger)\n- **Version**: `src/version.ts` (getVersion)\n- **Types**: `src/types/index.ts` (DiscoveryResult)\n### config/\n<!-- Generated by agents-reverse-engineer -->\n\n# config\n\nExports YAML-based configuration loading with Zod validation, resource-adaptive concurrency calculation, and default constant definitions for gitignore patterns, vendor directories, binary extensions, and AI service parameters.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Defines ConfigSchema (Zod) with nested ExcludeSchema (patterns/vendorDirs/binaryExtensions arrays), OptionsSchema (followSymlinks boolean, maxFileSize number), OutputSchema (colors boolean), AISchema (backend enum, model string, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Exports Config, ExcludeConfig, OptionsConfig, OutputConfig, AIConfig types. Applies validation constraints: backend restricted to `'claude' | 'gemini' | 'opencode' | 'auto'`, timeoutMs requires positive integer, concurrency clamped to 1-20, maxFileSize requires positive integer, keepRuns requires non-negative integer.\n\n**[loader.ts](./loader.ts)** — Exports loadConfig(root, options?) reading `.agents-reverse-engineer/config.yaml`, parsing YAML, validating via ConfigSchema.parse(), emitting `config:loaded` trace event (configPath/model/concurrency fields), returning validated Config or defaults on ENOENT, wrapping ZodError/parse failures in ConfigError. Exports configExists(root) checking file accessibility via fs.access(). Exports writeDefaultConfig(root) generating commented YAML template with DEFAULT_EXCLUDE_PATTERNS/DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS arrays, yamlScalar() escaping special characters.\n\n**[defaults.ts](./defaults.ts)** — Exports getDefaultConcurrency() computing worker pool size via `clamp(os.availableParallelism() * CONCURRENCY_MULTIPLIER, MIN_CONCURRENCY, min(MAX_CONCURRENCY, memCap))` where memCap = `floor(os.totalmem() * MEMORY_FRACTION / SUBPROCESS_HEAP_GB)` with MEMORY_FRACTION=0.5, SUBPROCESS_HEAP_GB=0.512, CONCURRENCY_MULTIPLIER=5, MIN_CONCURRENCY=2, MAX_CONCURRENCY=20. Exports DEFAULT_VENDOR_DIRS (18 directories: node_modules/vendor/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini), DEFAULT_EXCLUDE_PATTERNS (26 patterns: AGENTS.md/CLAUDE.md/OPENCODE.md/GEMINI.md/lock files/dotfiles/logs/sum files), DEFAULT_BINARY_EXTENSIONS (26 extensions: images/archives/executables/media/documents/fonts/compiled files), DEFAULT_MAX_FILE_SIZE (1MB), DEFAULT_CONFIG (composite object spreading constants).\n\n## Data Flow\n\n1. CLI commands invoke loadConfig(root, { tracer, debug }) from loader.ts\n2. loader.ts reads `.agents-reverse-engineer/config.yaml`, parses YAML, validates via ConfigSchema.parse() from schema.ts\n3. schema.ts applies defaults from defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_EXCLUDE_PATTERNS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency())\n4. On validation failure, loader.ts extracts ZodError.issues[] and wraps in ConfigError with formatted message\n5. On success, loader.ts emits `config:loaded` trace event and returns Config object to CLI orchestrators\n6. writeDefaultConfig() generates YAML template by embedding DEFAULT_ constants and calling yamlScalar() for pattern escaping\n\n## Resource Adaptive Concurrency\n\ngetDefaultConcurrency() prevents RAM exhaustion by computing memory cap: on 4GB systems, `memCap = floor(4 * 0.5 / 0.512) = 3` limits concurrency to 3 workers despite CPU count * 5 formula suggesting higher values. SUBPROCESS_HEAP_GB synchronizes with `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for subprocess resource limiting. Formula `cores * CONCURRENCY_MULTIPLIER` optimized for I/O-bound AI subprocess workloads with high wait-to-compute ratios.\n\n## Error Handling Strategy\n\nloader.ts distinguishes three error cases: ENOENT errors return `ConfigSchema.parse({})` defaults without throwing, ZodError instances map issues[] to multi-line validation report wrapped in ConfigError, existing ConfigError instances re-thrown without modification. ConfigError extends Error adding filePath: string and optional cause: Error properties for stack trace preservation.\n\n## File System Contracts\n\n- Configuration directory: `.agents-reverse-engineer/`\n- Configuration file: `config.yaml` (YAML format with commented sections)\n- Telemetry logs: `.agents-reverse-engineer/logs/` (referenced by AISchema.telemetry.keepRuns)\n- Trace output: `.agents-reverse-engineer/traces/` (referenced by ITraceWriter interface)\n\n## Behavioral Contracts\n\n**YAML Special Character Escaping:**\nyamlScalar() quotes values matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` and escapes `\\\\` → `\\\\\\\\`, `\\\"` → `\\\\\\\"` within quoted strings.\n\n**Default Exclusion Patterns:**\n```typescript\nDEFAULT_EXCLUDE_PATTERNS = [\n  'AGENTS.md', 'CLAUDE.md', 'OPENCODE.md', 'GEMINI.md',\n  '**/AGENTS.md', '**/CLAUDE.md', '**/OPENCODE.md', '**/GEMINI.md',\n  '*.lock', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\n  'bun.lock', 'bun.lockb', 'Gemfile.lock', 'Cargo.lock',\n  'poetry.lock', 'composer.lock', 'go.sum',\n  '.gitignore', '.gitattributes', '.gitkeep', '.env', '**/.env', '**/.env.*',\n  '*.log', '*.sum', '**/*.sum', '**/SKILL.md'\n]\n```\n\n**Vendor Directory Exclusions:**\n```typescript\nDEFAULT_VENDOR_DIRS = [\n  'node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__',\n  '.next', 'venv', '.venv', 'target', '.cargo', '.gradle',\n  '.agents-reverse-engineer', '.agents', '.planning',\n  '.claude', '.opencode', '.gemini'\n]\n```\n\n**Binary Extension Detection:**\n```typescript\nDEFAULT_BINARY_EXTENSIONS = [\n  '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp',\n  '.zip', '.tar', '.gz', '.rar', '.7z',\n  '.exe', '.dll', '.so', '.dylib',\n  '.mp3', '.mp4', '.wav',\n  '.pdf',\n  '.woff', '.woff2', '.ttf', '.eot',\n  '.class', '.pyc'\n]\n```\n\n**Concurrency Formula:**\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n## Cross-Module Dependencies\n\n- **schema.ts** imports defaults.ts (DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency)\n- **loader.ts** imports schema.ts (ConfigSchema, Config type), defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency), `../orchestration/trace.js` (ITraceWriter type)\n### discovery/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery pipeline using fast-glob directory traversal with composable four-stage filter chain (gitignore, vendor, binary, custom) and bounded-concurrency short-circuit evaluation, exposing `discoverFiles()` facade for all CLI commands.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[run.ts](./run.ts)** — `discoverFiles(root, config, options?)` facade composes `walkDirectory()` with four-stage filter chain: `createGitignoreFilter()` parses `.gitignore`, `createVendorFilter()` excludes `node_modules`/`.git`/`dist` via `config.exclude.vendorDirs`, `createBinaryFilter()` applies 96-extension allowlist + `isBinaryFile()` fallback with `config.options.maxFileSize` threshold, `createCustomFilter()` processes user glob patterns from `config.exclude.patterns`. Returns `FilterResult` with `included: string[]` and `excluded: Array<{file, reason, filter}>`. Exports `DiscoveryConfig` interface (structural subset of `Config`) and `DiscoverFilesOptions` (`tracer?: ITraceWriter`, `debug?: boolean`).\n\n**[walker.ts](./walker.ts)** — `walkDirectory(options)` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `suppressErrors: true`, hardcoded `ignore: ['**/.git/**']`, and `followSymbolicLinks: options.followSymlinks ?? false`. Requires `WalkerOptions` with `cwd: string` root, optional `dot?: boolean` (default `true` for dotfiles), `followSymlinks?: boolean` (default `false`). Returns raw `string[]` file paths without filtering logic (deferred to filter chain).\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` (`included[]`, `excluded[]`), `WalkerOptions` (`cwd`, `followSymlinks?`, `dot?`).\n\n## Subdirectory\n\n**[filters/](./filters/)** — Five modules implementing `FileFilter` contract: `gitignore.ts` (`createGitignoreFilter()` async factory with `ignore` library), `binary.ts` (96-extension set + `isBinaryFile()` content analysis), `vendor.ts` (single-segment Set + path-pattern array matching), `custom.ts` (gitignore-style glob parsing), `index.ts` (`applyFilters()` with 30-worker bounded concurrency, short-circuit evaluation, `filter:applied` trace emission).\n\n## Filter Chain Architecture\n\n`discoverFiles()` instantiates filters in priority order (gitignore → vendor → binary → custom), passes to `applyFilters()` which runs filters sequentially per file until `shouldExclude()` returns `true`. Concurrency pool prevents file descriptor exhaustion during `isBinaryFile()` I/O. Filters receive absolute paths; gitignore/custom filters convert to relative via `path.relative()` before pattern matching.\n\n## Configuration Surface\n\n- `config.exclude.vendorDirs: string[]` — directory names like `node_modules` (default 10 entries)\n- `config.exclude.binaryExtensions: string[]` — additional extensions beyond 96-entry `BINARY_EXTENSIONS` set\n- `config.exclude.patterns: string[]` — gitignore-style globs processed by `ignore` library\n- `config.options.maxFileSize: number` — binary detection threshold (default 1MB)\n- `config.options.followSymlinks: boolean` — symlink traversal toggle (default `false`)\n\n## Integration Points\n\nConsumed by `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` via `discoverFiles()` facade. Imports `ITraceWriter` from `../orchestration/trace.js` for `filter:applied` event emission. Filter factories depend on `ignore` library (gitignore parsing), `isbinaryfile` library (content analysis), `fast-glob` (directory traversal), `node:fs` (Stats objects).\n\n## Behavioral Contracts\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS`: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Binary Extension Set\n96 extensions including: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm` (full list in `filters/binary.ts`)\n\n### Concurrency Limit\n`applyFilters()` uses `CONCURRENCY = 30` workers via iterator-based pool pattern from `src/orchestration/pool.ts`\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```\n### generation/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates ARE's three-phase documentation generation pipeline: Phase 1 concurrent file analysis via worker pools generating `.sum` files with SHA-256 hashes, Phase 2 post-order directory aggregation synthesizing `AGENTS.md` from child summaries, Phase 3 sequential root document synthesis producing `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` from complete corpus.**\n\n## Contents\n\n### Pipeline Coordination\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` class exports `createPlan()` method constructing `GenerationPlan` via four-step workflow: `prepareFiles()` loads file content into `PreparedFile[]`, `analyzeComplexity()` computes `directoryDepth` and unique `directories` set, `buildProjectStructure()` formats compact directory tree, `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()`, `createDirectoryTasks()` groups files by directory returning directory-level tasks. Emits trace events (`phase:start`, `plan:created`, `phase:end`) via injected `ITraceWriter`. Clears `PreparedFile.content` after prompt embedding to free heap memory. Returns `{ files, tasks, complexity, projectStructure }`.\n\n**[executor.ts](./executor.ts)** — Transforms `GenerationPlan` into dependency-aware `ExecutionPlan` via `buildExecutionPlan()`: groups files by directory into `directoryFileMap`, creates `fileTasks[]` with `id: 'file:${path}'` and empty `dependencies[]`, sorts by depth descending for leaf-first processing, creates `directoryTasks[]` depending on child file task IDs for post-order traversal, creates `rootTasks[]` depending on all directory task IDs. Exports `isDirectoryComplete()` predicate checking child `.sum` file existence via `sumFileExists()` and `getReadyDirectories()` async filter. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with three-phase checklist grouped by directory depth.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` via `calculateDirectoryDepth()` (max depth via `split(sep).length - 1`) and `extractDirectories()` (unique directories via upward `dirname()` traversal). Returns `{ fileCount, directoryDepth, files, directories }` consumed by orchestrator for concurrency tuning and Phase 2 directory queue construction.\n\n**[collector.ts](./collector.ts)** — Exports `collectAgentsDocs()` recursively walking project tree collecting `AGENTS.md` files as `AgentsDocs` array of `{ relativePath, content }` sorted alphabetically, and `collectAnnexFiles()` similarly collecting `.annex.md` files. Both skip `SKIP_DIRS` set (13 entries: node_modules, .git, vendor, dist, build, etc.) and silently suppress permission errors.\n\n**[types.ts](./types.ts)** — Defines `AnalysisResult` containing `summary: string` and `metadata: SummaryMetadata` returned by Phase 1 AI subprocess calls, `SummaryMetadata` YAML frontmatter schema with `purpose`, `criticalTodos?`, `relatedFiles?` fields, and `SummaryOptions` for summary verbosity configuration.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Template-based prompt construction pipeline: `buildFilePrompt()` injects file path/content/imports into `FILE_USER_PROMPT` with density rules and identifier preservation constraints, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` with manifest detection (9 types) and import maps via `extractDirectoryImports()`, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus with synthesis-only constraints prohibiting invented features. Exports six prompt constants with mustache-style placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`), prohibited filler phrases, YAML frontmatter format, and annex reference format.\n\n**[writers/](./writers/)** — YAML frontmatter-based file I/O layer: `writeSumFile()`/`readSumFile()` implement `.sum` persistence with SHA-256 `content_hash` via regex-based field extraction and dual-format YAML array handling (inline `[a,b,c]` vs multi-line), `writeAgentsMd()` preserves user-authored `AGENTS.md` by renaming to `AGENTS.local.md` and prepending above generated content with `GENERATED_MARKER` injection/stripping, `writeAnnexFile()` archives verbatim source for reproduction-critical files (prompt templates, config schemas). Exports `sumFileExists()` predicate for change detection and `isGeneratedAgentsMd()` marker detection.\n\n## Three-Phase Execution Strategy\n\n**Phase 1: Concurrent File Analysis**\n- Orchestrator creates `fileTasks[]` with prompts via `buildFilePrompt()` embedding import maps and project structure\n- Runner spawns worker pool (`src/orchestration/pool.ts`) executing tasks concurrently (default concurrency: 2 for WSL, 5 elsewhere)\n- Each worker calls `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`--max-old-space-size=512`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`)\n- Worker writes `AnalysisResult` via `writeSumFile()` with YAML frontmatter containing SHA-256 `content_hash` and markdown summary body\n\n**Phase 2: Post-Order Directory Aggregation**\n- Executor sorts `directoryTasks[]` by depth descending (deepest first) via `getDirectoryDepth()` ensuring child directories complete before parents\n- Runner sequentially processes directories checking readiness via `isDirectoryComplete()` predicate polling for child `.sum` file existence\n- Prompt builder calls `buildDirectoryPrompt()` reading child `.sum` files via `readSumFile()`, aggregating subdirectory `AGENTS.md`, extracting imports via `extractDirectoryImports()`, detecting manifests (9 types: package.json, Cargo.toml, go.mod, etc.)\n- Runner writes `AGENTS.md` via `writeAgentsMd()` preserving any `AGENTS.local.md` user content above generated sections\n\n**Phase 3: Sequential Root Synthesis**\n- Executor creates `rootTasks[]` depending on all directory task IDs enforcing sequential execution (concurrency=1)\n- Prompt builder calls `buildRootPrompt()` consuming all `AGENTS.md` files via `collectAgentsDocs()`, reading root `package.json` metadata, embedding synthesis constraints prohibiting invented features\n- Runner writes platform-specific root documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) via `src/integration/generate.ts`\n\n## Post-Order Traversal Mechanism\n\nExecutor sorts directory tasks by depth descending:\n```typescript\ndirectoryTasks.sort((a, b) => \n  getDirectoryDepth(b.path) - getDirectoryDepth(a.path)\n)\n```\n\nwhere `getDirectoryDepth('.')` returns `0`, `getDirectoryDepth('src')` returns `1`, `getDirectoryDepth('src/cli')` returns `2`. Deepest directories process first ensuring child `AGENTS.md` exist before parent aggregation attempts. Runner polls `isDirectoryComplete()` checking all expected `.sum` files exist via `sumFileExists()` before processing directory task.\n\n## Memory Management Pattern\n\nOrchestrator clears `PreparedFile.content` after prompt construction:\n```typescript\nfor (const file of files) {\n  (file as { content: string }).content = ''\n}\n```\n\nThis frees heap memory since file content already embedded in `AnalysisTask.userPrompt` strings. Runner re-reads files from disk during execution if needed. Prevents memory exhaustion on large codebases (10k+ files).\n\n## Integration Points\n\nConsumes:\n- `DiscoveryResult` from `src/discovery/walker.ts` (file list input)\n- `Config` from `src/config/schema.ts` (concurrency, timeout, model settings)\n- `ITraceWriter` from `src/orchestration/trace.ts` (event emission)\n- `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt` from `./prompts/builder.ts`\n- `writeSumFile`, `writeAgentsMd` from `./writers/`\n- `extractDirectoryImports` from `src/imports/extractor.ts`\n- `collectAgentsDocs` from `./collector.ts`\n\nProduces:\n- `GenerationPlan` consumed by `src/orchestration/runner.ts`\n- `ExecutionPlan` consumed by Phase 1/2/3 execution loops\n- `.sum` files consumed by `src/update/orchestrator.ts` for change detection\n- `AGENTS.md` files consumed by Phase 3 root synthesis and `src/specify/index.ts`\n- `GENERATION-PLAN.md` consumed by progress tracking\n\nReferenced by:\n- `src/cli/generate.ts` (command entry point)\n- `src/cli/update.ts` (incremental update workflow)\n- `src/orchestration/runner.ts` (phase execution orchestrator)\n\n## Behavioral Contracts\n\n### Depth Calculation (executor.ts)\n```typescript\ngetDirectoryDepth('.')          → 0\ngetDirectoryDepth('src')        → 1\ngetDirectoryDepth('src/cli')    → 2\ngetDirectoryDepth('a/b/c/d')    → 4\n```\n\n### File Task Dependencies (executor.ts)\n```typescript\nfileTasks.forEach(task => task.dependencies = [])  // No dependencies, all parallel\n```\n\n### Directory Task Dependencies (executor.ts)\n```typescript\ndirectoryTask.dependencies = directoryFileMap[dirPath].map(f => `file:${f}`)\n```\n\n### Root Task Dependencies (executor.ts)\n```typescript\nrootTask.dependencies = directoryTasks.map(t => t.id)  // All directories\n```\n\n### SKIP_DIRS Set (collector.ts)\n```typescript\n['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', \n 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle']\n```\n\n### Manifest Detection Array (prompts/builder.ts)\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n### Language Detection Map (prompts/builder.ts)\n```typescript\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n## Annex References\n\n- Full prompt template text: [prompts/templates.ts.annex.md](./prompts/templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [prompts/types.ts.annex.md](./prompts/types.ts.annex.md)\n- Phase 2/3 execution workflow details: [../orchestration/runner.ts.annex.md](../orchestration/runner.ts.annex.md)\n### imports/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Extracts and classifies TypeScript/JavaScript import statements from source files via regex-based parsing, producing structured import maps with internal/external partitioning for LLM prompt integration during `.sum` generation and directory aggregation phases.**\n\n## Contents\n\n### Core Implementation\n\n**[extractor.ts](./extractor.ts)** — Parses source text via `IMPORT_REGEX` to extract import declarations with `extractImports()` returning `ImportEntry[]` (specifier, symbols, typeOnly), aggregates directory-level imports via `extractDirectoryImports()` reading first 100 lines per file with relative specifier filtering (`.` or `..` prefix) and internal/external classification, formats import maps for LLM prompts via `formatImportMap()` producing structured `fileName:\\n  specifier → symbol1, symbol2 (type)` output.\n\n**[types.ts](./types.ts)** — Defines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` fields representing single import statements, and `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` fields partitioning imports by external (parent directories/npm packages) versus internal (same-directory sibling files) origin.\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` functions and `ImportEntry`, `FileImports` types from implementation modules for single-point import surface consumed by prompt builders.\n\n## Import Extraction Strategy\n\n**Regex Pattern:** `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` with capture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nAnchored with `^` to match only lines starting with `import`, avoiding dynamic imports/comments/string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset for multi-import files.\n\n**Classification Logic:**\n- **Internal:** Specifier starts with `./` (same directory)\n- **External:** Specifier starts with `../` (parent directory)\n- **Excluded:** Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nApplied via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before internal/external partitioning in `extractDirectoryImports()`.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing in `extractDirectoryImports()`, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Silently skips unreadable files via empty catch block.\n\n## Integration Points\n\nConsumed by `../generation/prompts/builder.ts`:\n- `buildFilePrompt()` includes import context from `extractDirectoryImports()` to inform LLM about cross-file dependencies when generating `.sum` summaries\n- `buildDirectoryPrompt()` embeds `formatImportMap()` output in Phase 2 directory aggregation prompts to show module coupling within `AGENTS.md` synthesis\n\nOutput format embeds in prompt templates defined in `../generation/prompts/templates.ts` for structured import relationship documentation.\n\n## Behavioral Contracts\n\n**Import Regex Pattern (IMPORT_REGEX):**\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n**Alias Stripping Pattern:**\n```\n.replace(/\\s+as\\s+\\w+/, '')\n```\n\n**Relative Specifier Filter:**\n```javascript\n.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))\n```\n\n**Import Map Format Template:**\n```\nfileName:\n  specifier → symbol1, symbol2 (type)\n```\n\n**First N Lines Slice:**\n```javascript\ncontent.split('\\n').slice(0, 100).join('\\n')\n```\n### installer/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer/\n\n**npx-driven installer orchestrating IDE command/hook deployment across Claude Code, OpenCode, and Gemini CLI with interactive prompts, platform-specific settings.json registration, detached version-check processes, and recursive empty directory cleanup.**\n\n## Contents\n\n### [banner.ts](./banner.ts)\nASCII art rendering and terminal output styling. `displayBanner()` renders 7-line \"ARE\" logo with version string, `showHelp()` prints usage with cyan-highlighted examples, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` output prefixed symbols (`✓`/`✗`/`!`/`>`), `showNextSteps()` displays numbered command list with `/are-*` shortcuts.\n\n### [index.ts](./index.ts)\nMain entry point orchestrating install/uninstall workflow. `runInstaller()` parses CLI args, displays banner, prompts for missing runtime/location in interactive mode, enforces non-interactive requirements (`--runtime` + `-g/-l` flags mandatory when `!isInteractive()`), dispatches to `runInstall()`/`runUninstall()`, aggregates results. `parseInstallerArgs()` handles short/long flags (`-g`/`--global`, `-h`/`--help`).\n\n### [operations.ts](./operations.ts)\nFile copying, hook registration, permission configuration. `installFilesForRuntime()` writes command templates to `${basePath}/${relativePath}`, copies hooks from bundled `hooks/dist/`, calls `registerClaudeHooks()`/`registerGeminiHooks()` with nested `HookEvent.hooks[]` or flat `GeminiHook[]` schemas, adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` file. `getPackageVersion()` reads package.json via `fileURLToPath(import.meta.url)`.\n\n### [paths.ts](./paths.ts)\nCross-platform path resolution with environment overrides. `getRuntimePaths()` returns `{ global, local, settingsFile }` applying `CLAUDE_CONFIG_DIR`/`OPENCODE_CONFIG_DIR`/`GEMINI_CONFIG_DIR` precedence, `resolveInstallPath()` joins project root with local paths (`.claude`/`.opencode`/`.gemini`), `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` check directory existence via `stat()`.\n\n### [prompts.ts](./prompts.ts)\nInteractive selection with arrow-key navigation or numbered fallback. `arrowKeySelect()` enables raw mode (`process.stdin.setRawMode(true)`), listens for `key.name === 'up'|'down'|'return'`, re-renders with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line). `numberedSelect()` prints 1-indexed list for non-TTY. `cleanupRawMode()` registered on `process.on('exit')` and `SIGINT`.\n\n### [types.ts](./types.ts)\nInterface definitions for installer workflow. `InstallerArgs` with `runtime?: Runtime`, `global/local/uninstall/force/help/quiet: boolean`. `InstallerResult` with `success`, `runtime: Exclude<Runtime, 'all'>`, `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered?`, `versionWritten?`. `RuntimePaths` with `global/local/settingsFile` strings.\n\n### [uninstall.ts](./uninstall.ts)\nArtifact removal and hook deregistration. `uninstallFilesForRuntime()` deletes command templates, hooks/plugins, `ARE-VERSION` file, calls `unregisterClaudeHooks()`/`unregisterGeminiHooks()` filtering `settings.json` by `getHookPatterns()` (current + legacy formats), `unregisterPermissions()` removes `ARE_PERMISSIONS` from Claude `permissions.allow[]`. `cleanupEmptyDirs()` recursively removes directories via `rmdirSync()`. `cleanupLegacyGeminiFiles()` deletes pre-TOML `are-*.md` and old TOML subdirectories.\n\n## Installation Workflow\n\n**Interactive mode (TTY):** `runInstaller()` displays banner → prompts for runtime via `selectRuntime()` (options: claude/opencode/gemini/all) → prompts for location via `selectLocation()` (global: `~/.claude`, local: `./.claude`) → confirms action → dispatches.\n\n**Non-interactive mode (CI):** requires `--runtime <value>` and `-g`/`-l` flags, exits with error via `showError()` + `process.exit(1)` if missing.\n\n**Installation:** `runInstall()` calls `installFiles()` → `installFilesForRuntime()` copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}` (path component after runtime prefix), reads bundled hooks from `hooks/dist/${filename}` via `readBundledHook()`, writes to `${basePath}/hooks/` or `${basePath}/plugins/`, updates `settings.json` with hook/permission entries, writes `ARE-VERSION` from `getPackageVersion()`.\n\n**Uninstallation:** `runUninstall()` calls `uninstallFiles()` → `uninstallFilesForRuntime()` deletes files, unregisters hooks by filtering `settings.json` arrays with `getHookPatterns()` (matches current `node ${runtimeDir}/hooks/${filename}` and legacy `node hooks/${filename}` formats), removes permissions, deletes empty directories via `cleanupEmptyDirs()`, calls `deleteConfigFolder()` for local installs.\n\n## Settings.json Hook Registration\n\n**Claude Code format (nested structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"node ~/.claude/hooks/are-check-update.js\" }\n        ]\n      }\n    ]\n  }\n}\n```\nSchema: `SettingsJson.hooks[event]` contains `HookEvent[]` where each `HookEvent` has `hooks: SessionHook[]` array with `{ type: 'command', command: string }` entries.\n\n**Gemini CLI format (flat structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      { \"name\": \"are-check-update\", \"type\": \"command\", \"command\": \"node ~/.gemini/hooks/are-check-update.js\" }\n    ]\n  }\n}\n```\nSchema: `GeminiSettingsJson.hooks[event]` contains flat `GeminiHook[]` array with `{ name: string, type: 'command', command: string }` entries.\n\n**OpenCode (plugin system):** Copies `ARE_PLUGINS` files (`opencode-are-check-update.js` → `are-check-update.js`) to `${basePath}/plugins/`, sets `hookRegistered = true` without settings.json modification.\n\n## Behavioral Contracts\n\n**Hook definitions (operations.ts):**\n- `ARE_HOOKS: HookDefinition[]` currently empty (commented: \"causing issues\")\n- Intended entries: `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**Plugin definitions (operations.ts):**\n- `ARE_PLUGINS: PluginDefinition[]` with one active entry: `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- Commented: `{ srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }`\n\n**Permission patterns (operations.ts):**\n```javascript\nARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)'\n]\n```\n\n**Bundled hook paths:** `__dirname/../../hooks/dist/${hookName}` where `__dirname` from `fileURLToPath(import.meta.url)`, built via `scripts/build-hooks.js` during `npm run build:hooks`.\n\n**Environment variable precedence:**\n- Claude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- OpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` overrides `~/.config/opencode`\n- Gemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n**ANSI escape sequences (prompts.ts):**\n- `\\x1b[${n}A` — cursor up n lines\n- `\\x1b[1B` — cursor down 1 line\n- `\\x1b[2K` — clear entire line\n\n**Keypress matching (prompts.ts):** `key.name === 'up' | 'down' | 'return'`, interrupt via `key.ctrl && key.name === 'c'`.\n\n## File System Conventions\n\n**Template path extraction:** Remove runtime prefix via `template.path.split('/').slice(1).join('/')`, e.g., `.claude/commands/are/generate.md` → `commands/are/generate.md`.\n\n**Installation paths:**\n- Commands: `${basePath}/${relativePath}`\n- Hooks: `${basePath}/hooks/${filename}` (Claude/Gemini)\n- Plugins: `${basePath}/plugins/${destFilename}` (OpenCode)\n- Settings: `${basePath}/settings.json`\n- Version: `${basePath}/ARE-VERSION`\n\n**Config folder:** `.agents-reverse-engineer` deleted only for local uninstalls via `deleteConfigFolder(location === 'local', dryRun)`.\n\n## Error Handling\n\nFile write failures append to `errors[]` with format `\"Failed to write ${fullPath}: ${err}\"`. JSON parse failures in hook registration silently reset settings to `{}`. Missing bundled hooks throw Error in `readBundledHook()`. Directory cleanup ignores errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` for graceful missing path handling. `InstallerResult.success = errors.length === 0`.\n\n## Dependencies\n\n**Core modules:** `node:os` (homedir), `node:path` (join/dirname), `node:fs` (writeFileSync/readFileSync/unlinkSync/mkdirSync/readdirSync/rmdirSync/rmSync/existsSync), `node:fs/promises` (stat), `node:url` (fileURLToPath).\n\n**External libraries:** `picocolors` (ANSI colors: green/red/yellow/cyan/dim/bold), `readline` (keypress events/interface creation).\n\n**Internal imports:**\n- `../version.js` → getVersion (banner.ts)\n- `../integration/templates.js` → getClaudeTemplates/getOpenCodeTemplates/getGeminiTemplates (operations.ts, uninstall.ts)\n- `./paths.js` → getRuntimePaths/resolveInstallPath/getAllRuntimes (operations.ts, uninstall.ts, index.ts)\n- `./types.js` → Runtime/Location/InstallerArgs/InstallerResult/RuntimePaths (all files)\n- `./banner.js` → display functions (index.ts)\n- `./prompts.js` → selectRuntime/selectLocation/confirmAction/isInteractive (index.ts)\n### integration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific integration file generation for AI coding assistants (Claude Code, OpenCode, Gemini CLI, Aider) via environment detection, template materialization with frontmatter variants, and bundled hook deployment.**\n\n## Contents\n\n**[detect.ts](./detect.ts)** — `detectEnvironments(projectRoot)` scans for `.claude/`, `CLAUDE.md`, `.opencode/`, `.aider.conf.yml`, `.aider/`, `.gemini/` artifacts via `existsSync()` and returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment(projectRoot, type)` predicate tests for specific platform presence.\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles(projectRoot, options)` orchestrates template generation with skip-if-exists behavior (unless `force=true`) and optional `dryRun` preview. Dispatches to platform-specific template getters via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` after `ensureDir()` parent directory creation, copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays.\n\n**[templates.ts](./templates.ts)** — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generate command file arrays via `buildTemplate(platform, commandName, command)` with platform-specific paths (.claude/skills/are-{command}/SKILL.md, .opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml). `COMMANDS` constant defines seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. `PLATFORM_CONFIGS` maps frontmatter schemas (Claude `name:` field, OpenCode `agent: build` directive, Gemini `description`/`prompt` TOML structure). `buildFrontmatter()` handles Markdown variants, `buildGeminiToml()` formats triple-quoted prompts. Placeholder substitution replaces `COMMAND_PREFIX` (`/are-`) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Architecture\n\n### Environment Detection\n\n`detectEnvironments()` applies existence checks via `path.join(projectRoot, relativePath)` → `existsSync()` for platform-specific markers:\n- Claude: `.claude/` directory OR `CLAUDE.md` file → `{ type: 'claude', configDir: '.claude', detected: true }`\n- OpenCode: `.opencode/` directory → `{ type: 'opencode', configDir: '.opencode', detected: true }`\n- Aider: `.aider.conf.yml` file OR `.aider/` directory → `{ type: 'aider', configDir: '.aider', detected: true }`\n- Gemini: `.gemini/` directory → `{ type: 'gemini', configDir: '.gemini', detected: true }`\n\nNo recursive parent directory scanning—only direct children of `projectRoot` tested.\n\n### Template Generation Flow\n\n1. `generateIntegrationFiles()` receives `projectRoot` and optional `GenerateOptions` (`dryRun`, `force`, `environment`)\n2. If `options.environment` specified, constructs single-element array via `configDirMap: Record<EnvironmentType, string>` lookup (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n3. Otherwise calls `detectEnvironments(projectRoot)` to get auto-detected platforms\n4. For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve `IntegrationTemplate[]` from `templates.ts`\n5. Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file via `writeFileSync(fullPath, template.content, 'utf-8')` if not exists or `force=true`\n6. Special case for `claude` environment: after template processing, reads bundled hook via `getBundledHookPath('are-session-end.js')` → `readFileSync(hookPath, 'utf-8')`, writes to `.claude/hooks/are-session-end.js`\n7. Tracks created/skipped paths in `IntegrationResult` per environment\n\n### Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines variants for frontmatter generation and path construction:\n- **Claude**: Nested directory structure (.claude/skills/are-{command}/SKILL.md), frontmatter with `name: /are-{command}`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: Flat file structure (.opencode/commands/are-{command}.md), frontmatter with `agent: build`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: TOML format (.gemini/commands/are-{command}.toml), `description = \"...\"` and `prompt = \"\"\"...\"\"\"` fields, `versionFilePath: '.gemini/ARE-VERSION'`\n- **Aider**: Returns empty array (no templates defined)\n\n`buildFrontmatter(platform, commandName, description)` emits Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n### Command Template Structure\n\n`COMMANDS` constant defines command metadata with `description`, `argumentHint`, `content` fields for seven commands:\n\n- **`generate`**: Three-phase pipeline documentation (Discovery → File Analysis → Directory/Root Documents). Embeds background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, completion summarization covering file counts, failures, inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n- **`update`**: Incremental change detection workflow with `--uncommitted` flag, hash-based comparison, orphan cleanup, affected directory regeneration. Monitoring pattern identical to `generate`.\n- **`discover`**: Enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay.\n- **`clean`**: Mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview.\n- **`specify`**: AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n- **`rebuild`**: Spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation, ===FILE:=== delimited output parsing, exit code semantics (0=success, 1=partial failure, 2=total failure). Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n- **`help`**: Command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n`buildTemplate(platform, commandName, command)` materializes templates via placeholder substitution: `COMMAND_PREFIX` → `/are-`, `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`.\n\n### Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Bundled Hook Deployment\n\n`generateIntegrationFiles()` includes special-case logic for Claude environment: after template generation loop completes, calls `readBundledHook('are-session-end.js')` which resolves path via `getBundledHookPath(hookName)` using `fileURLToPath(import.meta.url)` to navigate from `dist/integration/` up two levels to project root, then into `hooks/dist/`. Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false. Writes hook content to `.claude/hooks/are-session-end.js` via `writeFileSync(fullHookPath, hookContent, 'utf-8')`. Hook files populated in `hooks/dist/` by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Integration with Project\n\nCalled by `src/installer/operations.ts` installer CLI commands to create IDE-specific command/hook files. Consumed by `src/cli/init.ts` initialization workflows requiring platform-specific configuration. Environment detection (`detectEnvironments()`) supports auto-discovery mode when `options.environment` undefined, enabling multi-platform batch generation (`npx agents-reverse-engineer --runtime all`). Single-environment targeting via `options.environment` parameter supports focused installs (`npx agents-reverse-engineer --runtime claude -g`).\n\n## Import Map\n\n- **Local**: `./detect.js` (`detectEnvironments`, `hasEnvironment`), `./templates.js` (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`), `./types.js` (`EnvironmentType`, `DetectedEnvironment`, `IntegrationTemplate`, `IntegrationResult`)\n- **External**: `path` (Node.js), `fs` (`existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`), `url` (`fileURLToPath`), `../installer/paths.js` (referenced in integration context)\n### orchestration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker-pool concurrency control with iterator-based task distribution, progress telemetry via ETA calculation, serialized plan/log/trace writers for concurrent-safe output, and high-level command workflows integrating three-phase AI-driven documentation pipelines.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export aggregating `runPool`, `createTraceWriter`, `cleanupOldTraces`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, plus types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`)\n\n**[pool.ts](./pool.ts)** — `runPool<T>(tasks, options, onComplete?)` iterator-based worker pool sharing single `tasks.entries()` iterator across N workers, returns `Promise<TaskResult<T>[]>` preserving task index, enforces `options.concurrency` cap via `Math.min(options.concurrency, tasks.length)`, supports fail-fast abort via shared `aborted` flag, emits trace events (`worker:start/end`, `task:pickup/done`) with `activeTasks` counter\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates `executeGenerate(plan)` via three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis) with pre/post validation phases (pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks, post-phase-2 phantom path resolution), `executeUpdate(filesToAnalyze)` runs Phase 1 only for incremental workflows, helpers `stripPreamble()` remove LLM conversational prefix via `\\n---\\n` or bold purpose detection, `extractPurpose()` scans lines skipping `PREAMBLE_PREFIXES` (`['now i', 'perfect', 'based on', ...]`)\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams build-log output with format `[X/Y] ANALYZING/DONE/FAIL path` for files, `[dir X/Y] ANALYZING/DONE dirPath/AGENTS.md` for directories, `[root] DONE docPath` for roots, calculates ETA via moving average of last 10 completion times in `completionTimes[]`/`dirCompletionTimes[]` arrays, `printSummary(summary)` outputs end-of-run aggregates; `ProgressLog` mirrors console output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with lazy file handle creation, `stripAnsi(str)` removes color codes via regex `/\\x1b\\[[0-9;]*m/g`\n\n**[trace.ts](./trace.ts)** — `ITraceWriter` interface with `emit(event)`, `finalize()`, `filePath`, `createTraceWriter(projectRoot, enabled)` factory returns `TraceWriter` writing NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` or `NullTraceWriter` no-op stub, `TraceWriter` auto-populates `TraceEventBase` fields (`seq`, `ts`, `pid`, `elapsedMs` via `process.hrtime.bigint()`), serializes via promise-chain pattern, `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces keeping most recent, event types: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via `markDone(itemPath)` replacing `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``, maintains `content: string` in-memory, serializes writes via `writeQueue: Promise<void>` chain pattern, `initialize()` creates parent directory and writes initial markdown, `flush()` drains pending writes before command completion\n\n**[types.ts](./types.ts)** — `FileTaskResult` (path, success, tokensIn/Out, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed/Failed/Skipped, totalCalls, token totals, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc/CodeVsCode, phantomPaths?, inconsistencyReport?), `ProgressEvent` discriminated by type (`start|done|error|dir-done|root-done`), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?, progressLog?)\n\n## Architecture Patterns\n\n### Iterator-Based Worker Pool\n\n`pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag on error, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern. Prevents NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Three-Phase Execution Pipeline\n\n`runner.ts` executes:\n1. **Pre-Phase-1**: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection\n2. **Phase 1**: Concurrent file analysis via `runPool()` calling `aiService.call()` with `buildFilePrompt()`, writes `.sum` with YAML frontmatter (generatedAt, contentHash, purpose), caches source in `sourceContentCache`\n3. **Post-Phase-1**: Quality validation via `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions), `checkCodeVsCode()` for duplicate exports\n4. **Phase 2**: Post-order directory aggregation sorted by depth descending via `sort((a,b) => b-a)`, waits for all child `.sum` files via implicit dependency, calls `aiService.call()` with `buildDirectoryPrompt()`, writes `AGENTS.md` via `writeAgentsMd()` merging `AGENTS.local.md`\n5. **Post-Phase-2**: Phantom path validation via `checkPhantomPaths()` extracting path-like strings with three regex patterns (markdown links, backtick paths, prose-embedded), resolves via `existsSync()` with `.ts`/`.js` fallback\n6. **Phase 3**: Sequential root synthesis (concurrency=1) via `runPool()` calling `aiService.call()` with `buildRootPrompt()` injecting all `AGENTS.md` via `collectAgentsDocs()`, strips conversational preamble via markdown start detection (`indexOf('# ')`), writes to `rootTask.outputPath`\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes[]` and `dirCompletionTimes[]` with max size `windowSize=10`. Records task duration on completion, computes moving average via `reduce((a,b)=>a+b, 0)/length`, multiplies by remaining tasks. Formats via `formatETA()` as `~12s remaining` or `~2m 30s remaining`. Returns empty string if fewer than 2 completions (insufficient sample).\n\n### Trace Event Emission\n\nTracer threaded via `CommandRunOptions.tracer` → pool options → AIService. `TraceWriter` auto-populates base fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (process.pid), `elapsedMs` (high-resolution fractional via `process.hrtime.bigint()` delta). Events: `phase:start` (taskCount, concurrency, phase), `phase:end` (durationMs, tasksCompleted, tasksFailed, phase), `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks), `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode).\n\n## Incremental Update Strategy\n\n`runner.executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code duplicate detection). Returns `RunSummary` with `filesSkipped: 0`. Caller (`src/update/orchestrator.ts`) handles Phase 2 `AGENTS.md` regeneration for `affectedDirs`.\n\n## Quality Validation Phases\n\n**Pre-Phase-1 Cache**: Reads existing `.sum` files into `oldSumCache: Map<string, SumFileContent>` for stale documentation detection in post-phase-1 code-vs-doc checks.\n\n**Post-Phase-1 Code-vs-Doc**: Calls `checkCodeVsDoc()` twice per file — once against `oldSumCache` (detect stale exports), once against freshly written `.sum` (detect omissions). Extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Post-Phase-1 Code-vs-Code**: Groups files by directory via `path.dirname()`, calls `checkCodeVsCode()` aggregating exports per directory into `Map<symbol, string[]>`, detects duplicates.\n\n**Post-Phase-2 Phantom Paths**: Reads each `AGENTS.md`, calls `checkPhantomPaths()` extracting path-like strings via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`.\n\n## Behavioral Contracts\n\n### PlanTracker Checkbox Update Pattern\n```typescript\nmarkDone(itemPath: string) {\n  // Replaces:  - [ ] `${itemPath}`\n  // With:      - [x] `${itemPath}`\n}\n```\n\n### ProgressReporter Output Formats\n```\nFile progress:\n  [X/Y] ANALYZING path\n  [X/Y] DONE path Xs in/out tok model ~Ns remaining\n  [X/Y] FAIL path error\n\nDirectory progress:\n  [dir X/Y] ANALYZING dirPath/AGENTS.md\n  [dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA\n\nRoot progress:\n  [root] DONE docPath\n\nSummary format:\n  === Run Summary ===\n    ARE version:     <version>\n    Files processed: <count>\n    Files failed:    <count>\n    Files skipped:   <count>\n    Total calls:     <count>\n    Tokens:          <totalIn> in / <totalOut> out\n    Cache:           <cacheRead> read / <cacheCreated> created\n    Files read:      <total> (<unique> unique)\n    Total time:      <elapsed>s\n    Errors:          <count>\n    Retries:         <count>\n```\n\n### Preamble Stripping Patterns\n```typescript\nstripPreamble(responseText: string): string {\n  // Pattern 1: Content after \\n---\\n separator within first 500 chars\n  // Pattern 2: Content starting with bold purpose **[A-Z] if preceding text <300 chars and lacks ##\n}\n\nextractPurpose(responseText: string): string {\n  // Skip lines: empty, #headers, ---separators\n  // Skip prefixes (case-insensitive): 'now i', 'perfect', 'based on', 'let me', \n  //   'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'\n  // Strip bold wrapper **, truncate to 120 chars with ... suffix\n}\n```\n\n### TraceEvent Base Field Population\n```typescript\nemit(event: TraceEventPayload): void {\n  // Auto-populate:\n  //   seq: monotonic counter\n  //   ts: new Date().toISOString()\n  //   pid: process.pid\n  //   elapsedMs: Number(process.hrtime.bigint() - startHr) / 1e6\n}\n```\n\n### Pool Worker Task Distribution\n```typescript\nrunPool<T>(tasks, options, onComplete?) {\n  const iterator = tasks.entries(); // Shared across workers\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) => \n    worker(iterator, workerId)\n  );\n  // Each worker pulls [index, task] via for...of over shared iterator\n  // Iterator protocol guarantees atomic .next() calls → no duplicate pickups\n}\n```\n\n## Reproduction-Critical Constants\n\nFull prompt template texts referenced in `runner.ts`:\n- [runner.ts.annex.md](./runner.ts.annex.md)\n### output/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal logging interface providing picocolors-based formatted output for CLI progress reporting, file discovery summaries, warnings, and errors with runtime color toggle support.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (6 methods: `info`, `file`, `excluded`, `summary`, `warn`, `error`), `createLogger(options: LoggerOptions)` factory with conditional color mode (`picocolors` vs identity passthrough), `createSilentLogger()` no-op factory for testing, and `LoggerOptions` configuration interface (`colors: boolean`). Implements format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes.\n\n## Color Mode Abstraction\n\n`ColorFunctions` internal interface wraps `picocolors` methods (`green`, `dim`, `red`, `bold`, `yellow`). Conditional logic `options.colors ? pc : noColor` selects between `picocolors` and `noColor` identity wrapper. `noColor` constant implements all five color methods as `identity: (s: string): string => s` passthrough, enabling compile-time type safety for color-enabled/disabled code paths without runtime string inspection.\n\n## Output Format Specification\n\n- **File discovery**: `c.green('  +') + ' ' + path` (included), `c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`)` (excluded)\n- **Summary**: `c.bold(\\`\\nDiscovered ${included} files\\`) + c.dim(` (${excluded} excluded)`)`\n- **Warnings**: `c.yellow('Warning: ') + message` via `console.warn()`\n- **Errors**: `c.red('Error: ') + message` via `console.error()`\n\n## Integration Points\n\nConsumed by CLI commands (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging (`file()`, `excluded()`, `summary()` calls) and `src/orchestration/progress.ts` for phase execution updates (`info()`, `warn()` calls during pool orchestration).\n### quality/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation suite detecting three classes of documentation drift: code-vs-doc (exported symbols missing from `.sum` files), code-vs-code (duplicate exports within directory scope), and phantom-paths (unresolvable file references in `AGENTS.md`). Aggregates findings into `InconsistencyReport` objects with severity stratification and plain-text CLI formatting.\n\n## Contents\n\n### Barrel Exports\n\n**[index.ts](./index.ts)** — Re-exports `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` from types.ts; `extractExports()`, `checkCodeVsDoc()` from inconsistency/code-vs-doc.js; `checkCodeVsCode()` from inconsistency/code-vs-code.js; `buildInconsistencyReport()`, `formatReportForCli()` from inconsistency/reporter.js; `checkPhantomPaths()` from phantom-paths/index.js; `validateFindability()`, `FindabilityResult` from density/validator.js.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency` (code-vs-doc | code-vs-code | phantom-path) with shared `severity: 'info' | 'warning' | 'error'` field. `CodeDocInconsistency` contains `filePath`, `sumPath`, `details.missingFromDoc[]`. `CodeCodeInconsistency` contains `files[]`, `pattern: 'duplicate-export'`. `PhantomPathInconsistency` contains `agentsMdPath`, `details.referencedPath`, `details.resolvedTo`, `details.context`. `InconsistencyReport` aggregates `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, `summary` counts by type/severity.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — Implements code-vs-doc detection via regex export extraction and substring matching, code-vs-code duplicate symbol detection across per-directory file groups, and report aggregation with plain-text formatting. Exports `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`.\n\n**[phantom-paths/](./phantom-paths/)** — Scans `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location filesystem resolution with `.ts`/`.js` fallback, reports unresolved references as `PhantomPathInconsistency` with 120-char contextLine. Exports `checkPhantomPaths()`.\n\n**[density/](./density/)** — Stub module returning empty `FindabilityResult[]` array, disabled after removal of `SumFileContent.metadata.publicInterface` schema field. Exports `validateFindability()` signature for future symbol extraction support.\n\n## Validation Pipeline\n\n**Code-vs-Doc:** `extractExports()` parses source via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies extracted symbols appear in `SumFileContent.summary` via substring search, returns `CodeDocInconsistency` with `missingFromDoc[]` on detection gaps.\n\n**Code-vs-Code:** `checkCodeVsCode()` aggregates exports into `Map<string, string[]>` across scoped file group, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Requires caller-enforced per-directory scoping to prevent cross-module false positives.\n\n**Phantom Paths:** `checkPhantomPaths()` extracts paths from `AGENTS.md` via three regex patterns, attempts `existsSync()` resolution at agentsMdDir-relative, projectRoot-relative, and `.ts` fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), reports unresolved references with severity='warning'.\n\n**Report Synthesis:** `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with metadata (timestamp, projectRoot, filesChecked, durationMs) and summary counts (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info). `formatReportForCli()` renders plain-text output with `[ERROR]`/`[WARN]`/`[INFO]` severity tags.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` Phase 1 post-processing: constructs `InconsistencyReport` via `buildInconsistencyReport()` after `.sum` generation completes, logs summary counts to `.agents-reverse-engineer/progress.log` via `src/orchestration/progress.ts` streaming reporter, includes report metadata in telemetry run logs via `src/ai/telemetry/run-log.ts`.\n\n**Upstream Inputs:**\n- Source files for `extractExports()` regex parsing\n- `.sum` files via `SumFileContent` schema from `src/generation/writers/sum.ts`\n- `AGENTS.md` files for `checkPhantomPaths()` scanning\n- Export maps from `src/imports/extractor.ts` for `checkCodeVsCode()` aggregation\n\n**Downstream Consumers:**\n- `src/orchestration/progress.ts` logs summary counts via `formatReportForCli()`\n- `src/ai/telemetry/run-log.ts` persists report metadata to NDJSON run logs\n- CLI output displays validation results after Phase 1 completion\n\n## Behavioral Contracts\n\n**Export extraction regex (inconsistency/code-vs-doc.ts):**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path extraction patterns (phantom-paths/validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (phantom-paths/validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n\n**Severity levels:** `'info' | 'warning' | 'error'` mapped to CLI tags `[INFO]`, `[WARN]`, `[ERROR]`.\n\n**Inconsistency type discriminants:** `code-vs-doc`, `code-vs-code`, `phantom-path` enable exhaustive pattern matching via `type` field.\n\n## Detection Limitations\n\n**Regex-based export extraction:** Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n**Substring matching:** `sumText.includes(e)` yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**Code-vs-code name-only comparison:** Cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions without AST context.\n\n**Phantom path resolution:** Four-location strategy (`agentsMdDir`, `projectRoot`, .ts fallback) does not resolve symlinks, aliased imports, or paths requiring module resolution algorithms.\n### rebuild/\n<!-- Generated by agents-reverse-engineer -->\n\n# rebuild\n\nImplements AI-driven project reconstruction from specification documents via ordered phase execution with checkpoint-based resumability, concurrent file generation within dependency groups, and progressive context accumulation from previously built modules.\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — Executes three-stage rebuild workflow: spec partitioning into `RebuildUnit[]` via `partitionSpec()`, sequential order group processing with concurrent AI calls per group via `runPool()`, and progressive context accumulation from generated file exports with LRU truncation at 100k chars.\n\n**[checkpoint.ts](./checkpoint.ts)** — Provides session continuity via `CheckpointManager` class with SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), and promise-chain write serialization to `.rebuild-checkpoint` JSON file.\n\n### Specification Processing\n\n**[spec-reader.ts](./spec-reader.ts)** — Reads markdown files from `specs/` directory via `readSpecFiles()`, partitions content into `RebuildUnit[]` via Build Plan phase extraction (`### Phase N:`) or top-level heading fallback, injects targeted context (Architecture, filtered API subsections, Data Structures, Behavioral Contracts) based on Defines/Consumes keyword matching.\n\n**[output-parser.ts](./output-parser.ts)** — Extracts file paths and contents from AI responses using delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback (```` ```language:path ````), returning `Map<string, string>` for filesystem writes.\n\n### Prompt Engineering\n\n**[prompts.ts](./prompts.ts)** — Defines `REBUILD_SYSTEM_PROMPT` enforcing delimiter format, exact spec compliance (no synonym substitution), and production code constraints (no tests/stubs/placeholders), plus `buildRebuildPrompt()` constructing per-unit prompts with full spec, phase-specific content, and accumulated built context.\n\n### Type System\n\n**[types.ts](./types.ts)** — Exports `RebuildCheckpointSchema` Zod validator with `specHashes`/`modules` fields, `RebuildUnit` interface with `name`/`specContent`/`order`, `RebuildPlan` with `units[]`/`outputDir`, and `RebuildResult` with token counts and `filesWritten[]`.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting all public types, schemas (`RebuildCheckpointSchema`), functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`), classes (`CheckpointManager`), and constants (`REBUILD_SYSTEM_PROMPT`).\n\n## Architecture\n\n### Sequential Order Groups with Concurrent Execution\n\n`executeRebuild()` groups `RebuildUnit[]` by `order` field into `Map<number, RebuildUnit[]>`, processes order values sequentially (ascending), executes units within each group concurrently via `runPool()` with configurable concurrency. Accumulates exported symbols from completed groups into `builtContext` string for injection into subsequent groups as \"Already Built\" context, enabling correct import resolution across phases.\n\n### Checkpoint-Based Resume Workflow\n\n`CheckpointManager.load()` compares SHA-256 hashes of current spec files against `checkpoint.specHashes`, returns `isResume: false` if hash count differs or any individual hash mismatches (drift detected). Filters `pendingUnits[]` via `checkpoint.isDone(unitName)` predicate, increments `modulesSkipped` counter for already-complete work. Workers call `checkpoint.markDone(unitName, filesWritten)` or `checkpoint.markFailed(unitName, errorMsg)` on completion, serializing writes via promise chain to prevent corruption.\n\n### Context Accumulation with LRU Truncation\n\nAfter each order group completes, `orchestrator.ts` reads all `filesWrittenInGroup` via `readFile()`, filters out non-source files (`.md`/`.json`/`.yml`), appends to `builtContext` with `// === ${filePath} ===` delimiters. When `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits on delimiter pattern, preserves recent files in full, truncates older files to first `TRUNCATED_HEAD_LINES` (20 lines) with `// ... (truncated)` marker.\n\n### Targeted API Injection (Change 2 Format)\n\n`extractFromBuildPlan()` detects `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern in phase content, extracts keywords from Defines/Consumes lists and file path references, calls `findRelevantSubsections()` with fuzzy matching (substring, word overlap) to filter API/Data/Behavioral subsections. Injects matched subsections under `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase` headings. Falls back to full `## Public API Surface` inclusion for legacy specs without Defines/Consumes (graceful degradation).\n\n## Behavioral Contracts\n\n**Delimiter format (primary output parsing):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block format (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Build Plan phase extraction:** `/^### Phase (\\d+):\\s*(.+)$/gm`\n\n**Change 2 format detection:** `/^\\*\\*Defines:\\*\\*|^Defines:/m`\n\n**Section heading pattern:** `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m`\n\n**Subsection heading pattern:** `/^### (.+)$/gm`\n\n**File path extraction:** `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`\n\n**Context delimiter format:** `// === ${filePath} ===` (used for splitting truncated context)\n\n## Integration Points\n\nInvoked by `src/cli/rebuild.ts` command handler. Consumes `../ai/index.js` `AIService` for subprocess spawning, `../orchestration/index.js` `runPool`/`ProgressReporter`/`ITraceWriter` for concurrency control, `../change-detection/index.js` `computeContentHashFromString` for drift detection. Writes checkpoint to `<outputDir>/.rebuild-checkpoint` JSON, generated source files to `<outputDir>/<relativePath>` as specified in parsed AI responses.\n### specify/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify\n\n**Project specification synthesis from AGENTS.md documentation corpus: prompt engineering, heading-based file splitting, and overwrite-protected filesystem operations for single-file or multi-file spec generation.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export for specification synthesis API: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions`\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs, annexFiles?)` constructs `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration) and verbatim annex reproduction mandates\n\n**[writer.ts](./writer.ts)** — `writeSpec(content, options)` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings`), throws `SpecExistsError` on conflicts unless `force=true`, returns written paths array\n\n## Architecture\n\n### Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompts by concatenating:\n1. Header directive: `\"Generate a comprehensive project specification...\"`\n2. Documentation sections: `### ${doc.relativePath}` with embedded content from `AgentsDocs` array\n3. Optional annex block: `## Annex Files` with verbatim source code for reproduction-critical modules (prompt templates, IDE configs)\n4. Requirement checklist: 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n5. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim\"`\n6. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\nSystem prompt enforces:\n- **Audience**: AI agents requiring instruction-oriented language (not human documentation)\n- **Anti-patterns**: Prohibits folder-mirroring, file path prescription, directory-derived headings\n- **Module boundary focus**: Describe interfaces/exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for external dependencies\n- **Build Plan**: Phased implementation with explicit \"Defines:\"/\"Consumes:\" interface contract cross-references\n- **Behavioral contracts**: Exact error types/codes, verbatim regex patterns in backticks, format strings with structure, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical sections**: Sections 10-11 reproduce annex content without summarization\n\n### Multi-File Splitting\n\n`splitByHeadings` partitions markdown on `/^(?=# )/m` regex (positive lookahead for top-level headings):\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify` (lowercase + whitespace→hyphens + strip non-alphanumeric + collapse consecutive hyphens + trim)\n- Pre-heading content assigned to `00-preamble.md`\n- Each section object contains `{ filename, content }` with trailing newline\n\nConflict detection: Before writing files, iterates sections to check existence via `fileExists` (wraps `fs.access(..., F_OK)`). Aggregates conflicts and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write (all-or-nothing).\n\n## Integration Points\n\n### Consumed By\n\n`src/cli/specify.ts` command invokes `writeSpec(aiOutput, { outputPath: config.output.specPath, force: cliFlags.force, multiFile: cliFlags.multiFile })` after `AIService.call(buildSpecPrompt(docs, annexFiles))` completes.\n\n### Dependencies\n\n**[../generation/collector.js](../generation/collector.ts)** — `collectAgentsDocs()` provides recursive `AGENTS.md` traversal results via `AgentsDocs` type\n\n**node:fs/promises** — `mkdir`, `writeFile`, `access` for filesystem operations\n\n**node:path** — `dirname`, `join` for path resolution\n\n## Behavioral Contracts\n\n### Runtime Behavior\n\n- `writeSpec` throws `SpecExistsError` when target paths exist and `force=false`\n- Single-file mode creates parent directory recursively via `mkdir(..., { recursive: true })`\n- Multi-file mode writes all sections or none (atomic operation via upfront conflict check)\n\n### Implementation Contracts\n\n**Heading regex**: `/^(?=# )/m` (matches lines starting with `# ` via positive lookahead)\n\n**Heading extraction**: `/^# (.+)/` (captures heading text after `# ` prefix)\n\n**Slugification transforms**:\n- Whitespace→hyphens: `/\\s+/g` → `'-'`\n- Strip non-alphanumeric: `/[^a-z0-9-]/g` → `''`\n- Collapse hyphens: `/-+/g` → `'-'`\n- Trim hyphens: `/^-|-$/g` → `''`\n\n**SpecExistsError message format**: `\"Specification file(s) already exist:\\n\" + paths.map(p => \"  - \" + p).join(\"\\n\") + \"\\n\\nUse --force to overwrite.\"`\n\n**Preamble filename**: `00-preamble.md` (for content before first `# ` heading)\n### types/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types/\n\n**Core TypeScript interface definitions for file discovery results, exclusion metadata, and discovery statistics shared across discovery, change-detection, generation, and quality validation modules.**\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `ExcludedFile`, `DiscoveryStats` interfaces consumed by orchestrators and reporters.\n\n## Exported Interfaces\n\n**ExcludedFile** — Represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** — Encapsulates file discovery output:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics/reporting)\n\n**DiscoveryStats** — Aggregates discovery metrics:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Histogram of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n**Producers:**\n- `discoverFiles()` in `src/discovery/run.ts` returns `DiscoveryResult`\n- Filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) produce `ExcludedFile` instances aggregated into `DiscoveryResult.excluded`\n\n**Consumers:**\n- `generateDocumentation()` in `src/generation/orchestrator.ts` consumes `DiscoveryResult.files` for Phase 1 pool execution\n- `updateDocumentation()` in `src/update/orchestrator.ts` merges `DiscoveryResult` with `FileChange[]` from change detector\n- Progress reporters and telemetry loggers in `src/output/logger.ts` compute `DiscoveryStats` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, written to `GENERATION-PLAN.md` and `.agents-reverse-engineer/progress.log`\n\n## File Relationships\n\n`DiscoveryResult` flows from discovery filters → orchestrator → worker pool → generation writers. `DiscoveryStats` computed post-discovery via reduction over `excluded[]` array for reporting and plan documentation.\n### update/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nIncremental documentation update workflow coordinating SHA-256 hash-based change detection, orphaned artifact cleanup, and selective `.sum` + `AGENTS.md` regeneration for modified files and affected directories.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export exposing UpdateOrchestrator, createUpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, UpdatePlan, UpdateOptions, UpdateResult, UpdateProgress, CleanupResult types.\n\n**[orchestrator.ts](./orchestrator.ts)** — UpdateOrchestrator compares YAML frontmatter `content_hash` fields from existing `.sum` files against SHA-256 hashes of current file content via computeContentHash(), generates UpdatePlan segregating files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), and `cleanup` (orphaned), computes `affectedDirs` requiring `AGENTS.md` regeneration sorted deepest-first. Factory createUpdateOrchestrator() constructs instances with optional ITraceWriter injection.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — cleanupOrphans() deletes `.sum` and `.annex.md` files for deleted/renamed source files (targeting `oldPath` for renames), invokes cleanupEmptyDirectoryDocs() removing `AGENTS.md` from directories with no remaining source files (excludes hidden files, `GENERATED_FILES` set, `.sum`/`.annex.md` from emptiness check). getAffectedDirectories() walks parent directories of non-deleted FileChange entries via path.dirname() to project root.\n\n**[types.ts](./types.ts)** — Defines CleanupResult (`deletedSumFiles[]`, `deletedAgentsMd[]`), UpdateOptions (`includeUncommitted`, `dryRun` flags), UpdateResult (execution summary with `analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`), UpdateProgress (event callbacks: `onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Workflow\n\n1. **Prerequisites** — UpdateOrchestrator.checkPrerequisites() validates git repository via isGitRepo(), throws Error if not a git repo\n2. **Plan Creation** — preparePlan() calls discoverFiles(), iterates results:\n   - Constructs `.sum` path via getSumPath()\n   - Calls readSumFile() extracting `contentHash` from frontmatter\n   - Missing `.sum`: adds `{ path, status: 'added' }` to `filesToAnalyze`\n   - Existing `.sum`: calls computeContentHash(), compares hashes\n   - Hash mismatch: adds `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: adds path to `filesToSkip`\n3. **Orphan Cleanup** — cleanupOrphans() deletes `.sum`/`.annex.md` for deleted/renamed files, cleanupEmptyDirectoryDocs() removes `AGENTS.md` from directories with no source files\n4. **Affected Directories** — getAffectedDirectories() computes parent paths of changed files, sorts by depth descending (deepest-first processing)\n5. **Regeneration** — External caller (src/cli/update.ts) invokes Phase 1 pool execution for `filesToAnalyze`, Phase 2 sequential directory aggregation for `affectedDirs` (skips Phase 3 root synthesis)\n\n## Integration\n\n**Upstream callers**: src/cli/update.ts constructs UpdateOrchestrator via createUpdateOrchestrator(), calls preparePlan() returning UpdatePlan, passes `filesToAnalyze` to src/generation/executor.ts Phase 1 file analysis pool, passes `affectedDirs` to Phase 2 directory aggregation.\n\n**Dependencies**: src/change-detection/detector.ts (computeContentHash, isGitRepo, getCurrentCommit), src/generation/writers/sum.ts (readSumFile, getSumPath), src/discovery/run.ts (discoverFiles), src/orchestration/trace.ts (ITraceWriter).\n\n## Trace Events\n\nUpdateOrchestrator emits via optional ITraceWriter:\n\n- `phase:start` at plan creation with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after completion with `durationMs` computed via process.hrtime.bigint() delta\n\n## Frontmatter Mode\n\nNo-op methods (recordFileAnalyzed, removeFileState, recordRun, getLastRun) retain signatures for API compatibility with hypothetical database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by writeSumFile(). Run history tracking unavailable (getLastRun() returns undefined, recordRun() returns 0).\n\n## Dry-Run Support\n\nWhen `UpdateOptions.dryRun === true`, orphan-cleaner skips unlink() calls but executes stat() checks, returning would-be deletions in CleanupResult. Used for preview mode in src/cli/update.ts.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Root source directory implementing ARE's three-phase documentation generation pipeline via TypeScript modules organized into nine functional subsystems: AI service orchestration (subprocess pooling, retry logic, telemetry), file discovery (gitignore-aware filtering, binary detection), change detection (git diff parsing, SHA-256 hashing), generation (concurrent file analysis, post-order directory aggregation, root synthesis), quality validation (code-vs-doc, phantom paths), CLI commands (generate, update, specify, rebuild, clean), configuration (YAML schema, resource-adaptive concurrency), IDE integration (template generation for Claude/OpenCode/Gemini), and installer (npx-driven hook deployment).**\n\n## Contents\n\n**[version.ts](./version.ts)** — Exports `getVersion()` extracting package version from `../package.json` via `fileURLToPath(import.meta.url)` + `dirname()` + `join(__dirname, '..', 'package.json')`, returns `'unknown'` string literal on read/parse failure. Consumed by CLI `--version` flag, session lifecycle hooks for npm version comparison, and telemetry run log metadata.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI CLI orchestration layer with subprocess pooling (10MB maxBuffer, SIGTERM/SIGKILL timeout enforcement, process group termination via `kill(-pid)`), exponential backoff retry detecting rate limits via stderr patterns (`[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`), token cost telemetry accumulating `TelemetryEntry[]` with NDJSON persistence to `.agents-reverse-engineer/logs/run-<timestamp>.json`, and trace emission (`subprocess:spawn/exit`, `retry` events). Backends: ClaudeBackend (Zod-validated JSON parsing), GeminiBackend/OpenCodeBackend (stubs throwing `SUBPROCESS_ERROR`). Resource limits: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`.\n\n**[change-detection/](./change-detection/)** — Git-based delta computation via `git diff --name-status -M` with tab-split parsing extracting status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, SHA-256 content hashing via `createHash('sha256').update(content).digest('hex')`, uncommitted change merging from `git.status()` with linear `Array.some()` deduplication. Exports `getChangedFiles()` returning `ChangeDetectionResult` with `changes[]` discriminated union (`FileChange` with optional `oldPath` for renames).\n\n**[cli/](./cli/)** — Command entry points parsing `process.argv` via `parseArgs()`, routing to `init`/`discover`/`generate`/`update`/`specify`/`rebuild`/`clean` handlers, managing shared flags (`--debug`, `--trace`, `--dry-run`, `--concurrency`, `--model`). Integrates `ProgressLog` streaming to `.agents-reverse-engineer/progress.log`, `TraceWriter` emitting NDJSON to `traces/`, `AIService` backend resolution with `CLI_NOT_FOUND` exit code 2. Model resolution: CLI overrides config, specify/rebuild upgrade sonnet → opus for quality-critical synthesis.\n\n**[config/](./config/)** — YAML configuration loading via `loadConfig()` reading `.agents-reverse-engineer/config.yaml`, Zod validation with `ConfigSchema` applying defaults from `defaults.ts` (18-entry `DEFAULT_VENDOR_DIRS`, 26-entry `DEFAULT_EXCLUDE_PATTERNS`, 26-entry `DEFAULT_BINARY_EXTENSIONS`). Resource-adaptive concurrency formula: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` preventing RAM exhaustion on low-memory systems. Environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery via `discoverFiles()` facade composing `walkDirectory()` (fast-glob with `absolute: true`, `onlyFiles: true`, hardcoded `ignore: ['**/.git/**']`) and four-stage filter chain: `createGitignoreFilter()` (parses `.gitignore` via `ignore` library), `createVendorFilter()` (excludes 10 default directories via Set + path-pattern matching), `createBinaryFilter()` (96-extension allowlist + `isBinaryFile()` content analysis at 1MB threshold), `createCustomFilter()` (user glob patterns). Returns `FilterResult` with `included[]` and `excluded[]` arrays, latter containing `{ file, reason, filter }` objects.\n\n**[generation/](./generation/)** — Orchestrates three-phase pipeline: `GenerationOrchestrator.createPlan()` builds `AnalysisTask[]` via `buildFilePrompt()` with import maps and project structure, clears `PreparedFile.content` after prompt embedding to free heap. `buildExecutionPlan()` constructs dependency-aware `ExecutionPlan` with post-order directory sorting (deepest-first via `getDirectoryDepth()`), `isDirectoryComplete()` predicate polling child `.sum` existence. Phase 1: concurrent file analysis writes `.sum` with SHA-256 `content_hash` frontmatter. Phase 2: directory aggregation via `buildDirectoryPrompt()` reading child `.sum` + manifest detection (9 types) + import maps, writes `AGENTS.md` via `writeAgentsMd()` preserving `AGENTS.local.md` user content. Phase 3: sequential root synthesis via `buildRootPrompt()` consuming all `AGENTS.md`, writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n**[imports/](./imports/)** — Extracts TypeScript/JavaScript import statements via `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`, classifies as internal (`.` prefix) or external (`..` prefix), formats as structured import maps via `formatImportMap()` producing `fileName:\\n  specifier → symbol1, symbol2 (type)` output. Reads first 100 lines per file assuming imports clustered at top, consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` for LLM prompt context.\n\n**[installer/](./installer/)** — npx-driven installer orchestrating command/hook deployment via interactive prompts (`arrowKeySelect()` with raw mode `\\x1b[${n}A` ANSI sequences) or non-interactive mode (requires `--runtime` + `-g/-l` flags). Copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}`, registers hooks with nested `HookEvent.hooks[]` schema (Claude) or flat `GeminiHook[]` (Gemini), adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` from `getPackageVersion()`. Uninstalls via `unregisterClaudeHooks()` filtering by `getHookPatterns()` (matches current + legacy formats), `cleanupEmptyDirs()` recursively.\n\n**[integration/](./integration/)** — Platform-specific integration file generation via `generateIntegrationFiles()` detecting `.claude/`, `.opencode/`, `.aider.conf.yml`, `.gemini/` artifacts. Templates from `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` with frontmatter variants (Claude `name:` field, OpenCode `agent: build`, Gemini TOML `description`/`prompt`). Background execution pattern: Read `VERSION_FILE_PATH`, run `npx agents-reverse-engineer@latest {command}` with `run_in_background: true`, poll `.agents-reverse-engineer/progress.log` with `offset` parameter, check `TaskOutput` with `block: false`. Copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`.\n\n**[orchestration/](./orchestration/)** — Worker pool via `runPool()` sharing single `tasks.entries()` iterator across N workers (atomic `.next()` calls prevent duplicate pickups), fail-fast via shared `aborted` flag. `CommandRunner.executeGenerate()` runs three-phase pipeline with quality validation: pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks via regex export extraction and substring matching, post-phase-2 phantom path resolution via three regex patterns. `ProgressReporter` streams `[X/Y] ANALYZING/DONE/FAIL path` to console + `.agents-reverse-engineer/progress.log`, calculates ETA via moving average of last 10 task durations. `TraceWriter` emits NDJSON with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields, serializes via promise-chain pattern. `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via regex `/- \\[ \\] \\`${itemPath}\\`/` → `/- \\[x\\] \\`${itemPath}\\`/`.\n\n**[output/](./output/)** — Terminal logging via `createLogger()` with conditional color mode (picocolors vs identity passthrough). Format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes. `stripAnsi(str)` removes color codes via `/\\x1b\\[[0-9;]*m/g` for progress.log persistence.\n\n**[quality/](./quality/)** — Post-generation validation detecting code-vs-doc (exported symbols missing from `.sum`), code-vs-code (duplicate exports within directory), phantom-paths (unresolvable file references in `AGENTS.md`). `extractExports()` via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies substring presence. `checkPhantomPaths()` extracts paths via three regex patterns (markdown links, backtick paths, prose-embedded src/), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax). `buildInconsistencyReport()` aggregates `Inconsistency[]` discriminated union into `InconsistencyReport` with severity stratification (`info`/`warning`/`error`).\n\n**[rebuild/](./rebuild/)** — AI-driven project reconstruction from specification documents via `executeRebuild()` executing ordered phase workflow: `partitionSpec()` extracts `RebuildUnit[]` from Build Plan phases (`### Phase N:`), sequential order group processing with concurrent `runPool()` execution per group, progressive context accumulation from generated file exports with LRU truncation at 100k chars. `CheckpointManager` provides session continuity via SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), promise-chain write serialization to `.rebuild-checkpoint` JSON. `parseModuleOutput()` extracts file paths via delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback. Targeted API injection via Defines/Consumes keyword matching filtering subsections with fuzzy matching (substring, word overlap).\n\n**[specify/](./specify/)** — Project specification synthesis via `buildSpecPrompt()` constructing `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration). `writeSpec()` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings` partitioning on `/^(?=# )/m` with slugification transforms), throws `SpecExistsError` on conflicts unless `force=true`.\n\n**[types/](./types/)** — Core TypeScript interfaces for file discovery results: `DiscoveryResult` (`files[]`, `excluded[]`), `ExcludedFile` (`path`, `reason`), `DiscoveryStats` (`totalFiles`, `includedFiles`, `excludedFiles`, `exclusionReasons` histogram). Flows from discovery filters → orchestrator → worker pool → generation writers.\n\n**[update/](./update/)** — Incremental documentation update via `UpdateOrchestrator.preparePlan()` comparing YAML frontmatter `content_hash` against SHA-256 hashes of current content via `computeContentHash()`, segregating into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files (targeting `oldPath` for renames), `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to project root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Architecture\n\n### Three-Phase Pipeline Coordination\n\nPhase 1 (`generation/orchestrator.ts`): `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()` embedding import maps from `imports/extractor.ts`, clears `PreparedFile.content` after prompt construction to free heap. `runPool()` spawns N workers sharing `tasks.entries()` iterator (atomic `.next()` calls), each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits, writes `.sum` with YAML frontmatter (`generated_at`, `content_hash` SHA-256 hex, `purpose`, `critical_todos`, `related_files`).\n\nPhase 2 (`generation/executor.ts`): `buildExecutionPlan()` sorts directories by depth descending via `getDirectoryDepth()` (deepest-first), creates `directoryTasks[]` depending on child file task IDs for post-order traversal. `isDirectoryComplete()` predicate polls child `.sum` existence before processing. `buildDirectoryPrompt()` reads child `.sum` via `readSumFile()`, aggregates subdirectory `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects manifests (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). `writeAgentsMd()` preserves `AGENTS.local.md` user content above generated sections with `GENERATED_MARKER` injection.\n\nPhase 3 (`generation/orchestrator.ts`): Sequential execution (concurrency=1) via `rootTasks[]` depending on all directory task IDs. `buildRootPrompt()` consumes all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json`, enforces synthesis-only constraints (no invented features). Strips conversational preamble via markdown start detection (`indexOf('# ')`), writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n### Iterator-Based Worker Pool\n\n`orchestration/pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern preventing NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Quality Validation Phases\n\nPre-Phase-1: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection.\n\nPost-Phase-1: `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions) via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, substring matching in summary. `checkCodeVsCode()` aggregates exports into `Map<symbol, string[]>` per directory, detects duplicates.\n\nPost-Phase-2: `checkPhantomPaths()` extracts paths via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS`.\n\n### Incremental Update Strategy\n\n`update/orchestrator.ts` workflow: `preparePlan()` calls `discoverFiles()`, iterates results constructing `.sum` path via `getSumPath()`, calls `readSumFile()` extracting `contentHash` from frontmatter, missing `.sum` adds `{ path, status: 'added' }` to `filesToAnalyze`, existing `.sum` calls `computeContentHash()` comparing hashes, mismatch adds `{ path, status: 'modified' }`, match adds to `filesToSkip`. `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Behavioral Contracts\n\n### Default Concurrency Formula (config/defaults.ts)\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n### Subprocess Resource Limits (ai/subprocess.ts)\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts (ai/subprocess.ts)\n- SIGTERM sent at `timeoutMs` via `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n- Process group termination: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Patterns (ai/service.ts)\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula (ai/retry.ts)\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n### Default Retry Configuration (ai/retry.ts)\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Git Diff Format (change-detection/detector.ts)\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n### Binary Extension Set (discovery/filters/binary.ts)\n96 extensions including `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Extraction Patterns (quality/phantom-paths/validator.ts)\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n### Import Regex Pattern (imports/extractor.ts)\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Build Plan Phase Extraction (rebuild/spec-reader.ts)\n```regex\n/^### Phase (\\d+):\\s*(.+)$/gm\n```\n\n### Delimiter Format (rebuild/output-parser.ts)\n```regex\n/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g\n```\n\n### Heading Regex (specify/writer.ts)\n```regex\n/^(?=# )/m\n```\n\n### Slugification Transforms (specify/writer.ts)\n```typescript\n.toLowerCase()\n.replace(/\\s+/g, '-')\n.replace(/[^a-z0-9-]/g, '')\n.replace(/-+/g, '-')\n.replace(/^-|-$/g, '')\n```\n\n## Dependencies\n\n**External**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (change detection), `yaml` (config parsing), `zod` (schema validation), `ora` (spinner UI), `picocolors` (terminal colors).\n\n**Internal**: `node:fs/promises` (async file I/O), `node:fs` (sync file I/O), `node:path` (path manipulation), `node:url` (ES module URL resolution), `node:child_process` (subprocess spawning), `node:os` (system metrics), `node:crypto` (SHA-256 hashing), `node:readline` (interactive prompts).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 5884,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 41676,
      "latencyMs": 144779,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:35:51.487Z",
      "prompt": "Generate AGENTS.md for directory: \".\" (agents-reverse-engineer)\n\n## File Summaries (5 files)\n\n### LANGUAGES-MANIFEST.md\n**Purpose:** LANGUAGES-MANIFEST.md catalogues package manifest file patterns across 25 programming languages with support status i...\n\n**LANGUAGES-MANIFEST.md catalogues package manifest file patterns across 25 programming languages with support status indicators for the ARE directory aggregation phase.**\n\n## Reference Table Structure\n\nThe markdown table contains four columns: `Language`, `Manifest File(s)`, `Package Manager`, and `Supported`. Each row maps a language ecosystem to its dependency declaration files. The `Supported` column contains `✓` marks for languages currently handled by `detectManifestFiles()` in the directory aggregation phase (Phase 2).\n\n## Supported Manifest Detection\n\nLanguages marked with `✓` in the `Supported` column correspond to patterns in `src/generation/prompts/builder.ts`:\n- **JavaScript/TypeScript**: `package.json` detected via npm/yarn/pnpm\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` detected via pip/poetry/pipenv\n- **Go**: `go.mod` detected via go modules\n- **Rust**: `Cargo.toml` detected via cargo\n\nAdditional detected patterns without explicit `✓` marks:\n- **Java**: `pom.xml` (Maven), `build.gradle` (Gradle)\n- **PHP**: `composer.json` (Composer)\n- **C/C++**: `CMakeLists.txt`, `Makefile` (CMake, make)\n\nThe `detectManifestFiles()` function searches for exactly nine manifest types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, and `CMakeLists.txt`/`Makefile`. Discrepancies exist between the `✓` marks in this table and actual detection logic (e.g., Ruby `Gemfile` is detected but unmarked; Python `setup.py`/`Pipfile` are marked but not in the detection array).\n\n## Unsupported Languages\n\nLanguages without `✓` marks have documented manifest patterns but no active detection in the codebase:\n- **Ruby**: `Gemfile` (bundler) — detection exists but unmarked\n- **Kotlin**: `build.gradle.kts`, `build.gradle` (Gradle)\n- **C#/.NET**: `*.csproj`, `packages.config`, `*.fsproj` (NuGet)\n- **Swift**: `Package.swift` (Swift PM)\n- **Elixir**: `mix.exs` (Mix)\n- **Erlang**: `rebar.config` (rebar3)\n- **Scala**: `build.sbt` (sbt)\n- **Clojure**: `deps.edn`, `project.clj` (clj/Leiningen)\n- **Haskell**: `package.yaml`, `*.cabal`, `stack.yaml` (cabal/stack)\n- **Dart/Flutter**: `pubspec.yaml` (pub)\n- **Lua**: `*.rockspec` (LuaRocks)\n- **R**: `DESCRIPTION` (CRAN)\n- **Julia**: `Project.toml` (Pkg)\n- **Zig**: `build.zig.zon` (zig)\n- **Nim**: `*.nimble` (nimble)\n- **OCaml**: `dune-project`, `*.opam` (dune/opam)\n- **C/C++**: `conanfile.txt`, `vcpkg.json` (Conan/vcpkg) — `CMakeLists.txt` is detected\n\n## Integration with Directory Aggregation\n\nDuring Phase 2 directory aggregation (`buildDirectoryPrompt()` in `src/generation/prompts/builder.ts`), `detectManifestFiles()` scans the directory for manifest patterns and includes detected files in the AI prompt context. The prompt instructs the AI to recognize these files as dependency declarations and incorporate package information into the `AGENTS.md` directory summary.\n\nThis reference document serves as the canonical list for future manifest detection expansion. Adding support for additional languages requires:\n1. Updating the `COMMON_MANIFEST_FILES` array in `src/generation/prompts/builder.ts`\n2. Marking the language row with `✓` in this table\n### LICENSE\n**Purpose:** LICENSE defines MIT License terms granting unrestricted usage, modification, and distribution rights for agents-rever...\n\n**LICENSE defines MIT License terms granting unrestricted usage, modification, and distribution rights for agents-reverse-engineer project with liability disclaimer.**\n\n## License Terms\n\nGrants permissions without restriction:\n- Use, copy, modify, merge, publish, distribute, sublicense, sell\n- Applies to \"Software\" (this codebase and associated documentation files)\n- Requires copyright notice and permission notice inclusion in all copies or substantial portions\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT\n\n## Liability and Warranty\n\nSoftware provided \"AS IS\" without warranty of any kind (express or implied). No warranties for MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, or NONINFRINGEMENT. Authors not liable for any claims, damages, or liabilities arising from the software.\n\n## Integration Context\n\nReferenced in `package.json` via `\"license\": \"MIT\"` field. Governs distribution via npm registry (publish workflow in `.github/workflows/publish.yml`). No programmatic parsing required—human-readable legal text only.\n### README.md\n**Purpose:** README.md serves as the primary user-facing documentation entry point, providing installation workflows, command refe...\n\n**README.md serves as the primary user-facing documentation entry point, providing installation workflows, command reference, configuration schema, and generated documentation structure for the agents-reverse-engineer CLI tool.**\n\n## Installation Patterns\n\nInteractive installer invoked via `npx agents-reverse-engineer@latest` prompts for runtime selection (`claude`, `opencode`, `gemini`, `all`) and location (`-g` global or `-l` local). Non-interactive mode uses `--runtime <rt>` with `-g`/`-l` flags. Uninstallation via `npx agents-reverse-engineer@latest uninstall` removes command files, session hooks, ARE permissions from `settings.json`, and `.agents-reverse-engineer` folder (local installs only). Version check via `--version` flag.\n\n## Command Interface\n\nCLI commands: `are install` (interactive or flagged), `are init` (creates `.agents-reverse-engineer/config.yaml`), `are discover` (scans files with `--plan` to generate `GENERATION-PLAN.md`, `--show-excluded` to display exclusion reasons), `are generate` (three-phase documentation generation), `are update` (incremental regeneration), `are specify` (synthesizes `specs/SPEC.md` with `--multi-file` split option and `--dry-run` preview), `are clean` (removes generated artifacts).\n\nAI assistant skill commands: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` available in Claude Code, OpenCode, and Gemini CLI runtimes.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` structure:\n- `exclude.patterns` (custom glob patterns like `[\"*.log\", \"temp/**\"]`)\n- `exclude.vendorDirs` (third-party directories: `node_modules`, `dist`, `.git`)\n- `exclude.binaryExtensions` (file types: `.png`, `.jpg`, `.pdf`)\n- `options.followSymlinks` (boolean, default `false`)\n- `options.maxFileSize` (bytes, default `1048576` for 1MB)\n- `output.colors` (boolean terminal ANSI flag)\n- `output.verbose` (boolean per-file logging)\n- `ai.backend` (enum: `'claude'`, `'gemini'`, `'opencode'`, `'auto'`)\n- `ai.model` (string, backend-specific identifier like `sonnet`)\n- `ai.timeoutMs` (number, default `300000` for 5-minute subprocess limit)\n- `ai.maxRetries` (number, default `3` for exponential backoff)\n- `ai.concurrency` (number 1-10, default `5`, reduced to `2` for WSL/resource-constrained environments)\n- `ai.telemetry.keepRuns` (number, default `50` log retention count)\n- `ai.telemetry.costThresholdUsd` (number, optional USD warning threshold)\n- `ai.pricing.claude-opus-4.inputCostPerMTok` (USD per 1M input tokens, default `15.0`)\n- `ai.pricing.claude-opus-4.outputCostPerMTok` (USD per 1M output tokens, default `75.0`)\n\n## Generated Documentation Formats\n\n`.sum` file structure includes YAML frontmatter with `file_type`, `generated_at` timestamp, followed by markdown sections: `## Purpose` (single-line role statement), `## Public Interface` (exported functions/classes with signatures), `## Dependencies` (library usage with purpose), `## Implementation Notes` (critical runtime behaviors).\n\n`AGENTS.md` per-directory format includes directory role description, files grouped by purpose (Types, Services, Utils), subdirectories with brief descriptions.\n\nRoot documents: `CLAUDE.md` (auto-loaded Claude Code entry point), `GEMINI.md` (Gemini CLI entry point), `OPENCODE.md` (OpenCode entry point), `AGENTS.md` (universal format root directory overview).\n\n## Workflow Sequence\n\nInstallation workflow: run `npx agents-reverse-engineer@latest` → select runtime(s) → select location → installs commands and session hooks. Documentation generation workflow: run `/are-init` → `/are-discover` (creates `GENERATION-PLAN.md` with post-order traversal plan) → `/are-generate` (Phase 1: file analysis creates `.sum` files, Phase 2: directory docs creates `AGENTS.md`, Phase 3: root docs creates `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) → `/are-update` (incremental regeneration for changed files only). Project specification synthesis via `/are-specify` aggregates all `AGENTS.md` into `specs/SPEC.md`.\n\n## Requirements\n\nNode.js 18+ runtime required. Supported AI assistants: Claude Code (full support with session hooks), Gemini CLI (full support with session hooks), OpenCode (AGENTS.md format supported), any assistant supporting `AGENTS.md` format.\n\n## Badge Links\n\n`[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)` links to npm package page. `[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)` links to LICENSE file.\n\n## Concurrency Tuning\n\n`ai.concurrency` range 1-10 controls parallel AI subprocess count. Default `5` (changed to `2` in WSL environments per v0.4.8+ resource management constraints). Lower values recommended for resource-constrained environments (reduces memory pressure), higher values accelerate generation (increase throughput). Timeout `ai.timeoutMs` default `300000` (5 minutes) per file analysis subprocess, increase for large files or slow network connections.\n### package.json\n**Purpose:** package.json defines npm package metadata, dependencies, build scripts, and distribution configuration for the agents...\n\n**package.json defines npm package metadata, dependencies, build scripts, and distribution configuration for the agents-reverse-engineer CLI tool.**\n\n## Package Identity\n\n**name**: `\"agents-reverse-engineer\"`  \n**version**: `\"0.7.1\"`  \n**description**: `\"CLI tool for reverse-engineering codebase documentation for AI agents\"`  \n**author**: `\"GeoloeG-IsT\"`  \n**license**: `\"MIT\"`  \n**type**: `\"module\"` (ES module package)\n\n## Binary Entry Points\n\nExports two CLI binaries mapping to `dist/cli/index.js`:\n- `agents-reverse-engineer` — full command name\n- `are` — short alias\n\n## Runtime Requirements\n\n**engines.node**: `\">=18.0.0\"` (minimum Node.js version)\n\n## Build Scripts\n\n**build**: `\"tsc\"` — compiles TypeScript source tree to `dist/` directory  \n**build:hooks**: `\"node scripts/build-hooks.js\"` — copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion  \n**prepack**: `\"rm -f LICENSE.sum README.md.sum\"` — removes documentation artifacts before tarball creation  \n**prepublishOnly**: `\"npm run build && npm run build:hooks\"` — sequential compilation before npm publish  \n**dev**: `\"tsx watch src/cli/index.ts\"` — hot-reload development mode via tsx watch\n\n## Production Dependencies\n\n**fast-glob** (`^3.3.3`) — file discovery with glob patterns  \n**ignore** (`^7.0.3`) — gitignore parsing  \n**isbinaryfile** (`^5.0.4`) — binary file detection  \n**ora** (`^8.1.1`) — terminal spinner UI  \n**picocolors** (`^1.1.1`) — ANSI color formatting  \n**simple-git** (`^3.27.0`) — git diff parsing for change detection  \n**yaml** (`^2.7.0`) — YAML config parsing  \n**zod** (`^3.24.1`) — schema validation\n\n## Development Dependencies\n\n**@types/node** (`^22.10.7`) — Node.js type definitions  \n**tsx** (`^4.19.2`) — TypeScript execution with watch mode  \n**typescript** (`^5.7.3`) — TypeScript compiler\n\n## Distribution Configuration\n\n**main**: `\"dist/cli/index.js\"` — CommonJS entry point fallback  \n**files**: `[\"dist\", \"hooks/dist\", \"README.md\", \"LICENSE\"]` — npm tarball inclusion list excludes source (`src/`), tests, and configuration files\n\n## Repository Links\n\n**repository.url**: `\"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"`  \n**bugs.url**: `\"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"`  \n**homepage**: `\"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"`\n\n## NPM Keywords\n\nSearch tags: `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]`\n### tsconfig.json\n**Purpose:** TypeScript compiler configuration defining ES2022 target with NodeNext module resolution, strict type-checking, and s...\n\n**TypeScript compiler configuration defining ES2022 target with NodeNext module resolution, strict type-checking, and source map generation for the agents-reverse-engineer CLI tool.**\n\n## Compiler Options\n\n**target**: `\"ES2022\"` — Emits JavaScript compatible with ECMAScript 2022 runtime (requires Node.js ≥18.0.0).\n\n**module**: `\"NodeNext\"` — Uses Node.js native ES module resolution with package.json `\"type\": \"module\"` support.\n\n**moduleResolution**: `\"NodeNext\"` — Resolves imports using Node.js package exports field and .js extension requirements for relative imports.\n\n**lib**: `[\"ES2022\"]` — Includes ES2022 standard library type definitions (Promise, Array methods, Object.hasOwn, etc.).\n\n**outDir**: `\"dist\"` — Compiles all TypeScript output to `dist/` directory (referenced by package.json `\"main\"` field and `bin` entry points).\n\n**rootDir**: `\"src\"` — Establishes `src/` as base directory for computing relative output paths in `dist/`.\n\n**strict**: `true` — Enables all strict type-checking flags (noImplicitAny, strictNullChecks, strictFunctionTypes, strictBindCallApply, strictPropertyInitialization, noImplicitThis, alwaysStrict).\n\n**esModuleInterop**: `true` — Enables CommonJS/ES module interoperability helpers for default imports from CJS modules.\n\n**skipLibCheck**: `true` — Skips type-checking of .d.ts files in node_modules to improve compilation speed.\n\n**forceConsistentCasingInFileNames**: `true` — Enforces case-sensitive file path imports (prevents Windows/macOS case-insensitivity bugs).\n\n**declaration**: `true` — Generates .d.ts type declaration files alongside compiled JavaScript (enables TypeScript consumers to import with type safety).\n\n**declarationMap**: `true` — Emits .d.ts.map files mapping type declarations back to original TypeScript source for IDE navigation.\n\n**sourceMap**: `true` — Generates .js.map files for debugging with original TypeScript line numbers in stack traces.\n\n**resolveJsonModule**: `true` — Allows importing JSON files with type inference (used for package.json version reading in src/version.ts).\n\n**isolatedModules**: `true` — Ensures each file can be transpiled independently without cross-file type information (required for build tools like esbuild/swc).\n\n## File Inclusion\n\n**include**: `[\"src/**/*\"]` — Compiles all TypeScript files recursively under `src/` directory (matches src/cli/index.ts, src/ai/service.ts, src/generation/prompts/templates.ts, etc.).\n\n**exclude**: `[\"node_modules\", \"dist\"]` — Excludes third-party dependencies and compiled output from compilation scope (prevents reprocessing of emitted .d.ts files).\n\n## Build Integration\n\nReferenced by `npm run build` script which invokes `tsc` to compile src/ → dist/. The `prepublishOnly` hook chains `build` + `build:hooks` before npm publish. Binary entry points in package.json (`bin.are`, `bin.agents-reverse-engineer`) resolve to `dist/cli/index.js` after compilation.\n\n## NodeNext Module Implications\n\nRequires explicit `.js` extensions in relative imports despite source files using `.ts` extension (e.g., `import { foo } from './bar.js'` resolves to `src/bar.ts` at compile time, `dist/bar.js` at runtime). Package.json must include `\"type\": \"module\"` to enable ES module semantics in emitted JavaScript.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  templates.ts.annex.md\n  types.ts\n  types.ts.annex.md\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  runner.ts.annex.md\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### docs/\n<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\n**Original vision document defining the Recursive Language Model (RLM) algorithm for agents-reverse-engineer, establishing post-order traversal methodology, directory documentation schema, multi-platform requirements, and integration references.**\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nDefines RLM algorithm executing post-order traversal: build project tree, analyze leaf files to generate `{filename}.sum` summaries, synthesize directory `AGENTS.md` when all children complete, recurse to root producing `CLAUDE.md` and platform-specific docs.\n\n## Recursive Language Model Algorithm\n\n**Core traversal strategy:**\n1. Build complete project structure tree via directory walk\n2. Execute at first leaf node (file level)\n3. Generate `{filename}.sum` per file via AI analysis\n4. When directory leaves complete, synthesize `AGENTS.md` consuming child `.sum` files\n5. Recurse backward to root producing final integration docs\n\n**Guarantees:** Child summaries exist before parent directory analysis, enabling bottom-up synthesis where directory docs reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nINPUT.md defines nine supplementary document types referenceable from `AGENTS.md`:\n\n- `ARCHITECTURE.md` — system design and component relationships\n- `STRUCTURE.md` — directory organization rationale\n- `STACK.md` — technology stack and dependencies\n- `INTEGRATIONS.md` — external service integrations\n- `INFRASTRUCTURE.md` — deployment and runtime configuration\n- `CONVENTIONS.md` — coding standards and naming patterns\n- `TESTING.md` — test strategy and frameworks\n- `PATTERNS.md` — design patterns and architectural idioms\n- `CONCERNS.md` — known issues and technical debt\n\n## Multi-Platform Requirements\n\n**Target runtimes:** Claude Code, OpenCode, and alternative LLM agent tools requiring brownfield project context.\n\n**Core features:**\n- Command execution using RLM with Claude Code or alternative backends\n- Session-end hook for incremental updates of impacted files\n- `AGENTS.md` generation in every project directory\n- Platform-agnostic documentation format\n\n**Usage commands:**\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references three methodologies for implementation analysis:\n\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — repository structure and brownfield approaches (primary)\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — partial analysis of special commands and codebase patterns\n- **SpecKit**: https://github.com/github/spec-kit — complementary tooling for specification synthesis\n### hooks/\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**Session lifecycle automation scripts for IDE integration: background version checking at session start and automatic documentation updates at session end across Claude Code, Gemini CLI, and OpenCode platforms via detached subprocess spawning with git-based change detection.**\n\n## Contents\n\n### Platform-Agnostic Scripts\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached `npm view` query comparing registry version against local ARE-VERSION files, writing availability status to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`. Prioritizes `.claude/ARE-VERSION` over `~/.claude/ARE-VERSION`, defaults to `'0.0.0'` if missing. Executes via shebang `#!/usr/bin/env node`.\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached process when `git status --porcelain` detects uncommitted changes. Exits early if `ARE_DISABLE_HOOK=1` environment variable set or `.agents-reverse-engineer.yaml` contains `hook_enabled: false` substring. ES module using `import` syntax.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — Exports `AreCheckUpdate()` async factory returning `{ event: { 'session.created': AsyncFunction } }` plugin object. Handler spawns detached Node.js subprocess writing to `~/.config/opencode/cache/are-update-check.json`. Reads version from `.opencode/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`, queries npm registry with 10s timeout, creates cache directory via `mkdirSync(cacheDir, { recursive: true })`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — Exports `AreSessionEnd()` async factory returning `{ event: { 'session.deleted': AsyncFunction } }` plugin object. Implements four-stage gate: environment check (`ARE_DISABLE_HOOK`), config substring search (`hook_enabled: false`), git status parsing, detached `npx` spawn with `stdio: 'ignore'` and `child.unref()`.\n\n## Architecture\n\n### Detached Background Execution Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\nInline script injection via `-e` flag avoids filesystem dependencies. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion. `windowsHide: true` prevents console window flash on Windows.\n\n### Version File Resolution Priority\n\nCheck hooks prioritize project-local over global version files:\n\n1. **Claude/Gemini**: `.claude/ARE-VERSION` → `~/.claude/ARE-VERSION`\n2. **OpenCode**: `.opencode/ARE-VERSION` → `~/.config/opencode/ARE-VERSION`\n\nResolution via `existsSync()` + `readFileSync()` with `'0.0.0'` fallback when neither exists.\n\n### Cache File Schema\n\nUpdate check hooks write JSON with four-field schema:\n\n```json\n{\n  \"update_available\": boolean,  // installed !== latest (whitespace-trimmed)\n  \"installed\": string,          // version from ARE-VERSION or '0.0.0'\n  \"latest\": string,             // npm registry response or 'unknown' on error\n  \"checked\": number             // Unix timestamp via Math.floor(Date.now() / 1000)\n}\n```\n\nCache locations:\n- **Claude/Gemini**: `~/.claude/cache/are-update-check.json`\n- **OpenCode**: `~/.config/opencode/cache/are-update-check.json`\n\nDirectory creation via `mkdirSync(cacheDir, { recursive: true })` before write.\n\n### Disable Mechanisms\n\nSession-end hooks implement two gates:\n\n1. **Environment**: Exit if `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file**: Exit if `.agents-reverse-engineer.yaml` exists and `readFileSync().includes('hook_enabled: false')` (substring search, no YAML parser)\n\nBoth gates return silently without error signaling.\n\n## Platform Integration\n\n### Claude Code / Gemini CLI\n\nPlatform-agnostic scripts installed via `src/installer/operations.ts`:\n- Check hook: `~/.claude/hooks/are-check-update.js` or `~/.gemini/hooks/are-check-update.js`\n- End hook: `~/.claude/hooks/are-session-end.js` or `~/.gemini/hooks/are-session-end.js`\n\nRuntime invocation via SessionStart/SessionEnd trigger names.\n\n### OpenCode\n\nPlugin factories installed to `~/.config/opencode/plugins/` or `$OPENCODE_CONFIG_DIR/plugins/`. Event handler contracts:\n- `AreCheckUpdate()` → `event['session.created']`\n- `AreSessionEnd()` → `event['session.deleted']`\n\nPlugin objects returned from async factories conform to OpenCode event system shape `{ event: { [eventName]: AsyncFunction } }`.\n\n## npm Registry Query\n\nAll check hooks execute `npm view agents-reverse-engineer version` via `execSync()` with options:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n\nTry-catch wrapper sets `latest = 'unknown'` on network/timeout errors without propagating exceptions.\n\n## Git Change Detection\n\nSession-end hooks invoke `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exit early if:\n- `status.trim()` returns empty string (clean working tree)\n- `execSync()` throws error (non-git repo or git unavailable)\n\nNo error logging — silent exit on all failure modes.\n\n## File System Operations\n\n**Shared imports across all hooks:**\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `child_process`: `spawn`, `execSync`\n- `path`: `join`\n- `os`: `homedir`\n\n**Module types:**\n- `are-session-end.js`: ES module (`import` syntax)\n- `are-check-update.js`, `opencode-*`: CommonJS-compatible (`#!/usr/bin/env node` shebang)\n\n## Build Integration\n\nHooks copied to distribution via `scripts/build-hooks.js` during `npm run build:hooks` (invoked by `prepublishOnly`). No TypeScript compilation — plain JavaScript for direct execution in IDE runtime environments.\n### scripts/\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\n**Build automation directory containing pre-publish hook file preparation script that copies session lifecycle hooks from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Contents\n\n### Build Scripts\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` files from `hooks/` to `hooks/dist/`, invoked by `npm run prepublishOnly` to prepare session lifecycle hooks for npm tarball inclusion.\n\n## Build Pipeline Integration\n\n`build-hooks.js` executes during npm publish workflow via `prepublishOnly` lifecycle hook defined in root `package.json`. The script ensures `hooks/dist/` contains all four session lifecycle hooks before `npm publish` bundles the package tarball:\n\n1. **are-check-update.js** — Claude/Gemini SessionStart hook for version checking\n2. **are-session-end.js** — Claude/Gemini SessionEnd hook for auto-update\n3. **opencode-are-check-update.js** — OpenCode plugin for version checking\n4. **opencode-are-session-end.js** — OpenCode plugin for session-end updates\n\nThe copied files become the source for installer operations (`src/installer/operations.ts`) which deploy hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).\n\n## Execution Flow\n\n1. Resolves project root via `fileURLToPath(import.meta.url)` → `path.dirname()` → `path.join(__dirname, '..')`\n2. Creates `hooks/dist/` via `mkdirSync(HOOKS_DIST, { recursive: true })` if directory missing\n3. Filters `readdirSync('hooks/')` for `.js` files, excluding `'dist'` directory itself\n4. Copies each file via `copyFileSync(src, dest)` where `src = 'hooks/<file>'` and `dest = 'hooks/dist/<file>'`\n5. Logs each operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n6. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Package.json Integration\n\nThe root `package.json` declares:\n- **files**: `[\"hooks/dist\"]` — includes `hooks/dist/` in npm tarball\n- **scripts.build:hooks**: `\"node scripts/build-hooks.js\"` — manual build command\n- **scripts.prepublishOnly**: `\"npm run build && npm run build:hooks\"` — automatic execution before `npm publish`\n\nCI/CD workflow (`.github/workflows/publish.yml`) runs `npm ci` which triggers `prepublishOnly`, ensuring `hooks/dist/` exists before registry upload with Sigstore-signed provenance attestation.\n### src/\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Root source directory implementing ARE's three-phase documentation generation pipeline via TypeScript modules organized into nine functional subsystems: AI service orchestration (subprocess pooling, retry logic, telemetry), file discovery (gitignore-aware filtering, binary detection), change detection (git diff parsing, SHA-256 hashing), generation (concurrent file analysis, post-order directory aggregation, root synthesis), quality validation (code-vs-doc, phantom paths), CLI commands (generate, update, specify, rebuild, clean), configuration (YAML schema, resource-adaptive concurrency), IDE integration (template generation for Claude/OpenCode/Gemini), and installer (npx-driven hook deployment).**\n\n## Contents\n\n**[version.ts](./version.ts)** — Exports `getVersion()` extracting package version from `../package.json` via `fileURLToPath(import.meta.url)` + `dirname()` + `join(__dirname, '..', 'package.json')`, returns `'unknown'` string literal on read/parse failure. Consumed by CLI `--version` flag, session lifecycle hooks for npm version comparison, and telemetry run log metadata.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI CLI orchestration layer with subprocess pooling (10MB maxBuffer, SIGTERM/SIGKILL timeout enforcement, process group termination via `kill(-pid)`), exponential backoff retry detecting rate limits via stderr patterns (`[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`), token cost telemetry accumulating `TelemetryEntry[]` with NDJSON persistence to `.agents-reverse-engineer/logs/run-<timestamp>.json`, and trace emission (`subprocess:spawn/exit`, `retry` events). Backends: ClaudeBackend (Zod-validated JSON parsing), GeminiBackend/OpenCodeBackend (stubs throwing `SUBPROCESS_ERROR`). Resource limits: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`.\n\n**[change-detection/](./change-detection/)** — Git-based delta computation via `git diff --name-status -M` with tab-split parsing extracting status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, SHA-256 content hashing via `createHash('sha256').update(content).digest('hex')`, uncommitted change merging from `git.status()` with linear `Array.some()` deduplication. Exports `getChangedFiles()` returning `ChangeDetectionResult` with `changes[]` discriminated union (`FileChange` with optional `oldPath` for renames).\n\n**[cli/](./cli/)** — Command entry points parsing `process.argv` via `parseArgs()`, routing to `init`/`discover`/`generate`/`update`/`specify`/`rebuild`/`clean` handlers, managing shared flags (`--debug`, `--trace`, `--dry-run`, `--concurrency`, `--model`). Integrates `ProgressLog` streaming to `.agents-reverse-engineer/progress.log`, `TraceWriter` emitting NDJSON to `traces/`, `AIService` backend resolution with `CLI_NOT_FOUND` exit code 2. Model resolution: CLI overrides config, specify/rebuild upgrade sonnet → opus for quality-critical synthesis.\n\n**[config/](./config/)** — YAML configuration loading via `loadConfig()` reading `.agents-reverse-engineer/config.yaml`, Zod validation with `ConfigSchema` applying defaults from `defaults.ts` (18-entry `DEFAULT_VENDOR_DIRS`, 26-entry `DEFAULT_EXCLUDE_PATTERNS`, 26-entry `DEFAULT_BINARY_EXTENSIONS`). Resource-adaptive concurrency formula: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` preventing RAM exhaustion on low-memory systems. Environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery via `discoverFiles()` facade composing `walkDirectory()` (fast-glob with `absolute: true`, `onlyFiles: true`, hardcoded `ignore: ['**/.git/**']`) and four-stage filter chain: `createGitignoreFilter()` (parses `.gitignore` via `ignore` library), `createVendorFilter()` (excludes 10 default directories via Set + path-pattern matching), `createBinaryFilter()` (96-extension allowlist + `isBinaryFile()` content analysis at 1MB threshold), `createCustomFilter()` (user glob patterns). Returns `FilterResult` with `included[]` and `excluded[]` arrays, latter containing `{ file, reason, filter }` objects.\n\n**[generation/](./generation/)** — Orchestrates three-phase pipeline: `GenerationOrchestrator.createPlan()` builds `AnalysisTask[]` via `buildFilePrompt()` with import maps and project structure, clears `PreparedFile.content` after prompt embedding to free heap. `buildExecutionPlan()` constructs dependency-aware `ExecutionPlan` with post-order directory sorting (deepest-first via `getDirectoryDepth()`), `isDirectoryComplete()` predicate polling child `.sum` existence. Phase 1: concurrent file analysis writes `.sum` with SHA-256 `content_hash` frontmatter. Phase 2: directory aggregation via `buildDirectoryPrompt()` reading child `.sum` + manifest detection (9 types) + import maps, writes `AGENTS.md` via `writeAgentsMd()` preserving `AGENTS.local.md` user content. Phase 3: sequential root synthesis via `buildRootPrompt()` consuming all `AGENTS.md`, writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n**[imports/](./imports/)** — Extracts TypeScript/JavaScript import statements via `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`, classifies as internal (`.` prefix) or external (`..` prefix), formats as structured import maps via `formatImportMap()` producing `fileName:\\n  specifier → symbol1, symbol2 (type)` output. Reads first 100 lines per file assuming imports clustered at top, consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` for LLM prompt context.\n\n**[installer/](./installer/)** — npx-driven installer orchestrating command/hook deployment via interactive prompts (`arrowKeySelect()` with raw mode `\\x1b[${n}A` ANSI sequences) or non-interactive mode (requires `--runtime` + `-g/-l` flags). Copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}`, registers hooks with nested `HookEvent.hooks[]` schema (Claude) or flat `GeminiHook[]` (Gemini), adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` from `getPackageVersion()`. Uninstalls via `unregisterClaudeHooks()` filtering by `getHookPatterns()` (matches current + legacy formats), `cleanupEmptyDirs()` recursively.\n\n**[integration/](./integration/)** — Platform-specific integration file generation via `generateIntegrationFiles()` detecting `.claude/`, `.opencode/`, `.aider.conf.yml`, `.gemini/` artifacts. Templates from `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` with frontmatter variants (Claude `name:` field, OpenCode `agent: build`, Gemini TOML `description`/`prompt`). Background execution pattern: Read `VERSION_FILE_PATH`, run `npx agents-reverse-engineer@latest {command}` with `run_in_background: true`, poll `.agents-reverse-engineer/progress.log` with `offset` parameter, check `TaskOutput` with `block: false`. Copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`.\n\n**[orchestration/](./orchestration/)** — Worker pool via `runPool()` sharing single `tasks.entries()` iterator across N workers (atomic `.next()` calls prevent duplicate pickups), fail-fast via shared `aborted` flag. `CommandRunner.executeGenerate()` runs three-phase pipeline with quality validation: pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks via regex export extraction and substring matching, post-phase-2 phantom path resolution via three regex patterns. `ProgressReporter` streams `[X/Y] ANALYZING/DONE/FAIL path` to console + `.agents-reverse-engineer/progress.log`, calculates ETA via moving average of last 10 task durations. `TraceWriter` emits NDJSON with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields, serializes via promise-chain pattern. `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via regex `/- \\[ \\] \\`${itemPath}\\`/` → `/- \\[x\\] \\`${itemPath}\\`/`.\n\n**[output/](./output/)** — Terminal logging via `createLogger()` with conditional color mode (picocolors vs identity passthrough). Format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes. `stripAnsi(str)` removes color codes via `/\\x1b\\[[0-9;]*m/g` for progress.log persistence.\n\n**[quality/](./quality/)** — Post-generation validation detecting code-vs-doc (exported symbols missing from `.sum`), code-vs-code (duplicate exports within directory), phantom-paths (unresolvable file references in `AGENTS.md`). `extractExports()` via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies substring presence. `checkPhantomPaths()` extracts paths via three regex patterns (markdown links, backtick paths, prose-embedded src/), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax). `buildInconsistencyReport()` aggregates `Inconsistency[]` discriminated union into `InconsistencyReport` with severity stratification (`info`/`warning`/`error`).\n\n**[rebuild/](./rebuild/)** — AI-driven project reconstruction from specification documents via `executeRebuild()` executing ordered phase workflow: `partitionSpec()` extracts `RebuildUnit[]` from Build Plan phases (`### Phase N:`), sequential order group processing with concurrent `runPool()` execution per group, progressive context accumulation from generated file exports with LRU truncation at 100k chars. `CheckpointManager` provides session continuity via SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), promise-chain write serialization to `.rebuild-checkpoint` JSON. `parseModuleOutput()` extracts file paths via delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback. Targeted API injection via Defines/Consumes keyword matching filtering subsections with fuzzy matching (substring, word overlap).\n\n**[specify/](./specify/)** — Project specification synthesis via `buildSpecPrompt()` constructing `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration). `writeSpec()` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings` partitioning on `/^(?=# )/m` with slugification transforms), throws `SpecExistsError` on conflicts unless `force=true`.\n\n**[types/](./types/)** — Core TypeScript interfaces for file discovery results: `DiscoveryResult` (`files[]`, `excluded[]`), `ExcludedFile` (`path`, `reason`), `DiscoveryStats` (`totalFiles`, `includedFiles`, `excludedFiles`, `exclusionReasons` histogram). Flows from discovery filters → orchestrator → worker pool → generation writers.\n\n**[update/](./update/)** — Incremental documentation update via `UpdateOrchestrator.preparePlan()` comparing YAML frontmatter `content_hash` against SHA-256 hashes of current content via `computeContentHash()`, segregating into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files (targeting `oldPath` for renames), `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to project root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Architecture\n\n### Three-Phase Pipeline Coordination\n\nPhase 1 (`generation/orchestrator.ts`): `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()` embedding import maps from `imports/extractor.ts`, clears `PreparedFile.content` after prompt construction to free heap. `runPool()` spawns N workers sharing `tasks.entries()` iterator (atomic `.next()` calls), each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits, writes `.sum` with YAML frontmatter (`generated_at`, `content_hash` SHA-256 hex, `purpose`, `critical_todos`, `related_files`).\n\nPhase 2 (`generation/executor.ts`): `buildExecutionPlan()` sorts directories by depth descending via `getDirectoryDepth()` (deepest-first), creates `directoryTasks[]` depending on child file task IDs for post-order traversal. `isDirectoryComplete()` predicate polls child `.sum` existence before processing. `buildDirectoryPrompt()` reads child `.sum` via `readSumFile()`, aggregates subdirectory `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects manifests (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). `writeAgentsMd()` preserves `AGENTS.local.md` user content above generated sections with `GENERATED_MARKER` injection.\n\nPhase 3 (`generation/orchestrator.ts`): Sequential execution (concurrency=1) via `rootTasks[]` depending on all directory task IDs. `buildRootPrompt()` consumes all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json`, enforces synthesis-only constraints (no invented features). Strips conversational preamble via markdown start detection (`indexOf('# ')`), writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n### Iterator-Based Worker Pool\n\n`orchestration/pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern preventing NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Quality Validation Phases\n\nPre-Phase-1: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection.\n\nPost-Phase-1: `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions) via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, substring matching in summary. `checkCodeVsCode()` aggregates exports into `Map<symbol, string[]>` per directory, detects duplicates.\n\nPost-Phase-2: `checkPhantomPaths()` extracts paths via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS`.\n\n### Incremental Update Strategy\n\n`update/orchestrator.ts` workflow: `preparePlan()` calls `discoverFiles()`, iterates results constructing `.sum` path via `getSumPath()`, calls `readSumFile()` extracting `contentHash` from frontmatter, missing `.sum` adds `{ path, status: 'added' }` to `filesToAnalyze`, existing `.sum` calls `computeContentHash()` comparing hashes, mismatch adds `{ path, status: 'modified' }`, match adds to `filesToSkip`. `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Behavioral Contracts\n\n### Default Concurrency Formula (config/defaults.ts)\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n### Subprocess Resource Limits (ai/subprocess.ts)\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts (ai/subprocess.ts)\n- SIGTERM sent at `timeoutMs` via `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n- Process group termination: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Patterns (ai/service.ts)\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula (ai/retry.ts)\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n### Default Retry Configuration (ai/retry.ts)\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Git Diff Format (change-detection/detector.ts)\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n### Binary Extension Set (discovery/filters/binary.ts)\n96 extensions including `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Extraction Patterns (quality/phantom-paths/validator.ts)\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n### Import Regex Pattern (imports/extractor.ts)\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Build Plan Phase Extraction (rebuild/spec-reader.ts)\n```regex\n/^### Phase (\\d+):\\s*(.+)$/gm\n```\n\n### Delimiter Format (rebuild/output-parser.ts)\n```regex\n/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g\n```\n\n### Heading Regex (specify/writer.ts)\n```regex\n/^(?=# )/m\n```\n\n### Slugification Transforms (specify/writer.ts)\n```typescript\n.toLowerCase()\n.replace(/\\s+/g, '-')\n.replace(/[^a-z0-9-]/g, '')\n.replace(/-+/g, '-')\n.replace(/^-|-$/g, '')\n```\n\n## Dependencies\n\n**External**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (change detection), `yaml` (config parsing), `zod` (schema validation), `ora` (spinner UI), `picocolors` (terminal colors).\n\n**Internal**: `node:fs/promises` (async file I/O), `node:fs` (sync file I/O), `node:path` (path manipulation), `node:url` (ES module URL resolution), `node:child_process` (subprocess spawning), `node:os` (system metrics), `node:crypto` (SHA-256 hashing), `node:readline` (interactive prompts).\n\n## Directory Hints\n\nContains manifest file(s): package.json — likely a package or project root.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Configuration & Metadata\n\n**[package.json](./package.json)** — Defines npm package identity (`agents-reverse-engineer@0.7.1`, MIT license), binary entry points (`are`, `agents-reverse-engineer` → `dist/cli/index.js`), dependencies (`fast-glob`, `ignore`, `isbinaryfile`, `simple-git`, `yaml`, `zod`, `ora`, `picocolors`), build scripts (`tsc`, `build-hooks.js`), and distribution whitelist (`[\"dist\", \"hooks/dist\", \"README.md\", \"LICENSE\"]`).\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler targeting ES2022 with NodeNext module resolution, strict type-checking, declaration/source map generation, compiling `src/**/*` → `dist/` with JSON module support for `package.json` version extraction.\n\n**[LICENSE](./LICENSE)** — MIT License granting unrestricted use/modification/distribution rights with liability disclaimer, copyright held by GeoloeG-IsT (2026).\n\n### Documentation\n\n**[README.md](./README.md)** — User-facing installation guide (interactive `npx` installer with `--runtime`/`-g`/`-l` flags), command reference (`are init/discover/generate/update/specify/clean`, `/are-*` AI skills), configuration schema (`.agents-reverse-engineer/config.yaml` with `exclude.patterns`, `ai.backend`, `ai.concurrency`), generated artifact formats (`.sum` YAML frontmatter with `content_hash`, `AGENTS.md` directory overviews, root platform documents), workflow sequences (init → discover → generate → update), and concurrency tuning recommendations (1-10 range, default 2 for WSL/5 elsewhere).\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Reference table mapping 25 programming language ecosystems to manifest file patterns (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`) with `✓` marks indicating detection support in Phase 2 directory aggregation via `detectManifestFiles()` (9 active patterns: JavaScript/Python/Go/Rust/Java/Ruby/PHP/C/C++).\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD automation for npm publishing with Sigstore-signed provenance attestation via `actions/setup-node@v4` and `npm publish --provenance --access public` on `release[published]` events.\n\n**[docs/](./docs/)** — Original vision document (`INPUT.md`) defining RLM algorithm with post-order traversal methodology, directory documentation schema (9 supplementary doc types: ARCHITECTURE.md, STACK.md, PATTERNS.md), multi-platform requirements (Claude/OpenCode/Gemini), and research references (GSD, BMAD, SpecKit).\n\n**[hooks/](./hooks/)** — Session lifecycle automation scripts: `are-check-update.js` (SessionStart hook spawning detached `npm view` query with cache to `~/.claude/cache/are-update-check.json`), `are-session-end.js` (SessionEnd hook triggering `npx agents-reverse-engineer@latest update --quiet` on git changes), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting `event['session.created']`/`event['session.deleted']` handlers).\n\n**[scripts/](./scripts/)** — Build automation via `build-hooks.js` copying `.js` files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle for npm tarball inclusion.\n\n**[src/](./src/)** — TypeScript source tree implementing nine functional subsystems: `ai/` (subprocess pooling with retry/telemetry), `change-detection/` (git diff + SHA-256 hashing), `cli/` (command entry points with `parseArgs()` routing), `config/` (YAML schema validation with resource-adaptive concurrency), `discovery/` (gitignore-aware filtering with binary detection), `generation/` (three-phase orchestrator with prompt builders), `imports/` (regex-based import extraction), `installer/` (npx-driven hook deployment), `integration/` (platform-specific template generation), `orchestration/` (iterator-based worker pool with progress/trace/plan tracking), `output/` (picocolors terminal logging), `quality/` (code-vs-doc/code-vs-code/phantom-paths validation), `rebuild/` (AI-driven project reconstruction from specs), `specify/` (11-section spec synthesis from `AGENTS.md` corpus), `types/` (shared discovery result interfaces), `update/` (incremental regeneration with orphan cleanup).\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1 (Concurrent File Analysis)**: Iterator-based worker pool shares `tasks.entries()` across N workers via atomic `.next()` calls. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash` for incremental update detection.\n\n**Phase 2 (Post-Order Directory Aggregation)**: Sorts directories by depth descending via `path.relative().split(path.sep).length`. `isDirectoryComplete()` predicate polls child `.sum` existence before processing directory. `buildDirectoryPrompt()` aggregates child summaries via `readSumFile()`, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, and manifest detection (9 types). Writes `AGENTS.md` preserving `AGENTS.local.md` user content above generated sections.\n\n**Phase 3 (Root Document Synthesis)**: Sequential execution (concurrency=1) generates `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` consuming all `AGENTS.md` via `collectAgentsDocs()` recursive traversal, parsing root `package.json` for project metadata, enforcing synthesis-only constraints (no feature invention).\n\n### Resource Management\n\n**Subprocess limits**: 512MB heap (`--max-old-space-size`), 4 libuv threads (`UV_THREADPOOL_SIZE`), process group killing (`kill(-pid)`) on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: `[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`).\n\n**Concurrency formula**: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` prevents RAM exhaustion in WSL/low-memory environments.\n\n### Incremental Update Strategy\n\nHash-based change detection via `.sum` frontmatter `content_hash` comparison against SHA-256 of current content. Segregates files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed sources. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining files. `getAffectedDirectories()` walks parent paths to root, sorts deepest-first. Regenerates Phase 1 for changed files, Phase 2 for affected directories (skips Phase 3).\n\n### Quality Validation\n\n**Code-vs-Doc**: Extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in `.sum` summaries.\n\n**Code-vs-Code**: Aggregates exports into `Map<symbol, string[]>` per directory, reports duplicates.\n\n**Phantom Paths**: Extracts file references via three patterns (markdown links, backtick paths, prose-embedded `src/` mentions), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax).\n\n### Telemetry & Tracing\n\n**Run logs** (`.agents-reverse-engineer/logs/run-<timestamp>.json`): Aggregates token counts, costs, durations, errors per AI call. Tracks `filesRead[]` with `path`, `sizeBytes`, `linesRead`. Enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace events** (`.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`): Emits `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry` events with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields. Promise-chain serialization ensures NDJSON line order.\n\n**Progress log** (`.agents-reverse-engineer/progress.log`): Streams `[X/Y] ANALYZING/DONE/FAIL path` with ETA calculation via moving average of last 10 task durations.\n\n## Workflows\n\n### Installation\n\nInteractive installer via `npx agents-reverse-engineer@latest` prompts for runtime (`claude`, `opencode`, `gemini`, `all`) and location (`-g` global, `-l` local). Non-interactive mode uses `--runtime <rt>` with `-g`/`-l` flags. Installs commands to `~/.claude/commands/`, hooks to `~/.claude/hooks/`, registers ARE permissions in `settings.json`, writes `ARE-VERSION` file.\n\n### Documentation Generation\n\n1. **Initialize**: `are init` → creates `.agents-reverse-engineer/config.yaml`\n2. **Discover**: `are discover --plan` → writes `GENERATION-PLAN.md` with post-order traversal plan\n3. **Generate**: `are generate` → Phase 1 (`.sum`), Phase 2 (`AGENTS.md`), Phase 3 (root docs)\n4. **Update**: `are update --uncommitted` → incremental regeneration for changed files only\n5. **Clean**: `are clean` → removes `.sum`, generated `AGENTS.md`, root docs\n\n### Project Specification Synthesis\n\n`are specify` aggregates all `AGENTS.md` into `specs/SPEC.md` with 11-section structure: Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts (Runtime Behavior + Implementation Contracts), Test Contracts, Build Plan, Prompt Templates, IDE Integration. Use `--multi-file` for split specs (`specs/<slug>.md`), `--dry-run` for preview.\n\n## Behavioral Contracts\n\n### Subprocess Resource Limits\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts\n- SIGTERM at `timeoutMs`\n- SIGKILL at `timeoutMs + 5000ms`\n- Process group kill: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Detection Patterns\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n// Default: { maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Import Extraction Regex\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Phantom Path Extraction Patterns\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g                          // Markdown links\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g              // Backtick paths\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi     // Prose-embedded paths\n```\n\n### Git Diff Format\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n## IDE Integration\n\nCommands installed via `npx agents-reverse-engineer@latest --runtime <rt> -g`:\n\n- `/are-init` — Create `.agents-reverse-engineer/config.yaml`\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation (long-running: use `run_in_background: true`, poll `progress.log` with `tail -5`)\n- `/are-update` — Incremental update\n- `/are-specify` — Synthesize project specification\n- `/are-clean` — Remove generated artifacts\n\nSession hooks:\n- **SessionStart**: `are-check-update.js` (version check with cache)\n- **SessionEnd**: `are-session-end.js` (auto-update on git changes)\n\nDisable hooks: `ARE_DISABLE_HOOK=1` or `hook_enabled: false` in config.\n\n## Dependencies\n\n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n**Core**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (git diff), `yaml` (config parsing), `zod` (schema validation)\n\n**UI**: `ora` (spinners), `picocolors` (ANSI colors)\n\n**Build**: TypeScript 5.7.3 (ES2022 target, NodeNext resolution, strict mode), `tsx` (dev watch mode)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3676,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 21673,
      "latencyMs": 94263,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T21:37:25.808Z",
      "prompt": "Generate CLAUDE.md for the project root.\n\nSynthesize the following AGENTS.md files into a single comprehensive project overview document.\nUse ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\n\n## AGENTS.md Files (30 directories)\n\n### .github/workflows/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\n**GitHub Actions workflow for automated npm package publishing with cryptographic provenance attestation on release events.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Automates npm publish on `release[published]` events with Sigstore-signed provenance linking published tarball to source commit SHA via GitHub OIDC provider.\n\n## Workflow Architecture\n\n**Trigger Conditions**: Executes on `release[published]` GitHub webhook events and manual `workflow_dispatch` invocation.\n\n**Runtime Environment**: Single `publish` job on `ubuntu-latest` runner with Node.js 20, npm registry authentication via `NODE_AUTH_TOKEN` secret, and `id-token: write` permission enabling OIDC-based attestation signing.\n\n**Build Pipeline**: Sequential steps execute `npm ci` (lockfile-based clean install), `npm run build` (invokes `prepublishOnly` hook running TypeScript compilation via `tsc` and hook file copying via `scripts/build-hooks.js`), then `npm publish --provenance --access public` generating Sigstore transparency log entry cryptographically binding published artifact to GitHub Actions workflow run and source commit.\n\n## Integration Points\n\n**Package Build System**: Depends on `prepublishOnly` script in root `package.json` executing `npm run build && npm run build:hooks`, compiling `src/` to `dist/` and copying `hooks/` to `hooks/dist/` for tarball inclusion.\n\n**Security Model**: Uses GitHub Actions secrets (`NPM_TOKEN`) for registry write access and native GitHub OIDC provider for provenance signing without long-lived cryptographic keys.\n\n**Provenance Chain**: The `--provenance` flag generates SLSA build attestation including workflow file path (`.github/workflows/publish.yml`), runner environment metadata, and commit SHA, recorded in Sigstore's public transparency log for post-publication verification via `npm audit signatures`.\n### AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Configuration & Metadata\n\n**[package.json](./package.json)** — Defines npm package identity (`agents-reverse-engineer@0.7.1`, MIT license), binary entry points (`are`, `agents-reverse-engineer` → `dist/cli/index.js`), dependencies (`fast-glob`, `ignore`, `isbinaryfile`, `simple-git`, `yaml`, `zod`, `ora`, `picocolors`), build scripts (`tsc`, `build-hooks.js`), and distribution whitelist (`[\"dist\", \"hooks/dist\", \"README.md\", \"LICENSE\"]`).\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler targeting ES2022 with NodeNext module resolution, strict type-checking, declaration/source map generation, compiling `src/**/*` → `dist/` with JSON module support for `package.json` version extraction.\n\n**[LICENSE](./LICENSE)** — MIT License granting unrestricted use/modification/distribution rights with liability disclaimer, copyright held by GeoloeG-IsT (2026).\n\n### Documentation\n\n**[README.md](./README.md)** — User-facing installation guide (interactive `npx` installer with `--runtime`/`-g`/`-l` flags), command reference (`are init/discover/generate/update/specify/clean`, `/are-*` AI skills), configuration schema (`.agents-reverse-engineer/config.yaml` with `exclude.patterns`, `ai.backend`, `ai.concurrency`), generated artifact formats (`.sum` YAML frontmatter with `content_hash`, `AGENTS.md` directory overviews, root platform documents), workflow sequences (init → discover → generate → update), and concurrency tuning recommendations (1-10 range, default 2 for WSL/5 elsewhere).\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Reference table mapping 25 programming language ecosystems to manifest file patterns (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`) with `✓` marks indicating detection support in Phase 2 directory aggregation via `detectManifestFiles()` (9 active patterns: JavaScript/Python/Go/Rust/Java/Ruby/PHP/C/C++).\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD automation for npm publishing with Sigstore-signed provenance attestation via `actions/setup-node@v4` and `npm publish --provenance --access public` on `release[published]` events.\n\n**[docs/](./docs/)** — Original vision document (`INPUT.md`) defining RLM algorithm with post-order traversal methodology, directory documentation schema (9 supplementary doc types: ARCHITECTURE.md, STACK.md, PATTERNS.md), multi-platform requirements (Claude/OpenCode/Gemini), and research references (GSD, BMAD, SpecKit).\n\n**[hooks/](./hooks/)** — Session lifecycle automation scripts: `are-check-update.js` (SessionStart hook spawning detached `npm view` query with cache to `~/.claude/cache/are-update-check.json`), `are-session-end.js` (SessionEnd hook triggering `npx agents-reverse-engineer@latest update --quiet` on git changes), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting `event['session.created']`/`event['session.deleted']` handlers).\n\n**[scripts/](./scripts/)** — Build automation via `build-hooks.js` copying `.js` files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle for npm tarball inclusion.\n\n**[src/](./src/)** — TypeScript source tree implementing nine functional subsystems: `ai/` (subprocess pooling with retry/telemetry), `change-detection/` (git diff + SHA-256 hashing), `cli/` (command entry points with `parseArgs()` routing), `config/` (YAML schema validation with resource-adaptive concurrency), `discovery/` (gitignore-aware filtering with binary detection), `generation/` (three-phase orchestrator with prompt builders), `imports/` (regex-based import extraction), `installer/` (npx-driven hook deployment), `integration/` (platform-specific template generation), `orchestration/` (iterator-based worker pool with progress/trace/plan tracking), `output/` (picocolors terminal logging), `quality/` (code-vs-doc/code-vs-code/phantom-paths validation), `rebuild/` (AI-driven project reconstruction from specs), `specify/` (11-section spec synthesis from `AGENTS.md` corpus), `types/` (shared discovery result interfaces), `update/` (incremental regeneration with orphan cleanup).\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1 (Concurrent File Analysis)**: Iterator-based worker pool shares `tasks.entries()` across N workers via atomic `.next()` calls. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash` for incremental update detection.\n\n**Phase 2 (Post-Order Directory Aggregation)**: Sorts directories by depth descending via `path.relative().split(path.sep).length`. `isDirectoryComplete()` predicate polls child `.sum` existence before processing directory. `buildDirectoryPrompt()` aggregates child summaries via `readSumFile()`, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, and manifest detection (9 types). Writes `AGENTS.md` preserving `AGENTS.local.md` user content above generated sections.\n\n**Phase 3 (Root Document Synthesis)**: Sequential execution (concurrency=1) generates `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` consuming all `AGENTS.md` via `collectAgentsDocs()` recursive traversal, parsing root `package.json` for project metadata, enforcing synthesis-only constraints (no feature invention).\n\n### Resource Management\n\n**Subprocess limits**: 512MB heap (`--max-old-space-size`), 4 libuv threads (`UV_THREADPOOL_SIZE`), process group killing (`kill(-pid)`) on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: `[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`).\n\n**Concurrency formula**: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` prevents RAM exhaustion in WSL/low-memory environments.\n\n### Incremental Update Strategy\n\nHash-based change detection via `.sum` frontmatter `content_hash` comparison against SHA-256 of current content. Segregates files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed sources. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining files. `getAffectedDirectories()` walks parent paths to root, sorts deepest-first. Regenerates Phase 1 for changed files, Phase 2 for affected directories (skips Phase 3).\n\n### Quality Validation\n\n**Code-vs-Doc**: Extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in `.sum` summaries.\n\n**Code-vs-Code**: Aggregates exports into `Map<symbol, string[]>` per directory, reports duplicates.\n\n**Phantom Paths**: Extracts file references via three patterns (markdown links, backtick paths, prose-embedded `src/` mentions), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax).\n\n### Telemetry & Tracing\n\n**Run logs** (`.agents-reverse-engineer/logs/run-<timestamp>.json`): Aggregates token counts, costs, durations, errors per AI call. Tracks `filesRead[]` with `path`, `sizeBytes`, `linesRead`. Enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace events** (`.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`): Emits `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry` events with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields. Promise-chain serialization ensures NDJSON line order.\n\n**Progress log** (`.agents-reverse-engineer/progress.log`): Streams `[X/Y] ANALYZING/DONE/FAIL path` with ETA calculation via moving average of last 10 task durations.\n\n## Workflows\n\n### Installation\n\nInteractive installer via `npx agents-reverse-engineer@latest` prompts for runtime (`claude`, `opencode`, `gemini`, `all`) and location (`-g` global, `-l` local). Non-interactive mode uses `--runtime <rt>` with `-g`/`-l` flags. Installs commands to `~/.claude/commands/`, hooks to `~/.claude/hooks/`, registers ARE permissions in `settings.json`, writes `ARE-VERSION` file.\n\n### Documentation Generation\n\n1. **Initialize**: `are init` → creates `.agents-reverse-engineer/config.yaml`\n2. **Discover**: `are discover --plan` → writes `GENERATION-PLAN.md` with post-order traversal plan\n3. **Generate**: `are generate` → Phase 1 (`.sum`), Phase 2 (`AGENTS.md`), Phase 3 (root docs)\n4. **Update**: `are update --uncommitted` → incremental regeneration for changed files only\n5. **Clean**: `are clean` → removes `.sum`, generated `AGENTS.md`, root docs\n\n### Project Specification Synthesis\n\n`are specify` aggregates all `AGENTS.md` into `specs/SPEC.md` with 11-section structure: Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts (Runtime Behavior + Implementation Contracts), Test Contracts, Build Plan, Prompt Templates, IDE Integration. Use `--multi-file` for split specs (`specs/<slug>.md`), `--dry-run` for preview.\n\n## Behavioral Contracts\n\n### Subprocess Resource Limits\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts\n- SIGTERM at `timeoutMs`\n- SIGKILL at `timeoutMs + 5000ms`\n- Process group kill: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Detection Patterns\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n// Default: { maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Import Extraction Regex\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Phantom Path Extraction Patterns\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g                          // Markdown links\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g              // Backtick paths\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi     // Prose-embedded paths\n```\n\n### Git Diff Format\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n## IDE Integration\n\nCommands installed via `npx agents-reverse-engineer@latest --runtime <rt> -g`:\n\n- `/are-init` — Create `.agents-reverse-engineer/config.yaml`\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation (long-running: use `run_in_background: true`, poll `progress.log` with `tail -5`)\n- `/are-update` — Incremental update\n- `/are-specify` — Synthesize project specification\n- `/are-clean` — Remove generated artifacts\n\nSession hooks:\n- **SessionStart**: `are-check-update.js` (version check with cache)\n- **SessionEnd**: `are-session-end.js` (auto-update on git changes)\n\nDisable hooks: `ARE_DISABLE_HOOK=1` or `hook_enabled: false` in config.\n\n## Dependencies\n\n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n**Core**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (git diff), `yaml` (config parsing), `zod` (schema validation)\n\n**UI**: `ora` (spinners), `picocolors` (ANSI colors)\n\n**Build**: TypeScript 5.7.3 (ES2022 target, NodeNext resolution, strict mode), `tsx` (dev watch mode)\n### docs/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\n**Original vision document defining the Recursive Language Model (RLM) algorithm for agents-reverse-engineer, establishing post-order traversal methodology, directory documentation schema, multi-platform requirements, and integration references.**\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nDefines RLM algorithm executing post-order traversal: build project tree, analyze leaf files to generate `{filename}.sum` summaries, synthesize directory `AGENTS.md` when all children complete, recurse to root producing `CLAUDE.md` and platform-specific docs.\n\n## Recursive Language Model Algorithm\n\n**Core traversal strategy:**\n1. Build complete project structure tree via directory walk\n2. Execute at first leaf node (file level)\n3. Generate `{filename}.sum` per file via AI analysis\n4. When directory leaves complete, synthesize `AGENTS.md` consuming child `.sum` files\n5. Recurse backward to root producing final integration docs\n\n**Guarantees:** Child summaries exist before parent directory analysis, enabling bottom-up synthesis where directory docs reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nINPUT.md defines nine supplementary document types referenceable from `AGENTS.md`:\n\n- `ARCHITECTURE.md` — system design and component relationships\n- `STRUCTURE.md` — directory organization rationale\n- `STACK.md` — technology stack and dependencies\n- `INTEGRATIONS.md` — external service integrations\n- `INFRASTRUCTURE.md` — deployment and runtime configuration\n- `CONVENTIONS.md` — coding standards and naming patterns\n- `TESTING.md` — test strategy and frameworks\n- `PATTERNS.md` — design patterns and architectural idioms\n- `CONCERNS.md` — known issues and technical debt\n\n## Multi-Platform Requirements\n\n**Target runtimes:** Claude Code, OpenCode, and alternative LLM agent tools requiring brownfield project context.\n\n**Core features:**\n- Command execution using RLM with Claude Code or alternative backends\n- Session-end hook for incremental updates of impacted files\n- `AGENTS.md` generation in every project directory\n- Platform-agnostic documentation format\n\n**Usage commands:**\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references three methodologies for implementation analysis:\n\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — repository structure and brownfield approaches (primary)\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — partial analysis of special commands and codebase patterns\n- **SpecKit**: https://github.com/github/spec-kit — complementary tooling for specification synthesis\n### hooks/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**Session lifecycle automation scripts for IDE integration: background version checking at session start and automatic documentation updates at session end across Claude Code, Gemini CLI, and OpenCode platforms via detached subprocess spawning with git-based change detection.**\n\n## Contents\n\n### Platform-Agnostic Scripts\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached `npm view` query comparing registry version against local ARE-VERSION files, writing availability status to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`. Prioritizes `.claude/ARE-VERSION` over `~/.claude/ARE-VERSION`, defaults to `'0.0.0'` if missing. Executes via shebang `#!/usr/bin/env node`.\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached process when `git status --porcelain` detects uncommitted changes. Exits early if `ARE_DISABLE_HOOK=1` environment variable set or `.agents-reverse-engineer.yaml` contains `hook_enabled: false` substring. ES module using `import` syntax.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — Exports `AreCheckUpdate()` async factory returning `{ event: { 'session.created': AsyncFunction } }` plugin object. Handler spawns detached Node.js subprocess writing to `~/.config/opencode/cache/are-update-check.json`. Reads version from `.opencode/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`, queries npm registry with 10s timeout, creates cache directory via `mkdirSync(cacheDir, { recursive: true })`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — Exports `AreSessionEnd()` async factory returning `{ event: { 'session.deleted': AsyncFunction } }` plugin object. Implements four-stage gate: environment check (`ARE_DISABLE_HOOK`), config substring search (`hook_enabled: false`), git status parsing, detached `npx` spawn with `stdio: 'ignore'` and `child.unref()`.\n\n## Architecture\n\n### Detached Background Execution Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\nInline script injection via `-e` flag avoids filesystem dependencies. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion. `windowsHide: true` prevents console window flash on Windows.\n\n### Version File Resolution Priority\n\nCheck hooks prioritize project-local over global version files:\n\n1. **Claude/Gemini**: `.claude/ARE-VERSION` → `~/.claude/ARE-VERSION`\n2. **OpenCode**: `.opencode/ARE-VERSION` → `~/.config/opencode/ARE-VERSION`\n\nResolution via `existsSync()` + `readFileSync()` with `'0.0.0'` fallback when neither exists.\n\n### Cache File Schema\n\nUpdate check hooks write JSON with four-field schema:\n\n```json\n{\n  \"update_available\": boolean,  // installed !== latest (whitespace-trimmed)\n  \"installed\": string,          // version from ARE-VERSION or '0.0.0'\n  \"latest\": string,             // npm registry response or 'unknown' on error\n  \"checked\": number             // Unix timestamp via Math.floor(Date.now() / 1000)\n}\n```\n\nCache locations:\n- **Claude/Gemini**: `~/.claude/cache/are-update-check.json`\n- **OpenCode**: `~/.config/opencode/cache/are-update-check.json`\n\nDirectory creation via `mkdirSync(cacheDir, { recursive: true })` before write.\n\n### Disable Mechanisms\n\nSession-end hooks implement two gates:\n\n1. **Environment**: Exit if `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file**: Exit if `.agents-reverse-engineer.yaml` exists and `readFileSync().includes('hook_enabled: false')` (substring search, no YAML parser)\n\nBoth gates return silently without error signaling.\n\n## Platform Integration\n\n### Claude Code / Gemini CLI\n\nPlatform-agnostic scripts installed via `src/installer/operations.ts`:\n- Check hook: `~/.claude/hooks/are-check-update.js` or `~/.gemini/hooks/are-check-update.js`\n- End hook: `~/.claude/hooks/are-session-end.js` or `~/.gemini/hooks/are-session-end.js`\n\nRuntime invocation via SessionStart/SessionEnd trigger names.\n\n### OpenCode\n\nPlugin factories installed to `~/.config/opencode/plugins/` or `$OPENCODE_CONFIG_DIR/plugins/`. Event handler contracts:\n- `AreCheckUpdate()` → `event['session.created']`\n- `AreSessionEnd()` → `event['session.deleted']`\n\nPlugin objects returned from async factories conform to OpenCode event system shape `{ event: { [eventName]: AsyncFunction } }`.\n\n## npm Registry Query\n\nAll check hooks execute `npm view agents-reverse-engineer version` via `execSync()` with options:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n\nTry-catch wrapper sets `latest = 'unknown'` on network/timeout errors without propagating exceptions.\n\n## Git Change Detection\n\nSession-end hooks invoke `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exit early if:\n- `status.trim()` returns empty string (clean working tree)\n- `execSync()` throws error (non-git repo or git unavailable)\n\nNo error logging — silent exit on all failure modes.\n\n## File System Operations\n\n**Shared imports across all hooks:**\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `child_process`: `spawn`, `execSync`\n- `path`: `join`\n- `os`: `homedir`\n\n**Module types:**\n- `are-session-end.js`: ES module (`import` syntax)\n- `are-check-update.js`, `opencode-*`: CommonJS-compatible (`#!/usr/bin/env node` shebang)\n\n## Build Integration\n\nHooks copied to distribution via `scripts/build-hooks.js` during `npm run build:hooks` (invoked by `prepublishOnly`). No TypeScript compilation — plain JavaScript for direct execution in IDE runtime environments.\n### scripts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\n**Build automation directory containing pre-publish hook file preparation script that copies session lifecycle hooks from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Contents\n\n### Build Scripts\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` files from `hooks/` to `hooks/dist/`, invoked by `npm run prepublishOnly` to prepare session lifecycle hooks for npm tarball inclusion.\n\n## Build Pipeline Integration\n\n`build-hooks.js` executes during npm publish workflow via `prepublishOnly` lifecycle hook defined in root `package.json`. The script ensures `hooks/dist/` contains all four session lifecycle hooks before `npm publish` bundles the package tarball:\n\n1. **are-check-update.js** — Claude/Gemini SessionStart hook for version checking\n2. **are-session-end.js** — Claude/Gemini SessionEnd hook for auto-update\n3. **opencode-are-check-update.js** — OpenCode plugin for version checking\n4. **opencode-are-session-end.js** — OpenCode plugin for session-end updates\n\nThe copied files become the source for installer operations (`src/installer/operations.ts`) which deploy hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).\n\n## Execution Flow\n\n1. Resolves project root via `fileURLToPath(import.meta.url)` → `path.dirname()` → `path.join(__dirname, '..')`\n2. Creates `hooks/dist/` via `mkdirSync(HOOKS_DIST, { recursive: true })` if directory missing\n3. Filters `readdirSync('hooks/')` for `.js` files, excluding `'dist'` directory itself\n4. Copies each file via `copyFileSync(src, dest)` where `src = 'hooks/<file>'` and `dest = 'hooks/dist/<file>'`\n5. Logs each operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n6. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Package.json Integration\n\nThe root `package.json` declares:\n- **files**: `[\"hooks/dist\"]` — includes `hooks/dist/` in npm tarball\n- **scripts.build:hooks**: `\"node scripts/build-hooks.js\"` — manual build command\n- **scripts.prepublishOnly**: `\"npm run build && npm run build:hooks\"` — automatic execution before `npm publish`\n\nCI/CD workflow (`.github/workflows/publish.yml`) runs `npm ci` which triggers `prepublishOnly`, ensuring `hooks/dist/` exists before registry upload with Sigstore-signed provenance attestation.\n### src/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Root source directory implementing ARE's three-phase documentation generation pipeline via TypeScript modules organized into nine functional subsystems: AI service orchestration (subprocess pooling, retry logic, telemetry), file discovery (gitignore-aware filtering, binary detection), change detection (git diff parsing, SHA-256 hashing), generation (concurrent file analysis, post-order directory aggregation, root synthesis), quality validation (code-vs-doc, phantom paths), CLI commands (generate, update, specify, rebuild, clean), configuration (YAML schema, resource-adaptive concurrency), IDE integration (template generation for Claude/OpenCode/Gemini), and installer (npx-driven hook deployment).**\n\n## Contents\n\n**[version.ts](./version.ts)** — Exports `getVersion()` extracting package version from `../package.json` via `fileURLToPath(import.meta.url)` + `dirname()` + `join(__dirname, '..', 'package.json')`, returns `'unknown'` string literal on read/parse failure. Consumed by CLI `--version` flag, session lifecycle hooks for npm version comparison, and telemetry run log metadata.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI CLI orchestration layer with subprocess pooling (10MB maxBuffer, SIGTERM/SIGKILL timeout enforcement, process group termination via `kill(-pid)`), exponential backoff retry detecting rate limits via stderr patterns (`[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`), token cost telemetry accumulating `TelemetryEntry[]` with NDJSON persistence to `.agents-reverse-engineer/logs/run-<timestamp>.json`, and trace emission (`subprocess:spawn/exit`, `retry` events). Backends: ClaudeBackend (Zod-validated JSON parsing), GeminiBackend/OpenCodeBackend (stubs throwing `SUBPROCESS_ERROR`). Resource limits: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`.\n\n**[change-detection/](./change-detection/)** — Git-based delta computation via `git diff --name-status -M` with tab-split parsing extracting status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, SHA-256 content hashing via `createHash('sha256').update(content).digest('hex')`, uncommitted change merging from `git.status()` with linear `Array.some()` deduplication. Exports `getChangedFiles()` returning `ChangeDetectionResult` with `changes[]` discriminated union (`FileChange` with optional `oldPath` for renames).\n\n**[cli/](./cli/)** — Command entry points parsing `process.argv` via `parseArgs()`, routing to `init`/`discover`/`generate`/`update`/`specify`/`rebuild`/`clean` handlers, managing shared flags (`--debug`, `--trace`, `--dry-run`, `--concurrency`, `--model`). Integrates `ProgressLog` streaming to `.agents-reverse-engineer/progress.log`, `TraceWriter` emitting NDJSON to `traces/`, `AIService` backend resolution with `CLI_NOT_FOUND` exit code 2. Model resolution: CLI overrides config, specify/rebuild upgrade sonnet → opus for quality-critical synthesis.\n\n**[config/](./config/)** — YAML configuration loading via `loadConfig()` reading `.agents-reverse-engineer/config.yaml`, Zod validation with `ConfigSchema` applying defaults from `defaults.ts` (18-entry `DEFAULT_VENDOR_DIRS`, 26-entry `DEFAULT_EXCLUDE_PATTERNS`, 26-entry `DEFAULT_BINARY_EXTENSIONS`). Resource-adaptive concurrency formula: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` preventing RAM exhaustion on low-memory systems. Environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery via `discoverFiles()` facade composing `walkDirectory()` (fast-glob with `absolute: true`, `onlyFiles: true`, hardcoded `ignore: ['**/.git/**']`) and four-stage filter chain: `createGitignoreFilter()` (parses `.gitignore` via `ignore` library), `createVendorFilter()` (excludes 10 default directories via Set + path-pattern matching), `createBinaryFilter()` (96-extension allowlist + `isBinaryFile()` content analysis at 1MB threshold), `createCustomFilter()` (user glob patterns). Returns `FilterResult` with `included[]` and `excluded[]` arrays, latter containing `{ file, reason, filter }` objects.\n\n**[generation/](./generation/)** — Orchestrates three-phase pipeline: `GenerationOrchestrator.createPlan()` builds `AnalysisTask[]` via `buildFilePrompt()` with import maps and project structure, clears `PreparedFile.content` after prompt embedding to free heap. `buildExecutionPlan()` constructs dependency-aware `ExecutionPlan` with post-order directory sorting (deepest-first via `getDirectoryDepth()`), `isDirectoryComplete()` predicate polling child `.sum` existence. Phase 1: concurrent file analysis writes `.sum` with SHA-256 `content_hash` frontmatter. Phase 2: directory aggregation via `buildDirectoryPrompt()` reading child `.sum` + manifest detection (9 types) + import maps, writes `AGENTS.md` via `writeAgentsMd()` preserving `AGENTS.local.md` user content. Phase 3: sequential root synthesis via `buildRootPrompt()` consuming all `AGENTS.md`, writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n**[imports/](./imports/)** — Extracts TypeScript/JavaScript import statements via `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`, classifies as internal (`.` prefix) or external (`..` prefix), formats as structured import maps via `formatImportMap()` producing `fileName:\\n  specifier → symbol1, symbol2 (type)` output. Reads first 100 lines per file assuming imports clustered at top, consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` for LLM prompt context.\n\n**[installer/](./installer/)** — npx-driven installer orchestrating command/hook deployment via interactive prompts (`arrowKeySelect()` with raw mode `\\x1b[${n}A` ANSI sequences) or non-interactive mode (requires `--runtime` + `-g/-l` flags). Copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}`, registers hooks with nested `HookEvent.hooks[]` schema (Claude) or flat `GeminiHook[]` (Gemini), adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` from `getPackageVersion()`. Uninstalls via `unregisterClaudeHooks()` filtering by `getHookPatterns()` (matches current + legacy formats), `cleanupEmptyDirs()` recursively.\n\n**[integration/](./integration/)** — Platform-specific integration file generation via `generateIntegrationFiles()` detecting `.claude/`, `.opencode/`, `.aider.conf.yml`, `.gemini/` artifacts. Templates from `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` with frontmatter variants (Claude `name:` field, OpenCode `agent: build`, Gemini TOML `description`/`prompt`). Background execution pattern: Read `VERSION_FILE_PATH`, run `npx agents-reverse-engineer@latest {command}` with `run_in_background: true`, poll `.agents-reverse-engineer/progress.log` with `offset` parameter, check `TaskOutput` with `block: false`. Copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`.\n\n**[orchestration/](./orchestration/)** — Worker pool via `runPool()` sharing single `tasks.entries()` iterator across N workers (atomic `.next()` calls prevent duplicate pickups), fail-fast via shared `aborted` flag. `CommandRunner.executeGenerate()` runs three-phase pipeline with quality validation: pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks via regex export extraction and substring matching, post-phase-2 phantom path resolution via three regex patterns. `ProgressReporter` streams `[X/Y] ANALYZING/DONE/FAIL path` to console + `.agents-reverse-engineer/progress.log`, calculates ETA via moving average of last 10 task durations. `TraceWriter` emits NDJSON with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields, serializes via promise-chain pattern. `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via regex `/- \\[ \\] \\`${itemPath}\\`/` → `/- \\[x\\] \\`${itemPath}\\`/`.\n\n**[output/](./output/)** — Terminal logging via `createLogger()` with conditional color mode (picocolors vs identity passthrough). Format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes. `stripAnsi(str)` removes color codes via `/\\x1b\\[[0-9;]*m/g` for progress.log persistence.\n\n**[quality/](./quality/)** — Post-generation validation detecting code-vs-doc (exported symbols missing from `.sum`), code-vs-code (duplicate exports within directory), phantom-paths (unresolvable file references in `AGENTS.md`). `extractExports()` via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies substring presence. `checkPhantomPaths()` extracts paths via three regex patterns (markdown links, backtick paths, prose-embedded src/), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax). `buildInconsistencyReport()` aggregates `Inconsistency[]` discriminated union into `InconsistencyReport` with severity stratification (`info`/`warning`/`error`).\n\n**[rebuild/](./rebuild/)** — AI-driven project reconstruction from specification documents via `executeRebuild()` executing ordered phase workflow: `partitionSpec()` extracts `RebuildUnit[]` from Build Plan phases (`### Phase N:`), sequential order group processing with concurrent `runPool()` execution per group, progressive context accumulation from generated file exports with LRU truncation at 100k chars. `CheckpointManager` provides session continuity via SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), promise-chain write serialization to `.rebuild-checkpoint` JSON. `parseModuleOutput()` extracts file paths via delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback. Targeted API injection via Defines/Consumes keyword matching filtering subsections with fuzzy matching (substring, word overlap).\n\n**[specify/](./specify/)** — Project specification synthesis via `buildSpecPrompt()` constructing `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration). `writeSpec()` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings` partitioning on `/^(?=# )/m` with slugification transforms), throws `SpecExistsError` on conflicts unless `force=true`.\n\n**[types/](./types/)** — Core TypeScript interfaces for file discovery results: `DiscoveryResult` (`files[]`, `excluded[]`), `ExcludedFile` (`path`, `reason`), `DiscoveryStats` (`totalFiles`, `includedFiles`, `excludedFiles`, `exclusionReasons` histogram). Flows from discovery filters → orchestrator → worker pool → generation writers.\n\n**[update/](./update/)** — Incremental documentation update via `UpdateOrchestrator.preparePlan()` comparing YAML frontmatter `content_hash` against SHA-256 hashes of current content via `computeContentHash()`, segregating into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files (targeting `oldPath` for renames), `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to project root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Architecture\n\n### Three-Phase Pipeline Coordination\n\nPhase 1 (`generation/orchestrator.ts`): `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()` embedding import maps from `imports/extractor.ts`, clears `PreparedFile.content` after prompt construction to free heap. `runPool()` spawns N workers sharing `tasks.entries()` iterator (atomic `.next()` calls), each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits, writes `.sum` with YAML frontmatter (`generated_at`, `content_hash` SHA-256 hex, `purpose`, `critical_todos`, `related_files`).\n\nPhase 2 (`generation/executor.ts`): `buildExecutionPlan()` sorts directories by depth descending via `getDirectoryDepth()` (deepest-first), creates `directoryTasks[]` depending on child file task IDs for post-order traversal. `isDirectoryComplete()` predicate polls child `.sum` existence before processing. `buildDirectoryPrompt()` reads child `.sum` via `readSumFile()`, aggregates subdirectory `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects manifests (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). `writeAgentsMd()` preserves `AGENTS.local.md` user content above generated sections with `GENERATED_MARKER` injection.\n\nPhase 3 (`generation/orchestrator.ts`): Sequential execution (concurrency=1) via `rootTasks[]` depending on all directory task IDs. `buildRootPrompt()` consumes all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json`, enforces synthesis-only constraints (no invented features). Strips conversational preamble via markdown start detection (`indexOf('# ')`), writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n### Iterator-Based Worker Pool\n\n`orchestration/pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern preventing NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Quality Validation Phases\n\nPre-Phase-1: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection.\n\nPost-Phase-1: `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions) via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, substring matching in summary. `checkCodeVsCode()` aggregates exports into `Map<symbol, string[]>` per directory, detects duplicates.\n\nPost-Phase-2: `checkPhantomPaths()` extracts paths via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS`.\n\n### Incremental Update Strategy\n\n`update/orchestrator.ts` workflow: `preparePlan()` calls `discoverFiles()`, iterates results constructing `.sum` path via `getSumPath()`, calls `readSumFile()` extracting `contentHash` from frontmatter, missing `.sum` adds `{ path, status: 'added' }` to `filesToAnalyze`, existing `.sum` calls `computeContentHash()` comparing hashes, mismatch adds `{ path, status: 'modified' }`, match adds to `filesToSkip`. `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Behavioral Contracts\n\n### Default Concurrency Formula (config/defaults.ts)\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n### Subprocess Resource Limits (ai/subprocess.ts)\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts (ai/subprocess.ts)\n- SIGTERM sent at `timeoutMs` via `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n- Process group termination: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Patterns (ai/service.ts)\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula (ai/retry.ts)\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n### Default Retry Configuration (ai/retry.ts)\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Git Diff Format (change-detection/detector.ts)\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n### Binary Extension Set (discovery/filters/binary.ts)\n96 extensions including `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Extraction Patterns (quality/phantom-paths/validator.ts)\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n### Import Regex Pattern (imports/extractor.ts)\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Build Plan Phase Extraction (rebuild/spec-reader.ts)\n```regex\n/^### Phase (\\d+):\\s*(.+)$/gm\n```\n\n### Delimiter Format (rebuild/output-parser.ts)\n```regex\n/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g\n```\n\n### Heading Regex (specify/writer.ts)\n```regex\n/^(?=# )/m\n```\n\n### Slugification Transforms (specify/writer.ts)\n```typescript\n.toLowerCase()\n.replace(/\\s+/g, '-')\n.replace(/[^a-z0-9-]/g, '')\n.replace(/-+/g, '-')\n.replace(/^-|-$/g, '')\n```\n\n## Dependencies\n\n**External**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (change detection), `yaml` (config parsing), `zod` (schema validation), `ora` (spinner UI), `picocolors` (terminal colors).\n\n**Internal**: `node:fs/promises` (async file I/O), `node:fs` (sync file I/O), `node:path` (path manipulation), `node:url` (ES module URL resolution), `node:child_process` (subprocess spawning), `node:os` (system metrics), `node:crypto` (SHA-256 hashing), `node:readline` (interactive prompts).\n### src/ai/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\n**Backend-agnostic AI CLI orchestration layer implementing subprocess pooling, exponential backoff retry, token cost telemetry with NDJSON persistence, and trace emission for concurrency debugging.**\n\n## Contents\n\n**[index.ts](./index.ts)** — Public API barrel exporting `AIService`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from types, `BackendRegistry`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()` from registry, `withRetry()`, `DEFAULT_RETRY_OPTIONS` from retry, `runSubprocess()`, `isCommandOnPath()` from subprocess/backends.\n\n**[registry.ts](./registry.ts)** — `BackendRegistry` stores AIBackend instances by name with insertion-order priority (Claude → Gemini → OpenCode), `createBackendRegistry()` pre-populates with backend adapters, `detectBackend()` returns first available via `backend.isAvailable()` iteration, `resolveBackend()` handles explicit names and 'auto' mode throwing `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on miss.\n\n**[retry.ts](./retry.ts)** — `withRetry<T>()` executes async operations with exponential backoff via `baseDelayMs * multiplier^attempt` capped at `maxDelayMs` plus 0-500ms jitter, terminates immediately on non-retryable errors via `isRetryable()` predicate, invokes `onRetry()` callback before each attempt, throws last error after exhausting `maxRetries`, `DEFAULT_RETRY_OPTIONS` defines 3 retries with 1s base delay, 8s max delay, 2x multiplier.\n\n**[service.ts](./service.ts)** — `AIService` orchestrates AI calls via `call()` wrapping `runSubprocess()` with `withRetry()`, detects rate limits via stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\"), refuses to retry timeouts (inline comment: \"spawning another heavyweight subprocess on a struggling system makes things worse\"), emits `subprocess:spawn/exit` and `retry` trace events, accumulates `TelemetryEntry` records via `TelemetryLogger`, serializes writes to subprocess log files via promise-chain queue, exposes `finalize()` for `RunLog` persistence and retention enforcement via `cleanupOldLogs()`, tracks active subprocesses for debug logging with heap/RSS metrics.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess()` spawns CLI via `execFile()` with 10MB `maxBuffer`, SIGTERM timeout via `killSignal`, stdin piping for prompt delivery, unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, process group termination via `kill(-pid, 'SIGKILL')` on completion, module-level `activeSubprocesses` Map tracking PIDs with spawn timestamps, exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency debugging, never throws (always resolves `SubprocessResult`).\n\n**[types.ts](./types.ts)** — Defines `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` methods, `AICallOptions` with `prompt`, optional `systemPrompt/model/timeoutMs/maxTurns/taskLabel`, `AIResponse` with normalized `text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw`, `SubprocessResult` with `stdout/stderr/exitCode/signal/durationMs/timedOut/childPid`, `RetryOptions` with exponential backoff config and predicates, `TelemetryEntry` per-call log with token counts and file reads, `RunLog` aggregate summary, `AIServiceError` with typed codes `CLI_NOT_FOUND/TIMEOUT/PARSE_ERROR/SUBPROCESS_ERROR/RATE_LIMIT`, `FileRead` with path/sizeBytes.\n\n## Architecture\n\n**Three-Layer Abstraction**\n\n1. **Subprocess Layer** (`subprocess.ts`): Raw `execFile()` wrapper with timeout enforcement, process group killing, stdin piping, active subprocess tracking. Returns `SubprocessResult` on all code paths (never throws).\n\n2. **Retry Layer** (`retry.ts`): Generic exponential backoff with jitter, caller-provided `isRetryable()` predicate, optional `onRetry()` callback. Terminates immediately on permanent errors.\n\n3. **Service Layer** (`service.ts`): Integrates subprocess + retry + telemetry + tracing. Detects rate limits via stderr patterns, refuses to retry timeouts, accumulates token counts, emits trace events for concurrency debugging, serializes subprocess log writes via promise chain.\n\n**Backend Registry**\n\n`BackendRegistry` stores `AIBackend` instances with insertion-order priority determining auto-detection sequence. `createBackendRegistry()` registers ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. `resolveBackend()` handles two modes: explicit name lookups with availability validation, 'auto' mode calling `detectBackend()` which iterates backends invoking `isAvailable()` until first match. Throws `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on failure.\n\n**Retry Strategy**\n\nRate limit detection via `isRateLimitStderr()` checking lowercase stderr for patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\". Only rate limit errors are retryable—timeouts are permanent failures (inline rationale: \"spawning another heavyweight subprocess on a system that's already struggling or against an unresponsive API makes things worse and can exhaust system resources\"). `withRetry()` invoked with custom predicate: `error instanceof AIServiceError && error.code === 'RATE_LIMIT'`.\n\n**Telemetry Pipeline**\n\n`TelemetryLogger` accumulates `TelemetryEntry` records in memory via `addEntry()` calls after each subprocess completion. Each entry captures `timestamp`, `prompt`, `systemPrompt`, `response`, `model`, token counts (`inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`), `latencyMs`, `exitCode`, `retryCount`, `filesRead[]` array. Command orchestrator calls `addFilesReadToLastEntry()` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`). `finalize()` invokes `logger.toRunLog()` producing aggregate `RunLog` with summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead` via Set deduplication), then serializes via `writeRunLog()` to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace Emission**\n\n`AIService` accepts optional `ITraceWriter` via `setTracer()` for concurrency debugging. Emits three event types: `subprocess:spawn` with `childPid/taskLabel/timestamp`, `subprocess:exit` with `childPid/exitCode/signal/durationMs/timedOut`, `retry` with `attempt/taskLabel/errorCode`. Trace writer (from `src/orchestration/trace.ts`) serializes events to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via promise-chain serialization ensuring write order matches emission order despite concurrent workers.\n\n**Subprocess Resource Management**\n\n`runSubprocess()` tracks active subprocesses in module-level Map with PID → `{ command, spawnedAt }` mapping. Exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for debugging. `AIService.call()` increments `activeSubprocesses` counter before spawn, decrements after completion, logs pre-spawn debug message with active count, heap usage, RSS via `process.memoryUsage()`, and configured timeout. Post-completion debug log includes PID, exit code, duration, updated active count.\n\n**Process Lifecycle**\n\n`runSubprocess()` sequence: spawn with 10MB `maxBuffer` and SIGTERM `killSignal`, track in `activeSubprocesses` Map, invoke `onSpawn()` callback synchronously for trace emission, write `options.input` to stdin then `.end()` to close stream, set unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, on callback clear timer, attempt process group kill via `kill(-pid, 'SIGKILL')` with single-process fallback, remove from `activeSubprocesses`, resolve with `SubprocessResult`. Timeout detection via `error.killed === true` from execFile. Exit code extracted from `error.code` (if number) else `child.exitCode` else defaults to 1 (failure) or 0 (success).\n\n## Behavioral Contracts\n\n**Rate Limit Patterns** (service.ts `RATE_LIMIT_PATTERNS`):\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n**Exponential Backoff Formula** (retry.ts):\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n**Default Retry Configuration** (retry.ts `DEFAULT_RETRY_OPTIONS`):\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n**Subprocess Timeout** (subprocess.ts):\n- SIGTERM sent at `timeoutMs` via execFile `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n\n**Process Group Termination** (subprocess.ts):\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // Negative PID targets process group\n```\n\n**Debug Memory Formatting** (service.ts `formatBytes()`):\n- `< 1024` → `${bytes}B`\n- `< 1048576` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1048576` → `${(bytes / 1048576).toFixed(1)}MB`\n\n**Telemetry Filename Format** (telemetry/run-log.ts):\n```typescript\n`run-${runLog.startTime.replace(/[:.]/g, '-')}.json`\n// \"2026-02-09T12:34:56.789Z\" → \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Subprocess Log Filename Sanitization** (service.ts):\n```typescript\ntaskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')\n```\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting token counts from usage/modelUsage objects, GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR` until output formats stabilize, shared `isCommandOnPath()` splitting `process.env.PATH` by platform delimiter with Windows `PATHEXT` iteration, CLI arguments with stdin prompt delivery and platform-specific flags (`--output-format json`, `--permission-mode bypassPermissions`).\n\n**[telemetry/](./telemetry/)** — `TelemetryLogger` accumulating `TelemetryEntry` records in memory with `addEntry()` and `getSummary()` computing token sums/error counts/unique file deduplication, `writeRunLog()` serializing `RunLog` to `.agents-reverse-engineer/logs/run-<timestamp>.json` with ISO timestamp transformation, `cleanupOldLogs()` deleting oldest files beyond retention threshold via lexicographic sort.\n\n## Integration Points\n\n**Upstream Consumers:**\n- `src/orchestration/runner.ts` — Instantiates AIService with resolved backend and config options, invokes `call()` per task, attaches file reads via `addFilesReadToLastEntry()`, finalizes telemetry via `finalize()`.\n- `src/generation/orchestrator.ts` — Threads AIService through three-phase pipeline (file analysis, directory aggregation, root synthesis).\n\n**Downstream Dependencies:**\n- `src/orchestration/trace.ts` — `ITraceWriter` interface for trace event emission.\n- `src/config/schema.ts` — `AIConfig` with backend selection, timeout, concurrency, telemetry retention.\n- `node:child_process` — `execFile()` for subprocess spawning.\n- `node:fs/promises` — File I/O for subprocess logs and telemetry persistence.\n### src/ai/backends/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\nBackend adapters implementing AIBackend interface for three AI CLI tools: ClaudeBackend with full JSON parsing via Zod schema validation, GeminiBackend/OpenCodeBackend as stub implementations throwing AIServiceError until stable output formats arrive.\n\n## Contents\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection splitting process.env.PATH by platform delimiter, buildArgs() constructing ['--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions'] with optional --model/--system-prompt/--max-turns, parseResponse() slicing stdout from first '{' character to strip upgrade notices then validating against ClaudeResponseSchema extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens and model name from modelUsage object keys, getInstallInstructions() returning npm command for @anthropic-ai/claude-code.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub with isAvailable() delegating to isCommandOnPath('gemini'), buildArgs() returning ['-p', '--output-format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning npm command for @anthropic-ai/gemini-cli with GitHub URL (implementation deferred per RESEARCH.md Open Question 2 pending stable JSON format).\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub with isAvailable() delegating to isCommandOnPath('opencode'), buildArgs() returning ['run', '--format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning curl install command for https://opencode.ai (implementation deferred per RESEARCH.md Open Question 3 pending JSONL parsing).\n\n## Architecture\n\n**Backend Adapter Contract**\n\nAIBackend interface defines five methods: isAvailable() checking CLI binary presence on PATH, buildArgs(options) constructing subprocess argument arrays with prompt delivered via stdin, parseResponse(stdout, durationMs, exitCode) extracting AIResponse with normalized token counts, getInstallInstructions() returning user-facing setup guidance, name/cliCommand properties identifying backend.\n\n**PATH Detection Strategy**\n\nisCommandOnPath() splits process.env.PATH by path.delimiter (';' on Windows, ':' elsewhere), iterates directories calling fs.stat() on potential executable paths, on Windows iterates process.env.PATHEXT extensions ['.exe', '.cmd', '.bat', '.com'] to match platform conventions, returns true on first match. Function exported from claude.ts and reused by gemini.ts/opencode.ts for cross-backend consistency.\n\n**CLI Argument Patterns**\n\nAll backends use stdin for prompt delivery (not CLI arguments) to avoid shell escaping issues. ClaudeBackend appends --model/--system-prompt/--max-turns conditionally via buildArgs() inspecting AICallOptions properties. --permission-mode bypassPermissions enables non-interactive subprocess execution per PITFALLS.md §8. --no-session-persistence prevents state file writes.\n\n**JSON Response Parsing**\n\nClaudeBackend.parseResponse() handles stdout prefix content (upgrade notices, warnings) by finding first '{' via indexOf() before JSON.parse(). ClaudeResponseSchema validates against v2.1.31 output format with type: 'result', usage object containing input_tokens/cache_creation_input_tokens/cache_read_input_tokens/output_tokens, modelUsage record mapping model names to detailed statistics. Throws AIServiceError with code PARSE_ERROR on validation failure or missing JSON.\n\n**Stub Backend Pattern**\n\nGeminiBackend and OpenCodeBackend implement full interface surface but throw AIServiceError from parseResponse() to prevent runtime usage until output format research completes. Enables backend registration in src/ai/registry.ts and auto-detection via detectFirstAvailableBackend() while blocking production execution.\n\n## Integration Dependencies\n\nImports AIBackend/AICallOptions/AIResponse/AIServiceError from ../types.js. ClaudeBackend requires zod for ClaudeResponseSchema validation. Consumed by AIService in src/ai/service.ts via BackendRegistry.getBackend(name) lookup. Backend selection happens via config.ai.backend or auto-detection iterating ['claude', 'gemini', 'opencode'] until isAvailable() returns true.\n\n## Behavioral Contracts\n\n**ClaudeBackend Argument Construction**\n```javascript\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n// Appends conditionally:\n['--model', options.model] if options.model present\n['--system-prompt', options.systemPrompt] if options.systemPrompt present\n['--max-turns', String(options.maxTurns)] if options.maxTurns defined\n```\n\n**ClaudeResponseSchema Token Fields**\n```typescript\nusage: z.object({\n  input_tokens: z.number(),\n  cache_creation_input_tokens: z.number(),\n  cache_read_input_tokens: z.number(),\n  output_tokens: z.number()\n})\n```\n\n**GeminiBackend Arguments** — `['-p', '--output-format', 'json']`\n\n**OpenCodeBackend Arguments** — `['run', '--format', 'json']`\n\n**Stub Error Message Pattern** — `'<Backend> backend is not yet implemented. Use Claude backend.'` with AIServiceError code `'SUBPROCESS_ERROR'`\n### src/ai/telemetry/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\n**Accumulates per-subprocess telemetry entries in memory via TelemetryLogger, serializes aggregate RunLog summaries to timestamped NDJSON files in `.agents-reverse-engineer/logs/`, and enforces retention limits via lexicographic filename sorting.**\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes oldest `run-*.json` files beyond retention threshold via lexicographic sort, catches `ENOENT` for missing logs directory.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates `TelemetryEntry` instances in memory, computes aggregate statistics via `getSummary()` (token sums, error counts, unique file deduplication), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` serializes `RunLog` to `.agents-reverse-engineer/logs/run-<safeTimestamp>.json` with ISO timestamp transformation (`2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z.json`).\n\n## Data Flow\n\n1. **AIService** (`src/ai/service.ts`) invokes `TelemetryLogger.addEntry()` after each subprocess call with `TelemetryEntry` containing `inputTokens`, `outputTokens`, `cacheReadTokens`, `latencyMs`, `error`, `filesRead[]`.\n2. **Command orchestrator** (`src/orchestration/runner.ts`) calls `logger.setFilesReadOnLastEntry(filesRead)` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`).\n3. **Run completion**: orchestrator calls `logger.toRunLog()` to produce `RunLog` with aggregated summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`).\n4. **Persistence**: `writeRunLog(projectRoot, runLog)` serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n5. **Retention enforcement**: `cleanupOldLogs(projectRoot, keepCount)` deletes oldest logs beyond `config.ai.telemetry.keepRuns` threshold (default 50).\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService instantiates TelemetryLogger once per run, calls `addEntry()` per subprocess, invokes `toRunLog()` at finalization.\n- `src/orchestration/runner.ts` — Orchestrates telemetry lifecycle: logger creation, entry enrichment with file reads, run log serialization, cleanup invocation.\n\n**Consumes:**\n- `src/ai/types.ts` — `TelemetryEntry`, `RunLog`, `FileRead` type definitions with token count fields, latency metrics, error summaries.\n\n## Behavioral Contracts\n\n**Filename transformation** (run-log.ts):\n```typescript\nrunLog.startTime.replace(/[:.]/g, '-')\n// Input:  \"2026-02-09T12:34:56.789Z\"\n// Output: \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Lexicographic sorting** (cleanup.ts):\n- ISO 8601 timestamps in filenames sort chronologically when treated as strings.\n- `sort()` then `reverse()` produces newest-first order.\n- `slice(keepCount)` targets oldest files for deletion.\n\n**Aggregate statistics** (logger.ts `getSummary()`):\n- `totalCalls` = `entries.length`\n- `totalInputTokens` = `Σ(entry.inputTokens)`\n- `uniqueFilesRead` = distinct count via `Set<string>` deduplication of `entry.filesRead.map(f => f.path)`\n\n## Retention Policy\n\nDefault retention: 50 runs (`config.ai.telemetry.keepRuns`). `cleanupOldLogs()` invoked after every `writeRunLog()` call. Missing logs directory handled gracefully (returns 0 without error). Propagates non-`ENOENT` filesystem errors to caller.\n### src/change-detection/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\n**Git-based change detection with SHA-256 content hashing for incremental documentation updates, supporting rename tracking via `git diff -M`, uncommitted change merging via `git status --porcelain`, and hash-only fallback for non-git workflows.**\n\n## Contents\n\n### [detector.ts](./detector.ts)\nImplements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, and `computeContentHashFromString()` for delta computation. Parses `git diff --name-status -M` output with tab-split line parsing to extract status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, maps to `FileChange` objects with optional `oldPath` for renames. Optionally merges uncommitted changes from `git.status()` (`modified`, `deleted`, `not_added`, `staged`) when `includeUncommitted: true`, using linear `Array.some()` deduplication to prevent duplicate entries. SHA-256 hashing via `node:crypto` `createHash('sha256')` with hex digest output.\n\n### [types.ts](./types.ts)\nDefines `ChangeType` (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` (discriminated by `status` with optional `oldPath` for `'renamed'`), `ChangeDetectionResult` (`currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`), and `ChangeDetectionOptions` (`includeUncommitted?: boolean`). Discriminated union pattern enables type-safe `oldPath` access after `status === 'renamed'` check.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting all functions (`isGitRepo`, `getCurrentCommit`, `getChangedFiles`, `computeContentHash`, `computeContentHashFromString`) and types (`ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions`) from `detector.ts` and `types.ts`. Serves as public API consumed by `../update/orchestrator.ts` and `../cli/update.ts`.\n\n## Integration Points\n\n**Incremental Update Workflow:**\n1. `src/update/orchestrator.ts` calls `getChangedFiles(projectRoot, baseCommit, options)` to compute `changes[]` array\n2. For each changed file, reads `.sum` YAML frontmatter `content_hash` via `src/generation/writers/sum.ts`\n3. Calls `computeContentHash(filePath)` to compute current file SHA-256 hash\n4. Hash mismatch → add to `filesToAnalyze` as `FileChange`, hash match → add to `filesToSkip`\n5. Detects orphans via `changes.filter(c => c.status === 'renamed')` and calls `cleanupOrphans()` to delete `.sum` files for `oldPath` entries\n6. Session-end hooks (`hooks/are-session-end.js`) check `git status --porcelain` and spawn `npx agents-reverse-engineer@latest update --quiet` if changes detected\n\n**Non-Git Fallback:**\nWhen `isGitRepo(projectRoot)` returns `false`, consumers skip git operations and rely on SHA-256 hash comparison alone for change detection (reads all `.sum` files, compares hashes).\n\n## Behavioral Contracts\n\n**Diff output format:** `git diff --name-status -M` produces tab-delimited lines where:\n- Add/Modify/Delete: `STATUS\\tFILE` (e.g., `M\\tsrc/foo.ts`)\n- Rename: `R<percentage>\\tOLDPATH\\tNEWPATH` (e.g., `R100\\tsrc/old.ts\\tsrc/new.ts`)\n\n**Rename similarity threshold:** `-M` flag uses git default 50% content similarity for rename detection.\n\n**Status code mapping:**\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n**Uncommitted change sources (when `includeUncommitted: true`):**\n- `status.modified` — modified files not staged for commit\n- `status.deleted` — staged deletions\n- `status.not_added` — untracked files\n- `status.staged` — files added to git index\n\n**Deduplication strategy:** Linear `changes.some(c => c.path === file)` scan before appending uncommitted entries prevents duplicate `FileChange` objects when working tree changes overlap with committed diff.\n\n**SHA-256 output format:** 64-character hex string from `createHash('sha256').update(content).digest('hex')`.\n### src/cli/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand-line interface layer parsing `process.argv`, routing to command handlers, managing shared flags (--debug, --trace, --dry-run, --concurrency, --model), and integrating ProgressLog/TraceWriter/AIService across init/discover/generate/update/specify/rebuild/clean workflows.\n\n## Command Entry Points\n\n**[index.ts](./index.ts)** — Main router parsing args via `parseArgs()`, dispatching to command handlers, handling `--version`/`--help` flags, launching interactive installer when invoked with no arguments or installer-specific flags (--runtime, -g, -l).\n\n**[init.ts](./init.ts)** — `initCommand(root, { force? })` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, exits with warning if config exists without `--force`, catches `EACCES`/`EPERM` with `process.exit(1)`.\n\n**[discover.ts](./discover.ts)** — `discoverCommand(targetPath, { tracer?, debug? })` runs file discovery via `discoverFiles()`, writes included/excluded files to console and `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`, emits `discovery:start/end` trace events.\n\n**[generate.ts](./generate.ts)** — `generateCommand(targetPath, GenerateOptions)` orchestrates three-phase pipeline: discovers files → creates GenerationPlan via `createOrchestrator().createPlan()` → resolves AI backend → builds ExecutionPlan → executes via `CommandRunner.executeGenerate()` → finalizes telemetry/trace/progress, exits with codes 0 (success), 1 (partial failure), 2 (total failure/no CLI).\n\n**[update.ts](./update.ts)** — `updateCommand(targetPath, UpdateCommandOptions)` prepares UpdatePlan via hash comparison, cleans orphaned `.sum`/`AGENTS.md` artifacts, analyzes changed files (Phase 1), regenerates `AGENTS.md` for `affectedDirs` (Phase 2), finalizes telemetry, exits with codes 0/1/2.\n\n**[specify.ts](./specify.ts)** — `specifyCommand(targetPath, SpecifyOptions)` synthesizes project spec from `AGENTS.md` corpus via AI, auto-invokes `generateCommand()` if docs missing, supports single/multi-file output (`--multi-file`), enforces 15min timeout and opus model default, exits with code 1 on `SpecExistsError` (conflicts), 2 on `CLI_NOT_FOUND`.\n\n**[rebuild.ts](./rebuild.ts)** — `rebuildCommand(targetPath, RebuildOptions)` reconstructs project from specs via `partitionSpec()` + `executeRebuild()`, enforces 15min timeout and opus default, supports checkpoint-based resumption via `CheckpointManager.load()`, writes generated code to `rebuild/` (or custom `--output`), exits with codes 0/1/2.\n\n**[clean.ts](./clean.ts)** — `cleanCommand(targetPath, { dryRun })` removes `.sum`, `.annex.md`, generated `AGENTS.md` (via `GENERATED_MARKER` detection), `CLAUDE.md`, `GENERATION-PLAN.md`, restores `AGENTS.local.md` → `AGENTS.md`, logs deletion counts with picocolors formatting.\n\n## Shared Option Types\n\n**GenerateOptions** (`generate.ts`, `rebuild.ts` reuses subset):\n```typescript\n{\n  dryRun?: boolean;       // Show plan without AI calls\n  concurrency?: number;   // Worker pool size (1-10)\n  failFast?: boolean;     // Abort on first failure\n  debug?: boolean;        // Verbose subprocess logging\n  trace?: boolean;        // NDJSON trace emission\n  model?: string;         // Override AI model\n}\n```\n\n**UpdateCommandOptions** (`update.ts`):\n```typescript\n{\n  uncommitted?: boolean;  // Include staged+working changes\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**SpecifyOptions** (`specify.ts`):\n```typescript\n{\n  output?: string;        // Custom spec file path (default: specs/SPEC.md)\n  force?: boolean;        // Overwrite existing files\n  dryRun?: boolean;\n  multiFile?: boolean;    // Split into per-directory specs\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (`rebuild.ts`):\n```typescript\n{\n  output?: string;        // Custom output directory (default: rebuild/)\n  force?: boolean;        // Wipe output dir and restart\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (`clean.ts`):\n```typescript\n{\n  dryRun?: boolean;       // Preview deletions without filesystem writes\n}\n```\n\n## Argument Parsing Protocol\n\n`parseArgs(args: string[])` in `index.ts` returns `{ command, positional, flags, values }`:\n- **Long flags**: `--key value` → `values['key'] = 'value'`, `--flag` → `flags.add('flag')`\n- **Short flags**: `-V` → `flags.add('version')`, `-h` → `flags.add('help')`, `-g` → `flags.add('global')`, `-l` → `flags.add('local')`\n- **Command**: First non-flag argument\n- **Positional**: Subsequent non-flag arguments after command\n- **Values map**: `--concurrency 3`, `--output ./spec.md`, `--model sonnet`, `--runtime claude`\n- **Installer detection**: `hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` value\n\n## Model Resolution Strategy\n\n**generate/update**: `options.model ?? config.ai.model` — CLI flag overrides config, no hardcoded default.\n\n**specify/rebuild**: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)` — upgrades sonnet → opus for quality-critical synthesis, respects explicit opus/haiku config.\n\n## Progress Tracking Infrastructure\n\nAll commands (except `init`, `clean`) create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`:\n- **Header format**: `=== ARE <Command> (${ISO-8601}) ===\\nProject: ${absolutePath}\\n...`\n- **Real-time monitoring**: `tail -f .agents-reverse-engineer/progress.log`\n- **Phase boundaries**: `=== Phase 1: File Analysis ===`, `=== Phase 2: Directory AGENTS.md ===`\n- **Task progress**: `[worker-0] Analyzing src/foo.ts (ETA: 2m 15s)`\n- **Summary line**: `Tokens: 12345 in / 6789 out | Duration: 45s | Exit: 0`\n- **Finalization**: `await progressLog.finalize()` before exit\n\n## Trace Integration\n\nWhen `--trace` flag present:\n- `createTraceWriter(absolutePath, true)` creates `.agents-reverse-engineer/traces/trace-<ISO-timestamp>.ndjson` writer\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner()`, `AIService()`\n- Emits events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs`\n- Finalization: `await tracer.finalize()` + `cleanupOldTraces(absolutePath)` keeps 500 most recent traces\n\n## Backend Resolution Flow\n\nAll commands except `init`, `discover`, `clean`:\n1. `createBackendRegistry()` → enumerates installed CLIs (Claude, Gemini, OpenCode)\n2. `resolveBackend(registry, config.ai.backend)` → throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found\n3. Catch block → `getInstallInstructions(registry)` prints installation commands, `process.exit(2)` (distinct from task failure code 1)\n\n## AIService Configuration\n\nInstantiation pattern (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min min for specify/rebuild\n  maxRetries: config.ai.maxRetries,\n  model: effectiveModel,\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }\n});\n\nif (options.debug) {\n  aiService.setDebug(true);\n  console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n  console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n}\n\nif (options.trace) {\n  const logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', timestamp);\n  aiService.setSubprocessLogDir(logDir);\n  console.error(pc.dim(`[debug] Subprocess logs: ${logDir}`));\n}\n```\n\n## Finalization Sequence\n\nStandard cleanup before exit (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nawait aiService.finalize(absolutePath);  // Write run log, enforce retention\nawait progressLog.finalize();            // Close progress.log stream\nawait tracer.finalize();                 // Close trace NDJSON stream\nif (options.trace) {\n  cleanupOldTraces(absolutePath);        // Keep 500 most recent traces\n}\n```\n\n## Exit Code Conventions\n\n- **0**: Success (all tasks completed or no tasks to process)\n- **1**: Partial failure (some tasks succeeded, some failed) OR file conflict (specify/clean) OR first-run detection (update)\n- **2**: Total failure (no tasks succeeded, only failures) OR AI CLI not found\n\n## Dry-Run Behavior\n\n**generate.ts**: Builds `ExecutionPlan` via `buildExecutionPlan()`, logs file/directory/root task counts, returns without AI backend resolution.\n\n**update.ts**: Prepares `UpdatePlan` via `orchestrator.preparePlan({ includeUncommitted, dryRun: true })`, logs changed files with status markers (M=modified, +=added, R=renamed), cleanup actions (deleted `.sum`, empty dir `AGENTS.md`), affected directories, returns without AI calls.\n\n**specify.ts**: Collects `AGENTS.md` + annex files, estimates tokens via `totalChars / 4 / 1000`, logs counts with cyan styling, warns if input exceeds 150K tokens or docs missing, returns without backend resolution.\n\n**rebuild.ts**: Reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads checkpoint via `CheckpointManager.load()`, logs unit count and checkpoint status (completed vs. pending modules), returns without AIService instantiation.\n\n**clean.ts**: Discovers artifacts via `fast-glob` (`.sum`, `.annex.md`, `AGENTS.md`, `AGENTS.local.md`), filters via `GENERATED_MARKER` substring search, logs deletion preview with picocolors formatting (yellow warning \"Dry run — no files were changed\"), returns without `unlink()` calls.\n\n## Error Handling Patterns\n\n**Directory access errors** (all commands):\n```typescript\nawait access(resolvedPath, constants.R_OK);\n// Catches ENOENT → 'Directory not found: ${path}', exit 1\n// Catches EACCES/EPERM → 'Permission denied: ${path}', exit 1\n```\n\n**Config load errors** (all commands except `init`):\n```typescript\nconst config = await loadConfig(absolutePath, { tracer, debug });\n// Throws on invalid YAML or schema validation failure\n// Caught by top-level try/catch in index.ts main()\n```\n\n**Backend resolution errors** (generate/update/specify/rebuild):\n```typescript\ntry {\n  backend = resolveBackend(registry, config.ai.backend);\n} catch (err) {\n  if (err instanceof AIServiceError && err.code === 'CLI_NOT_FOUND') {\n    console.error(pc.red('No AI CLI found...'));\n    console.error(getInstallInstructions(registry));\n    process.exit(2);\n  }\n  throw err;\n}\n```\n\n**File conflict errors** (specify):\n```typescript\ntry {\n  await writeSpec(...);\n} catch (err) {\n  if (err instanceof SpecExistsError) {\n    await progressLog.finalize();\n    console.error(pc.red(err.message));\n    process.exit(1);\n  }\n  throw err;\n}\n```\n\n## Dependencies\n\n**External**: `picocolors` (as `pc`), `node:path`, `node:fs/promises` (`access`, `readFile`, `rename`, `unlink`, `mkdir`, `readdir`), `node:fs` (`constants.F_OK/R_OK`).\n\n**Internal**:\n- **Config**: `src/config/loader.ts` (loadConfig, configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE)\n- **Discovery**: `src/discovery/run.ts` (discoverFiles)\n- **Generation**: `src/generation/orchestrator.ts` (createOrchestrator, GenerationPlan), `src/generation/executor.ts` (buildExecutionPlan, formatExecutionPlanAsMarkdown), `src/generation/collector.ts` (collectAgentsDocs, collectAnnexFiles), `src/generation/writers/agents-md.ts` (writeAgentsMd, GENERATED_MARKER), `src/generation/prompts/index.ts` (buildDirectoryPrompt)\n- **AI**: `src/ai/index.ts` (AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions)\n- **Orchestration**: `src/orchestration/index.ts` (CommandRunner, ProgressLog, ProgressReporter, createTraceWriter, cleanupOldTraces), `src/orchestration/trace.ts` (ITraceWriter)\n- **Update**: `src/update/index.ts` (createUpdateOrchestrator, UpdatePlan)\n- **Specify**: `src/specify/index.ts` (buildSpecPrompt, writeSpec, SpecExistsError)\n- **Rebuild**: `src/rebuild/index.ts` (readSpecFiles, partitionSpec, CheckpointManager, executeRebuild)\n- **Installer**: `src/installer/index.ts` (runInstaller, parseInstallerArgs)\n- **Output**: `src/output/logger.ts` (createLogger)\n- **Version**: `src/version.ts` (getVersion)\n- **Types**: `src/types/index.ts` (DiscoveryResult)\n### src/config/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# config\n\nExports YAML-based configuration loading with Zod validation, resource-adaptive concurrency calculation, and default constant definitions for gitignore patterns, vendor directories, binary extensions, and AI service parameters.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Defines ConfigSchema (Zod) with nested ExcludeSchema (patterns/vendorDirs/binaryExtensions arrays), OptionsSchema (followSymlinks boolean, maxFileSize number), OutputSchema (colors boolean), AISchema (backend enum, model string, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Exports Config, ExcludeConfig, OptionsConfig, OutputConfig, AIConfig types. Applies validation constraints: backend restricted to `'claude' | 'gemini' | 'opencode' | 'auto'`, timeoutMs requires positive integer, concurrency clamped to 1-20, maxFileSize requires positive integer, keepRuns requires non-negative integer.\n\n**[loader.ts](./loader.ts)** — Exports loadConfig(root, options?) reading `.agents-reverse-engineer/config.yaml`, parsing YAML, validating via ConfigSchema.parse(), emitting `config:loaded` trace event (configPath/model/concurrency fields), returning validated Config or defaults on ENOENT, wrapping ZodError/parse failures in ConfigError. Exports configExists(root) checking file accessibility via fs.access(). Exports writeDefaultConfig(root) generating commented YAML template with DEFAULT_EXCLUDE_PATTERNS/DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS arrays, yamlScalar() escaping special characters.\n\n**[defaults.ts](./defaults.ts)** — Exports getDefaultConcurrency() computing worker pool size via `clamp(os.availableParallelism() * CONCURRENCY_MULTIPLIER, MIN_CONCURRENCY, min(MAX_CONCURRENCY, memCap))` where memCap = `floor(os.totalmem() * MEMORY_FRACTION / SUBPROCESS_HEAP_GB)` with MEMORY_FRACTION=0.5, SUBPROCESS_HEAP_GB=0.512, CONCURRENCY_MULTIPLIER=5, MIN_CONCURRENCY=2, MAX_CONCURRENCY=20. Exports DEFAULT_VENDOR_DIRS (18 directories: node_modules/vendor/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini), DEFAULT_EXCLUDE_PATTERNS (26 patterns: AGENTS.md/CLAUDE.md/OPENCODE.md/GEMINI.md/lock files/dotfiles/logs/sum files), DEFAULT_BINARY_EXTENSIONS (26 extensions: images/archives/executables/media/documents/fonts/compiled files), DEFAULT_MAX_FILE_SIZE (1MB), DEFAULT_CONFIG (composite object spreading constants).\n\n## Data Flow\n\n1. CLI commands invoke loadConfig(root, { tracer, debug }) from loader.ts\n2. loader.ts reads `.agents-reverse-engineer/config.yaml`, parses YAML, validates via ConfigSchema.parse() from schema.ts\n3. schema.ts applies defaults from defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_EXCLUDE_PATTERNS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency())\n4. On validation failure, loader.ts extracts ZodError.issues[] and wraps in ConfigError with formatted message\n5. On success, loader.ts emits `config:loaded` trace event and returns Config object to CLI orchestrators\n6. writeDefaultConfig() generates YAML template by embedding DEFAULT_ constants and calling yamlScalar() for pattern escaping\n\n## Resource Adaptive Concurrency\n\ngetDefaultConcurrency() prevents RAM exhaustion by computing memory cap: on 4GB systems, `memCap = floor(4 * 0.5 / 0.512) = 3` limits concurrency to 3 workers despite CPU count * 5 formula suggesting higher values. SUBPROCESS_HEAP_GB synchronizes with `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for subprocess resource limiting. Formula `cores * CONCURRENCY_MULTIPLIER` optimized for I/O-bound AI subprocess workloads with high wait-to-compute ratios.\n\n## Error Handling Strategy\n\nloader.ts distinguishes three error cases: ENOENT errors return `ConfigSchema.parse({})` defaults without throwing, ZodError instances map issues[] to multi-line validation report wrapped in ConfigError, existing ConfigError instances re-thrown without modification. ConfigError extends Error adding filePath: string and optional cause: Error properties for stack trace preservation.\n\n## File System Contracts\n\n- Configuration directory: `.agents-reverse-engineer/`\n- Configuration file: `config.yaml` (YAML format with commented sections)\n- Telemetry logs: `.agents-reverse-engineer/logs/` (referenced by AISchema.telemetry.keepRuns)\n- Trace output: `.agents-reverse-engineer/traces/` (referenced by ITraceWriter interface)\n\n## Behavioral Contracts\n\n**YAML Special Character Escaping:**\nyamlScalar() quotes values matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` and escapes `\\\\` → `\\\\\\\\`, `\\\"` → `\\\\\\\"` within quoted strings.\n\n**Default Exclusion Patterns:**\n```typescript\nDEFAULT_EXCLUDE_PATTERNS = [\n  'AGENTS.md', 'CLAUDE.md', 'OPENCODE.md', 'GEMINI.md',\n  '**/AGENTS.md', '**/CLAUDE.md', '**/OPENCODE.md', '**/GEMINI.md',\n  '*.lock', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\n  'bun.lock', 'bun.lockb', 'Gemfile.lock', 'Cargo.lock',\n  'poetry.lock', 'composer.lock', 'go.sum',\n  '.gitignore', '.gitattributes', '.gitkeep', '.env', '**/.env', '**/.env.*',\n  '*.log', '*.sum', '**/*.sum', '**/SKILL.md'\n]\n```\n\n**Vendor Directory Exclusions:**\n```typescript\nDEFAULT_VENDOR_DIRS = [\n  'node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__',\n  '.next', 'venv', '.venv', 'target', '.cargo', '.gradle',\n  '.agents-reverse-engineer', '.agents', '.planning',\n  '.claude', '.opencode', '.gemini'\n]\n```\n\n**Binary Extension Detection:**\n```typescript\nDEFAULT_BINARY_EXTENSIONS = [\n  '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp',\n  '.zip', '.tar', '.gz', '.rar', '.7z',\n  '.exe', '.dll', '.so', '.dylib',\n  '.mp3', '.mp4', '.wav',\n  '.pdf',\n  '.woff', '.woff2', '.ttf', '.eot',\n  '.class', '.pyc'\n]\n```\n\n**Concurrency Formula:**\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n## Cross-Module Dependencies\n\n- **schema.ts** imports defaults.ts (DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency)\n- **loader.ts** imports schema.ts (ConfigSchema, Config type), defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency), `../orchestration/trace.js` (ITraceWriter type)\n### src/discovery/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery pipeline using fast-glob directory traversal with composable four-stage filter chain (gitignore, vendor, binary, custom) and bounded-concurrency short-circuit evaluation, exposing `discoverFiles()` facade for all CLI commands.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[run.ts](./run.ts)** — `discoverFiles(root, config, options?)` facade composes `walkDirectory()` with four-stage filter chain: `createGitignoreFilter()` parses `.gitignore`, `createVendorFilter()` excludes `node_modules`/`.git`/`dist` via `config.exclude.vendorDirs`, `createBinaryFilter()` applies 96-extension allowlist + `isBinaryFile()` fallback with `config.options.maxFileSize` threshold, `createCustomFilter()` processes user glob patterns from `config.exclude.patterns`. Returns `FilterResult` with `included: string[]` and `excluded: Array<{file, reason, filter}>`. Exports `DiscoveryConfig` interface (structural subset of `Config`) and `DiscoverFilesOptions` (`tracer?: ITraceWriter`, `debug?: boolean`).\n\n**[walker.ts](./walker.ts)** — `walkDirectory(options)` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `suppressErrors: true`, hardcoded `ignore: ['**/.git/**']`, and `followSymbolicLinks: options.followSymlinks ?? false`. Requires `WalkerOptions` with `cwd: string` root, optional `dot?: boolean` (default `true` for dotfiles), `followSymlinks?: boolean` (default `false`). Returns raw `string[]` file paths without filtering logic (deferred to filter chain).\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` (`included[]`, `excluded[]`), `WalkerOptions` (`cwd`, `followSymlinks?`, `dot?`).\n\n## Subdirectory\n\n**[filters/](./filters/)** — Five modules implementing `FileFilter` contract: `gitignore.ts` (`createGitignoreFilter()` async factory with `ignore` library), `binary.ts` (96-extension set + `isBinaryFile()` content analysis), `vendor.ts` (single-segment Set + path-pattern array matching), `custom.ts` (gitignore-style glob parsing), `index.ts` (`applyFilters()` with 30-worker bounded concurrency, short-circuit evaluation, `filter:applied` trace emission).\n\n## Filter Chain Architecture\n\n`discoverFiles()` instantiates filters in priority order (gitignore → vendor → binary → custom), passes to `applyFilters()` which runs filters sequentially per file until `shouldExclude()` returns `true`. Concurrency pool prevents file descriptor exhaustion during `isBinaryFile()` I/O. Filters receive absolute paths; gitignore/custom filters convert to relative via `path.relative()` before pattern matching.\n\n## Configuration Surface\n\n- `config.exclude.vendorDirs: string[]` — directory names like `node_modules` (default 10 entries)\n- `config.exclude.binaryExtensions: string[]` — additional extensions beyond 96-entry `BINARY_EXTENSIONS` set\n- `config.exclude.patterns: string[]` — gitignore-style globs processed by `ignore` library\n- `config.options.maxFileSize: number` — binary detection threshold (default 1MB)\n- `config.options.followSymlinks: boolean` — symlink traversal toggle (default `false`)\n\n## Integration Points\n\nConsumed by `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` via `discoverFiles()` facade. Imports `ITraceWriter` from `../orchestration/trace.js` for `filter:applied` event emission. Filter factories depend on `ignore` library (gitignore parsing), `isbinaryfile` library (content analysis), `fast-glob` (directory traversal), `node:fs` (Stats objects).\n\n## Behavioral Contracts\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS`: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Binary Extension Set\n96 extensions including: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm` (full list in `filters/binary.ts`)\n\n### Concurrency Limit\n`applyFilters()` uses `CONCURRENCY = 30` workers via iterator-based pool pattern from `src/orchestration/pool.ts`\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```\n### src/discovery/filters/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters for discovery pipeline: gitignore pattern matching, binary file detection, vendor directory exclusion, and custom glob rules with short-circuit evaluation and bounded-concurrency application.\n\n## Contents\n\n### Filter Orchestration\n\n**[index.ts](./index.ts)** — `applyFilters(files, filters, options)` executes filter chain with short-circuit evaluation (stops at first exclusion) using 30-worker bounded concurrency pool to prevent file descriptor exhaustion during `isBinaryFile()` I/O operations. Returns `FilterResult` with `included[]` and `excluded[]` arrays, emits `filter:applied` trace events with per-filter `filesMatched`/`filesRejected` counts. Re-exports all filter factory functions.\n\n### Filter Implementations\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter(root)` async factory reads `.gitignore`, parses via `ignore` library, returns `FileFilter` that converts absolute paths to relative before calling `ig.ignores()`. Returns `false` for paths outside root tree (relative path starts with `..`).\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter(options?)` constructs fast-path extension lookup in `BINARY_EXTENSIONS` set (96 extensions: `.png`, `.exe`, `.mp4`, `.pdf`, `.woff`, `.class`, `.db`, etc.) with fallback to `isBinaryFile()` content analysis. Excludes files exceeding `maxFileSize` (default 1MB via `DEFAULT_MAX_FILE_SIZE`). Returns `true` on `fs.stat()` errors.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter(vendorDirs)` partitions input into single-segment Set (e.g., `node_modules`) for O(1) membership testing and path-pattern array (e.g., `.agents/skills`) for substring matching. `DEFAULT_VENDOR_DIRS` contains `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter(patterns, root)` parses gitignore-style glob patterns via `ignore` library, converts absolute paths to relative before evaluation. Returns `false` immediately for empty patterns array or paths outside root.\n\n## Concurrency Strategy\n\n`applyFilters()` uses iterator-based worker pool with `CONCURRENCY = 30` to share single `files.entries()` iterator across workers. Each file runs through `filters[]` sequentially until `shouldExclude()` returns `true` (short-circuit). BinaryFilter calls synchronous `isBinaryFile()`, making concurrency bounds critical to avoid ulimit violations.\n\n## Filter Contract\n\nAll factories return `FileFilter` interface from `../types.js`:\n- `name: string` — filter identifier for logging\n- `shouldExclude(absolutePath: string): boolean` — exclusion predicate\n\n## Behavioral Contracts\n\n### Binary Extension Set\n`BINARY_EXTENSIONS` (96 entries): `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`, `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`, `.db`, `.sqlite`, `.sqlite3`, `.mdb`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS` (10 entries): `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Path Normalization\n- Gitignore filter: `path.relative(normalizedRoot, absolutePath)` before `ig.ignores()`\n- Vendor filter: `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling\n- Binary filter: `path.extname(absolutePath).toLowerCase()` for extension extraction\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```\n### src/generation/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates ARE's three-phase documentation generation pipeline: Phase 1 concurrent file analysis via worker pools generating `.sum` files with SHA-256 hashes, Phase 2 post-order directory aggregation synthesizing `AGENTS.md` from child summaries, Phase 3 sequential root document synthesis producing `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` from complete corpus.**\n\n## Contents\n\n### Pipeline Coordination\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` class exports `createPlan()` method constructing `GenerationPlan` via four-step workflow: `prepareFiles()` loads file content into `PreparedFile[]`, `analyzeComplexity()` computes `directoryDepth` and unique `directories` set, `buildProjectStructure()` formats compact directory tree, `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()`, `createDirectoryTasks()` groups files by directory returning directory-level tasks. Emits trace events (`phase:start`, `plan:created`, `phase:end`) via injected `ITraceWriter`. Clears `PreparedFile.content` after prompt embedding to free heap memory. Returns `{ files, tasks, complexity, projectStructure }`.\n\n**[executor.ts](./executor.ts)** — Transforms `GenerationPlan` into dependency-aware `ExecutionPlan` via `buildExecutionPlan()`: groups files by directory into `directoryFileMap`, creates `fileTasks[]` with `id: 'file:${path}'` and empty `dependencies[]`, sorts by depth descending for leaf-first processing, creates `directoryTasks[]` depending on child file task IDs for post-order traversal, creates `rootTasks[]` depending on all directory task IDs. Exports `isDirectoryComplete()` predicate checking child `.sum` file existence via `sumFileExists()` and `getReadyDirectories()` async filter. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with three-phase checklist grouped by directory depth.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` via `calculateDirectoryDepth()` (max depth via `split(sep).length - 1`) and `extractDirectories()` (unique directories via upward `dirname()` traversal). Returns `{ fileCount, directoryDepth, files, directories }` consumed by orchestrator for concurrency tuning and Phase 2 directory queue construction.\n\n**[collector.ts](./collector.ts)** — Exports `collectAgentsDocs()` recursively walking project tree collecting `AGENTS.md` files as `AgentsDocs` array of `{ relativePath, content }` sorted alphabetically, and `collectAnnexFiles()` similarly collecting `.annex.md` files. Both skip `SKIP_DIRS` set (13 entries: node_modules, .git, vendor, dist, build, etc.) and silently suppress permission errors.\n\n**[types.ts](./types.ts)** — Defines `AnalysisResult` containing `summary: string` and `metadata: SummaryMetadata` returned by Phase 1 AI subprocess calls, `SummaryMetadata` YAML frontmatter schema with `purpose`, `criticalTodos?`, `relatedFiles?` fields, and `SummaryOptions` for summary verbosity configuration.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Template-based prompt construction pipeline: `buildFilePrompt()` injects file path/content/imports into `FILE_USER_PROMPT` with density rules and identifier preservation constraints, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` with manifest detection (9 types) and import maps via `extractDirectoryImports()`, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus with synthesis-only constraints prohibiting invented features. Exports six prompt constants with mustache-style placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`), prohibited filler phrases, YAML frontmatter format, and annex reference format.\n\n**[writers/](./writers/)** — YAML frontmatter-based file I/O layer: `writeSumFile()`/`readSumFile()` implement `.sum` persistence with SHA-256 `content_hash` via regex-based field extraction and dual-format YAML array handling (inline `[a,b,c]` vs multi-line), `writeAgentsMd()` preserves user-authored `AGENTS.md` by renaming to `AGENTS.local.md` and prepending above generated content with `GENERATED_MARKER` injection/stripping, `writeAnnexFile()` archives verbatim source for reproduction-critical files (prompt templates, config schemas). Exports `sumFileExists()` predicate for change detection and `isGeneratedAgentsMd()` marker detection.\n\n## Three-Phase Execution Strategy\n\n**Phase 1: Concurrent File Analysis**\n- Orchestrator creates `fileTasks[]` with prompts via `buildFilePrompt()` embedding import maps and project structure\n- Runner spawns worker pool (`src/orchestration/pool.ts`) executing tasks concurrently (default concurrency: 2 for WSL, 5 elsewhere)\n- Each worker calls `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`--max-old-space-size=512`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`)\n- Worker writes `AnalysisResult` via `writeSumFile()` with YAML frontmatter containing SHA-256 `content_hash` and markdown summary body\n\n**Phase 2: Post-Order Directory Aggregation**\n- Executor sorts `directoryTasks[]` by depth descending (deepest first) via `getDirectoryDepth()` ensuring child directories complete before parents\n- Runner sequentially processes directories checking readiness via `isDirectoryComplete()` predicate polling for child `.sum` file existence\n- Prompt builder calls `buildDirectoryPrompt()` reading child `.sum` files via `readSumFile()`, aggregating subdirectory `AGENTS.md`, extracting imports via `extractDirectoryImports()`, detecting manifests (9 types: package.json, Cargo.toml, go.mod, etc.)\n- Runner writes `AGENTS.md` via `writeAgentsMd()` preserving any `AGENTS.local.md` user content above generated sections\n\n**Phase 3: Sequential Root Synthesis**\n- Executor creates `rootTasks[]` depending on all directory task IDs enforcing sequential execution (concurrency=1)\n- Prompt builder calls `buildRootPrompt()` consuming all `AGENTS.md` files via `collectAgentsDocs()`, reading root `package.json` metadata, embedding synthesis constraints prohibiting invented features\n- Runner writes platform-specific root documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) via `src/integration/generate.ts`\n\n## Post-Order Traversal Mechanism\n\nExecutor sorts directory tasks by depth descending:\n```typescript\ndirectoryTasks.sort((a, b) => \n  getDirectoryDepth(b.path) - getDirectoryDepth(a.path)\n)\n```\n\nwhere `getDirectoryDepth('.')` returns `0`, `getDirectoryDepth('src')` returns `1`, `getDirectoryDepth('src/cli')` returns `2`. Deepest directories process first ensuring child `AGENTS.md` exist before parent aggregation attempts. Runner polls `isDirectoryComplete()` checking all expected `.sum` files exist via `sumFileExists()` before processing directory task.\n\n## Memory Management Pattern\n\nOrchestrator clears `PreparedFile.content` after prompt construction:\n```typescript\nfor (const file of files) {\n  (file as { content: string }).content = ''\n}\n```\n\nThis frees heap memory since file content already embedded in `AnalysisTask.userPrompt` strings. Runner re-reads files from disk during execution if needed. Prevents memory exhaustion on large codebases (10k+ files).\n\n## Integration Points\n\nConsumes:\n- `DiscoveryResult` from `src/discovery/walker.ts` (file list input)\n- `Config` from `src/config/schema.ts` (concurrency, timeout, model settings)\n- `ITraceWriter` from `src/orchestration/trace.ts` (event emission)\n- `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt` from `./prompts/builder.ts`\n- `writeSumFile`, `writeAgentsMd` from `./writers/`\n- `extractDirectoryImports` from `src/imports/extractor.ts`\n- `collectAgentsDocs` from `./collector.ts`\n\nProduces:\n- `GenerationPlan` consumed by `src/orchestration/runner.ts`\n- `ExecutionPlan` consumed by Phase 1/2/3 execution loops\n- `.sum` files consumed by `src/update/orchestrator.ts` for change detection\n- `AGENTS.md` files consumed by Phase 3 root synthesis and `src/specify/index.ts`\n- `GENERATION-PLAN.md` consumed by progress tracking\n\nReferenced by:\n- `src/cli/generate.ts` (command entry point)\n- `src/cli/update.ts` (incremental update workflow)\n- `src/orchestration/runner.ts` (phase execution orchestrator)\n\n## Behavioral Contracts\n\n### Depth Calculation (executor.ts)\n```typescript\ngetDirectoryDepth('.')          → 0\ngetDirectoryDepth('src')        → 1\ngetDirectoryDepth('src/cli')    → 2\ngetDirectoryDepth('a/b/c/d')    → 4\n```\n\n### File Task Dependencies (executor.ts)\n```typescript\nfileTasks.forEach(task => task.dependencies = [])  // No dependencies, all parallel\n```\n\n### Directory Task Dependencies (executor.ts)\n```typescript\ndirectoryTask.dependencies = directoryFileMap[dirPath].map(f => `file:${f}`)\n```\n\n### Root Task Dependencies (executor.ts)\n```typescript\nrootTask.dependencies = directoryTasks.map(t => t.id)  // All directories\n```\n\n### SKIP_DIRS Set (collector.ts)\n```typescript\n['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', \n 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle']\n```\n\n### Manifest Detection Array (prompts/builder.ts)\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n### Language Detection Map (prompts/builder.ts)\n```typescript\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n## Annex References\n\n- Full prompt template text: [prompts/templates.ts.annex.md](./prompts/templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [prompts/types.ts.annex.md](./prompts/types.ts.annex.md)\n- Phase 2/3 execution workflow details: [../orchestration/runner.ts.annex.md](../orchestration/runner.ts.annex.md)\n### src/generation/prompts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# prompts\n\nExports prompt construction pipeline for ARE's three-phase documentation generation: `buildFilePrompt()` constructs Phase 1 file analysis prompts with import maps and project structure trees, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` for Phase 2 directory synthesis, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus in Phase 3.\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements prompt builders via template interpolation: `buildFilePrompt()` injects `filePath`/`content`/detected language (via `detectLanguage()`) into `FILE_USER_PROMPT`, appending `contextFiles[]` and optional `projectPlan`. `buildDirectoryPrompt()` aggregates `.sum` files via `readSumFile()`, child `AGENTS.md` from subdirectories, import maps via `extractDirectoryImports()`, and manifest files (9 types: package.json, Cargo.toml, go.mod, etc.), returning `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided. `buildRootPrompt()` collects all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json` metadata, embeds synthesis constraints prohibiting invented features. Returns `{system, user}` prompt pairs with incremental update system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when existing content provided.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` for AI service call construction.\n\n### [templates.ts](./templates.ts)\nExports six prompt constants: `FILE_SYSTEM_PROMPT` (density rules, identifier preservation, behavioral contract verbatim reproduction), `FILE_USER_PROMPT` (contains `{{FILE_PATH}}`, `{{CONTENT}}` placeholders), `DIRECTORY_SYSTEM_PROMPT` (adaptive section strategy, path accuracy constraints, annex linking), `FILE_UPDATE_SYSTEM_PROMPT` (preserve unchanged sections verbatim), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (modify only affected entries), `ROOT_SYSTEM_PROMPT` (synthesis-only constraint: no invented features). Defines prohibited filler phrases (\"this file\", \"provides\", \"responsible for\"), annex reference format, YAML frontmatter structure for `.sum` files.\n\n### [types.ts](./types.ts)\nDefines `PromptContext` interface (fields: `filePath`, `content`, `contextFiles?`, `projectPlan?`, `existingSum?`) and `SUMMARY_GUIDELINES` constant specifying target word count (300-500), 8 required content categories (purpose, public interface, patterns, dependencies, signatures, related files, behavioral contracts, annex references), 3 excluded categories (control flow, generic TODOs, broad architecture).\n\n## Behavioral Contracts\n\n**Mustache-Style Placeholder Substitution** (from templates.ts):\n```\n{{FILE_PATH}}  — Replaced with source file relative path\n{{CONTENT}}    — Replaced with source file content (unescaped)\n{{LANG}}       — Replaced with detected language identifier\n{{PROJECT_PLAN_SECTION}} — Replaced with project structure tree or empty string\n```\n\n**Prohibited Filler Phrases** (from templates.ts):\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**YAML Frontmatter Format** (from templates.ts):\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Language Detection Map** (from builder.ts):\n```\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n**Annex Reference Format** (from templates.ts):\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n**Manifest Detection Array** (from builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n## Annex References\n\n- Full prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [types.ts.annex.md](./types.ts.annex.md)\n\n## File Relationships\n\nbuilder.ts depends on templates.ts constants (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, etc.) for interpolation, calls `detectLanguage()` for syntax highlighting identifiers, reads `.sum` files via `readSumFile()` (from `../writers/sum.js`), checks `GENERATED_MARKER` (from `../writers/agents-md.js`) to distinguish user-authored `AGENTS.md`, extracts imports via `extractDirectoryImports()` + `formatImportMap()` (from `../../imports/index.js`), collects root corpus via `collectAgentsDocs()` (from `../collector.js`). types.ts defines `PromptContext` interface consumed by all builder functions. index.ts re-exports public API consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts`.\n### src/generation/writers/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n**YAML frontmatter-based file I/O layer implementing `.sum` file persistence with SHA-256 hashing, `AGENTS.md` generation with user content preservation, and `.annex.md` verbatim source archival for reproduction-critical artifacts.**\n\n## Contents\n\n### Core Writers\n\n**[sum.ts](./sum.ts)** — SHA-256-tracked `.sum` file I/O with YAML frontmatter serialization via `writeSumFile()`/`readSumFile()`/`formatSumFile()`/`parseSumFile()`, regex-based field extraction (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), dual-format YAML array handling (inline `[a,b,c]` vs multi-line `  - item`) via `parseYamlArray()`/`formatYamlArray()`, `.annex.md` verbatim source archival via `writeAnnexFile()` for reproduction-critical files, path resolution via `getSumPath()`/`getAnnexPath()`.\n\n**[agents-md.ts](./agents-md.ts)** — `AGENTS.md` preservation logic via `writeAgentsMd()` four-step workflow: (1) detects user-authored files lacking `GENERATED_MARKER` and renames to `AGENTS.local.md`, (2) reads preserved `AGENTS.local.md` content, (3) strips marker prefix from LLM content, (4) assembles final output with marker header + user content block (`<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter) + horizontal rule + LLM content. Exports `isGeneratedAgentsMd()` marker detection predicate.\n\n**[index.ts](./index.ts)** — Barrel re-exporting `writeSumFile`/`readSumFile`/`getSumPath`/`sumFileExists`/`SumFileContent` from sum.ts and `writeAgentsMd` from agents-md.ts for unified import in `src/generation/executor.ts`.\n\n## File Naming Conventions\n\n- **Summary files**: `<sourcePath>.sum` (e.g., `foo.ts` → `foo.ts.sum`)\n- **Annex files**: `<sourcePath>.annex.md` (e.g., `foo.ts` → `foo.ts.annex.md`)\n- **Preserved user docs**: `AGENTS.md` → `AGENTS.local.md` (renamed on first generation)\n\n## YAML Frontmatter Structure\n\n`.sum` files use YAML frontmatter block delimited by `---\\n...\\n---\\n`:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex digest)\npurpose: One-line file purpose statement\ncritical_todos: [Security issue, Performance bottleneck]\nrelated_files: [../config/schema.ts, ./index.ts]\n---\n\nMarkdown summary content...\n```\n\n**Field serialization rules** (via `formatYamlArray()`):\n- Arrays with ≤3 items where all items <40 chars: inline format `key: [a, b, c]`\n- Otherwise: multi-line format with `  - ` prefix per item\n\n**Parsing patterns** (via `parseSumFile()`):\n- Frontmatter block extraction: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`\n- Arrays: inline `/key:\\s*\\[([^\\]]*)\\]/`, multi-line `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n## User Content Preservation Strategy\n\n`writeAgentsMd()` implements two-path detection for existing `AGENTS.md` files:\n\n1. **First-time generation**: If existing `AGENTS.md` lacks `GENERATED_MARKER` (via `isGeneratedAgentsMd()` substring search), rename to `AGENTS.local.md` to preserve user content\n2. **Subsequent runs**: Read `AGENTS.local.md` if already exists from prior rename operation\n\nFinal assembly concatenates:\n1. `GENERATED_MARKER` constant: `'<!-- Generated by agents-reverse-engineer -->'`\n2. User content block (if present): `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\\n<content>\\n---`\n3. LLM-generated content (with marker prefix stripped if present)\n\nEnsures AI assistants see user-defined directory context before generated summaries during codebase navigation.\n\n## Annex File Pattern\n\n**Purpose**: Archive verbatim source content for reproduction-critical files (e.g., prompt templates with regex patterns, config schemas with magic constants) whose behavioral contracts cannot fit within `.sum` word limits.\n\n**Generated format** (via `writeAnnexFile()`):\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<triple-backtick code fence with full source content>\n```\n```\n\n**Reference pattern**: `.sum` files include `## Annex References` section linking to annex: `[templates.ts.annex.md](./templates.ts.annex.md)`.\n\n## Integration with Generation Pipeline\n\n**Phase 1 (File Analysis)**:\n- `src/generation/executor.ts` calls `writeSumFile()` after LLM subprocess returns `SumFileContent`\n- `writeSumFile()` creates `.sum` alongside source file via `mkdir({ recursive: true })`\n- Optional `writeAnnexFile()` call for files flagged as reproduction-critical\n\n**Phase 2 (Directory Aggregation)**:\n- `src/generation/executor.ts` reads child `.sum` files via `readSumFile()` for aggregation prompts\n- After LLM generates directory summary, calls `writeAgentsMd()` to assemble final `AGENTS.md`\n- `writeAgentsMd()` preserves any existing `AGENTS.local.md` content above generated sections\n\n**Phase 3 (Root Synthesis)**:\n- Root document generators consume all `AGENTS.md` files via `src/generation/collector.ts`\n- No direct writer interaction (root docs written by `src/integration/generate.ts`)\n\n## Change Detection Integration\n\n`readSumFile()` exposes `contentHash` field parsed from YAML frontmatter, consumed by `src/update/orchestrator.ts` for SHA-256-based incremental update detection:\n\n1. Read current file content hash via `src/change-detection/detector.ts`\n2. Read `.sum` file's stored `contentHash` via `readSumFile()`\n3. Hash mismatch → add to regeneration queue\n4. Hash match → skip (file unchanged)\n\n## Error Handling\n\n- **`readSumFile()`**: Returns `null` on missing file or parse failure (no exceptions thrown)\n- **`sumFileExists()`**: Returns `false` if `readSumFile()` returns `null`\n- **`isGeneratedAgentsMd()`**: Returns `false` on read errors (treats missing files as non-generated)\n- **`writeAgentsMd()`**: Silent try-catch blocks for existing `AGENTS.md` / `AGENTS.local.md` reads, treats absence as `null` user content\n- **`writeSumFile()` / `writeAnnexFile()`**: No error handling—writeFile failures bubble to caller\n\n## Behavioral Contracts\n\n### Regex Patterns (sum.ts)\n\n**Frontmatter extraction**:\n- Block delimiter: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**Value normalization**:\n- Quote stripping: `/^[\"']|[\"']$/g` (removes leading/trailing quotes from inline array items)\n- Leading newline removal: `/^\\n+/` (strips marker prefix from LLM content)\n\n### Constants\n\n**GENERATED_MARKER** (agents-md.ts): `'<!-- Generated by agents-reverse-engineer -->'` — marker for tool-generated `AGENTS.md` detection, referenced by `isGeneratedAgentsMd()` substring search and `writeAgentsMd()` marker injection/stripping.\n### src/imports/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Extracts and classifies TypeScript/JavaScript import statements from source files via regex-based parsing, producing structured import maps with internal/external partitioning for LLM prompt integration during `.sum` generation and directory aggregation phases.**\n\n## Contents\n\n### Core Implementation\n\n**[extractor.ts](./extractor.ts)** — Parses source text via `IMPORT_REGEX` to extract import declarations with `extractImports()` returning `ImportEntry[]` (specifier, symbols, typeOnly), aggregates directory-level imports via `extractDirectoryImports()` reading first 100 lines per file with relative specifier filtering (`.` or `..` prefix) and internal/external classification, formats import maps for LLM prompts via `formatImportMap()` producing structured `fileName:\\n  specifier → symbol1, symbol2 (type)` output.\n\n**[types.ts](./types.ts)** — Defines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` fields representing single import statements, and `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` fields partitioning imports by external (parent directories/npm packages) versus internal (same-directory sibling files) origin.\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` functions and `ImportEntry`, `FileImports` types from implementation modules for single-point import surface consumed by prompt builders.\n\n## Import Extraction Strategy\n\n**Regex Pattern:** `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` with capture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nAnchored with `^` to match only lines starting with `import`, avoiding dynamic imports/comments/string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset for multi-import files.\n\n**Classification Logic:**\n- **Internal:** Specifier starts with `./` (same directory)\n- **External:** Specifier starts with `../` (parent directory)\n- **Excluded:** Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nApplied via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before internal/external partitioning in `extractDirectoryImports()`.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing in `extractDirectoryImports()`, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Silently skips unreadable files via empty catch block.\n\n## Integration Points\n\nConsumed by `../generation/prompts/builder.ts`:\n- `buildFilePrompt()` includes import context from `extractDirectoryImports()` to inform LLM about cross-file dependencies when generating `.sum` summaries\n- `buildDirectoryPrompt()` embeds `formatImportMap()` output in Phase 2 directory aggregation prompts to show module coupling within `AGENTS.md` synthesis\n\nOutput format embeds in prompt templates defined in `../generation/prompts/templates.ts` for structured import relationship documentation.\n\n## Behavioral Contracts\n\n**Import Regex Pattern (IMPORT_REGEX):**\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n**Alias Stripping Pattern:**\n```\n.replace(/\\s+as\\s+\\w+/, '')\n```\n\n**Relative Specifier Filter:**\n```javascript\n.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))\n```\n\n**Import Map Format Template:**\n```\nfileName:\n  specifier → symbol1, symbol2 (type)\n```\n\n**First N Lines Slice:**\n```javascript\ncontent.split('\\n').slice(0, 100).join('\\n')\n```\n### src/installer/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer/\n\n**npx-driven installer orchestrating IDE command/hook deployment across Claude Code, OpenCode, and Gemini CLI with interactive prompts, platform-specific settings.json registration, detached version-check processes, and recursive empty directory cleanup.**\n\n## Contents\n\n### [banner.ts](./banner.ts)\nASCII art rendering and terminal output styling. `displayBanner()` renders 7-line \"ARE\" logo with version string, `showHelp()` prints usage with cyan-highlighted examples, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` output prefixed symbols (`✓`/`✗`/`!`/`>`), `showNextSteps()` displays numbered command list with `/are-*` shortcuts.\n\n### [index.ts](./index.ts)\nMain entry point orchestrating install/uninstall workflow. `runInstaller()` parses CLI args, displays banner, prompts for missing runtime/location in interactive mode, enforces non-interactive requirements (`--runtime` + `-g/-l` flags mandatory when `!isInteractive()`), dispatches to `runInstall()`/`runUninstall()`, aggregates results. `parseInstallerArgs()` handles short/long flags (`-g`/`--global`, `-h`/`--help`).\n\n### [operations.ts](./operations.ts)\nFile copying, hook registration, permission configuration. `installFilesForRuntime()` writes command templates to `${basePath}/${relativePath}`, copies hooks from bundled `hooks/dist/`, calls `registerClaudeHooks()`/`registerGeminiHooks()` with nested `HookEvent.hooks[]` or flat `GeminiHook[]` schemas, adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` file. `getPackageVersion()` reads package.json via `fileURLToPath(import.meta.url)`.\n\n### [paths.ts](./paths.ts)\nCross-platform path resolution with environment overrides. `getRuntimePaths()` returns `{ global, local, settingsFile }` applying `CLAUDE_CONFIG_DIR`/`OPENCODE_CONFIG_DIR`/`GEMINI_CONFIG_DIR` precedence, `resolveInstallPath()` joins project root with local paths (`.claude`/`.opencode`/`.gemini`), `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` check directory existence via `stat()`.\n\n### [prompts.ts](./prompts.ts)\nInteractive selection with arrow-key navigation or numbered fallback. `arrowKeySelect()` enables raw mode (`process.stdin.setRawMode(true)`), listens for `key.name === 'up'|'down'|'return'`, re-renders with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line). `numberedSelect()` prints 1-indexed list for non-TTY. `cleanupRawMode()` registered on `process.on('exit')` and `SIGINT`.\n\n### [types.ts](./types.ts)\nInterface definitions for installer workflow. `InstallerArgs` with `runtime?: Runtime`, `global/local/uninstall/force/help/quiet: boolean`. `InstallerResult` with `success`, `runtime: Exclude<Runtime, 'all'>`, `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered?`, `versionWritten?`. `RuntimePaths` with `global/local/settingsFile` strings.\n\n### [uninstall.ts](./uninstall.ts)\nArtifact removal and hook deregistration. `uninstallFilesForRuntime()` deletes command templates, hooks/plugins, `ARE-VERSION` file, calls `unregisterClaudeHooks()`/`unregisterGeminiHooks()` filtering `settings.json` by `getHookPatterns()` (current + legacy formats), `unregisterPermissions()` removes `ARE_PERMISSIONS` from Claude `permissions.allow[]`. `cleanupEmptyDirs()` recursively removes directories via `rmdirSync()`. `cleanupLegacyGeminiFiles()` deletes pre-TOML `are-*.md` and old TOML subdirectories.\n\n## Installation Workflow\n\n**Interactive mode (TTY):** `runInstaller()` displays banner → prompts for runtime via `selectRuntime()` (options: claude/opencode/gemini/all) → prompts for location via `selectLocation()` (global: `~/.claude`, local: `./.claude`) → confirms action → dispatches.\n\n**Non-interactive mode (CI):** requires `--runtime <value>` and `-g`/`-l` flags, exits with error via `showError()` + `process.exit(1)` if missing.\n\n**Installation:** `runInstall()` calls `installFiles()` → `installFilesForRuntime()` copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}` (path component after runtime prefix), reads bundled hooks from `hooks/dist/${filename}` via `readBundledHook()`, writes to `${basePath}/hooks/` or `${basePath}/plugins/`, updates `settings.json` with hook/permission entries, writes `ARE-VERSION` from `getPackageVersion()`.\n\n**Uninstallation:** `runUninstall()` calls `uninstallFiles()` → `uninstallFilesForRuntime()` deletes files, unregisters hooks by filtering `settings.json` arrays with `getHookPatterns()` (matches current `node ${runtimeDir}/hooks/${filename}` and legacy `node hooks/${filename}` formats), removes permissions, deletes empty directories via `cleanupEmptyDirs()`, calls `deleteConfigFolder()` for local installs.\n\n## Settings.json Hook Registration\n\n**Claude Code format (nested structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"node ~/.claude/hooks/are-check-update.js\" }\n        ]\n      }\n    ]\n  }\n}\n```\nSchema: `SettingsJson.hooks[event]` contains `HookEvent[]` where each `HookEvent` has `hooks: SessionHook[]` array with `{ type: 'command', command: string }` entries.\n\n**Gemini CLI format (flat structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      { \"name\": \"are-check-update\", \"type\": \"command\", \"command\": \"node ~/.gemini/hooks/are-check-update.js\" }\n    ]\n  }\n}\n```\nSchema: `GeminiSettingsJson.hooks[event]` contains flat `GeminiHook[]` array with `{ name: string, type: 'command', command: string }` entries.\n\n**OpenCode (plugin system):** Copies `ARE_PLUGINS` files (`opencode-are-check-update.js` → `are-check-update.js`) to `${basePath}/plugins/`, sets `hookRegistered = true` without settings.json modification.\n\n## Behavioral Contracts\n\n**Hook definitions (operations.ts):**\n- `ARE_HOOKS: HookDefinition[]` currently empty (commented: \"causing issues\")\n- Intended entries: `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**Plugin definitions (operations.ts):**\n- `ARE_PLUGINS: PluginDefinition[]` with one active entry: `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- Commented: `{ srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }`\n\n**Permission patterns (operations.ts):**\n```javascript\nARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)'\n]\n```\n\n**Bundled hook paths:** `__dirname/../../hooks/dist/${hookName}` where `__dirname` from `fileURLToPath(import.meta.url)`, built via `scripts/build-hooks.js` during `npm run build:hooks`.\n\n**Environment variable precedence:**\n- Claude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- OpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` overrides `~/.config/opencode`\n- Gemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n**ANSI escape sequences (prompts.ts):**\n- `\\x1b[${n}A` — cursor up n lines\n- `\\x1b[1B` — cursor down 1 line\n- `\\x1b[2K` — clear entire line\n\n**Keypress matching (prompts.ts):** `key.name === 'up' | 'down' | 'return'`, interrupt via `key.ctrl && key.name === 'c'`.\n\n## File System Conventions\n\n**Template path extraction:** Remove runtime prefix via `template.path.split('/').slice(1).join('/')`, e.g., `.claude/commands/are/generate.md` → `commands/are/generate.md`.\n\n**Installation paths:**\n- Commands: `${basePath}/${relativePath}`\n- Hooks: `${basePath}/hooks/${filename}` (Claude/Gemini)\n- Plugins: `${basePath}/plugins/${destFilename}` (OpenCode)\n- Settings: `${basePath}/settings.json`\n- Version: `${basePath}/ARE-VERSION`\n\n**Config folder:** `.agents-reverse-engineer` deleted only for local uninstalls via `deleteConfigFolder(location === 'local', dryRun)`.\n\n## Error Handling\n\nFile write failures append to `errors[]` with format `\"Failed to write ${fullPath}: ${err}\"`. JSON parse failures in hook registration silently reset settings to `{}`. Missing bundled hooks throw Error in `readBundledHook()`. Directory cleanup ignores errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` for graceful missing path handling. `InstallerResult.success = errors.length === 0`.\n\n## Dependencies\n\n**Core modules:** `node:os` (homedir), `node:path` (join/dirname), `node:fs` (writeFileSync/readFileSync/unlinkSync/mkdirSync/readdirSync/rmdirSync/rmSync/existsSync), `node:fs/promises` (stat), `node:url` (fileURLToPath).\n\n**External libraries:** `picocolors` (ANSI colors: green/red/yellow/cyan/dim/bold), `readline` (keypress events/interface creation).\n\n**Internal imports:**\n- `../version.js` → getVersion (banner.ts)\n- `../integration/templates.js` → getClaudeTemplates/getOpenCodeTemplates/getGeminiTemplates (operations.ts, uninstall.ts)\n- `./paths.js` → getRuntimePaths/resolveInstallPath/getAllRuntimes (operations.ts, uninstall.ts, index.ts)\n- `./types.js` → Runtime/Location/InstallerArgs/InstallerResult/RuntimePaths (all files)\n- `./banner.js` → display functions (index.ts)\n- `./prompts.js` → selectRuntime/selectLocation/confirmAction/isInteractive (index.ts)\n### src/integration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific integration file generation for AI coding assistants (Claude Code, OpenCode, Gemini CLI, Aider) via environment detection, template materialization with frontmatter variants, and bundled hook deployment.**\n\n## Contents\n\n**[detect.ts](./detect.ts)** — `detectEnvironments(projectRoot)` scans for `.claude/`, `CLAUDE.md`, `.opencode/`, `.aider.conf.yml`, `.aider/`, `.gemini/` artifacts via `existsSync()` and returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment(projectRoot, type)` predicate tests for specific platform presence.\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles(projectRoot, options)` orchestrates template generation with skip-if-exists behavior (unless `force=true`) and optional `dryRun` preview. Dispatches to platform-specific template getters via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` after `ensureDir()` parent directory creation, copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays.\n\n**[templates.ts](./templates.ts)** — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generate command file arrays via `buildTemplate(platform, commandName, command)` with platform-specific paths (.claude/skills/are-{command}/SKILL.md, .opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml). `COMMANDS` constant defines seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. `PLATFORM_CONFIGS` maps frontmatter schemas (Claude `name:` field, OpenCode `agent: build` directive, Gemini `description`/`prompt` TOML structure). `buildFrontmatter()` handles Markdown variants, `buildGeminiToml()` formats triple-quoted prompts. Placeholder substitution replaces `COMMAND_PREFIX` (`/are-`) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Architecture\n\n### Environment Detection\n\n`detectEnvironments()` applies existence checks via `path.join(projectRoot, relativePath)` → `existsSync()` for platform-specific markers:\n- Claude: `.claude/` directory OR `CLAUDE.md` file → `{ type: 'claude', configDir: '.claude', detected: true }`\n- OpenCode: `.opencode/` directory → `{ type: 'opencode', configDir: '.opencode', detected: true }`\n- Aider: `.aider.conf.yml` file OR `.aider/` directory → `{ type: 'aider', configDir: '.aider', detected: true }`\n- Gemini: `.gemini/` directory → `{ type: 'gemini', configDir: '.gemini', detected: true }`\n\nNo recursive parent directory scanning—only direct children of `projectRoot` tested.\n\n### Template Generation Flow\n\n1. `generateIntegrationFiles()` receives `projectRoot` and optional `GenerateOptions` (`dryRun`, `force`, `environment`)\n2. If `options.environment` specified, constructs single-element array via `configDirMap: Record<EnvironmentType, string>` lookup (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n3. Otherwise calls `detectEnvironments(projectRoot)` to get auto-detected platforms\n4. For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve `IntegrationTemplate[]` from `templates.ts`\n5. Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file via `writeFileSync(fullPath, template.content, 'utf-8')` if not exists or `force=true`\n6. Special case for `claude` environment: after template processing, reads bundled hook via `getBundledHookPath('are-session-end.js')` → `readFileSync(hookPath, 'utf-8')`, writes to `.claude/hooks/are-session-end.js`\n7. Tracks created/skipped paths in `IntegrationResult` per environment\n\n### Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines variants for frontmatter generation and path construction:\n- **Claude**: Nested directory structure (.claude/skills/are-{command}/SKILL.md), frontmatter with `name: /are-{command}`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: Flat file structure (.opencode/commands/are-{command}.md), frontmatter with `agent: build`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: TOML format (.gemini/commands/are-{command}.toml), `description = \"...\"` and `prompt = \"\"\"...\"\"\"` fields, `versionFilePath: '.gemini/ARE-VERSION'`\n- **Aider**: Returns empty array (no templates defined)\n\n`buildFrontmatter(platform, commandName, description)` emits Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n### Command Template Structure\n\n`COMMANDS` constant defines command metadata with `description`, `argumentHint`, `content` fields for seven commands:\n\n- **`generate`**: Three-phase pipeline documentation (Discovery → File Analysis → Directory/Root Documents). Embeds background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, completion summarization covering file counts, failures, inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n- **`update`**: Incremental change detection workflow with `--uncommitted` flag, hash-based comparison, orphan cleanup, affected directory regeneration. Monitoring pattern identical to `generate`.\n- **`discover`**: Enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay.\n- **`clean`**: Mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview.\n- **`specify`**: AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n- **`rebuild`**: Spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation, ===FILE:=== delimited output parsing, exit code semantics (0=success, 1=partial failure, 2=total failure). Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n- **`help`**: Command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n`buildTemplate(platform, commandName, command)` materializes templates via placeholder substitution: `COMMAND_PREFIX` → `/are-`, `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`.\n\n### Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Bundled Hook Deployment\n\n`generateIntegrationFiles()` includes special-case logic for Claude environment: after template generation loop completes, calls `readBundledHook('are-session-end.js')` which resolves path via `getBundledHookPath(hookName)` using `fileURLToPath(import.meta.url)` to navigate from `dist/integration/` up two levels to project root, then into `hooks/dist/`. Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false. Writes hook content to `.claude/hooks/are-session-end.js` via `writeFileSync(fullHookPath, hookContent, 'utf-8')`. Hook files populated in `hooks/dist/` by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Integration with Project\n\nCalled by `src/installer/operations.ts` installer CLI commands to create IDE-specific command/hook files. Consumed by `src/cli/init.ts` initialization workflows requiring platform-specific configuration. Environment detection (`detectEnvironments()`) supports auto-discovery mode when `options.environment` undefined, enabling multi-platform batch generation (`npx agents-reverse-engineer --runtime all`). Single-environment targeting via `options.environment` parameter supports focused installs (`npx agents-reverse-engineer --runtime claude -g`).\n\n## Import Map\n\n- **Local**: `./detect.js` (`detectEnvironments`, `hasEnvironment`), `./templates.js` (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`), `./types.js` (`EnvironmentType`, `DetectedEnvironment`, `IntegrationTemplate`, `IntegrationResult`)\n- **External**: `path` (Node.js), `fs` (`existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`), `url` (`fileURLToPath`), `../installer/paths.js` (referenced in integration context)\n### src/orchestration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker-pool concurrency control with iterator-based task distribution, progress telemetry via ETA calculation, serialized plan/log/trace writers for concurrent-safe output, and high-level command workflows integrating three-phase AI-driven documentation pipelines.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export aggregating `runPool`, `createTraceWriter`, `cleanupOldTraces`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, plus types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`)\n\n**[pool.ts](./pool.ts)** — `runPool<T>(tasks, options, onComplete?)` iterator-based worker pool sharing single `tasks.entries()` iterator across N workers, returns `Promise<TaskResult<T>[]>` preserving task index, enforces `options.concurrency` cap via `Math.min(options.concurrency, tasks.length)`, supports fail-fast abort via shared `aborted` flag, emits trace events (`worker:start/end`, `task:pickup/done`) with `activeTasks` counter\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates `executeGenerate(plan)` via three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis) with pre/post validation phases (pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks, post-phase-2 phantom path resolution), `executeUpdate(filesToAnalyze)` runs Phase 1 only for incremental workflows, helpers `stripPreamble()` remove LLM conversational prefix via `\\n---\\n` or bold purpose detection, `extractPurpose()` scans lines skipping `PREAMBLE_PREFIXES` (`['now i', 'perfect', 'based on', ...]`)\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams build-log output with format `[X/Y] ANALYZING/DONE/FAIL path` for files, `[dir X/Y] ANALYZING/DONE dirPath/AGENTS.md` for directories, `[root] DONE docPath` for roots, calculates ETA via moving average of last 10 completion times in `completionTimes[]`/`dirCompletionTimes[]` arrays, `printSummary(summary)` outputs end-of-run aggregates; `ProgressLog` mirrors console output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with lazy file handle creation, `stripAnsi(str)` removes color codes via regex `/\\x1b\\[[0-9;]*m/g`\n\n**[trace.ts](./trace.ts)** — `ITraceWriter` interface with `emit(event)`, `finalize()`, `filePath`, `createTraceWriter(projectRoot, enabled)` factory returns `TraceWriter` writing NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` or `NullTraceWriter` no-op stub, `TraceWriter` auto-populates `TraceEventBase` fields (`seq`, `ts`, `pid`, `elapsedMs` via `process.hrtime.bigint()`), serializes via promise-chain pattern, `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces keeping most recent, event types: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via `markDone(itemPath)` replacing `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``, maintains `content: string` in-memory, serializes writes via `writeQueue: Promise<void>` chain pattern, `initialize()` creates parent directory and writes initial markdown, `flush()` drains pending writes before command completion\n\n**[types.ts](./types.ts)** — `FileTaskResult` (path, success, tokensIn/Out, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed/Failed/Skipped, totalCalls, token totals, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc/CodeVsCode, phantomPaths?, inconsistencyReport?), `ProgressEvent` discriminated by type (`start|done|error|dir-done|root-done`), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?, progressLog?)\n\n## Architecture Patterns\n\n### Iterator-Based Worker Pool\n\n`pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag on error, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern. Prevents NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Three-Phase Execution Pipeline\n\n`runner.ts` executes:\n1. **Pre-Phase-1**: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection\n2. **Phase 1**: Concurrent file analysis via `runPool()` calling `aiService.call()` with `buildFilePrompt()`, writes `.sum` with YAML frontmatter (generatedAt, contentHash, purpose), caches source in `sourceContentCache`\n3. **Post-Phase-1**: Quality validation via `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions), `checkCodeVsCode()` for duplicate exports\n4. **Phase 2**: Post-order directory aggregation sorted by depth descending via `sort((a,b) => b-a)`, waits for all child `.sum` files via implicit dependency, calls `aiService.call()` with `buildDirectoryPrompt()`, writes `AGENTS.md` via `writeAgentsMd()` merging `AGENTS.local.md`\n5. **Post-Phase-2**: Phantom path validation via `checkPhantomPaths()` extracting path-like strings with three regex patterns (markdown links, backtick paths, prose-embedded), resolves via `existsSync()` with `.ts`/`.js` fallback\n6. **Phase 3**: Sequential root synthesis (concurrency=1) via `runPool()` calling `aiService.call()` with `buildRootPrompt()` injecting all `AGENTS.md` via `collectAgentsDocs()`, strips conversational preamble via markdown start detection (`indexOf('# ')`), writes to `rootTask.outputPath`\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes[]` and `dirCompletionTimes[]` with max size `windowSize=10`. Records task duration on completion, computes moving average via `reduce((a,b)=>a+b, 0)/length`, multiplies by remaining tasks. Formats via `formatETA()` as `~12s remaining` or `~2m 30s remaining`. Returns empty string if fewer than 2 completions (insufficient sample).\n\n### Trace Event Emission\n\nTracer threaded via `CommandRunOptions.tracer` → pool options → AIService. `TraceWriter` auto-populates base fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (process.pid), `elapsedMs` (high-resolution fractional via `process.hrtime.bigint()` delta). Events: `phase:start` (taskCount, concurrency, phase), `phase:end` (durationMs, tasksCompleted, tasksFailed, phase), `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks), `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode).\n\n## Incremental Update Strategy\n\n`runner.executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code duplicate detection). Returns `RunSummary` with `filesSkipped: 0`. Caller (`src/update/orchestrator.ts`) handles Phase 2 `AGENTS.md` regeneration for `affectedDirs`.\n\n## Quality Validation Phases\n\n**Pre-Phase-1 Cache**: Reads existing `.sum` files into `oldSumCache: Map<string, SumFileContent>` for stale documentation detection in post-phase-1 code-vs-doc checks.\n\n**Post-Phase-1 Code-vs-Doc**: Calls `checkCodeVsDoc()` twice per file — once against `oldSumCache` (detect stale exports), once against freshly written `.sum` (detect omissions). Extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Post-Phase-1 Code-vs-Code**: Groups files by directory via `path.dirname()`, calls `checkCodeVsCode()` aggregating exports per directory into `Map<symbol, string[]>`, detects duplicates.\n\n**Post-Phase-2 Phantom Paths**: Reads each `AGENTS.md`, calls `checkPhantomPaths()` extracting path-like strings via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`.\n\n## Behavioral Contracts\n\n### PlanTracker Checkbox Update Pattern\n```typescript\nmarkDone(itemPath: string) {\n  // Replaces:  - [ ] `${itemPath}`\n  // With:      - [x] `${itemPath}`\n}\n```\n\n### ProgressReporter Output Formats\n```\nFile progress:\n  [X/Y] ANALYZING path\n  [X/Y] DONE path Xs in/out tok model ~Ns remaining\n  [X/Y] FAIL path error\n\nDirectory progress:\n  [dir X/Y] ANALYZING dirPath/AGENTS.md\n  [dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA\n\nRoot progress:\n  [root] DONE docPath\n\nSummary format:\n  === Run Summary ===\n    ARE version:     <version>\n    Files processed: <count>\n    Files failed:    <count>\n    Files skipped:   <count>\n    Total calls:     <count>\n    Tokens:          <totalIn> in / <totalOut> out\n    Cache:           <cacheRead> read / <cacheCreated> created\n    Files read:      <total> (<unique> unique)\n    Total time:      <elapsed>s\n    Errors:          <count>\n    Retries:         <count>\n```\n\n### Preamble Stripping Patterns\n```typescript\nstripPreamble(responseText: string): string {\n  // Pattern 1: Content after \\n---\\n separator within first 500 chars\n  // Pattern 2: Content starting with bold purpose **[A-Z] if preceding text <300 chars and lacks ##\n}\n\nextractPurpose(responseText: string): string {\n  // Skip lines: empty, #headers, ---separators\n  // Skip prefixes (case-insensitive): 'now i', 'perfect', 'based on', 'let me', \n  //   'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'\n  // Strip bold wrapper **, truncate to 120 chars with ... suffix\n}\n```\n\n### TraceEvent Base Field Population\n```typescript\nemit(event: TraceEventPayload): void {\n  // Auto-populate:\n  //   seq: monotonic counter\n  //   ts: new Date().toISOString()\n  //   pid: process.pid\n  //   elapsedMs: Number(process.hrtime.bigint() - startHr) / 1e6\n}\n```\n\n### Pool Worker Task Distribution\n```typescript\nrunPool<T>(tasks, options, onComplete?) {\n  const iterator = tasks.entries(); // Shared across workers\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) => \n    worker(iterator, workerId)\n  );\n  // Each worker pulls [index, task] via for...of over shared iterator\n  // Iterator protocol guarantees atomic .next() calls → no duplicate pickups\n}\n```\n\n## Reproduction-Critical Constants\n\nFull prompt template texts referenced in `runner.ts`:\n- [runner.ts.annex.md](./runner.ts.annex.md)\n### src/output/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal logging interface providing picocolors-based formatted output for CLI progress reporting, file discovery summaries, warnings, and errors with runtime color toggle support.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (6 methods: `info`, `file`, `excluded`, `summary`, `warn`, `error`), `createLogger(options: LoggerOptions)` factory with conditional color mode (`picocolors` vs identity passthrough), `createSilentLogger()` no-op factory for testing, and `LoggerOptions` configuration interface (`colors: boolean`). Implements format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes.\n\n## Color Mode Abstraction\n\n`ColorFunctions` internal interface wraps `picocolors` methods (`green`, `dim`, `red`, `bold`, `yellow`). Conditional logic `options.colors ? pc : noColor` selects between `picocolors` and `noColor` identity wrapper. `noColor` constant implements all five color methods as `identity: (s: string): string => s` passthrough, enabling compile-time type safety for color-enabled/disabled code paths without runtime string inspection.\n\n## Output Format Specification\n\n- **File discovery**: `c.green('  +') + ' ' + path` (included), `c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`)` (excluded)\n- **Summary**: `c.bold(\\`\\nDiscovered ${included} files\\`) + c.dim(` (${excluded} excluded)`)`\n- **Warnings**: `c.yellow('Warning: ') + message` via `console.warn()`\n- **Errors**: `c.red('Error: ') + message` via `console.error()`\n\n## Integration Points\n\nConsumed by CLI commands (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging (`file()`, `excluded()`, `summary()` calls) and `src/orchestration/progress.ts` for phase execution updates (`info()`, `warn()` calls during pool orchestration).\n### src/quality/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation suite detecting three classes of documentation drift: code-vs-doc (exported symbols missing from `.sum` files), code-vs-code (duplicate exports within directory scope), and phantom-paths (unresolvable file references in `AGENTS.md`). Aggregates findings into `InconsistencyReport` objects with severity stratification and plain-text CLI formatting.\n\n## Contents\n\n### Barrel Exports\n\n**[index.ts](./index.ts)** — Re-exports `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` from types.ts; `extractExports()`, `checkCodeVsDoc()` from inconsistency/code-vs-doc.js; `checkCodeVsCode()` from inconsistency/code-vs-code.js; `buildInconsistencyReport()`, `formatReportForCli()` from inconsistency/reporter.js; `checkPhantomPaths()` from phantom-paths/index.js; `validateFindability()`, `FindabilityResult` from density/validator.js.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency` (code-vs-doc | code-vs-code | phantom-path) with shared `severity: 'info' | 'warning' | 'error'` field. `CodeDocInconsistency` contains `filePath`, `sumPath`, `details.missingFromDoc[]`. `CodeCodeInconsistency` contains `files[]`, `pattern: 'duplicate-export'`. `PhantomPathInconsistency` contains `agentsMdPath`, `details.referencedPath`, `details.resolvedTo`, `details.context`. `InconsistencyReport` aggregates `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, `summary` counts by type/severity.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — Implements code-vs-doc detection via regex export extraction and substring matching, code-vs-code duplicate symbol detection across per-directory file groups, and report aggregation with plain-text formatting. Exports `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`.\n\n**[phantom-paths/](./phantom-paths/)** — Scans `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location filesystem resolution with `.ts`/`.js` fallback, reports unresolved references as `PhantomPathInconsistency` with 120-char contextLine. Exports `checkPhantomPaths()`.\n\n**[density/](./density/)** — Stub module returning empty `FindabilityResult[]` array, disabled after removal of `SumFileContent.metadata.publicInterface` schema field. Exports `validateFindability()` signature for future symbol extraction support.\n\n## Validation Pipeline\n\n**Code-vs-Doc:** `extractExports()` parses source via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies extracted symbols appear in `SumFileContent.summary` via substring search, returns `CodeDocInconsistency` with `missingFromDoc[]` on detection gaps.\n\n**Code-vs-Code:** `checkCodeVsCode()` aggregates exports into `Map<string, string[]>` across scoped file group, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Requires caller-enforced per-directory scoping to prevent cross-module false positives.\n\n**Phantom Paths:** `checkPhantomPaths()` extracts paths from `AGENTS.md` via three regex patterns, attempts `existsSync()` resolution at agentsMdDir-relative, projectRoot-relative, and `.ts` fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), reports unresolved references with severity='warning'.\n\n**Report Synthesis:** `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with metadata (timestamp, projectRoot, filesChecked, durationMs) and summary counts (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info). `formatReportForCli()` renders plain-text output with `[ERROR]`/`[WARN]`/`[INFO]` severity tags.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` Phase 1 post-processing: constructs `InconsistencyReport` via `buildInconsistencyReport()` after `.sum` generation completes, logs summary counts to `.agents-reverse-engineer/progress.log` via `src/orchestration/progress.ts` streaming reporter, includes report metadata in telemetry run logs via `src/ai/telemetry/run-log.ts`.\n\n**Upstream Inputs:**\n- Source files for `extractExports()` regex parsing\n- `.sum` files via `SumFileContent` schema from `src/generation/writers/sum.ts`\n- `AGENTS.md` files for `checkPhantomPaths()` scanning\n- Export maps from `src/imports/extractor.ts` for `checkCodeVsCode()` aggregation\n\n**Downstream Consumers:**\n- `src/orchestration/progress.ts` logs summary counts via `formatReportForCli()`\n- `src/ai/telemetry/run-log.ts` persists report metadata to NDJSON run logs\n- CLI output displays validation results after Phase 1 completion\n\n## Behavioral Contracts\n\n**Export extraction regex (inconsistency/code-vs-doc.ts):**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path extraction patterns (phantom-paths/validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (phantom-paths/validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n\n**Severity levels:** `'info' | 'warning' | 'error'` mapped to CLI tags `[INFO]`, `[WARN]`, `[ERROR]`.\n\n**Inconsistency type discriminants:** `code-vs-doc`, `code-vs-code`, `phantom-path` enable exhaustive pattern matching via `type` field.\n\n## Detection Limitations\n\n**Regex-based export extraction:** Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n**Substring matching:** `sumText.includes(e)` yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**Code-vs-code name-only comparison:** Cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions without AST context.\n\n**Phantom path resolution:** Four-location strategy (`agentsMdDir`, `projectRoot`, .ts fallback) does not resolve symlinks, aliased imports, or paths requiring module resolution algorithms.\n### src/quality/density/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub findability validation module, currently disabled after removal of structured `publicInterface` metadata from `.sum` file schema. Preserved for future re-implementation via post-processing symbol extraction.\n\n## Contents\n\n### [validator.ts](./validator.ts)\nExports `validateFindability()` stub returning empty `FindabilityResult[]` array and `FindabilityResult` interface defining validation outcome structure with `filePath`, `symbolsTested`, `symbolsFound`, `symbolsMissing`, `score` fields.\n\n## Architecture\n\n**Validation Approach (Disabled):** Originally performed string-based substring matching to verify exported symbols from `.sum` files appeared in parent `AGENTS.md` content without LLM calls. Logic removed when `SumFileContent.metadata.publicInterface` field was deleted from schema.\n\n**Current State:** `validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>)` unconditionally returns `[]` with parameters prefixed by underscores indicating unused status. Function signature retained for future structured extraction support via post-processing passes.\n\n## Integration Points\n\nConsumed by `src/quality/index.ts` which aggregates quality validators (`validateCodeDocConsistency`, `validateCodeCodeConsistency`, `validatePhantomPaths`, `validateFindability`). Produces no findings in current implementation but return type `FindabilityResult[]` preserved in reporting pipeline.\n\n**Type Dependencies:**\n- `SumFileContent` imported from `../../generation/writers/sum.js` (verified path)\n- `FindabilityResult` consumed by quality reporting aggregator\n\n## Behavioral Contracts\n\n**Return Value:** Always `[]` (empty array) indicating zero validation findings.\n\n**Score Calculation (Historical):** Ratio of `symbolsFound.length / symbolsTested.length` yielding `0.0` (no symbols) to `1.0` (all symbols present) when validation was active.\n### src/quality/inconsistency/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\n**Detects three classes of code-documentation drift: code-vs-doc (exports missing from .sum files), code-vs-code (duplicate symbol exports within directory scope), and phantom-paths (broken file references in AGENTS.md), aggregating findings into structured InconsistencyReport objects with plain-text CLI formatting.**\n\n## Contents\n\n### Detection Modules\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts identifiers from TypeScript/JavaScript source via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` compares extracted symbols against SumFileContent.summary substring presence, returns CodeDocInconsistency with `missingFromDoc[]` when exports absent from documentation.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` aggregates exports via `Map<string, string[]>` across scoped file group, returns CodeCodeInconsistency array flagging symbols exported from multiple files with `pattern: 'duplicate-export'` sentinel, relies on caller to enforce per-directory scoping.\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates Inconsistency discriminated union into InconsistencyReport with summary counts across type/severity dimensions, `formatReportForCli()` renders plain-text output without picocolors dependency for testability.\n\n## File Relationships\n\ncode-vs-code.ts imports `extractExports()` from code-vs-doc.ts for shared regex-based export extraction. Both detection modules return type-specific inconsistency objects (CodeDocInconsistency, CodeCodeInconsistency) consumed by reporter.ts via Inconsistency discriminated union. Reporter aggregates findings from both modules into unified InconsistencyReport structure with metadata (timestamp, projectRoot, filesChecked, durationMs) and typed summary counts.\n\n## Behavioral Contracts\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nCaptures identifiers from `export function`, `export const`, `export default class`, `export type`, `export interface`, `export enum`. Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n### Inconsistency Type Discriminants\n- `code-vs-doc`: requires `filePath`, `sumPath`, `details.missingFromDoc[]`\n- `code-vs-code`: requires `files[]`, `pattern: 'duplicate-export'`\n- `phantom-path`: requires `agentsMdPath`, `details.referencedPath`\n\n### Severity Levels\nMapped to CLI tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`\n\n## Detection Limitations\n\n**code-vs-doc**: Substring matching (`sumText.includes(e)`) yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**code-vs-code**: Name-only comparison without AST analysis cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions. Requires caller-enforced per-directory scoping to prevent false positives across module boundaries.\n\n**Shared**: Regex-based extraction misses complex export patterns (destructured, namespace, dynamic). Both modules operate as pure heuristics without AI service calls.\n### src/quality/phantom-paths/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable file path references in generated `AGENTS.md` documentation through regex extraction, multi-strategy filesystem resolution with TypeScript/JavaScript extension fallback, and `PhantomPathInconsistency` issue reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Re-exports `checkPhantomPaths` from `validator.js` as barrel export for phantom path detection subsystem within parent `src/quality/` pipeline.\n\n**[validator.ts](./validator.ts)** — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` scanning AGENTS.md text via `PATH_PATTERNS` regex array (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location resolution (agentsMdDir-relative, projectRoot-relative, .ts fallback for both), filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), extracts 120-char contextLine from content on resolution failure, deduplicates via Set, returns `PhantomPathInconsistency[]` with severity='warning', type='phantom-path', referencedPath, resolvedTo, contextLine details.\n\n## Path Extraction Strategy\n\n`PATH_PATTERNS` contains three RegExp patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link syntax\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures code-formatted paths starting with src/, ./, ../\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures src/ paths following trigger keywords\n\n`SKIP_PATTERNS` excludes seven non-file reference types: node_modules, .git, HTTP(S) URLs, template placeholders (`{{`, `${`), glob wildcards, brace expansion syntax.\n\n## Resolution Protocol\n\nFor each extracted path, `checkPhantomPaths` attempts `existsSync()` validation at four filesystem locations in sequence:\n1. `path.resolve(agentsMdDir, rawPath)` — relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — relative to project root (handles absolute-style src/ paths)\n3. agentsMdDir-relative path with .js → .ts extension substitution\n4. projectRoot-relative path with .js → .ts extension substitution\n\nStops at first successful resolution; reports unresolved path as `PhantomPathInconsistency` with contextLine extracted via `lines.find((l) => l.includes(rawPath))`, trimmed and sliced to 120-char maximum.\n\n## Behavioral Contracts\n\n**Path extraction patterns (validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n### src/rebuild/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# rebuild\n\nImplements AI-driven project reconstruction from specification documents via ordered phase execution with checkpoint-based resumability, concurrent file generation within dependency groups, and progressive context accumulation from previously built modules.\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — Executes three-stage rebuild workflow: spec partitioning into `RebuildUnit[]` via `partitionSpec()`, sequential order group processing with concurrent AI calls per group via `runPool()`, and progressive context accumulation from generated file exports with LRU truncation at 100k chars.\n\n**[checkpoint.ts](./checkpoint.ts)** — Provides session continuity via `CheckpointManager` class with SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), and promise-chain write serialization to `.rebuild-checkpoint` JSON file.\n\n### Specification Processing\n\n**[spec-reader.ts](./spec-reader.ts)** — Reads markdown files from `specs/` directory via `readSpecFiles()`, partitions content into `RebuildUnit[]` via Build Plan phase extraction (`### Phase N:`) or top-level heading fallback, injects targeted context (Architecture, filtered API subsections, Data Structures, Behavioral Contracts) based on Defines/Consumes keyword matching.\n\n**[output-parser.ts](./output-parser.ts)** — Extracts file paths and contents from AI responses using delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback (```` ```language:path ````), returning `Map<string, string>` for filesystem writes.\n\n### Prompt Engineering\n\n**[prompts.ts](./prompts.ts)** — Defines `REBUILD_SYSTEM_PROMPT` enforcing delimiter format, exact spec compliance (no synonym substitution), and production code constraints (no tests/stubs/placeholders), plus `buildRebuildPrompt()` constructing per-unit prompts with full spec, phase-specific content, and accumulated built context.\n\n### Type System\n\n**[types.ts](./types.ts)** — Exports `RebuildCheckpointSchema` Zod validator with `specHashes`/`modules` fields, `RebuildUnit` interface with `name`/`specContent`/`order`, `RebuildPlan` with `units[]`/`outputDir`, and `RebuildResult` with token counts and `filesWritten[]`.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting all public types, schemas (`RebuildCheckpointSchema`), functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`), classes (`CheckpointManager`), and constants (`REBUILD_SYSTEM_PROMPT`).\n\n## Architecture\n\n### Sequential Order Groups with Concurrent Execution\n\n`executeRebuild()` groups `RebuildUnit[]` by `order` field into `Map<number, RebuildUnit[]>`, processes order values sequentially (ascending), executes units within each group concurrently via `runPool()` with configurable concurrency. Accumulates exported symbols from completed groups into `builtContext` string for injection into subsequent groups as \"Already Built\" context, enabling correct import resolution across phases.\n\n### Checkpoint-Based Resume Workflow\n\n`CheckpointManager.load()` compares SHA-256 hashes of current spec files against `checkpoint.specHashes`, returns `isResume: false` if hash count differs or any individual hash mismatches (drift detected). Filters `pendingUnits[]` via `checkpoint.isDone(unitName)` predicate, increments `modulesSkipped` counter for already-complete work. Workers call `checkpoint.markDone(unitName, filesWritten)` or `checkpoint.markFailed(unitName, errorMsg)` on completion, serializing writes via promise chain to prevent corruption.\n\n### Context Accumulation with LRU Truncation\n\nAfter each order group completes, `orchestrator.ts` reads all `filesWrittenInGroup` via `readFile()`, filters out non-source files (`.md`/`.json`/`.yml`), appends to `builtContext` with `// === ${filePath} ===` delimiters. When `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits on delimiter pattern, preserves recent files in full, truncates older files to first `TRUNCATED_HEAD_LINES` (20 lines) with `// ... (truncated)` marker.\n\n### Targeted API Injection (Change 2 Format)\n\n`extractFromBuildPlan()` detects `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern in phase content, extracts keywords from Defines/Consumes lists and file path references, calls `findRelevantSubsections()` with fuzzy matching (substring, word overlap) to filter API/Data/Behavioral subsections. Injects matched subsections under `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase` headings. Falls back to full `## Public API Surface` inclusion for legacy specs without Defines/Consumes (graceful degradation).\n\n## Behavioral Contracts\n\n**Delimiter format (primary output parsing):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block format (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Build Plan phase extraction:** `/^### Phase (\\d+):\\s*(.+)$/gm`\n\n**Change 2 format detection:** `/^\\*\\*Defines:\\*\\*|^Defines:/m`\n\n**Section heading pattern:** `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m`\n\n**Subsection heading pattern:** `/^### (.+)$/gm`\n\n**File path extraction:** `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`\n\n**Context delimiter format:** `// === ${filePath} ===` (used for splitting truncated context)\n\n## Integration Points\n\nInvoked by `src/cli/rebuild.ts` command handler. Consumes `../ai/index.js` `AIService` for subprocess spawning, `../orchestration/index.js` `runPool`/`ProgressReporter`/`ITraceWriter` for concurrency control, `../change-detection/index.js` `computeContentHashFromString` for drift detection. Writes checkpoint to `<outputDir>/.rebuild-checkpoint` JSON, generated source files to `<outputDir>/<relativePath>` as specified in parsed AI responses.\n### src/specify/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify\n\n**Project specification synthesis from AGENTS.md documentation corpus: prompt engineering, heading-based file splitting, and overwrite-protected filesystem operations for single-file or multi-file spec generation.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export for specification synthesis API: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions`\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs, annexFiles?)` constructs `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration) and verbatim annex reproduction mandates\n\n**[writer.ts](./writer.ts)** — `writeSpec(content, options)` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings`), throws `SpecExistsError` on conflicts unless `force=true`, returns written paths array\n\n## Architecture\n\n### Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompts by concatenating:\n1. Header directive: `\"Generate a comprehensive project specification...\"`\n2. Documentation sections: `### ${doc.relativePath}` with embedded content from `AgentsDocs` array\n3. Optional annex block: `## Annex Files` with verbatim source code for reproduction-critical modules (prompt templates, IDE configs)\n4. Requirement checklist: 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n5. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim\"`\n6. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\nSystem prompt enforces:\n- **Audience**: AI agents requiring instruction-oriented language (not human documentation)\n- **Anti-patterns**: Prohibits folder-mirroring, file path prescription, directory-derived headings\n- **Module boundary focus**: Describe interfaces/exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for external dependencies\n- **Build Plan**: Phased implementation with explicit \"Defines:\"/\"Consumes:\" interface contract cross-references\n- **Behavioral contracts**: Exact error types/codes, verbatim regex patterns in backticks, format strings with structure, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical sections**: Sections 10-11 reproduce annex content without summarization\n\n### Multi-File Splitting\n\n`splitByHeadings` partitions markdown on `/^(?=# )/m` regex (positive lookahead for top-level headings):\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify` (lowercase + whitespace→hyphens + strip non-alphanumeric + collapse consecutive hyphens + trim)\n- Pre-heading content assigned to `00-preamble.md`\n- Each section object contains `{ filename, content }` with trailing newline\n\nConflict detection: Before writing files, iterates sections to check existence via `fileExists` (wraps `fs.access(..., F_OK)`). Aggregates conflicts and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write (all-or-nothing).\n\n## Integration Points\n\n### Consumed By\n\n`src/cli/specify.ts` command invokes `writeSpec(aiOutput, { outputPath: config.output.specPath, force: cliFlags.force, multiFile: cliFlags.multiFile })` after `AIService.call(buildSpecPrompt(docs, annexFiles))` completes.\n\n### Dependencies\n\n**[../generation/collector.js](../generation/collector.ts)** — `collectAgentsDocs()` provides recursive `AGENTS.md` traversal results via `AgentsDocs` type\n\n**node:fs/promises** — `mkdir`, `writeFile`, `access` for filesystem operations\n\n**node:path** — `dirname`, `join` for path resolution\n\n## Behavioral Contracts\n\n### Runtime Behavior\n\n- `writeSpec` throws `SpecExistsError` when target paths exist and `force=false`\n- Single-file mode creates parent directory recursively via `mkdir(..., { recursive: true })`\n- Multi-file mode writes all sections or none (atomic operation via upfront conflict check)\n\n### Implementation Contracts\n\n**Heading regex**: `/^(?=# )/m` (matches lines starting with `# ` via positive lookahead)\n\n**Heading extraction**: `/^# (.+)/` (captures heading text after `# ` prefix)\n\n**Slugification transforms**:\n- Whitespace→hyphens: `/\\s+/g` → `'-'`\n- Strip non-alphanumeric: `/[^a-z0-9-]/g` → `''`\n- Collapse hyphens: `/-+/g` → `'-'`\n- Trim hyphens: `/^-|-$/g` → `''`\n\n**SpecExistsError message format**: `\"Specification file(s) already exist:\\n\" + paths.map(p => \"  - \" + p).join(\"\\n\") + \"\\n\\nUse --force to overwrite.\"`\n\n**Preamble filename**: `00-preamble.md` (for content before first `# ` heading)\n### src/types/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types/\n\n**Core TypeScript interface definitions for file discovery results, exclusion metadata, and discovery statistics shared across discovery, change-detection, generation, and quality validation modules.**\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `ExcludedFile`, `DiscoveryStats` interfaces consumed by orchestrators and reporters.\n\n## Exported Interfaces\n\n**ExcludedFile** — Represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** — Encapsulates file discovery output:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics/reporting)\n\n**DiscoveryStats** — Aggregates discovery metrics:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Histogram of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n**Producers:**\n- `discoverFiles()` in `src/discovery/run.ts` returns `DiscoveryResult`\n- Filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) produce `ExcludedFile` instances aggregated into `DiscoveryResult.excluded`\n\n**Consumers:**\n- `generateDocumentation()` in `src/generation/orchestrator.ts` consumes `DiscoveryResult.files` for Phase 1 pool execution\n- `updateDocumentation()` in `src/update/orchestrator.ts` merges `DiscoveryResult` with `FileChange[]` from change detector\n- Progress reporters and telemetry loggers in `src/output/logger.ts` compute `DiscoveryStats` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, written to `GENERATION-PLAN.md` and `.agents-reverse-engineer/progress.log`\n\n## File Relationships\n\n`DiscoveryResult` flows from discovery filters → orchestrator → worker pool → generation writers. `DiscoveryStats` computed post-discovery via reduction over `excluded[]` array for reporting and plan documentation.\n### src/update/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nIncremental documentation update workflow coordinating SHA-256 hash-based change detection, orphaned artifact cleanup, and selective `.sum` + `AGENTS.md` regeneration for modified files and affected directories.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export exposing UpdateOrchestrator, createUpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, UpdatePlan, UpdateOptions, UpdateResult, UpdateProgress, CleanupResult types.\n\n**[orchestrator.ts](./orchestrator.ts)** — UpdateOrchestrator compares YAML frontmatter `content_hash` fields from existing `.sum` files against SHA-256 hashes of current file content via computeContentHash(), generates UpdatePlan segregating files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), and `cleanup` (orphaned), computes `affectedDirs` requiring `AGENTS.md` regeneration sorted deepest-first. Factory createUpdateOrchestrator() constructs instances with optional ITraceWriter injection.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — cleanupOrphans() deletes `.sum` and `.annex.md` files for deleted/renamed source files (targeting `oldPath` for renames), invokes cleanupEmptyDirectoryDocs() removing `AGENTS.md` from directories with no remaining source files (excludes hidden files, `GENERATED_FILES` set, `.sum`/`.annex.md` from emptiness check). getAffectedDirectories() walks parent directories of non-deleted FileChange entries via path.dirname() to project root.\n\n**[types.ts](./types.ts)** — Defines CleanupResult (`deletedSumFiles[]`, `deletedAgentsMd[]`), UpdateOptions (`includeUncommitted`, `dryRun` flags), UpdateResult (execution summary with `analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`), UpdateProgress (event callbacks: `onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Workflow\n\n1. **Prerequisites** — UpdateOrchestrator.checkPrerequisites() validates git repository via isGitRepo(), throws Error if not a git repo\n2. **Plan Creation** — preparePlan() calls discoverFiles(), iterates results:\n   - Constructs `.sum` path via getSumPath()\n   - Calls readSumFile() extracting `contentHash` from frontmatter\n   - Missing `.sum`: adds `{ path, status: 'added' }` to `filesToAnalyze`\n   - Existing `.sum`: calls computeContentHash(), compares hashes\n   - Hash mismatch: adds `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: adds path to `filesToSkip`\n3. **Orphan Cleanup** — cleanupOrphans() deletes `.sum`/`.annex.md` for deleted/renamed files, cleanupEmptyDirectoryDocs() removes `AGENTS.md` from directories with no source files\n4. **Affected Directories** — getAffectedDirectories() computes parent paths of changed files, sorts by depth descending (deepest-first processing)\n5. **Regeneration** — External caller (src/cli/update.ts) invokes Phase 1 pool execution for `filesToAnalyze`, Phase 2 sequential directory aggregation for `affectedDirs` (skips Phase 3 root synthesis)\n\n## Integration\n\n**Upstream callers**: src/cli/update.ts constructs UpdateOrchestrator via createUpdateOrchestrator(), calls preparePlan() returning UpdatePlan, passes `filesToAnalyze` to src/generation/executor.ts Phase 1 file analysis pool, passes `affectedDirs` to Phase 2 directory aggregation.\n\n**Dependencies**: src/change-detection/detector.ts (computeContentHash, isGitRepo, getCurrentCommit), src/generation/writers/sum.ts (readSumFile, getSumPath), src/discovery/run.ts (discoverFiles), src/orchestration/trace.ts (ITraceWriter).\n\n## Trace Events\n\nUpdateOrchestrator emits via optional ITraceWriter:\n\n- `phase:start` at plan creation with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after completion with `durationMs` computed via process.hrtime.bigint() delta\n\n## Frontmatter Mode\n\nNo-op methods (recordFileAnalyzed, removeFileState, recordRun, getLastRun) retain signatures for API compatibility with hypothetical database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by writeSumFile(). Run history tracking unavailable (getLastRun() returns undefined, recordRun() returns 0).\n\n## Dry-Run Support\n\nWhen `UpdateOptions.dryRun === true`, orphan-cleaner skips unlink() calls but executes stat() checks, returning would-be deletions in CleanupResult. Used for preview mode in src/cli/update.ts.\n\n## Package Metadata (package.json)\n\n- **Name**: agents-reverse-engineer\n- **Version**: 0.7.1\n- **Description**: CLI tool for reverse-engineering codebase documentation for AI agents\n- **Scripts**:\n  - `build`: `tsc`\n  - `build:hooks`: `node scripts/build-hooks.js`\n  - `prepack`: `rm -f LICENSE.sum README.md.sum`\n  - `prepublishOnly`: `npm run build && npm run build:hooks`\n  - `dev`: `tsx watch src/cli/index.ts`\n\n## Output Requirements\n\nThe document MUST include:\n- Project purpose and description\n- Architecture overview with directory structure\n- Key directories table\n- Getting started (install, build, run commands)\n- Key technologies and dependencies\n\nThis document is the COMPREHENSIVE reference for the entire project.\nIt should contain architecture, configuration, build instructions, and project-wide patterns.\nIndividual file details belong in directory-level AGENTS.md files — reference them, don't duplicate them.\n\nOutput ONLY the markdown content. No preamble.",
      "systemPrompt": "You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided",
      "response": "# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Project Overview\n\nagents-reverse-engineer (ARE) automates brownfield documentation for AI coding assistants by executing a three-phase pipeline that discovers source files, analyzes them concurrently via AI CLI subprocesses (Claude Code, Gemini, OpenCode), generates file summaries with YAML frontmatter containing SHA-256 content hashes, synthesizes directory-level documentation from bottom-up via post-order traversal, and produces root integration documents tailored to each AI platform.\n\n**Version**: 0.7.1  \n**License**: MIT (GeoloeG-IsT, 2026)  \n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n### Core Capabilities\n\n- **Parallel file analysis** with configurable concurrency pools (default 2 workers for WSL, 5 elsewhere)\n- **Incremental updates** via SHA-256 content hash comparison (skip unchanged files)\n- **Multi-platform AI backend support** (Claude Code, Gemini CLI, OpenCode) with automatic detection\n- **Gitignore-aware file discovery** with binary detection and vendor directory exclusion\n- **Quality validation** detecting code-documentation inconsistencies and phantom path references\n- **Session lifecycle hooks** for automatic documentation refresh on IDE session end\n- **NDJSON telemetry logging** with token cost tracking and run retention management\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1: Concurrent File Analysis**\n\nIterator-based worker pool (`src/orchestration/pool.ts`) shares single task iterator across N workers to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits:\n\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` blocks subagents\n\nProcess group killing (`kill(-pid)`) terminates subprocess trees on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\").\n\nWrites `.sum` files with YAML frontmatter:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n\nMarkdown summary content...\n```\n\n**Phase 2: Post-Order Directory Aggregation**\n\nSorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory.\n\nPrompts include:\n\n- Aggregated child `.sum` content via `readSumFile()`\n- Subdirectory `AGENTS.md` files via recursive traversal\n- Import maps via `extractDirectoryImports()` with verified path constraints\n- Manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile)\n\nUser-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3: Root Document Synthesis**\n\nSequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal, parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching before writing output.\n\n### Directory Structure\n\n```\n.\n├── .github/workflows/    # CI/CD: npm publish workflow with provenance attestation\n├── docs/                 # Original vision document (INPUT.md) defining RLM algorithm\n├── hooks/                # Session lifecycle hooks for Claude/Gemini/OpenCode\n│   ├── are-check-update.js              # SessionStart: npm version check\n│   ├── are-session-end.js               # SessionEnd: auto-update on uncommitted changes\n│   ├── opencode-are-check-update.js     # OpenCode plugin: version check\n│   └── opencode-are-session-end.js      # OpenCode plugin: session end update\n├── scripts/              # Build automation: hook file copying (build-hooks.js)\n└── src/                  # TypeScript source tree\n    ├── ai/               # AI service orchestration with backend abstraction\n    ├── change-detection/ # Git-based delta computation and SHA-256 hashing\n    ├── cli/              # Command entry points (init, discover, generate, update, clean, specify)\n    ├── config/           # YAML config loading with Zod validation\n    ├── discovery/        # File walking with composable filters (gitignore, binary, vendor, custom)\n    ├── generation/       # Three-phase pipeline orchestration and prompt engineering\n    ├── imports/          # Static import analysis for dependency graphs\n    ├── installer/        # npx-based command/hook installation for IDE runtimes\n    ├── integration/      # Platform-specific template generation (Claude/OpenCode/Gemini)\n    ├── orchestration/    # Worker pool, progress reporting, trace emission, plan tracking\n    ├── output/           # Terminal logging with picocolors formatting\n    ├── quality/          # Code-doc consistency validation and phantom path detection\n    ├── rebuild/          # AI-driven project reconstruction from specs\n    ├── specify/          # Project specification synthesis from AGENTS.md corpus\n    ├── types/            # Shared interfaces for discovery results\n    └── update/           # Incremental update workflow with orphan cleanup\n```\n\n## Key Directories\n\n| Directory | Purpose |\n|-----------|---------|\n| **src/ai/** | Backend-agnostic AI service layer with subprocess management, retry logic, telemetry logging, and trace emission. Contains adapters for Claude Code, Gemini CLI, OpenCode. |\n| **src/generation/** | Orchestrates three-phase pipeline: file analysis prompt construction with import maps, directory aggregation with user content preservation, root document synthesis with stack detection. |\n| **src/orchestration/** | Iterator-based worker pool with shared task queue, streaming progress reporter with ETA calculation, promise-chain serialization for concurrent writes (GENERATION-PLAN.md, progress.log, trace NDJSON). |\n| **src/discovery/** | Directory walker with filter chain: gitignore parsing via `ignore` library, binary detection (extension + content analysis), vendor directory exclusion (node_modules/.git/dist), custom glob patterns. |\n| **src/change-detection/** | Git diff parsing (`git diff --name-status -M`) with rename detection and SHA-256 content hashing for non-git workflows. Detects added/modified/deleted/renamed files, optionally merges uncommitted changes. |\n| **src/quality/** | Post-generation validators: regex-based export extraction vs. substring search in summaries (code-vs-doc), duplicate symbol detection (code-vs-code), path resolution via `existsSync()` (phantom-paths). |\n| **src/config/** | Zod schema validation for `.agents-reverse-engineer/config.yaml` with defaults for vendor directories (10 entries), exclude patterns (gitignore-style globs), binary extensions (96 types), AI backend config (concurrency/timeout/model/pricing). |\n| **src/installer/** | npx install orchestrator supporting global (`~/.claude`) and local (`.claude`) installation modes with interactive prompts, platform-specific path resolution (environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`), hook registration, permission setup. |\n| **hooks/** | Detached background processes for version checking (compares `npm view agents-reverse-engineer version` against `~/.claude/ARE-VERSION`) and session-end updates (spawns `npx agents-reverse-engineer@latest update --quiet` when `git status --porcelain` detects changes). |\n\n## Getting Started\n\n### Installation\n\n**Global install:**\n```bash\nnpm install -g agents-reverse-engineer\n```\n\n**Project-local install:**\n```bash\nnpm install --save-dev agents-reverse-engineer\n```\n\n**npx usage (no install required):**\n```bash\nnpx agents-reverse-engineer init\n```\n\n### Prerequisites\n\n- Node.js ≥18.0.0\n- At least one AI CLI installed:\n  - Claude Code: `npm install -g @anthropic-ai/claude-code`\n  - Gemini CLI: Installation instructions at https://gemini.google.com/cli\n  - OpenCode: Installation instructions at https://opencode.dev\n\n### Basic Workflow\n\n**1. Initialize configuration:**\n```bash\nare init\n# Creates .agents-reverse-engineer/config.yaml with defaults\n```\n\n**2. Discover files (optional preview):**\n```bash\nare discover\n# Scans files, writes GENERATION-PLAN.md with phase breakdown\n```\n\n**3. Generate documentation:**\n```bash\nare generate\n# Three-phase execution: .sum files → AGENTS.md → CLAUDE.md\n# Progress logged to .agents-reverse-engineer/progress.log\n# Monitor with: tail -f .agents-reverse-engineer/progress.log\n```\n\n**4. Incremental updates:**\n```bash\nare update\n# Hash-based change detection, regenerates only modified files\n# Use --uncommitted flag to include working tree changes\n```\n\n**5. Clean artifacts:**\n```bash\nare clean\n# Removes .sum, AGENTS.md (generated only), CLAUDE.md, GENERATION-PLAN.md\n# Restores AGENTS.local.md → AGENTS.md\n```\n\n**6. Generate project spec:**\n```bash\nare specify\n# Synthesizes all AGENTS.md into specs/SPEC.md\n# Use --multi-file for split specs (specs/<dirname>.md)\n```\n\n### CLI Options\n\n**Global flags:**\n- `--debug` — Enable verbose subprocess logging with heap/RSS metrics\n- `--trace` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n- `--concurrency N` — Override worker pool size (1-10, default from config)\n- `--fail-fast` — Abort on first task failure\n- `--dry-run` — Preview operations without writing files\n\n**Generate/Update:**\n- `--uncommitted` — Include working tree changes (update only)\n- `--force` — Overwrite existing files (specify only)\n\n**Trace output:**\n```bash\nare generate --trace\n# Writes trace-<timestamp>.ndjson with events:\n# phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry\n```\n\n### IDE Integration\n\n**Install commands/hooks for Claude Code:**\n```bash\nnpx agents-reverse-engineer --runtime claude -g\n# Installs to ~/.claude/commands/ and ~/.claude/hooks/\n# Registers SessionStart hook: are-check-update.js\n# Registers SessionEnd hook: are-session-end.js\n```\n\n**Install for OpenCode:**\n```bash\nnpx agents-reverse-engineer --runtime opencode -g\n# Installs to ~/.config/opencode/commands/ and ~/.config/opencode/plugins/\n```\n\n**Install for Gemini:**\n```bash\nnpx agents-reverse-engineer --runtime gemini -g\n# Installs to ~/.gemini/commands/ with TOML format\n```\n\n**Available IDE commands:**\n- `/are-init` — Create configuration\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation\n- `/are-update` — Incremental update\n- `/are-clean` — Remove generated artifacts\n- `/are-specify` — Synthesize project specification\n- `/are-help` — Show command list\n\n## Key Technologies\n\n**Runtime & Language:**\n- Node.js ≥18.0.0 (ES modules)\n- TypeScript 5.7.3 (ES2022 target, NodeNext module resolution, strict mode)\n\n**Core Dependencies:**\n- `fast-glob` — File discovery with glob patterns\n- `ignore` — Gitignore parsing\n- `isbinaryfile` — Binary file detection\n- `simple-git` — Change detection via git diff\n- `yaml` — Config parsing\n- `zod` — Schema validation\n- `ora` — Spinner UI\n- `picocolors` — Terminal colors\n\n**Build & Distribution:**\n- TypeScript compiler (`tsc`) emits to `dist/`\n- Binary entry points: `are` and `agents-reverse-engineer` → `dist/cli/index.js`\n- Pre-publish hooks: `build` + `build:hooks` (copies hooks/ to hooks/dist/)\n\n**AI Backends:**\n- `@anthropic-ai/claude-code` (Claude Code CLI adapter)\n- Gemini CLI (stub implementation pending JSON output stability)\n- OpenCode CLI (stub implementation pending JSONL parsing)\n\n## Configuration\n\n**Config file:** `.agents-reverse-engineer/config.yaml`\n\n**Schema sections:**\n\n```yaml\nexclude:\n  patterns:              # Gitignore-style globs (e.g., \"*.log\", \"**/*.test.ts\")\n  vendorDirs:           # Third-party directories to skip (default: node_modules, .git, dist, etc.)\n  binaryExtensions:     # Non-text file extensions (default: .png, .jpg, .zip, .pdf, etc.)\n\noptions:\n  followSymlinks: false # Follow symbolic links during discovery\n  maxFileSize: 1048576  # Binary detection threshold (1MB default)\n\noutput:\n  colors: true          # Enable ANSI color codes in CLI output\n\nai:\n  backend: 'auto'       # 'claude' | 'gemini' | 'opencode' | 'auto' (detect first available)\n  model: null           # Override backend default model\n  timeoutMs: 120000     # Subprocess timeout (2 minutes)\n  maxRetries: 3         # Exponential backoff retry attempts\n  concurrency: 2        # Worker pool size (1-10, default 2 for WSL, 5 elsewhere)\n  \n  telemetry:\n    enabled: true       # Write run logs to .agents-reverse-engineer/logs/\n    keepRuns: 50        # Retention limit for historical logs\n    costThresholdUsd: 10  # Warning threshold for cumulative costs\n  \n  pricing:              # Per-backend token cost configuration (input/output/cacheRead/cacheWrite)\n    claude: { inputCostPer1kTokens: 0.003, outputCostPer1kTokens: 0.015, ... }\n```\n\n**Environment overrides:**\n- `CLAUDE_CONFIG_DIR` — Override `~/.claude` path\n- `OPENCODE_CONFIG_DIR` — Override `~/.config/opencode` path\n- `GEMINI_CONFIG_DIR` — Override `~/.gemini` path\n- `ARE_DISABLE_HOOK` — Disable session-end auto-update (set to `1`)\n\n## Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json`\n- Aggregates per-call token counts, costs, durations, errors\n- Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`\n- Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`\n- Enforces retention via `cleanupOldLogs(keepCount)` after each run\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`\n- Enabled via `--trace` flag\n- Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta)\n- Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers\n- Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`\n\n**Progress log:** `.agents-reverse-engineer/progress.log`\n- Human-readable streaming output mirroring console\n- ETA calculation via moving average of last 10 task durations\n- Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts\n- Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`\n\n## Quality Validation\n\n**Code-vs-Doc Consistency:**\n- Extracts exported symbols via regex: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- Verifies all exports appear in `.sum` summary text via substring search\n- Reports `CodeDocInconsistency` with `missingFromDoc` arrays\n\n**Code-vs-Code Duplicate Detection:**\n- Aggregates exports across per-directory file groups into `Map<symbol, string[]>`\n- Reports `CodeCodeInconsistency` for symbols appearing in multiple files (pattern: `'duplicate-export'`)\n\n**Phantom Path Resolution:**\n- Extracts path-like strings from `AGENTS.md` via three regex patterns:\n  - Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n  - Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n  - Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n- Resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback\n- Reports `PhantomPathInconsistency` for unresolved references\n\n**Report format:** `InconsistencyReport` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity).\n\n## Incremental Update Strategy\n\n**Workflow:**\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\n**Git integration:**\n- Supports both committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag)\n- Rename detection via `git diff -M` (50% similarity threshold)\n- Fallback to SHA-256 hashing for non-git workflows\n\n## Subprocess Resource Management\n\n**Problem:** Claude CLI spawns excessive Node.js threads (GitHub #5771: 200 instances reported). With concurrency=5, each subprocess spawns internal threads causing RAM exhaustion in WSL environments.\n\n**Mitigations applied in `src/ai/subprocess.ts`:**\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments\n\n**Timeout enforcement:**\n- SIGTERM sent at `timeoutMs`\n- SIGKILL escalation after 5s grace period\n- Unref'd timeout handle allows Node.js exit without cleanup blocking\n\n## Build & Development\n\n**Install dependencies:**\n```bash\nnpm install\n```\n\n**Development mode (hot reload):**\n```bash\nnpm run dev\n# Equivalent to: tsx watch src/cli/index.ts\n```\n\n**Build TypeScript:**\n```bash\nnpm run build\n# Compiles src/ → dist/ via tsc\n```\n\n**Prepare for publish:**\n```bash\nnpm run prepublishOnly\n# Executes: npm run build && npm run build:hooks\n# Copies hooks/ → hooks/dist/ for npm tarball inclusion\n```\n\n**Run from source:**\n```bash\nnode dist/cli/index.js generate\n# Or via npm link:\nnpm link\nare generate\n```\n\n## Session Hooks\n\n**Claude/Gemini session lifecycle:**\n- `are-check-update.js` (SessionStart): Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, caches result to `~/.claude/cache/are-update-check.json`\n- `are-session-end.js` (SessionEnd): Runs `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached background process if changes detected\n\n**OpenCode plugin system:**\n- `opencode-are-check-update.js`: Exports `AreCheckUpdate()` async factory returning plugin with `event['session.created']` handler\n- `opencode-are-session-end.js`: Exports `AreSessionEnd()` async factory returning plugin with `event['session.deleted']` handler\n\n**Detached spawn pattern:**\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\n**Disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config flag: `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no YAML parser)\n\n## Integration with AI Assistants\n\n**Platform-specific command formats:**\n\n**Claude Code:**\n- Location: `.claude/skills/are-generate/SKILL.md`\n- Frontmatter: `name: /are-generate`\n- Long-running pattern: Remove stale `progress.log`, run with `run_in_background: true`, poll via `tail -5` every 10-15s, check `TaskOutput` with `block: false`, summarize on completion\n\n**OpenCode:**\n- Location: `.opencode/commands/are-generate.md`\n- Frontmatter: `agent: build`\n- Identical progress monitoring pattern\n\n**Gemini:**\n- Location: `.gemini/commands/are-generate.toml`\n- Format: `description = \"...\"` and `prompt = \"\"\"...\"\"\"` triple-quoted multiline\n\n**Aider:**\n- Detection only via `.aider.conf.yml` or `.aider/` directory\n- No template generation (manual integration required)\n\n## CI/CD\n\n**GitHub Actions workflow:** `.github/workflows/publish.yml`\n- Triggers on `release[published]` events or manual `workflow_dispatch`\n- Executes `ubuntu-latest` job with `id-token: write` permission enabling Sigstore-signed provenance\n- Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`\n- Executes `npm ci`, `npm run build` (invokes `tsc` + `build:hooks` from `prepublishOnly`)\n- Publishes with `npm publish --provenance --access public` using `NPM_TOKEN` secret\n- Cryptographic attestation links published artifact to source commit SHA\n\n## Roadmap & Known Limitations\n\n**Stub Backends:**\n- Gemini CLI adapter throws `SUBPROCESS_ERROR` until JSON output format stabilizes\n- OpenCode CLI adapter throws `SUBPROCESS_ERROR` until JSONL parsing implemented\n\n**Quality Validator Limitations:**\n- Regex-based export extraction misses complex patterns (destructured, namespace, dynamic exports)\n- Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface\n- Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication\n\n**Disabled Features:**\n- Density validator (`validateFindability`) disabled after structured `publicInterface` removal from `SumFileContent` schema",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 6262,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 62876,
      "latencyMs": 152183,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 138,
    "totalInputTokens": 414,
    "totalOutputTokens": 157734,
    "totalCacheReadTokens": 2196360,
    "totalCacheCreationTokens": 1846471,
    "totalDurationMs": 4586824,
    "errorCount": 0,
    "totalFilesRead": 107,
    "uniqueFilesRead": 107
  }
}