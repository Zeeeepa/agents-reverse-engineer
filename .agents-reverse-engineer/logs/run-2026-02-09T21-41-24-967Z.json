{
  "runId": "2026-02-09T21:41:24.967Z",
  "startTime": "2026-02-09T21:41:24.967Z",
  "endTime": "2026-02-09T21:49:24.273Z",
  "entries": [
    {
      "timestamp": "2026-02-09T21:41:24.969Z",
      "prompt": "Generate a comprehensive project specification from the following documentation.\n\n## AGENTS.md Files (30 directories)\n\n### .github/workflows/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\n**GitHub Actions workflow for automated npm package publishing with cryptographic provenance attestation on release events.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Automates npm publish on `release[published]` events with Sigstore-signed provenance linking published tarball to source commit SHA via GitHub OIDC provider.\n\n## Workflow Architecture\n\n**Trigger Conditions**: Executes on `release[published]` GitHub webhook events and manual `workflow_dispatch` invocation.\n\n**Runtime Environment**: Single `publish` job on `ubuntu-latest` runner with Node.js 20, npm registry authentication via `NODE_AUTH_TOKEN` secret, and `id-token: write` permission enabling OIDC-based attestation signing.\n\n**Build Pipeline**: Sequential steps execute `npm ci` (lockfile-based clean install), `npm run build` (invokes `prepublishOnly` hook running TypeScript compilation via `tsc` and hook file copying via `scripts/build-hooks.js`), then `npm publish --provenance --access public` generating Sigstore transparency log entry cryptographically binding published artifact to GitHub Actions workflow run and source commit.\n\n## Integration Points\n\n**Package Build System**: Depends on `prepublishOnly` script in root `package.json` executing `npm run build && npm run build:hooks`, compiling `src/` to `dist/` and copying `hooks/` to `hooks/dist/` for tarball inclusion.\n\n**Security Model**: Uses GitHub Actions secrets (`NPM_TOKEN`) for registry write access and native GitHub OIDC provider for provenance signing without long-lived cryptographic keys.\n\n**Provenance Chain**: The `--provenance` flag generates SLSA build attestation including workflow file path (`.github/workflows/publish.yml`), runner environment metadata, and commit SHA, recorded in Sigstore's public transparency log for post-publication verification via `npm audit signatures`.\n### AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Configuration & Metadata\n\n**[package.json](./package.json)** — Defines npm package identity (`agents-reverse-engineer@0.7.1`, MIT license), binary entry points (`are`, `agents-reverse-engineer` → `dist/cli/index.js`), dependencies (`fast-glob`, `ignore`, `isbinaryfile`, `simple-git`, `yaml`, `zod`, `ora`, `picocolors`), build scripts (`tsc`, `build-hooks.js`), and distribution whitelist (`[\"dist\", \"hooks/dist\", \"README.md\", \"LICENSE\"]`).\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler targeting ES2022 with NodeNext module resolution, strict type-checking, declaration/source map generation, compiling `src/**/*` → `dist/` with JSON module support for `package.json` version extraction.\n\n**[LICENSE](./LICENSE)** — MIT License granting unrestricted use/modification/distribution rights with liability disclaimer, copyright held by GeoloeG-IsT (2026).\n\n### Documentation\n\n**[README.md](./README.md)** — User-facing installation guide (interactive `npx` installer with `--runtime`/`-g`/`-l` flags), command reference (`are init/discover/generate/update/specify/clean`, `/are-*` AI skills), configuration schema (`.agents-reverse-engineer/config.yaml` with `exclude.patterns`, `ai.backend`, `ai.concurrency`), generated artifact formats (`.sum` YAML frontmatter with `content_hash`, `AGENTS.md` directory overviews, root platform documents), workflow sequences (init → discover → generate → update), and concurrency tuning recommendations (1-10 range, default 2 for WSL/5 elsewhere).\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Reference table mapping 25 programming language ecosystems to manifest file patterns (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`) with `✓` marks indicating detection support in Phase 2 directory aggregation via `detectManifestFiles()` (9 active patterns: JavaScript/Python/Go/Rust/Java/Ruby/PHP/C/C++).\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD automation for npm publishing with Sigstore-signed provenance attestation via `actions/setup-node@v4` and `npm publish --provenance --access public` on `release[published]` events.\n\n**[docs/](./docs/)** — Original vision document (`INPUT.md`) defining RLM algorithm with post-order traversal methodology, directory documentation schema (9 supplementary doc types: ARCHITECTURE.md, STACK.md, PATTERNS.md), multi-platform requirements (Claude/OpenCode/Gemini), and research references (GSD, BMAD, SpecKit).\n\n**[hooks/](./hooks/)** — Session lifecycle automation scripts: `are-check-update.js` (SessionStart hook spawning detached `npm view` query with cache to `~/.claude/cache/are-update-check.json`), `are-session-end.js` (SessionEnd hook triggering `npx agents-reverse-engineer@latest update --quiet` on git changes), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting `event['session.created']`/`event['session.deleted']` handlers).\n\n**[scripts/](./scripts/)** — Build automation via `build-hooks.js` copying `.js` files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle for npm tarball inclusion.\n\n**[src/](./src/)** — TypeScript source tree implementing nine functional subsystems: `ai/` (subprocess pooling with retry/telemetry), `change-detection/` (git diff + SHA-256 hashing), `cli/` (command entry points with `parseArgs()` routing), `config/` (YAML schema validation with resource-adaptive concurrency), `discovery/` (gitignore-aware filtering with binary detection), `generation/` (three-phase orchestrator with prompt builders), `imports/` (regex-based import extraction), `installer/` (npx-driven hook deployment), `integration/` (platform-specific template generation), `orchestration/` (iterator-based worker pool with progress/trace/plan tracking), `output/` (picocolors terminal logging), `quality/` (code-vs-doc/code-vs-code/phantom-paths validation), `rebuild/` (AI-driven project reconstruction from specs), `specify/` (11-section spec synthesis from `AGENTS.md` corpus), `types/` (shared discovery result interfaces), `update/` (incremental regeneration with orphan cleanup).\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1 (Concurrent File Analysis)**: Iterator-based worker pool shares `tasks.entries()` across N workers via atomic `.next()` calls. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash` for incremental update detection.\n\n**Phase 2 (Post-Order Directory Aggregation)**: Sorts directories by depth descending via `path.relative().split(path.sep).length`. `isDirectoryComplete()` predicate polls child `.sum` existence before processing directory. `buildDirectoryPrompt()` aggregates child summaries via `readSumFile()`, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, and manifest detection (9 types). Writes `AGENTS.md` preserving `AGENTS.local.md` user content above generated sections.\n\n**Phase 3 (Root Document Synthesis)**: Sequential execution (concurrency=1) generates `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` consuming all `AGENTS.md` via `collectAgentsDocs()` recursive traversal, parsing root `package.json` for project metadata, enforcing synthesis-only constraints (no feature invention).\n\n### Resource Management\n\n**Subprocess limits**: 512MB heap (`--max-old-space-size`), 4 libuv threads (`UV_THREADPOOL_SIZE`), process group killing (`kill(-pid)`) on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: `[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`).\n\n**Concurrency formula**: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` prevents RAM exhaustion in WSL/low-memory environments.\n\n### Incremental Update Strategy\n\nHash-based change detection via `.sum` frontmatter `content_hash` comparison against SHA-256 of current content. Segregates files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed sources. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining files. `getAffectedDirectories()` walks parent paths to root, sorts deepest-first. Regenerates Phase 1 for changed files, Phase 2 for affected directories (skips Phase 3).\n\n### Quality Validation\n\n**Code-vs-Doc**: Extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in `.sum` summaries.\n\n**Code-vs-Code**: Aggregates exports into `Map<symbol, string[]>` per directory, reports duplicates.\n\n**Phantom Paths**: Extracts file references via three patterns (markdown links, backtick paths, prose-embedded `src/` mentions), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax).\n\n### Telemetry & Tracing\n\n**Run logs** (`.agents-reverse-engineer/logs/run-<timestamp>.json`): Aggregates token counts, costs, durations, errors per AI call. Tracks `filesRead[]` with `path`, `sizeBytes`, `linesRead`. Enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace events** (`.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`): Emits `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry` events with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields. Promise-chain serialization ensures NDJSON line order.\n\n**Progress log** (`.agents-reverse-engineer/progress.log`): Streams `[X/Y] ANALYZING/DONE/FAIL path` with ETA calculation via moving average of last 10 task durations.\n\n## Workflows\n\n### Installation\n\nInteractive installer via `npx agents-reverse-engineer@latest` prompts for runtime (`claude`, `opencode`, `gemini`, `all`) and location (`-g` global, `-l` local). Non-interactive mode uses `--runtime <rt>` with `-g`/`-l` flags. Installs commands to `~/.claude/commands/`, hooks to `~/.claude/hooks/`, registers ARE permissions in `settings.json`, writes `ARE-VERSION` file.\n\n### Documentation Generation\n\n1. **Initialize**: `are init` → creates `.agents-reverse-engineer/config.yaml`\n2. **Discover**: `are discover --plan` → writes `GENERATION-PLAN.md` with post-order traversal plan\n3. **Generate**: `are generate` → Phase 1 (`.sum`), Phase 2 (`AGENTS.md`), Phase 3 (root docs)\n4. **Update**: `are update --uncommitted` → incremental regeneration for changed files only\n5. **Clean**: `are clean` → removes `.sum`, generated `AGENTS.md`, root docs\n\n### Project Specification Synthesis\n\n`are specify` aggregates all `AGENTS.md` into `specs/SPEC.md` with 11-section structure: Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts (Runtime Behavior + Implementation Contracts), Test Contracts, Build Plan, Prompt Templates, IDE Integration. Use `--multi-file` for split specs (`specs/<slug>.md`), `--dry-run` for preview.\n\n## Behavioral Contracts\n\n### Subprocess Resource Limits\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts\n- SIGTERM at `timeoutMs`\n- SIGKILL at `timeoutMs + 5000ms`\n- Process group kill: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Detection Patterns\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n// Default: { maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Import Extraction Regex\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Phantom Path Extraction Patterns\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g                          // Markdown links\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g              // Backtick paths\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi     // Prose-embedded paths\n```\n\n### Git Diff Format\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n## IDE Integration\n\nCommands installed via `npx agents-reverse-engineer@latest --runtime <rt> -g`:\n\n- `/are-init` — Create `.agents-reverse-engineer/config.yaml`\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation (long-running: use `run_in_background: true`, poll `progress.log` with `tail -5`)\n- `/are-update` — Incremental update\n- `/are-specify` — Synthesize project specification\n- `/are-clean` — Remove generated artifacts\n\nSession hooks:\n- **SessionStart**: `are-check-update.js` (version check with cache)\n- **SessionEnd**: `are-session-end.js` (auto-update on git changes)\n\nDisable hooks: `ARE_DISABLE_HOOK=1` or `hook_enabled: false` in config.\n\n## Dependencies\n\n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n**Core**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (git diff), `yaml` (config parsing), `zod` (schema validation)\n\n**UI**: `ora` (spinners), `picocolors` (ANSI colors)\n\n**Build**: TypeScript 5.7.3 (ES2022 target, NodeNext resolution, strict mode), `tsx` (dev watch mode)\n### docs/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\n**Original vision document defining the Recursive Language Model (RLM) algorithm for agents-reverse-engineer, establishing post-order traversal methodology, directory documentation schema, multi-platform requirements, and integration references.**\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nDefines RLM algorithm executing post-order traversal: build project tree, analyze leaf files to generate `{filename}.sum` summaries, synthesize directory `AGENTS.md` when all children complete, recurse to root producing `CLAUDE.md` and platform-specific docs.\n\n## Recursive Language Model Algorithm\n\n**Core traversal strategy:**\n1. Build complete project structure tree via directory walk\n2. Execute at first leaf node (file level)\n3. Generate `{filename}.sum` per file via AI analysis\n4. When directory leaves complete, synthesize `AGENTS.md` consuming child `.sum` files\n5. Recurse backward to root producing final integration docs\n\n**Guarantees:** Child summaries exist before parent directory analysis, enabling bottom-up synthesis where directory docs reference already-generated file summaries and subdirectory `AGENTS.md` files.\n\n## Directory Documentation Schema\n\nINPUT.md defines nine supplementary document types referenceable from `AGENTS.md`:\n\n- `ARCHITECTURE.md` — system design and component relationships\n- `STRUCTURE.md` — directory organization rationale\n- `STACK.md` — technology stack and dependencies\n- `INTEGRATIONS.md` — external service integrations\n- `INFRASTRUCTURE.md` — deployment and runtime configuration\n- `CONVENTIONS.md` — coding standards and naming patterns\n- `TESTING.md` — test strategy and frameworks\n- `PATTERNS.md` — design patterns and architectural idioms\n- `CONCERNS.md` — known issues and technical debt\n\n## Multi-Platform Requirements\n\n**Target runtimes:** Claude Code, OpenCode, and alternative LLM agent tools requiring brownfield project context.\n\n**Core features:**\n- Command execution using RLM with Claude Code or alternative backends\n- Session-end hook for incremental updates of impacted files\n- `AGENTS.md` generation in every project directory\n- Platform-agnostic documentation format\n\n**Usage commands:**\n```bash\n/are-generate  # Full documentation generation via RLM\n/are-update    # Incremental update of changed files\n```\n\n## Research References\n\nDocument references three methodologies for implementation analysis:\n\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — repository structure and brownfield approaches (primary)\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — partial analysis of special commands and codebase patterns\n- **SpecKit**: https://github.com/github/spec-kit — complementary tooling for specification synthesis\n### hooks/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**Session lifecycle automation scripts for IDE integration: background version checking at session start and automatic documentation updates at session end across Claude Code, Gemini CLI, and OpenCode platforms via detached subprocess spawning with git-based change detection.**\n\n## Contents\n\n### Platform-Agnostic Scripts\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached `npm view` query comparing registry version against local ARE-VERSION files, writing availability status to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`. Prioritizes `.claude/ARE-VERSION` over `~/.claude/ARE-VERSION`, defaults to `'0.0.0'` if missing. Executes via shebang `#!/usr/bin/env node`.\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached process when `git status --porcelain` detects uncommitted changes. Exits early if `ARE_DISABLE_HOOK=1` environment variable set or `.agents-reverse-engineer.yaml` contains `hook_enabled: false` substring. ES module using `import` syntax.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — Exports `AreCheckUpdate()` async factory returning `{ event: { 'session.created': AsyncFunction } }` plugin object. Handler spawns detached Node.js subprocess writing to `~/.config/opencode/cache/are-update-check.json`. Reads version from `.opencode/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`, queries npm registry with 10s timeout, creates cache directory via `mkdirSync(cacheDir, { recursive: true })`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — Exports `AreSessionEnd()` async factory returning `{ event: { 'session.deleted': AsyncFunction } }` plugin object. Implements four-stage gate: environment check (`ARE_DISABLE_HOOK`), config substring search (`hook_enabled: false`), git status parsing, detached `npx` spawn with `stdio: 'ignore'` and `child.unref()`.\n\n## Architecture\n\n### Detached Background Execution Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\nInline script injection via `-e` flag avoids filesystem dependencies. `stdio: 'ignore'` suppresses output streams. `detached: true` creates independent process group. `unref()` allows parent process termination without waiting for child completion. `windowsHide: true` prevents console window flash on Windows.\n\n### Version File Resolution Priority\n\nCheck hooks prioritize project-local over global version files:\n\n1. **Claude/Gemini**: `.claude/ARE-VERSION` → `~/.claude/ARE-VERSION`\n2. **OpenCode**: `.opencode/ARE-VERSION` → `~/.config/opencode/ARE-VERSION`\n\nResolution via `existsSync()` + `readFileSync()` with `'0.0.0'` fallback when neither exists.\n\n### Cache File Schema\n\nUpdate check hooks write JSON with four-field schema:\n\n```json\n{\n  \"update_available\": boolean,  // installed !== latest (whitespace-trimmed)\n  \"installed\": string,          // version from ARE-VERSION or '0.0.0'\n  \"latest\": string,             // npm registry response or 'unknown' on error\n  \"checked\": number             // Unix timestamp via Math.floor(Date.now() / 1000)\n}\n```\n\nCache locations:\n- **Claude/Gemini**: `~/.claude/cache/are-update-check.json`\n- **OpenCode**: `~/.config/opencode/cache/are-update-check.json`\n\nDirectory creation via `mkdirSync(cacheDir, { recursive: true })` before write.\n\n### Disable Mechanisms\n\nSession-end hooks implement two gates:\n\n1. **Environment**: Exit if `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file**: Exit if `.agents-reverse-engineer.yaml` exists and `readFileSync().includes('hook_enabled: false')` (substring search, no YAML parser)\n\nBoth gates return silently without error signaling.\n\n## Platform Integration\n\n### Claude Code / Gemini CLI\n\nPlatform-agnostic scripts installed via `src/installer/operations.ts`:\n- Check hook: `~/.claude/hooks/are-check-update.js` or `~/.gemini/hooks/are-check-update.js`\n- End hook: `~/.claude/hooks/are-session-end.js` or `~/.gemini/hooks/are-session-end.js`\n\nRuntime invocation via SessionStart/SessionEnd trigger names.\n\n### OpenCode\n\nPlugin factories installed to `~/.config/opencode/plugins/` or `$OPENCODE_CONFIG_DIR/plugins/`. Event handler contracts:\n- `AreCheckUpdate()` → `event['session.created']`\n- `AreSessionEnd()` → `event['session.deleted']`\n\nPlugin objects returned from async factories conform to OpenCode event system shape `{ event: { [eventName]: AsyncFunction } }`.\n\n## npm Registry Query\n\nAll check hooks execute `npm view agents-reverse-engineer version` via `execSync()` with options:\n- `encoding: 'utf8'` for string output\n- `timeout: 10000` (10 second limit)\n- `windowsHide: true` to suppress console window on Windows\n\nTry-catch wrapper sets `latest = 'unknown'` on network/timeout errors without propagating exceptions.\n\n## Git Change Detection\n\nSession-end hooks invoke `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exit early if:\n- `status.trim()` returns empty string (clean working tree)\n- `execSync()` throws error (non-git repo or git unavailable)\n\nNo error logging — silent exit on all failure modes.\n\n## File System Operations\n\n**Shared imports across all hooks:**\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `child_process`: `spawn`, `execSync`\n- `path`: `join`\n- `os`: `homedir`\n\n**Module types:**\n- `are-session-end.js`: ES module (`import` syntax)\n- `are-check-update.js`, `opencode-*`: CommonJS-compatible (`#!/usr/bin/env node` shebang)\n\n## Build Integration\n\nHooks copied to distribution via `scripts/build-hooks.js` during `npm run build:hooks` (invoked by `prepublishOnly`). No TypeScript compilation — plain JavaScript for direct execution in IDE runtime environments.\n### scripts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\n**Build automation directory containing pre-publish hook file preparation script that copies session lifecycle hooks from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Contents\n\n### Build Scripts\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` files from `hooks/` to `hooks/dist/`, invoked by `npm run prepublishOnly` to prepare session lifecycle hooks for npm tarball inclusion.\n\n## Build Pipeline Integration\n\n`build-hooks.js` executes during npm publish workflow via `prepublishOnly` lifecycle hook defined in root `package.json`. The script ensures `hooks/dist/` contains all four session lifecycle hooks before `npm publish` bundles the package tarball:\n\n1. **are-check-update.js** — Claude/Gemini SessionStart hook for version checking\n2. **are-session-end.js** — Claude/Gemini SessionEnd hook for auto-update\n3. **opencode-are-check-update.js** — OpenCode plugin for version checking\n4. **opencode-are-session-end.js** — OpenCode plugin for session-end updates\n\nThe copied files become the source for installer operations (`src/installer/operations.ts`) which deploy hooks to IDE configuration directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`, etc.).\n\n## Execution Flow\n\n1. Resolves project root via `fileURLToPath(import.meta.url)` → `path.dirname()` → `path.join(__dirname, '..')`\n2. Creates `hooks/dist/` via `mkdirSync(HOOKS_DIST, { recursive: true })` if directory missing\n3. Filters `readdirSync('hooks/')` for `.js` files, excluding `'dist'` directory itself\n4. Copies each file via `copyFileSync(src, dest)` where `src = 'hooks/<file>'` and `dest = 'hooks/dist/<file>'`\n5. Logs each operation: `\"  Copied: ${file} -> hooks/dist/${file}\"`\n6. Prints final count: `\"Done. ${hookFiles.length} hook(s) built.\"`\n\n## Package.json Integration\n\nThe root `package.json` declares:\n- **files**: `[\"hooks/dist\"]` — includes `hooks/dist/` in npm tarball\n- **scripts.build:hooks**: `\"node scripts/build-hooks.js\"` — manual build command\n- **scripts.prepublishOnly**: `\"npm run build && npm run build:hooks\"` — automatic execution before `npm publish`\n\nCI/CD workflow (`.github/workflows/publish.yml`) runs `npm ci` which triggers `prepublishOnly`, ensuring `hooks/dist/` exists before registry upload with Sigstore-signed provenance attestation.\n### src/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Root source directory implementing ARE's three-phase documentation generation pipeline via TypeScript modules organized into nine functional subsystems: AI service orchestration (subprocess pooling, retry logic, telemetry), file discovery (gitignore-aware filtering, binary detection), change detection (git diff parsing, SHA-256 hashing), generation (concurrent file analysis, post-order directory aggregation, root synthesis), quality validation (code-vs-doc, phantom paths), CLI commands (generate, update, specify, rebuild, clean), configuration (YAML schema, resource-adaptive concurrency), IDE integration (template generation for Claude/OpenCode/Gemini), and installer (npx-driven hook deployment).**\n\n## Contents\n\n**[version.ts](./version.ts)** — Exports `getVersion()` extracting package version from `../package.json` via `fileURLToPath(import.meta.url)` + `dirname()` + `join(__dirname, '..', 'package.json')`, returns `'unknown'` string literal on read/parse failure. Consumed by CLI `--version` flag, session lifecycle hooks for npm version comparison, and telemetry run log metadata.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI CLI orchestration layer with subprocess pooling (10MB maxBuffer, SIGTERM/SIGKILL timeout enforcement, process group termination via `kill(-pid)`), exponential backoff retry detecting rate limits via stderr patterns (`[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]`), token cost telemetry accumulating `TelemetryEntry[]` with NDJSON persistence to `.agents-reverse-engineer/logs/run-<timestamp>.json`, and trace emission (`subprocess:spawn/exit`, `retry` events). Backends: ClaudeBackend (Zod-validated JSON parsing), GeminiBackend/OpenCodeBackend (stubs throwing `SUBPROCESS_ERROR`). Resource limits: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`.\n\n**[change-detection/](./change-detection/)** — Git-based delta computation via `git diff --name-status -M` with tab-split parsing extracting status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, SHA-256 content hashing via `createHash('sha256').update(content).digest('hex')`, uncommitted change merging from `git.status()` with linear `Array.some()` deduplication. Exports `getChangedFiles()` returning `ChangeDetectionResult` with `changes[]` discriminated union (`FileChange` with optional `oldPath` for renames).\n\n**[cli/](./cli/)** — Command entry points parsing `process.argv` via `parseArgs()`, routing to `init`/`discover`/`generate`/`update`/`specify`/`rebuild`/`clean` handlers, managing shared flags (`--debug`, `--trace`, `--dry-run`, `--concurrency`, `--model`). Integrates `ProgressLog` streaming to `.agents-reverse-engineer/progress.log`, `TraceWriter` emitting NDJSON to `traces/`, `AIService` backend resolution with `CLI_NOT_FOUND` exit code 2. Model resolution: CLI overrides config, specify/rebuild upgrade sonnet → opus for quality-critical synthesis.\n\n**[config/](./config/)** — YAML configuration loading via `loadConfig()` reading `.agents-reverse-engineer/config.yaml`, Zod validation with `ConfigSchema` applying defaults from `defaults.ts` (18-entry `DEFAULT_VENDOR_DIRS`, 26-entry `DEFAULT_EXCLUDE_PATTERNS`, 26-entry `DEFAULT_BINARY_EXTENSIONS`). Resource-adaptive concurrency formula: `clamp(os.availableParallelism() * 5, 2, min(20, floor(os.totalmem() * 0.5 / 0.512)))` preventing RAM exhaustion on low-memory systems. Environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery via `discoverFiles()` facade composing `walkDirectory()` (fast-glob with `absolute: true`, `onlyFiles: true`, hardcoded `ignore: ['**/.git/**']`) and four-stage filter chain: `createGitignoreFilter()` (parses `.gitignore` via `ignore` library), `createVendorFilter()` (excludes 10 default directories via Set + path-pattern matching), `createBinaryFilter()` (96-extension allowlist + `isBinaryFile()` content analysis at 1MB threshold), `createCustomFilter()` (user glob patterns). Returns `FilterResult` with `included[]` and `excluded[]` arrays, latter containing `{ file, reason, filter }` objects.\n\n**[generation/](./generation/)** — Orchestrates three-phase pipeline: `GenerationOrchestrator.createPlan()` builds `AnalysisTask[]` via `buildFilePrompt()` with import maps and project structure, clears `PreparedFile.content` after prompt embedding to free heap. `buildExecutionPlan()` constructs dependency-aware `ExecutionPlan` with post-order directory sorting (deepest-first via `getDirectoryDepth()`), `isDirectoryComplete()` predicate polling child `.sum` existence. Phase 1: concurrent file analysis writes `.sum` with SHA-256 `content_hash` frontmatter. Phase 2: directory aggregation via `buildDirectoryPrompt()` reading child `.sum` + manifest detection (9 types) + import maps, writes `AGENTS.md` via `writeAgentsMd()` preserving `AGENTS.local.md` user content. Phase 3: sequential root synthesis via `buildRootPrompt()` consuming all `AGENTS.md`, writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n**[imports/](./imports/)** — Extracts TypeScript/JavaScript import statements via `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`, classifies as internal (`.` prefix) or external (`..` prefix), formats as structured import maps via `formatImportMap()` producing `fileName:\\n  specifier → symbol1, symbol2 (type)` output. Reads first 100 lines per file assuming imports clustered at top, consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` for LLM prompt context.\n\n**[installer/](./installer/)** — npx-driven installer orchestrating command/hook deployment via interactive prompts (`arrowKeySelect()` with raw mode `\\x1b[${n}A` ANSI sequences) or non-interactive mode (requires `--runtime` + `-g/-l` flags). Copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}`, registers hooks with nested `HookEvent.hooks[]` schema (Claude) or flat `GeminiHook[]` (Gemini), adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` from `getPackageVersion()`. Uninstalls via `unregisterClaudeHooks()` filtering by `getHookPatterns()` (matches current + legacy formats), `cleanupEmptyDirs()` recursively.\n\n**[integration/](./integration/)** — Platform-specific integration file generation via `generateIntegrationFiles()` detecting `.claude/`, `.opencode/`, `.aider.conf.yml`, `.gemini/` artifacts. Templates from `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` with frontmatter variants (Claude `name:` field, OpenCode `agent: build`, Gemini TOML `description`/`prompt`). Background execution pattern: Read `VERSION_FILE_PATH`, run `npx agents-reverse-engineer@latest {command}` with `run_in_background: true`, poll `.agents-reverse-engineer/progress.log` with `offset` parameter, check `TaskOutput` with `block: false`. Copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`.\n\n**[orchestration/](./orchestration/)** — Worker pool via `runPool()` sharing single `tasks.entries()` iterator across N workers (atomic `.next()` calls prevent duplicate pickups), fail-fast via shared `aborted` flag. `CommandRunner.executeGenerate()` runs three-phase pipeline with quality validation: pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks via regex export extraction and substring matching, post-phase-2 phantom path resolution via three regex patterns. `ProgressReporter` streams `[X/Y] ANALYZING/DONE/FAIL path` to console + `.agents-reverse-engineer/progress.log`, calculates ETA via moving average of last 10 task durations. `TraceWriter` emits NDJSON with auto-populated `seq`/`ts`/`pid`/`elapsedMs` fields, serializes via promise-chain pattern. `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via regex `/- \\[ \\] \\`${itemPath}\\`/` → `/- \\[x\\] \\`${itemPath}\\`/`.\n\n**[output/](./output/)** — Terminal logging via `createLogger()` with conditional color mode (picocolors vs identity passthrough). Format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes. `stripAnsi(str)` removes color codes via `/\\x1b\\[[0-9;]*m/g` for progress.log persistence.\n\n**[quality/](./quality/)** — Post-generation validation detecting code-vs-doc (exported symbols missing from `.sum`), code-vs-code (duplicate exports within directory), phantom-paths (unresolvable file references in `AGENTS.md`). `extractExports()` via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies substring presence. `checkPhantomPaths()` extracts paths via three regex patterns (markdown links, backtick paths, prose-embedded src/), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax). `buildInconsistencyReport()` aggregates `Inconsistency[]` discriminated union into `InconsistencyReport` with severity stratification (`info`/`warning`/`error`).\n\n**[rebuild/](./rebuild/)** — AI-driven project reconstruction from specification documents via `executeRebuild()` executing ordered phase workflow: `partitionSpec()` extracts `RebuildUnit[]` from Build Plan phases (`### Phase N:`), sequential order group processing with concurrent `runPool()` execution per group, progressive context accumulation from generated file exports with LRU truncation at 100k chars. `CheckpointManager` provides session continuity via SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), promise-chain write serialization to `.rebuild-checkpoint` JSON. `parseModuleOutput()` extracts file paths via delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback. Targeted API injection via Defines/Consumes keyword matching filtering subsections with fuzzy matching (substring, word overlap).\n\n**[specify/](./specify/)** — Project specification synthesis via `buildSpecPrompt()` constructing `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration). `writeSpec()` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings` partitioning on `/^(?=# )/m` with slugification transforms), throws `SpecExistsError` on conflicts unless `force=true`.\n\n**[types/](./types/)** — Core TypeScript interfaces for file discovery results: `DiscoveryResult` (`files[]`, `excluded[]`), `ExcludedFile` (`path`, `reason`), `DiscoveryStats` (`totalFiles`, `includedFiles`, `excludedFiles`, `exclusionReasons` histogram). Flows from discovery filters → orchestrator → worker pool → generation writers.\n\n**[update/](./update/)** — Incremental documentation update via `UpdateOrchestrator.preparePlan()` comparing YAML frontmatter `content_hash` against SHA-256 hashes of current content via `computeContentHash()`, segregating into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), `cleanup` (orphaned). `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files (targeting `oldPath` for renames), `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no remaining source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to project root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Architecture\n\n### Three-Phase Pipeline Coordination\n\nPhase 1 (`generation/orchestrator.ts`): `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()` embedding import maps from `imports/extractor.ts`, clears `PreparedFile.content` after prompt construction to free heap. `runPool()` spawns N workers sharing `tasks.entries()` iterator (atomic `.next()` calls), each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits, writes `.sum` with YAML frontmatter (`generated_at`, `content_hash` SHA-256 hex, `purpose`, `critical_todos`, `related_files`).\n\nPhase 2 (`generation/executor.ts`): `buildExecutionPlan()` sorts directories by depth descending via `getDirectoryDepth()` (deepest-first), creates `directoryTasks[]` depending on child file task IDs for post-order traversal. `isDirectoryComplete()` predicate polls child `.sum` existence before processing. `buildDirectoryPrompt()` reads child `.sum` via `readSumFile()`, aggregates subdirectory `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects manifests (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). `writeAgentsMd()` preserves `AGENTS.local.md` user content above generated sections with `GENERATED_MARKER` injection.\n\nPhase 3 (`generation/orchestrator.ts`): Sequential execution (concurrency=1) via `rootTasks[]` depending on all directory task IDs. `buildRootPrompt()` consumes all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json`, enforces synthesis-only constraints (no invented features). Strips conversational preamble via markdown start detection (`indexOf('# ')`), writes `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`.\n\n### Iterator-Based Worker Pool\n\n`orchestration/pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern preventing NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Quality Validation Phases\n\nPre-Phase-1: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection.\n\nPost-Phase-1: `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions) via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, substring matching in summary. `checkCodeVsCode()` aggregates exports into `Map<symbol, string[]>` per directory, detects duplicates.\n\nPost-Phase-2: `checkPhantomPaths()` extracts paths via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves via `existsSync()` at `agentsMdDir`/`projectRoot`/.ts fallback locations, filters via `SKIP_PATTERNS`.\n\n### Incremental Update Strategy\n\n`update/orchestrator.ts` workflow: `preparePlan()` calls `discoverFiles()`, iterates results constructing `.sum` path via `getSumPath()`, calls `readSumFile()` extracting `contentHash` from frontmatter, missing `.sum` adds `{ path, status: 'added' }` to `filesToAnalyze`, existing `.sum` calls `computeContentHash()` comparing hashes, mismatch adds `{ path, status: 'modified' }`, match adds to `filesToSkip`. `cleanupOrphans()` deletes `.sum`/`.annex.md` for deleted/renamed files, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with no source files. `getAffectedDirectories()` walks parent paths via `path.dirname()` to root, sorts deepest-first. External caller invokes Phase 1 for `filesToAnalyze`, Phase 2 for `affectedDirs` (skips Phase 3).\n\n## Behavioral Contracts\n\n### Default Concurrency Formula (config/defaults.ts)\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n### Subprocess Resource Limits (ai/subprocess.ts)\n```\nNODE_OPTIONS='--max-old-space-size=512'\nUV_THREADPOOL_SIZE=4\nCLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1\n--disallowedTools Task\n```\n\n### Process Lifecycle Timeouts (ai/subprocess.ts)\n- SIGTERM sent at `timeoutMs` via `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n- Process group termination: `kill(-pid, 'SIGKILL')`\n\n### Rate Limit Patterns (ai/service.ts)\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n### Exponential Backoff Formula (ai/retry.ts)\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n### Default Retry Configuration (ai/retry.ts)\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n### Git Diff Format (change-detection/detector.ts)\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n### Binary Extension Set (discovery/filters/binary.ts)\n96 extensions including `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Extraction Patterns (quality/phantom-paths/validator.ts)\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n### Import Regex Pattern (imports/extractor.ts)\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n### Build Plan Phase Extraction (rebuild/spec-reader.ts)\n```regex\n/^### Phase (\\d+):\\s*(.+)$/gm\n```\n\n### Delimiter Format (rebuild/output-parser.ts)\n```regex\n/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g\n```\n\n### Heading Regex (specify/writer.ts)\n```regex\n/^(?=# )/m\n```\n\n### Slugification Transforms (specify/writer.ts)\n```typescript\n.toLowerCase()\n.replace(/\\s+/g, '-')\n.replace(/[^a-z0-9-]/g, '')\n.replace(/-+/g, '-')\n.replace(/^-|-$/g, '')\n```\n\n## Dependencies\n\n**External**: `fast-glob` (file discovery), `ignore` (gitignore parsing), `isbinaryfile` (binary detection), `simple-git` (change detection), `yaml` (config parsing), `zod` (schema validation), `ora` (spinner UI), `picocolors` (terminal colors).\n\n**Internal**: `node:fs/promises` (async file I/O), `node:fs` (sync file I/O), `node:path` (path manipulation), `node:url` (ES module URL resolution), `node:child_process` (subprocess spawning), `node:os` (system metrics), `node:crypto` (SHA-256 hashing), `node:readline` (interactive prompts).\n### src/ai/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\n**Backend-agnostic AI CLI orchestration layer implementing subprocess pooling, exponential backoff retry, token cost telemetry with NDJSON persistence, and trace emission for concurrency debugging.**\n\n## Contents\n\n**[index.ts](./index.ts)** — Public API barrel exporting `AIService`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from types, `BackendRegistry`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()` from registry, `withRetry()`, `DEFAULT_RETRY_OPTIONS` from retry, `runSubprocess()`, `isCommandOnPath()` from subprocess/backends.\n\n**[registry.ts](./registry.ts)** — `BackendRegistry` stores AIBackend instances by name with insertion-order priority (Claude → Gemini → OpenCode), `createBackendRegistry()` pre-populates with backend adapters, `detectBackend()` returns first available via `backend.isAvailable()` iteration, `resolveBackend()` handles explicit names and 'auto' mode throwing `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on miss.\n\n**[retry.ts](./retry.ts)** — `withRetry<T>()` executes async operations with exponential backoff via `baseDelayMs * multiplier^attempt` capped at `maxDelayMs` plus 0-500ms jitter, terminates immediately on non-retryable errors via `isRetryable()` predicate, invokes `onRetry()` callback before each attempt, throws last error after exhausting `maxRetries`, `DEFAULT_RETRY_OPTIONS` defines 3 retries with 1s base delay, 8s max delay, 2x multiplier.\n\n**[service.ts](./service.ts)** — `AIService` orchestrates AI calls via `call()` wrapping `runSubprocess()` with `withRetry()`, detects rate limits via stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\"), refuses to retry timeouts (inline comment: \"spawning another heavyweight subprocess on a struggling system makes things worse\"), emits `subprocess:spawn/exit` and `retry` trace events, accumulates `TelemetryEntry` records via `TelemetryLogger`, serializes writes to subprocess log files via promise-chain queue, exposes `finalize()` for `RunLog` persistence and retention enforcement via `cleanupOldLogs()`, tracks active subprocesses for debug logging with heap/RSS metrics.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess()` spawns CLI via `execFile()` with 10MB `maxBuffer`, SIGTERM timeout via `killSignal`, stdin piping for prompt delivery, unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, process group termination via `kill(-pid, 'SIGKILL')` on completion, module-level `activeSubprocesses` Map tracking PIDs with spawn timestamps, exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency debugging, never throws (always resolves `SubprocessResult`).\n\n**[types.ts](./types.ts)** — Defines `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` methods, `AICallOptions` with `prompt`, optional `systemPrompt/model/timeoutMs/maxTurns/taskLabel`, `AIResponse` with normalized `text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw`, `SubprocessResult` with `stdout/stderr/exitCode/signal/durationMs/timedOut/childPid`, `RetryOptions` with exponential backoff config and predicates, `TelemetryEntry` per-call log with token counts and file reads, `RunLog` aggregate summary, `AIServiceError` with typed codes `CLI_NOT_FOUND/TIMEOUT/PARSE_ERROR/SUBPROCESS_ERROR/RATE_LIMIT`, `FileRead` with path/sizeBytes.\n\n## Architecture\n\n**Three-Layer Abstraction**\n\n1. **Subprocess Layer** (`subprocess.ts`): Raw `execFile()` wrapper with timeout enforcement, process group killing, stdin piping, active subprocess tracking. Returns `SubprocessResult` on all code paths (never throws).\n\n2. **Retry Layer** (`retry.ts`): Generic exponential backoff with jitter, caller-provided `isRetryable()` predicate, optional `onRetry()` callback. Terminates immediately on permanent errors.\n\n3. **Service Layer** (`service.ts`): Integrates subprocess + retry + telemetry + tracing. Detects rate limits via stderr patterns, refuses to retry timeouts, accumulates token counts, emits trace events for concurrency debugging, serializes subprocess log writes via promise chain.\n\n**Backend Registry**\n\n`BackendRegistry` stores `AIBackend` instances with insertion-order priority determining auto-detection sequence. `createBackendRegistry()` registers ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. `resolveBackend()` handles two modes: explicit name lookups with availability validation, 'auto' mode calling `detectBackend()` which iterates backends invoking `isAvailable()` until first match. Throws `AIServiceError` with `CLI_NOT_FOUND` code and aggregated install instructions on failure.\n\n**Retry Strategy**\n\nRate limit detection via `isRateLimitStderr()` checking lowercase stderr for patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\". Only rate limit errors are retryable—timeouts are permanent failures (inline rationale: \"spawning another heavyweight subprocess on a system that's already struggling or against an unresponsive API makes things worse and can exhaust system resources\"). `withRetry()` invoked with custom predicate: `error instanceof AIServiceError && error.code === 'RATE_LIMIT'`.\n\n**Telemetry Pipeline**\n\n`TelemetryLogger` accumulates `TelemetryEntry` records in memory via `addEntry()` calls after each subprocess completion. Each entry captures `timestamp`, `prompt`, `systemPrompt`, `response`, `model`, token counts (`inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`), `latencyMs`, `exitCode`, `retryCount`, `filesRead[]` array. Command orchestrator calls `addFilesReadToLastEntry()` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`). `finalize()` invokes `logger.toRunLog()` producing aggregate `RunLog` with summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead` via Set deduplication), then serializes via `writeRunLog()` to `.agents-reverse-engineer/logs/run-<timestamp>.json`, enforces retention via `cleanupOldLogs(keepCount)`.\n\n**Trace Emission**\n\n`AIService` accepts optional `ITraceWriter` via `setTracer()` for concurrency debugging. Emits three event types: `subprocess:spawn` with `childPid/taskLabel/timestamp`, `subprocess:exit` with `childPid/exitCode/signal/durationMs/timedOut`, `retry` with `attempt/taskLabel/errorCode`. Trace writer (from `src/orchestration/trace.ts`) serializes events to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via promise-chain serialization ensuring write order matches emission order despite concurrent workers.\n\n**Subprocess Resource Management**\n\n`runSubprocess()` tracks active subprocesses in module-level Map with PID → `{ command, spawnedAt }` mapping. Exposes `getActiveSubprocessCount()` and `getActiveSubprocesses()` for debugging. `AIService.call()` increments `activeSubprocesses` counter before spawn, decrements after completion, logs pre-spawn debug message with active count, heap usage, RSS via `process.memoryUsage()`, and configured timeout. Post-completion debug log includes PID, exit code, duration, updated active count.\n\n**Process Lifecycle**\n\n`runSubprocess()` sequence: spawn with 10MB `maxBuffer` and SIGTERM `killSignal`, track in `activeSubprocesses` Map, invoke `onSpawn()` callback synchronously for trace emission, write `options.input` to stdin then `.end()` to close stream, set unref'd SIGKILL escalation timer at `timeoutMs + 5000ms`, on callback clear timer, attempt process group kill via `kill(-pid, 'SIGKILL')` with single-process fallback, remove from `activeSubprocesses`, resolve with `SubprocessResult`. Timeout detection via `error.killed === true` from execFile. Exit code extracted from `error.code` (if number) else `child.exitCode` else defaults to 1 (failure) or 0 (success).\n\n## Behavioral Contracts\n\n**Rate Limit Patterns** (service.ts `RATE_LIMIT_PATTERNS`):\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\n**Exponential Backoff Formula** (retry.ts):\n```typescript\nmin(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n**Default Retry Configuration** (retry.ts `DEFAULT_RETRY_OPTIONS`):\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n**Subprocess Timeout** (subprocess.ts):\n- SIGTERM sent at `timeoutMs` via execFile `killSignal` option\n- SIGKILL escalation at `timeoutMs + 5000ms` via unref'd timer\n\n**Process Group Termination** (subprocess.ts):\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // Negative PID targets process group\n```\n\n**Debug Memory Formatting** (service.ts `formatBytes()`):\n- `< 1024` → `${bytes}B`\n- `< 1048576` → `${(bytes / 1024).toFixed(1)}KB`\n- `≥ 1048576` → `${(bytes / 1048576).toFixed(1)}MB`\n\n**Telemetry Filename Format** (telemetry/run-log.ts):\n```typescript\n`run-${runLog.startTime.replace(/[:.]/g, '-')}.json`\n// \"2026-02-09T12:34:56.789Z\" → \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Subprocess Log Filename Sanitization** (service.ts):\n```typescript\ntaskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')\n```\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting token counts from usage/modelUsage objects, GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR` until output formats stabilize, shared `isCommandOnPath()` splitting `process.env.PATH` by platform delimiter with Windows `PATHEXT` iteration, CLI arguments with stdin prompt delivery and platform-specific flags (`--output-format json`, `--permission-mode bypassPermissions`).\n\n**[telemetry/](./telemetry/)** — `TelemetryLogger` accumulating `TelemetryEntry` records in memory with `addEntry()` and `getSummary()` computing token sums/error counts/unique file deduplication, `writeRunLog()` serializing `RunLog` to `.agents-reverse-engineer/logs/run-<timestamp>.json` with ISO timestamp transformation, `cleanupOldLogs()` deleting oldest files beyond retention threshold via lexicographic sort.\n\n## Integration Points\n\n**Upstream Consumers:**\n- `src/orchestration/runner.ts` — Instantiates AIService with resolved backend and config options, invokes `call()` per task, attaches file reads via `addFilesReadToLastEntry()`, finalizes telemetry via `finalize()`.\n- `src/generation/orchestrator.ts` — Threads AIService through three-phase pipeline (file analysis, directory aggregation, root synthesis).\n\n**Downstream Dependencies:**\n- `src/orchestration/trace.ts` — `ITraceWriter` interface for trace event emission.\n- `src/config/schema.ts` — `AIConfig` with backend selection, timeout, concurrency, telemetry retention.\n- `node:child_process` — `execFile()` for subprocess spawning.\n- `node:fs/promises` — File I/O for subprocess logs and telemetry persistence.\n### src/ai/backends/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\nBackend adapters implementing AIBackend interface for three AI CLI tools: ClaudeBackend with full JSON parsing via Zod schema validation, GeminiBackend/OpenCodeBackend as stub implementations throwing AIServiceError until stable output formats arrive.\n\n## Contents\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection splitting process.env.PATH by platform delimiter, buildArgs() constructing ['--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions'] with optional --model/--system-prompt/--max-turns, parseResponse() slicing stdout from first '{' character to strip upgrade notices then validating against ClaudeResponseSchema extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens and model name from modelUsage object keys, getInstallInstructions() returning npm command for @anthropic-ai/claude-code.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub with isAvailable() delegating to isCommandOnPath('gemini'), buildArgs() returning ['-p', '--output-format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning npm command for @anthropic-ai/gemini-cli with GitHub URL (implementation deferred per RESEARCH.md Open Question 2 pending stable JSON format).\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub with isAvailable() delegating to isCommandOnPath('opencode'), buildArgs() returning ['run', '--format', 'json'], parseResponse() unconditionally throwing AIServiceError with code SUBPROCESS_ERROR and message 'not yet implemented', getInstallInstructions() returning curl install command for https://opencode.ai (implementation deferred per RESEARCH.md Open Question 3 pending JSONL parsing).\n\n## Architecture\n\n**Backend Adapter Contract**\n\nAIBackend interface defines five methods: isAvailable() checking CLI binary presence on PATH, buildArgs(options) constructing subprocess argument arrays with prompt delivered via stdin, parseResponse(stdout, durationMs, exitCode) extracting AIResponse with normalized token counts, getInstallInstructions() returning user-facing setup guidance, name/cliCommand properties identifying backend.\n\n**PATH Detection Strategy**\n\nisCommandOnPath() splits process.env.PATH by path.delimiter (';' on Windows, ':' elsewhere), iterates directories calling fs.stat() on potential executable paths, on Windows iterates process.env.PATHEXT extensions ['.exe', '.cmd', '.bat', '.com'] to match platform conventions, returns true on first match. Function exported from claude.ts and reused by gemini.ts/opencode.ts for cross-backend consistency.\n\n**CLI Argument Patterns**\n\nAll backends use stdin for prompt delivery (not CLI arguments) to avoid shell escaping issues. ClaudeBackend appends --model/--system-prompt/--max-turns conditionally via buildArgs() inspecting AICallOptions properties. --permission-mode bypassPermissions enables non-interactive subprocess execution per PITFALLS.md §8. --no-session-persistence prevents state file writes.\n\n**JSON Response Parsing**\n\nClaudeBackend.parseResponse() handles stdout prefix content (upgrade notices, warnings) by finding first '{' via indexOf() before JSON.parse(). ClaudeResponseSchema validates against v2.1.31 output format with type: 'result', usage object containing input_tokens/cache_creation_input_tokens/cache_read_input_tokens/output_tokens, modelUsage record mapping model names to detailed statistics. Throws AIServiceError with code PARSE_ERROR on validation failure or missing JSON.\n\n**Stub Backend Pattern**\n\nGeminiBackend and OpenCodeBackend implement full interface surface but throw AIServiceError from parseResponse() to prevent runtime usage until output format research completes. Enables backend registration in src/ai/registry.ts and auto-detection via detectFirstAvailableBackend() while blocking production execution.\n\n## Integration Dependencies\n\nImports AIBackend/AICallOptions/AIResponse/AIServiceError from ../types.js. ClaudeBackend requires zod for ClaudeResponseSchema validation. Consumed by AIService in src/ai/service.ts via BackendRegistry.getBackend(name) lookup. Backend selection happens via config.ai.backend or auto-detection iterating ['claude', 'gemini', 'opencode'] until isAvailable() returns true.\n\n## Behavioral Contracts\n\n**ClaudeBackend Argument Construction**\n```javascript\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n// Appends conditionally:\n['--model', options.model] if options.model present\n['--system-prompt', options.systemPrompt] if options.systemPrompt present\n['--max-turns', String(options.maxTurns)] if options.maxTurns defined\n```\n\n**ClaudeResponseSchema Token Fields**\n```typescript\nusage: z.object({\n  input_tokens: z.number(),\n  cache_creation_input_tokens: z.number(),\n  cache_read_input_tokens: z.number(),\n  output_tokens: z.number()\n})\n```\n\n**GeminiBackend Arguments** — `['-p', '--output-format', 'json']`\n\n**OpenCodeBackend Arguments** — `['run', '--format', 'json']`\n\n**Stub Error Message Pattern** — `'<Backend> backend is not yet implemented. Use Claude backend.'` with AIServiceError code `'SUBPROCESS_ERROR'`\n### src/ai/telemetry/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\n**Accumulates per-subprocess telemetry entries in memory via TelemetryLogger, serializes aggregate RunLog summaries to timestamped NDJSON files in `.agents-reverse-engineer/logs/`, and enforces retention limits via lexicographic filename sorting.**\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes oldest `run-*.json` files beyond retention threshold via lexicographic sort, catches `ENOENT` for missing logs directory.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates `TelemetryEntry` instances in memory, computes aggregate statistics via `getSummary()` (token sums, error counts, unique file deduplication), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` serializes `RunLog` to `.agents-reverse-engineer/logs/run-<safeTimestamp>.json` with ISO timestamp transformation (`2026-02-07T12:00:00.000Z` → `2026-02-07T12-00-00-000Z.json`).\n\n## Data Flow\n\n1. **AIService** (`src/ai/service.ts`) invokes `TelemetryLogger.addEntry()` after each subprocess call with `TelemetryEntry` containing `inputTokens`, `outputTokens`, `cacheReadTokens`, `latencyMs`, `error`, `filesRead[]`.\n2. **Command orchestrator** (`src/orchestration/runner.ts`) calls `logger.setFilesReadOnLastEntry(filesRead)` post-execution to attach file metadata (`path`, `sizeBytes`, `linesRead`).\n3. **Run completion**: orchestrator calls `logger.toRunLog()` to produce `RunLog` with aggregated summary (`totalCalls`, `totalInputTokens`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`).\n4. **Persistence**: `writeRunLog(projectRoot, runLog)` serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n5. **Retention enforcement**: `cleanupOldLogs(projectRoot, keepCount)` deletes oldest logs beyond `config.ai.telemetry.keepRuns` threshold (default 50).\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService instantiates TelemetryLogger once per run, calls `addEntry()` per subprocess, invokes `toRunLog()` at finalization.\n- `src/orchestration/runner.ts` — Orchestrates telemetry lifecycle: logger creation, entry enrichment with file reads, run log serialization, cleanup invocation.\n\n**Consumes:**\n- `src/ai/types.ts` — `TelemetryEntry`, `RunLog`, `FileRead` type definitions with token count fields, latency metrics, error summaries.\n\n## Behavioral Contracts\n\n**Filename transformation** (run-log.ts):\n```typescript\nrunLog.startTime.replace(/[:.]/g, '-')\n// Input:  \"2026-02-09T12:34:56.789Z\"\n// Output: \"run-2026-02-09T12-34-56-789Z.json\"\n```\n\n**Lexicographic sorting** (cleanup.ts):\n- ISO 8601 timestamps in filenames sort chronologically when treated as strings.\n- `sort()` then `reverse()` produces newest-first order.\n- `slice(keepCount)` targets oldest files for deletion.\n\n**Aggregate statistics** (logger.ts `getSummary()`):\n- `totalCalls` = `entries.length`\n- `totalInputTokens` = `Σ(entry.inputTokens)`\n- `uniqueFilesRead` = distinct count via `Set<string>` deduplication of `entry.filesRead.map(f => f.path)`\n\n## Retention Policy\n\nDefault retention: 50 runs (`config.ai.telemetry.keepRuns`). `cleanupOldLogs()` invoked after every `writeRunLog()` call. Missing logs directory handled gracefully (returns 0 without error). Propagates non-`ENOENT` filesystem errors to caller.\n### src/change-detection/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\n**Git-based change detection with SHA-256 content hashing for incremental documentation updates, supporting rename tracking via `git diff -M`, uncommitted change merging via `git status --porcelain`, and hash-only fallback for non-git workflows.**\n\n## Contents\n\n### [detector.ts](./detector.ts)\nImplements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, and `computeContentHashFromString()` for delta computation. Parses `git diff --name-status -M` output with tab-split line parsing to extract status codes (`A`/`M`/`D`/`R<percentage>`) and file paths, maps to `FileChange` objects with optional `oldPath` for renames. Optionally merges uncommitted changes from `git.status()` (`modified`, `deleted`, `not_added`, `staged`) when `includeUncommitted: true`, using linear `Array.some()` deduplication to prevent duplicate entries. SHA-256 hashing via `node:crypto` `createHash('sha256')` with hex digest output.\n\n### [types.ts](./types.ts)\nDefines `ChangeType` (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` (discriminated by `status` with optional `oldPath` for `'renamed'`), `ChangeDetectionResult` (`currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`), and `ChangeDetectionOptions` (`includeUncommitted?: boolean`). Discriminated union pattern enables type-safe `oldPath` access after `status === 'renamed'` check.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting all functions (`isGitRepo`, `getCurrentCommit`, `getChangedFiles`, `computeContentHash`, `computeContentHashFromString`) and types (`ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions`) from `detector.ts` and `types.ts`. Serves as public API consumed by `../update/orchestrator.ts` and `../cli/update.ts`.\n\n## Integration Points\n\n**Incremental Update Workflow:**\n1. `src/update/orchestrator.ts` calls `getChangedFiles(projectRoot, baseCommit, options)` to compute `changes[]` array\n2. For each changed file, reads `.sum` YAML frontmatter `content_hash` via `src/generation/writers/sum.ts`\n3. Calls `computeContentHash(filePath)` to compute current file SHA-256 hash\n4. Hash mismatch → add to `filesToAnalyze` as `FileChange`, hash match → add to `filesToSkip`\n5. Detects orphans via `changes.filter(c => c.status === 'renamed')` and calls `cleanupOrphans()` to delete `.sum` files for `oldPath` entries\n6. Session-end hooks (`hooks/are-session-end.js`) check `git status --porcelain` and spawn `npx agents-reverse-engineer@latest update --quiet` if changes detected\n\n**Non-Git Fallback:**\nWhen `isGitRepo(projectRoot)` returns `false`, consumers skip git operations and rely on SHA-256 hash comparison alone for change detection (reads all `.sum` files, compares hashes).\n\n## Behavioral Contracts\n\n**Diff output format:** `git diff --name-status -M` produces tab-delimited lines where:\n- Add/Modify/Delete: `STATUS\\tFILE` (e.g., `M\\tsrc/foo.ts`)\n- Rename: `R<percentage>\\tOLDPATH\\tNEWPATH` (e.g., `R100\\tsrc/old.ts\\tsrc/new.ts`)\n\n**Rename similarity threshold:** `-M` flag uses git default 50% content similarity for rename detection.\n\n**Status code mapping:**\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n**Uncommitted change sources (when `includeUncommitted: true`):**\n- `status.modified` — modified files not staged for commit\n- `status.deleted` — staged deletions\n- `status.not_added` — untracked files\n- `status.staged` — files added to git index\n\n**Deduplication strategy:** Linear `changes.some(c => c.path === file)` scan before appending uncommitted entries prevents duplicate `FileChange` objects when working tree changes overlap with committed diff.\n\n**SHA-256 output format:** 64-character hex string from `createHash('sha256').update(content).digest('hex')`.\n### src/cli/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand-line interface layer parsing `process.argv`, routing to command handlers, managing shared flags (--debug, --trace, --dry-run, --concurrency, --model), and integrating ProgressLog/TraceWriter/AIService across init/discover/generate/update/specify/rebuild/clean workflows.\n\n## Command Entry Points\n\n**[index.ts](./index.ts)** — Main router parsing args via `parseArgs()`, dispatching to command handlers, handling `--version`/`--help` flags, launching interactive installer when invoked with no arguments or installer-specific flags (--runtime, -g, -l).\n\n**[init.ts](./init.ts)** — `initCommand(root, { force? })` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, exits with warning if config exists without `--force`, catches `EACCES`/`EPERM` with `process.exit(1)`.\n\n**[discover.ts](./discover.ts)** — `discoverCommand(targetPath, { tracer?, debug? })` runs file discovery via `discoverFiles()`, writes included/excluded files to console and `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`, emits `discovery:start/end` trace events.\n\n**[generate.ts](./generate.ts)** — `generateCommand(targetPath, GenerateOptions)` orchestrates three-phase pipeline: discovers files → creates GenerationPlan via `createOrchestrator().createPlan()` → resolves AI backend → builds ExecutionPlan → executes via `CommandRunner.executeGenerate()` → finalizes telemetry/trace/progress, exits with codes 0 (success), 1 (partial failure), 2 (total failure/no CLI).\n\n**[update.ts](./update.ts)** — `updateCommand(targetPath, UpdateCommandOptions)` prepares UpdatePlan via hash comparison, cleans orphaned `.sum`/`AGENTS.md` artifacts, analyzes changed files (Phase 1), regenerates `AGENTS.md` for `affectedDirs` (Phase 2), finalizes telemetry, exits with codes 0/1/2.\n\n**[specify.ts](./specify.ts)** — `specifyCommand(targetPath, SpecifyOptions)` synthesizes project spec from `AGENTS.md` corpus via AI, auto-invokes `generateCommand()` if docs missing, supports single/multi-file output (`--multi-file`), enforces 15min timeout and opus model default, exits with code 1 on `SpecExistsError` (conflicts), 2 on `CLI_NOT_FOUND`.\n\n**[rebuild.ts](./rebuild.ts)** — `rebuildCommand(targetPath, RebuildOptions)` reconstructs project from specs via `partitionSpec()` + `executeRebuild()`, enforces 15min timeout and opus default, supports checkpoint-based resumption via `CheckpointManager.load()`, writes generated code to `rebuild/` (or custom `--output`), exits with codes 0/1/2.\n\n**[clean.ts](./clean.ts)** — `cleanCommand(targetPath, { dryRun })` removes `.sum`, `.annex.md`, generated `AGENTS.md` (via `GENERATED_MARKER` detection), `CLAUDE.md`, `GENERATION-PLAN.md`, restores `AGENTS.local.md` → `AGENTS.md`, logs deletion counts with picocolors formatting.\n\n## Shared Option Types\n\n**GenerateOptions** (`generate.ts`, `rebuild.ts` reuses subset):\n```typescript\n{\n  dryRun?: boolean;       // Show plan without AI calls\n  concurrency?: number;   // Worker pool size (1-10)\n  failFast?: boolean;     // Abort on first failure\n  debug?: boolean;        // Verbose subprocess logging\n  trace?: boolean;        // NDJSON trace emission\n  model?: string;         // Override AI model\n}\n```\n\n**UpdateCommandOptions** (`update.ts`):\n```typescript\n{\n  uncommitted?: boolean;  // Include staged+working changes\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**SpecifyOptions** (`specify.ts`):\n```typescript\n{\n  output?: string;        // Custom spec file path (default: specs/SPEC.md)\n  force?: boolean;        // Overwrite existing files\n  dryRun?: boolean;\n  multiFile?: boolean;    // Split into per-directory specs\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**RebuildOptions** (`rebuild.ts`):\n```typescript\n{\n  output?: string;        // Custom output directory (default: rebuild/)\n  force?: boolean;        // Wipe output dir and restart\n  dryRun?: boolean;\n  concurrency?: number;\n  failFast?: boolean;\n  debug?: boolean;\n  trace?: boolean;\n  model?: string;\n}\n```\n\n**CleanOptions** (`clean.ts`):\n```typescript\n{\n  dryRun?: boolean;       // Preview deletions without filesystem writes\n}\n```\n\n## Argument Parsing Protocol\n\n`parseArgs(args: string[])` in `index.ts` returns `{ command, positional, flags, values }`:\n- **Long flags**: `--key value` → `values['key'] = 'value'`, `--flag` → `flags.add('flag')`\n- **Short flags**: `-V` → `flags.add('version')`, `-h` → `flags.add('help')`, `-g` → `flags.add('global')`, `-l` → `flags.add('local')`\n- **Command**: First non-flag argument\n- **Positional**: Subsequent non-flag arguments after command\n- **Values map**: `--concurrency 3`, `--output ./spec.md`, `--model sonnet`, `--runtime claude`\n- **Installer detection**: `hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` value\n\n## Model Resolution Strategy\n\n**generate/update**: `options.model ?? config.ai.model` — CLI flag overrides config, no hardcoded default.\n\n**specify/rebuild**: `options.model ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model)` — upgrades sonnet → opus for quality-critical synthesis, respects explicit opus/haiku config.\n\n## Progress Tracking Infrastructure\n\nAll commands (except `init`, `clean`) create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log`:\n- **Header format**: `=== ARE <Command> (${ISO-8601}) ===\\nProject: ${absolutePath}\\n...`\n- **Real-time monitoring**: `tail -f .agents-reverse-engineer/progress.log`\n- **Phase boundaries**: `=== Phase 1: File Analysis ===`, `=== Phase 2: Directory AGENTS.md ===`\n- **Task progress**: `[worker-0] Analyzing src/foo.ts (ETA: 2m 15s)`\n- **Summary line**: `Tokens: 12345 in / 6789 out | Duration: 45s | Exit: 0`\n- **Finalization**: `await progressLog.finalize()` before exit\n\n## Trace Integration\n\nWhen `--trace` flag present:\n- `createTraceWriter(absolutePath, true)` creates `.agents-reverse-engineer/traces/trace-<ISO-timestamp>.ndjson` writer\n- Threads `tracer` through `loadConfig()`, `discoverFiles()`, `createOrchestrator()`, `CommandRunner()`, `AIService()`\n- Emits events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs`\n- Finalization: `await tracer.finalize()` + `cleanupOldTraces(absolutePath)` keeps 500 most recent traces\n\n## Backend Resolution Flow\n\nAll commands except `init`, `discover`, `clean`:\n1. `createBackendRegistry()` → enumerates installed CLIs (Claude, Gemini, OpenCode)\n2. `resolveBackend(registry, config.ai.backend)` → throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found\n3. Catch block → `getInstallInstructions(registry)` prints installation commands, `process.exit(2)` (distinct from task failure code 1)\n\n## AIService Configuration\n\nInstantiation pattern (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min min for specify/rebuild\n  maxRetries: config.ai.maxRetries,\n  model: effectiveModel,\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }\n});\n\nif (options.debug) {\n  aiService.setDebug(true);\n  console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n  console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n}\n\nif (options.trace) {\n  const logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', timestamp);\n  aiService.setSubprocessLogDir(logDir);\n  console.error(pc.dim(`[debug] Subprocess logs: ${logDir}`));\n}\n```\n\n## Finalization Sequence\n\nStandard cleanup before exit (from `generate.ts`, `update.ts`, `specify.ts`, `rebuild.ts`):\n```typescript\nawait aiService.finalize(absolutePath);  // Write run log, enforce retention\nawait progressLog.finalize();            // Close progress.log stream\nawait tracer.finalize();                 // Close trace NDJSON stream\nif (options.trace) {\n  cleanupOldTraces(absolutePath);        // Keep 500 most recent traces\n}\n```\n\n## Exit Code Conventions\n\n- **0**: Success (all tasks completed or no tasks to process)\n- **1**: Partial failure (some tasks succeeded, some failed) OR file conflict (specify/clean) OR first-run detection (update)\n- **2**: Total failure (no tasks succeeded, only failures) OR AI CLI not found\n\n## Dry-Run Behavior\n\n**generate.ts**: Builds `ExecutionPlan` via `buildExecutionPlan()`, logs file/directory/root task counts, returns without AI backend resolution.\n\n**update.ts**: Prepares `UpdatePlan` via `orchestrator.preparePlan({ includeUncommitted, dryRun: true })`, logs changed files with status markers (M=modified, +=added, R=renamed), cleanup actions (deleted `.sum`, empty dir `AGENTS.md`), affected directories, returns without AI calls.\n\n**specify.ts**: Collects `AGENTS.md` + annex files, estimates tokens via `totalChars / 4 / 1000`, logs counts with cyan styling, warns if input exceeds 150K tokens or docs missing, returns without backend resolution.\n\n**rebuild.ts**: Reads specs via `readSpecFiles()`, partitions via `partitionSpec()`, loads checkpoint via `CheckpointManager.load()`, logs unit count and checkpoint status (completed vs. pending modules), returns without AIService instantiation.\n\n**clean.ts**: Discovers artifacts via `fast-glob` (`.sum`, `.annex.md`, `AGENTS.md`, `AGENTS.local.md`), filters via `GENERATED_MARKER` substring search, logs deletion preview with picocolors formatting (yellow warning \"Dry run — no files were changed\"), returns without `unlink()` calls.\n\n## Error Handling Patterns\n\n**Directory access errors** (all commands):\n```typescript\nawait access(resolvedPath, constants.R_OK);\n// Catches ENOENT → 'Directory not found: ${path}', exit 1\n// Catches EACCES/EPERM → 'Permission denied: ${path}', exit 1\n```\n\n**Config load errors** (all commands except `init`):\n```typescript\nconst config = await loadConfig(absolutePath, { tracer, debug });\n// Throws on invalid YAML or schema validation failure\n// Caught by top-level try/catch in index.ts main()\n```\n\n**Backend resolution errors** (generate/update/specify/rebuild):\n```typescript\ntry {\n  backend = resolveBackend(registry, config.ai.backend);\n} catch (err) {\n  if (err instanceof AIServiceError && err.code === 'CLI_NOT_FOUND') {\n    console.error(pc.red('No AI CLI found...'));\n    console.error(getInstallInstructions(registry));\n    process.exit(2);\n  }\n  throw err;\n}\n```\n\n**File conflict errors** (specify):\n```typescript\ntry {\n  await writeSpec(...);\n} catch (err) {\n  if (err instanceof SpecExistsError) {\n    await progressLog.finalize();\n    console.error(pc.red(err.message));\n    process.exit(1);\n  }\n  throw err;\n}\n```\n\n## Dependencies\n\n**External**: `picocolors` (as `pc`), `node:path`, `node:fs/promises` (`access`, `readFile`, `rename`, `unlink`, `mkdir`, `readdir`), `node:fs` (`constants.F_OK/R_OK`).\n\n**Internal**:\n- **Config**: `src/config/loader.ts` (loadConfig, configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE)\n- **Discovery**: `src/discovery/run.ts` (discoverFiles)\n- **Generation**: `src/generation/orchestrator.ts` (createOrchestrator, GenerationPlan), `src/generation/executor.ts` (buildExecutionPlan, formatExecutionPlanAsMarkdown), `src/generation/collector.ts` (collectAgentsDocs, collectAnnexFiles), `src/generation/writers/agents-md.ts` (writeAgentsMd, GENERATED_MARKER), `src/generation/prompts/index.ts` (buildDirectoryPrompt)\n- **AI**: `src/ai/index.ts` (AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions)\n- **Orchestration**: `src/orchestration/index.ts` (CommandRunner, ProgressLog, ProgressReporter, createTraceWriter, cleanupOldTraces), `src/orchestration/trace.ts` (ITraceWriter)\n- **Update**: `src/update/index.ts` (createUpdateOrchestrator, UpdatePlan)\n- **Specify**: `src/specify/index.ts` (buildSpecPrompt, writeSpec, SpecExistsError)\n- **Rebuild**: `src/rebuild/index.ts` (readSpecFiles, partitionSpec, CheckpointManager, executeRebuild)\n- **Installer**: `src/installer/index.ts` (runInstaller, parseInstallerArgs)\n- **Output**: `src/output/logger.ts` (createLogger)\n- **Version**: `src/version.ts` (getVersion)\n- **Types**: `src/types/index.ts` (DiscoveryResult)\n### src/config/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# config\n\nExports YAML-based configuration loading with Zod validation, resource-adaptive concurrency calculation, and default constant definitions for gitignore patterns, vendor directories, binary extensions, and AI service parameters.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Defines ConfigSchema (Zod) with nested ExcludeSchema (patterns/vendorDirs/binaryExtensions arrays), OptionsSchema (followSymlinks boolean, maxFileSize number), OutputSchema (colors boolean), AISchema (backend enum, model string, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Exports Config, ExcludeConfig, OptionsConfig, OutputConfig, AIConfig types. Applies validation constraints: backend restricted to `'claude' | 'gemini' | 'opencode' | 'auto'`, timeoutMs requires positive integer, concurrency clamped to 1-20, maxFileSize requires positive integer, keepRuns requires non-negative integer.\n\n**[loader.ts](./loader.ts)** — Exports loadConfig(root, options?) reading `.agents-reverse-engineer/config.yaml`, parsing YAML, validating via ConfigSchema.parse(), emitting `config:loaded` trace event (configPath/model/concurrency fields), returning validated Config or defaults on ENOENT, wrapping ZodError/parse failures in ConfigError. Exports configExists(root) checking file accessibility via fs.access(). Exports writeDefaultConfig(root) generating commented YAML template with DEFAULT_EXCLUDE_PATTERNS/DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS arrays, yamlScalar() escaping special characters.\n\n**[defaults.ts](./defaults.ts)** — Exports getDefaultConcurrency() computing worker pool size via `clamp(os.availableParallelism() * CONCURRENCY_MULTIPLIER, MIN_CONCURRENCY, min(MAX_CONCURRENCY, memCap))` where memCap = `floor(os.totalmem() * MEMORY_FRACTION / SUBPROCESS_HEAP_GB)` with MEMORY_FRACTION=0.5, SUBPROCESS_HEAP_GB=0.512, CONCURRENCY_MULTIPLIER=5, MIN_CONCURRENCY=2, MAX_CONCURRENCY=20. Exports DEFAULT_VENDOR_DIRS (18 directories: node_modules/vendor/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini), DEFAULT_EXCLUDE_PATTERNS (26 patterns: AGENTS.md/CLAUDE.md/OPENCODE.md/GEMINI.md/lock files/dotfiles/logs/sum files), DEFAULT_BINARY_EXTENSIONS (26 extensions: images/archives/executables/media/documents/fonts/compiled files), DEFAULT_MAX_FILE_SIZE (1MB), DEFAULT_CONFIG (composite object spreading constants).\n\n## Data Flow\n\n1. CLI commands invoke loadConfig(root, { tracer, debug }) from loader.ts\n2. loader.ts reads `.agents-reverse-engineer/config.yaml`, parses YAML, validates via ConfigSchema.parse() from schema.ts\n3. schema.ts applies defaults from defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_EXCLUDE_PATTERNS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency())\n4. On validation failure, loader.ts extracts ZodError.issues[] and wraps in ConfigError with formatted message\n5. On success, loader.ts emits `config:loaded` trace event and returns Config object to CLI orchestrators\n6. writeDefaultConfig() generates YAML template by embedding DEFAULT_ constants and calling yamlScalar() for pattern escaping\n\n## Resource Adaptive Concurrency\n\ngetDefaultConcurrency() prevents RAM exhaustion by computing memory cap: on 4GB systems, `memCap = floor(4 * 0.5 / 0.512) = 3` limits concurrency to 3 workers despite CPU count * 5 formula suggesting higher values. SUBPROCESS_HEAP_GB synchronizes with `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for subprocess resource limiting. Formula `cores * CONCURRENCY_MULTIPLIER` optimized for I/O-bound AI subprocess workloads with high wait-to-compute ratios.\n\n## Error Handling Strategy\n\nloader.ts distinguishes three error cases: ENOENT errors return `ConfigSchema.parse({})` defaults without throwing, ZodError instances map issues[] to multi-line validation report wrapped in ConfigError, existing ConfigError instances re-thrown without modification. ConfigError extends Error adding filePath: string and optional cause: Error properties for stack trace preservation.\n\n## File System Contracts\n\n- Configuration directory: `.agents-reverse-engineer/`\n- Configuration file: `config.yaml` (YAML format with commented sections)\n- Telemetry logs: `.agents-reverse-engineer/logs/` (referenced by AISchema.telemetry.keepRuns)\n- Trace output: `.agents-reverse-engineer/traces/` (referenced by ITraceWriter interface)\n\n## Behavioral Contracts\n\n**YAML Special Character Escaping:**\nyamlScalar() quotes values matching regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` and escapes `\\\\` → `\\\\\\\\`, `\\\"` → `\\\\\\\"` within quoted strings.\n\n**Default Exclusion Patterns:**\n```typescript\nDEFAULT_EXCLUDE_PATTERNS = [\n  'AGENTS.md', 'CLAUDE.md', 'OPENCODE.md', 'GEMINI.md',\n  '**/AGENTS.md', '**/CLAUDE.md', '**/OPENCODE.md', '**/GEMINI.md',\n  '*.lock', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\n  'bun.lock', 'bun.lockb', 'Gemfile.lock', 'Cargo.lock',\n  'poetry.lock', 'composer.lock', 'go.sum',\n  '.gitignore', '.gitattributes', '.gitkeep', '.env', '**/.env', '**/.env.*',\n  '*.log', '*.sum', '**/*.sum', '**/SKILL.md'\n]\n```\n\n**Vendor Directory Exclusions:**\n```typescript\nDEFAULT_VENDOR_DIRS = [\n  'node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__',\n  '.next', 'venv', '.venv', 'target', '.cargo', '.gradle',\n  '.agents-reverse-engineer', '.agents', '.planning',\n  '.claude', '.opencode', '.gemini'\n]\n```\n\n**Binary Extension Detection:**\n```typescript\nDEFAULT_BINARY_EXTENSIONS = [\n  '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp',\n  '.zip', '.tar', '.gz', '.rar', '.7z',\n  '.exe', '.dll', '.so', '.dylib',\n  '.mp3', '.mp4', '.wav',\n  '.pdf',\n  '.woff', '.woff2', '.ttf', '.eot',\n  '.class', '.pyc'\n]\n```\n\n**Concurrency Formula:**\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\n## Cross-Module Dependencies\n\n- **schema.ts** imports defaults.ts (DEFAULT_EXCLUDE_PATTERNS, DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, getDefaultConcurrency)\n- **loader.ts** imports schema.ts (ConfigSchema, Config type), defaults.ts (DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency), `../orchestration/trace.js` (ITraceWriter type)\n### src/discovery/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery pipeline using fast-glob directory traversal with composable four-stage filter chain (gitignore, vendor, binary, custom) and bounded-concurrency short-circuit evaluation, exposing `discoverFiles()` facade for all CLI commands.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[run.ts](./run.ts)** — `discoverFiles(root, config, options?)` facade composes `walkDirectory()` with four-stage filter chain: `createGitignoreFilter()` parses `.gitignore`, `createVendorFilter()` excludes `node_modules`/`.git`/`dist` via `config.exclude.vendorDirs`, `createBinaryFilter()` applies 96-extension allowlist + `isBinaryFile()` fallback with `config.options.maxFileSize` threshold, `createCustomFilter()` processes user glob patterns from `config.exclude.patterns`. Returns `FilterResult` with `included: string[]` and `excluded: Array<{file, reason, filter}>`. Exports `DiscoveryConfig` interface (structural subset of `Config`) and `DiscoverFilesOptions` (`tracer?: ITraceWriter`, `debug?: boolean`).\n\n**[walker.ts](./walker.ts)** — `walkDirectory(options)` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `suppressErrors: true`, hardcoded `ignore: ['**/.git/**']`, and `followSymbolicLinks: options.followSymlinks ?? false`. Requires `WalkerOptions` with `cwd: string` root, optional `dot?: boolean` (default `true` for dotfiles), `followSymlinks?: boolean` (default `false`). Returns raw `string[]` file paths without filtering logic (deferred to filter chain).\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` (`included[]`, `excluded[]`), `WalkerOptions` (`cwd`, `followSymlinks?`, `dot?`).\n\n## Subdirectory\n\n**[filters/](./filters/)** — Five modules implementing `FileFilter` contract: `gitignore.ts` (`createGitignoreFilter()` async factory with `ignore` library), `binary.ts` (96-extension set + `isBinaryFile()` content analysis), `vendor.ts` (single-segment Set + path-pattern array matching), `custom.ts` (gitignore-style glob parsing), `index.ts` (`applyFilters()` with 30-worker bounded concurrency, short-circuit evaluation, `filter:applied` trace emission).\n\n## Filter Chain Architecture\n\n`discoverFiles()` instantiates filters in priority order (gitignore → vendor → binary → custom), passes to `applyFilters()` which runs filters sequentially per file until `shouldExclude()` returns `true`. Concurrency pool prevents file descriptor exhaustion during `isBinaryFile()` I/O. Filters receive absolute paths; gitignore/custom filters convert to relative via `path.relative()` before pattern matching.\n\n## Configuration Surface\n\n- `config.exclude.vendorDirs: string[]` — directory names like `node_modules` (default 10 entries)\n- `config.exclude.binaryExtensions: string[]` — additional extensions beyond 96-entry `BINARY_EXTENSIONS` set\n- `config.exclude.patterns: string[]` — gitignore-style globs processed by `ignore` library\n- `config.options.maxFileSize: number` — binary detection threshold (default 1MB)\n- `config.options.followSymlinks: boolean` — symlink traversal toggle (default `false`)\n\n## Integration Points\n\nConsumed by `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` via `discoverFiles()` facade. Imports `ITraceWriter` from `../orchestration/trace.js` for `filter:applied` event emission. Filter factories depend on `ignore` library (gitignore parsing), `isbinaryfile` library (content analysis), `fast-glob` (directory traversal), `node:fs` (Stats objects).\n\n## Behavioral Contracts\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS`: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Binary Extension Set\n96 extensions including: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.class`, `.pyc`, `.o`, `.obj`, `.db`, `.sqlite`, `.wasm` (full list in `filters/binary.ts`)\n\n### Concurrency Limit\n`applyFilters()` uses `CONCURRENCY = 30` workers via iterator-based pool pattern from `src/orchestration/pool.ts`\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```\n### src/discovery/filters/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters for discovery pipeline: gitignore pattern matching, binary file detection, vendor directory exclusion, and custom glob rules with short-circuit evaluation and bounded-concurrency application.\n\n## Contents\n\n### Filter Orchestration\n\n**[index.ts](./index.ts)** — `applyFilters(files, filters, options)` executes filter chain with short-circuit evaluation (stops at first exclusion) using 30-worker bounded concurrency pool to prevent file descriptor exhaustion during `isBinaryFile()` I/O operations. Returns `FilterResult` with `included[]` and `excluded[]` arrays, emits `filter:applied` trace events with per-filter `filesMatched`/`filesRejected` counts. Re-exports all filter factory functions.\n\n### Filter Implementations\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter(root)` async factory reads `.gitignore`, parses via `ignore` library, returns `FileFilter` that converts absolute paths to relative before calling `ig.ignores()`. Returns `false` for paths outside root tree (relative path starts with `..`).\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter(options?)` constructs fast-path extension lookup in `BINARY_EXTENSIONS` set (96 extensions: `.png`, `.exe`, `.mp4`, `.pdf`, `.woff`, `.class`, `.db`, etc.) with fallback to `isBinaryFile()` content analysis. Excludes files exceeding `maxFileSize` (default 1MB via `DEFAULT_MAX_FILE_SIZE`). Returns `true` on `fs.stat()` errors.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter(vendorDirs)` partitions input into single-segment Set (e.g., `node_modules`) for O(1) membership testing and path-pattern array (e.g., `.agents/skills`) for substring matching. `DEFAULT_VENDOR_DIRS` contains `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter(patterns, root)` parses gitignore-style glob patterns via `ignore` library, converts absolute paths to relative before evaluation. Returns `false` immediately for empty patterns array or paths outside root.\n\n## Concurrency Strategy\n\n`applyFilters()` uses iterator-based worker pool with `CONCURRENCY = 30` to share single `files.entries()` iterator across workers. Each file runs through `filters[]` sequentially until `shouldExclude()` returns `true` (short-circuit). BinaryFilter calls synchronous `isBinaryFile()`, making concurrency bounds critical to avoid ulimit violations.\n\n## Filter Contract\n\nAll factories return `FileFilter` interface from `../types.js`:\n- `name: string` — filter identifier for logging\n- `shouldExclude(absolutePath: string): boolean` — exclusion predicate\n\n## Behavioral Contracts\n\n### Binary Extension Set\n`BINARY_EXTENSIONS` (96 entries): `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`, `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`, `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`, `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`, `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`, `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`, `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`, `.db`, `.sqlite`, `.sqlite3`, `.mdb`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories\n`DEFAULT_VENDOR_DIRS` (10 entries): `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Path Normalization\n- Gitignore filter: `path.relative(normalizedRoot, absolutePath)` before `ig.ignores()`\n- Vendor filter: `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling\n- Binary filter: `path.extname(absolutePath).toLowerCase()` for extension extraction\n\n### Trace Event Schema\n```typescript\n{\n  type: 'filter:applied',\n  filterName: string,\n  filesMatched: number,\n  filesRejected: number\n}\n```\n### src/generation/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates ARE's three-phase documentation generation pipeline: Phase 1 concurrent file analysis via worker pools generating `.sum` files with SHA-256 hashes, Phase 2 post-order directory aggregation synthesizing `AGENTS.md` from child summaries, Phase 3 sequential root document synthesis producing `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` from complete corpus.**\n\n## Contents\n\n### Pipeline Coordination\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` class exports `createPlan()` method constructing `GenerationPlan` via four-step workflow: `prepareFiles()` loads file content into `PreparedFile[]`, `analyzeComplexity()` computes `directoryDepth` and unique `directories` set, `buildProjectStructure()` formats compact directory tree, `createFileTasks()` builds `AnalysisTask[]` with prompts via `buildFilePrompt()`, `createDirectoryTasks()` groups files by directory returning directory-level tasks. Emits trace events (`phase:start`, `plan:created`, `phase:end`) via injected `ITraceWriter`. Clears `PreparedFile.content` after prompt embedding to free heap memory. Returns `{ files, tasks, complexity, projectStructure }`.\n\n**[executor.ts](./executor.ts)** — Transforms `GenerationPlan` into dependency-aware `ExecutionPlan` via `buildExecutionPlan()`: groups files by directory into `directoryFileMap`, creates `fileTasks[]` with `id: 'file:${path}'` and empty `dependencies[]`, sorts by depth descending for leaf-first processing, creates `directoryTasks[]` depending on child file task IDs for post-order traversal, creates `rootTasks[]` depending on all directory task IDs. Exports `isDirectoryComplete()` predicate checking child `.sum` file existence via `sumFileExists()` and `getReadyDirectories()` async filter. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with three-phase checklist grouped by directory depth.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` via `calculateDirectoryDepth()` (max depth via `split(sep).length - 1`) and `extractDirectories()` (unique directories via upward `dirname()` traversal). Returns `{ fileCount, directoryDepth, files, directories }` consumed by orchestrator for concurrency tuning and Phase 2 directory queue construction.\n\n**[collector.ts](./collector.ts)** — Exports `collectAgentsDocs()` recursively walking project tree collecting `AGENTS.md` files as `AgentsDocs` array of `{ relativePath, content }` sorted alphabetically, and `collectAnnexFiles()` similarly collecting `.annex.md` files. Both skip `SKIP_DIRS` set (13 entries: node_modules, .git, vendor, dist, build, etc.) and silently suppress permission errors.\n\n**[types.ts](./types.ts)** — Defines `AnalysisResult` containing `summary: string` and `metadata: SummaryMetadata` returned by Phase 1 AI subprocess calls, `SummaryMetadata` YAML frontmatter schema with `purpose`, `criticalTodos?`, `relatedFiles?` fields, and `SummaryOptions` for summary verbosity configuration.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Template-based prompt construction pipeline: `buildFilePrompt()` injects file path/content/imports into `FILE_USER_PROMPT` with density rules and identifier preservation constraints, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` with manifest detection (9 types) and import maps via `extractDirectoryImports()`, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus with synthesis-only constraints prohibiting invented features. Exports six prompt constants with mustache-style placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`), prohibited filler phrases, YAML frontmatter format, and annex reference format.\n\n**[writers/](./writers/)** — YAML frontmatter-based file I/O layer: `writeSumFile()`/`readSumFile()` implement `.sum` persistence with SHA-256 `content_hash` via regex-based field extraction and dual-format YAML array handling (inline `[a,b,c]` vs multi-line), `writeAgentsMd()` preserves user-authored `AGENTS.md` by renaming to `AGENTS.local.md` and prepending above generated content with `GENERATED_MARKER` injection/stripping, `writeAnnexFile()` archives verbatim source for reproduction-critical files (prompt templates, config schemas). Exports `sumFileExists()` predicate for change detection and `isGeneratedAgentsMd()` marker detection.\n\n## Three-Phase Execution Strategy\n\n**Phase 1: Concurrent File Analysis**\n- Orchestrator creates `fileTasks[]` with prompts via `buildFilePrompt()` embedding import maps and project structure\n- Runner spawns worker pool (`src/orchestration/pool.ts`) executing tasks concurrently (default concurrency: 2 for WSL, 5 elsewhere)\n- Each worker calls `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`--max-old-space-size=512`, `UV_THREADPOOL_SIZE=4`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS=1`)\n- Worker writes `AnalysisResult` via `writeSumFile()` with YAML frontmatter containing SHA-256 `content_hash` and markdown summary body\n\n**Phase 2: Post-Order Directory Aggregation**\n- Executor sorts `directoryTasks[]` by depth descending (deepest first) via `getDirectoryDepth()` ensuring child directories complete before parents\n- Runner sequentially processes directories checking readiness via `isDirectoryComplete()` predicate polling for child `.sum` file existence\n- Prompt builder calls `buildDirectoryPrompt()` reading child `.sum` files via `readSumFile()`, aggregating subdirectory `AGENTS.md`, extracting imports via `extractDirectoryImports()`, detecting manifests (9 types: package.json, Cargo.toml, go.mod, etc.)\n- Runner writes `AGENTS.md` via `writeAgentsMd()` preserving any `AGENTS.local.md` user content above generated sections\n\n**Phase 3: Sequential Root Synthesis**\n- Executor creates `rootTasks[]` depending on all directory task IDs enforcing sequential execution (concurrency=1)\n- Prompt builder calls `buildRootPrompt()` consuming all `AGENTS.md` files via `collectAgentsDocs()`, reading root `package.json` metadata, embedding synthesis constraints prohibiting invented features\n- Runner writes platform-specific root documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) via `src/integration/generate.ts`\n\n## Post-Order Traversal Mechanism\n\nExecutor sorts directory tasks by depth descending:\n```typescript\ndirectoryTasks.sort((a, b) => \n  getDirectoryDepth(b.path) - getDirectoryDepth(a.path)\n)\n```\n\nwhere `getDirectoryDepth('.')` returns `0`, `getDirectoryDepth('src')` returns `1`, `getDirectoryDepth('src/cli')` returns `2`. Deepest directories process first ensuring child `AGENTS.md` exist before parent aggregation attempts. Runner polls `isDirectoryComplete()` checking all expected `.sum` files exist via `sumFileExists()` before processing directory task.\n\n## Memory Management Pattern\n\nOrchestrator clears `PreparedFile.content` after prompt construction:\n```typescript\nfor (const file of files) {\n  (file as { content: string }).content = ''\n}\n```\n\nThis frees heap memory since file content already embedded in `AnalysisTask.userPrompt` strings. Runner re-reads files from disk during execution if needed. Prevents memory exhaustion on large codebases (10k+ files).\n\n## Integration Points\n\nConsumes:\n- `DiscoveryResult` from `src/discovery/walker.ts` (file list input)\n- `Config` from `src/config/schema.ts` (concurrency, timeout, model settings)\n- `ITraceWriter` from `src/orchestration/trace.ts` (event emission)\n- `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt` from `./prompts/builder.ts`\n- `writeSumFile`, `writeAgentsMd` from `./writers/`\n- `extractDirectoryImports` from `src/imports/extractor.ts`\n- `collectAgentsDocs` from `./collector.ts`\n\nProduces:\n- `GenerationPlan` consumed by `src/orchestration/runner.ts`\n- `ExecutionPlan` consumed by Phase 1/2/3 execution loops\n- `.sum` files consumed by `src/update/orchestrator.ts` for change detection\n- `AGENTS.md` files consumed by Phase 3 root synthesis and `src/specify/index.ts`\n- `GENERATION-PLAN.md` consumed by progress tracking\n\nReferenced by:\n- `src/cli/generate.ts` (command entry point)\n- `src/cli/update.ts` (incremental update workflow)\n- `src/orchestration/runner.ts` (phase execution orchestrator)\n\n## Behavioral Contracts\n\n### Depth Calculation (executor.ts)\n```typescript\ngetDirectoryDepth('.')          → 0\ngetDirectoryDepth('src')        → 1\ngetDirectoryDepth('src/cli')    → 2\ngetDirectoryDepth('a/b/c/d')    → 4\n```\n\n### File Task Dependencies (executor.ts)\n```typescript\nfileTasks.forEach(task => task.dependencies = [])  // No dependencies, all parallel\n```\n\n### Directory Task Dependencies (executor.ts)\n```typescript\ndirectoryTask.dependencies = directoryFileMap[dirPath].map(f => `file:${f}`)\n```\n\n### Root Task Dependencies (executor.ts)\n```typescript\nrootTask.dependencies = directoryTasks.map(t => t.id)  // All directories\n```\n\n### SKIP_DIRS Set (collector.ts)\n```typescript\n['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', \n 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle']\n```\n\n### Manifest Detection Array (prompts/builder.ts)\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n### Language Detection Map (prompts/builder.ts)\n```typescript\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n## Annex References\n\n- Full prompt template text: [prompts/templates.ts.annex.md](./prompts/templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [prompts/types.ts.annex.md](./prompts/types.ts.annex.md)\n- Phase 2/3 execution workflow details: [../orchestration/runner.ts.annex.md](../orchestration/runner.ts.annex.md)\n### src/generation/prompts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# prompts\n\nExports prompt construction pipeline for ARE's three-phase documentation generation: `buildFilePrompt()` constructs Phase 1 file analysis prompts with import maps and project structure trees, `buildDirectoryPrompt()` aggregates `.sum` files and child `AGENTS.md` for Phase 2 directory synthesis, `buildRootPrompt()` synthesizes `CLAUDE.md` from complete `AGENTS.md` corpus in Phase 3.\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements prompt builders via template interpolation: `buildFilePrompt()` injects `filePath`/`content`/detected language (via `detectLanguage()`) into `FILE_USER_PROMPT`, appending `contextFiles[]` and optional `projectPlan`. `buildDirectoryPrompt()` aggregates `.sum` files via `readSumFile()`, child `AGENTS.md` from subdirectories, import maps via `extractDirectoryImports()`, and manifest files (9 types: package.json, Cargo.toml, go.mod, etc.), returning `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided. `buildRootPrompt()` collects all `AGENTS.md` via `collectAgentsDocs()`, reads root `package.json` metadata, embeds synthesis constraints prohibiting invented features. Returns `{system, user}` prompt pairs with incremental update system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when existing content provided.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts` for AI service call construction.\n\n### [templates.ts](./templates.ts)\nExports six prompt constants: `FILE_SYSTEM_PROMPT` (density rules, identifier preservation, behavioral contract verbatim reproduction), `FILE_USER_PROMPT` (contains `{{FILE_PATH}}`, `{{CONTENT}}` placeholders), `DIRECTORY_SYSTEM_PROMPT` (adaptive section strategy, path accuracy constraints, annex linking), `FILE_UPDATE_SYSTEM_PROMPT` (preserve unchanged sections verbatim), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (modify only affected entries), `ROOT_SYSTEM_PROMPT` (synthesis-only constraint: no invented features). Defines prohibited filler phrases (\"this file\", \"provides\", \"responsible for\"), annex reference format, YAML frontmatter structure for `.sum` files.\n\n### [types.ts](./types.ts)\nDefines `PromptContext` interface (fields: `filePath`, `content`, `contextFiles?`, `projectPlan?`, `existingSum?`) and `SUMMARY_GUIDELINES` constant specifying target word count (300-500), 8 required content categories (purpose, public interface, patterns, dependencies, signatures, related files, behavioral contracts, annex references), 3 excluded categories (control flow, generic TODOs, broad architecture).\n\n## Behavioral Contracts\n\n**Mustache-Style Placeholder Substitution** (from templates.ts):\n```\n{{FILE_PATH}}  — Replaced with source file relative path\n{{CONTENT}}    — Replaced with source file content (unescaped)\n{{LANG}}       — Replaced with detected language identifier\n{{PROJECT_PLAN_SECTION}} — Replaced with project structure tree or empty string\n```\n\n**Prohibited Filler Phrases** (from templates.ts):\n- \"this file\", \"this module\", \"this directory\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n\n**YAML Frontmatter Format** (from templates.ts):\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n```\n\n**Language Detection Map** (from builder.ts):\n```\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n**Annex Reference Format** (from templates.ts):\n```markdown\n## Annex References\n- `CONSTANT_NAME` — description (line count)\n```\n\n**Manifest Detection Array** (from builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n## Annex References\n\n- Full prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)\n- `SUMMARY_GUIDELINES` object structure: [types.ts.annex.md](./types.ts.annex.md)\n\n## File Relationships\n\nbuilder.ts depends on templates.ts constants (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, etc.) for interpolation, calls `detectLanguage()` for syntax highlighting identifiers, reads `.sum` files via `readSumFile()` (from `../writers/sum.js`), checks `GENERATED_MARKER` (from `../writers/agents-md.js`) to distinguish user-authored `AGENTS.md`, extracts imports via `extractDirectoryImports()` + `formatImportMap()` (from `../../imports/index.js`), collects root corpus via `collectAgentsDocs()` (from `../collector.js`). types.ts defines `PromptContext` interface consumed by all builder functions. index.ts re-exports public API consumed by `src/generation/executor.ts` and `src/generation/orchestrator.ts`.\n### src/generation/writers/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n**YAML frontmatter-based file I/O layer implementing `.sum` file persistence with SHA-256 hashing, `AGENTS.md` generation with user content preservation, and `.annex.md` verbatim source archival for reproduction-critical artifacts.**\n\n## Contents\n\n### Core Writers\n\n**[sum.ts](./sum.ts)** — SHA-256-tracked `.sum` file I/O with YAML frontmatter serialization via `writeSumFile()`/`readSumFile()`/`formatSumFile()`/`parseSumFile()`, regex-based field extraction (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), dual-format YAML array handling (inline `[a,b,c]` vs multi-line `  - item`) via `parseYamlArray()`/`formatYamlArray()`, `.annex.md` verbatim source archival via `writeAnnexFile()` for reproduction-critical files, path resolution via `getSumPath()`/`getAnnexPath()`.\n\n**[agents-md.ts](./agents-md.ts)** — `AGENTS.md` preservation logic via `writeAgentsMd()` four-step workflow: (1) detects user-authored files lacking `GENERATED_MARKER` and renames to `AGENTS.local.md`, (2) reads preserved `AGENTS.local.md` content, (3) strips marker prefix from LLM content, (4) assembles final output with marker header + user content block (`<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` delimiter) + horizontal rule + LLM content. Exports `isGeneratedAgentsMd()` marker detection predicate.\n\n**[index.ts](./index.ts)** — Barrel re-exporting `writeSumFile`/`readSumFile`/`getSumPath`/`sumFileExists`/`SumFileContent` from sum.ts and `writeAgentsMd` from agents-md.ts for unified import in `src/generation/executor.ts`.\n\n## File Naming Conventions\n\n- **Summary files**: `<sourcePath>.sum` (e.g., `foo.ts` → `foo.ts.sum`)\n- **Annex files**: `<sourcePath>.annex.md` (e.g., `foo.ts` → `foo.ts.annex.md`)\n- **Preserved user docs**: `AGENTS.md` → `AGENTS.local.md` (renamed on first generation)\n\n## YAML Frontmatter Structure\n\n`.sum` files use YAML frontmatter block delimited by `---\\n...\\n---\\n`:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex digest)\npurpose: One-line file purpose statement\ncritical_todos: [Security issue, Performance bottleneck]\nrelated_files: [../config/schema.ts, ./index.ts]\n---\n\nMarkdown summary content...\n```\n\n**Field serialization rules** (via `formatYamlArray()`):\n- Arrays with ≤3 items where all items <40 chars: inline format `key: [a, b, c]`\n- Otherwise: multi-line format with `  - ` prefix per item\n\n**Parsing patterns** (via `parseSumFile()`):\n- Frontmatter block extraction: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`\n- Arrays: inline `/key:\\s*\\[([^\\]]*)\\]/`, multi-line `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n## User Content Preservation Strategy\n\n`writeAgentsMd()` implements two-path detection for existing `AGENTS.md` files:\n\n1. **First-time generation**: If existing `AGENTS.md` lacks `GENERATED_MARKER` (via `isGeneratedAgentsMd()` substring search), rename to `AGENTS.local.md` to preserve user content\n2. **Subsequent runs**: Read `AGENTS.local.md` if already exists from prior rename operation\n\nFinal assembly concatenates:\n1. `GENERATED_MARKER` constant: `'<!-- Generated by agents-reverse-engineer -->'`\n2. User content block (if present): `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\\n<content>\\n---`\n3. LLM-generated content (with marker prefix stripped if present)\n\nEnsures AI assistants see user-defined directory context before generated summaries during codebase navigation.\n\n## Annex File Pattern\n\n**Purpose**: Archive verbatim source content for reproduction-critical files (e.g., prompt templates with regex patterns, config schemas with magic constants) whose behavioral contracts cannot fit within `.sum` word limits.\n\n**Generated format** (via `writeAnnexFile()`):\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n# Annex: <fileName>\n\nReproduction-critical source content from `<fileName>`.\nReferenced by `<fileName>.sum`.\n\n```\n<triple-backtick code fence with full source content>\n```\n```\n\n**Reference pattern**: `.sum` files include `## Annex References` section linking to annex: `[templates.ts.annex.md](./templates.ts.annex.md)`.\n\n## Integration with Generation Pipeline\n\n**Phase 1 (File Analysis)**:\n- `src/generation/executor.ts` calls `writeSumFile()` after LLM subprocess returns `SumFileContent`\n- `writeSumFile()` creates `.sum` alongside source file via `mkdir({ recursive: true })`\n- Optional `writeAnnexFile()` call for files flagged as reproduction-critical\n\n**Phase 2 (Directory Aggregation)**:\n- `src/generation/executor.ts` reads child `.sum` files via `readSumFile()` for aggregation prompts\n- After LLM generates directory summary, calls `writeAgentsMd()` to assemble final `AGENTS.md`\n- `writeAgentsMd()` preserves any existing `AGENTS.local.md` content above generated sections\n\n**Phase 3 (Root Synthesis)**:\n- Root document generators consume all `AGENTS.md` files via `src/generation/collector.ts`\n- No direct writer interaction (root docs written by `src/integration/generate.ts`)\n\n## Change Detection Integration\n\n`readSumFile()` exposes `contentHash` field parsed from YAML frontmatter, consumed by `src/update/orchestrator.ts` for SHA-256-based incremental update detection:\n\n1. Read current file content hash via `src/change-detection/detector.ts`\n2. Read `.sum` file's stored `contentHash` via `readSumFile()`\n3. Hash mismatch → add to regeneration queue\n4. Hash match → skip (file unchanged)\n\n## Error Handling\n\n- **`readSumFile()`**: Returns `null` on missing file or parse failure (no exceptions thrown)\n- **`sumFileExists()`**: Returns `false` if `readSumFile()` returns `null`\n- **`isGeneratedAgentsMd()`**: Returns `false` on read errors (treats missing files as non-generated)\n- **`writeAgentsMd()`**: Silent try-catch blocks for existing `AGENTS.md` / `AGENTS.local.md` reads, treats absence as `null` user content\n- **`writeSumFile()` / `writeAnnexFile()`**: No error handling—writeFile failures bubble to caller\n\n## Behavioral Contracts\n\n### Regex Patterns (sum.ts)\n\n**Frontmatter extraction**:\n- Block delimiter: `/^---\\n([\\s\\S]*?)\\n---\\n/`\n- Scalar fields: `/<key>:\\s*(.+)/` (e.g., `generated_at`, `content_hash`, `purpose`)\n- Inline array: `/<key>:\\s*\\[([^\\]]*)\\]/`\n- Multi-line array: `/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`\n\n**Value normalization**:\n- Quote stripping: `/^[\"']|[\"']$/g` (removes leading/trailing quotes from inline array items)\n- Leading newline removal: `/^\\n+/` (strips marker prefix from LLM content)\n\n### Constants\n\n**GENERATED_MARKER** (agents-md.ts): `'<!-- Generated by agents-reverse-engineer -->'` — marker for tool-generated `AGENTS.md` detection, referenced by `isGeneratedAgentsMd()` substring search and `writeAgentsMd()` marker injection/stripping.\n### src/imports/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Extracts and classifies TypeScript/JavaScript import statements from source files via regex-based parsing, producing structured import maps with internal/external partitioning for LLM prompt integration during `.sum` generation and directory aggregation phases.**\n\n## Contents\n\n### Core Implementation\n\n**[extractor.ts](./extractor.ts)** — Parses source text via `IMPORT_REGEX` to extract import declarations with `extractImports()` returning `ImportEntry[]` (specifier, symbols, typeOnly), aggregates directory-level imports via `extractDirectoryImports()` reading first 100 lines per file with relative specifier filtering (`.` or `..` prefix) and internal/external classification, formats import maps for LLM prompts via `formatImportMap()` producing structured `fileName:\\n  specifier → symbol1, symbol2 (type)` output.\n\n**[types.ts](./types.ts)** — Defines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` fields representing single import statements, and `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` fields partitioning imports by external (parent directories/npm packages) versus internal (same-directory sibling files) origin.\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` functions and `ImportEntry`, `FileImports` types from implementation modules for single-point import surface consumed by prompt builders.\n\n## Import Extraction Strategy\n\n**Regex Pattern:** `IMPORT_REGEX = /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` with capture groups:\n- Group 1: `type` keyword for type-only imports (`import type`)\n- Group 2: Named imports between braces (`{ Foo, Bar }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier)\n- Group 5: Module specifier (string after `from`)\n\nAnchored with `^` to match only lines starting with `import`, avoiding dynamic imports/comments/string literals containing import syntax. Global + multiline flags (`gm`) enable `.exec()` iteration via `.lastIndex` reset for multi-import files.\n\n**Classification Logic:**\n- **Internal:** Specifier starts with `./` (same directory)\n- **External:** Specifier starts with `../` (parent directory)\n- **Excluded:** Bare specifiers (npm packages like `'react'`) and `node:` protocol builtins\n\nApplied via `.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))` before internal/external partitioning in `extractDirectoryImports()`.\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before parsing in `extractDirectoryImports()`, assuming imports clustered at file top per convention. Trades completeness for speed in large codebases where imports after line 100 are rare edge cases. Silently skips unreadable files via empty catch block.\n\n## Integration Points\n\nConsumed by `../generation/prompts/builder.ts`:\n- `buildFilePrompt()` includes import context from `extractDirectoryImports()` to inform LLM about cross-file dependencies when generating `.sum` summaries\n- `buildDirectoryPrompt()` embeds `formatImportMap()` output in Phase 2 directory aggregation prompts to show module coupling within `AGENTS.md` synthesis\n\nOutput format embeds in prompt templates defined in `../generation/prompts/templates.ts` for structured import relationship documentation.\n\n## Behavioral Contracts\n\n**Import Regex Pattern (IMPORT_REGEX):**\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n**Alias Stripping Pattern:**\n```\n.replace(/\\s+as\\s+\\w+/, '')\n```\n\n**Relative Specifier Filter:**\n```javascript\n.filter(i => i.specifier.startsWith('.') || i.specifier.startsWith('..'))\n```\n\n**Import Map Format Template:**\n```\nfileName:\n  specifier → symbol1, symbol2 (type)\n```\n\n**First N Lines Slice:**\n```javascript\ncontent.split('\\n').slice(0, 100).join('\\n')\n```\n### src/installer/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer/\n\n**npx-driven installer orchestrating IDE command/hook deployment across Claude Code, OpenCode, and Gemini CLI with interactive prompts, platform-specific settings.json registration, detached version-check processes, and recursive empty directory cleanup.**\n\n## Contents\n\n### [banner.ts](./banner.ts)\nASCII art rendering and terminal output styling. `displayBanner()` renders 7-line \"ARE\" logo with version string, `showHelp()` prints usage with cyan-highlighted examples, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` output prefixed symbols (`✓`/`✗`/`!`/`>`), `showNextSteps()` displays numbered command list with `/are-*` shortcuts.\n\n### [index.ts](./index.ts)\nMain entry point orchestrating install/uninstall workflow. `runInstaller()` parses CLI args, displays banner, prompts for missing runtime/location in interactive mode, enforces non-interactive requirements (`--runtime` + `-g/-l` flags mandatory when `!isInteractive()`), dispatches to `runInstall()`/`runUninstall()`, aggregates results. `parseInstallerArgs()` handles short/long flags (`-g`/`--global`, `-h`/`--help`).\n\n### [operations.ts](./operations.ts)\nFile copying, hook registration, permission configuration. `installFilesForRuntime()` writes command templates to `${basePath}/${relativePath}`, copies hooks from bundled `hooks/dist/`, calls `registerClaudeHooks()`/`registerGeminiHooks()` with nested `HookEvent.hooks[]` or flat `GeminiHook[]` schemas, adds `ARE_PERMISSIONS` patterns to Claude `settings.json`, writes `ARE-VERSION` file. `getPackageVersion()` reads package.json via `fileURLToPath(import.meta.url)`.\n\n### [paths.ts](./paths.ts)\nCross-platform path resolution with environment overrides. `getRuntimePaths()` returns `{ global, local, settingsFile }` applying `CLAUDE_CONFIG_DIR`/`OPENCODE_CONFIG_DIR`/`GEMINI_CONFIG_DIR` precedence, `resolveInstallPath()` joins project root with local paths (`.claude`/`.opencode`/`.gemini`), `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` check directory existence via `stat()`.\n\n### [prompts.ts](./prompts.ts)\nInteractive selection with arrow-key navigation or numbered fallback. `arrowKeySelect()` enables raw mode (`process.stdin.setRawMode(true)`), listens for `key.name === 'up'|'down'|'return'`, re-renders with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line). `numberedSelect()` prints 1-indexed list for non-TTY. `cleanupRawMode()` registered on `process.on('exit')` and `SIGINT`.\n\n### [types.ts](./types.ts)\nInterface definitions for installer workflow. `InstallerArgs` with `runtime?: Runtime`, `global/local/uninstall/force/help/quiet: boolean`. `InstallerResult` with `success`, `runtime: Exclude<Runtime, 'all'>`, `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered?`, `versionWritten?`. `RuntimePaths` with `global/local/settingsFile` strings.\n\n### [uninstall.ts](./uninstall.ts)\nArtifact removal and hook deregistration. `uninstallFilesForRuntime()` deletes command templates, hooks/plugins, `ARE-VERSION` file, calls `unregisterClaudeHooks()`/`unregisterGeminiHooks()` filtering `settings.json` by `getHookPatterns()` (current + legacy formats), `unregisterPermissions()` removes `ARE_PERMISSIONS` from Claude `permissions.allow[]`. `cleanupEmptyDirs()` recursively removes directories via `rmdirSync()`. `cleanupLegacyGeminiFiles()` deletes pre-TOML `are-*.md` and old TOML subdirectories.\n\n## Installation Workflow\n\n**Interactive mode (TTY):** `runInstaller()` displays banner → prompts for runtime via `selectRuntime()` (options: claude/opencode/gemini/all) → prompts for location via `selectLocation()` (global: `~/.claude`, local: `./.claude`) → confirms action → dispatches.\n\n**Non-interactive mode (CI):** requires `--runtime <value>` and `-g`/`-l` flags, exits with error via `showError()` + `process.exit(1)` if missing.\n\n**Installation:** `runInstall()` calls `installFiles()` → `installFilesForRuntime()` copies templates from `getTemplatesForRuntime()` to `${basePath}/${relativePath}` (path component after runtime prefix), reads bundled hooks from `hooks/dist/${filename}` via `readBundledHook()`, writes to `${basePath}/hooks/` or `${basePath}/plugins/`, updates `settings.json` with hook/permission entries, writes `ARE-VERSION` from `getPackageVersion()`.\n\n**Uninstallation:** `runUninstall()` calls `uninstallFiles()` → `uninstallFilesForRuntime()` deletes files, unregisters hooks by filtering `settings.json` arrays with `getHookPatterns()` (matches current `node ${runtimeDir}/hooks/${filename}` and legacy `node hooks/${filename}` formats), removes permissions, deletes empty directories via `cleanupEmptyDirs()`, calls `deleteConfigFolder()` for local installs.\n\n## Settings.json Hook Registration\n\n**Claude Code format (nested structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"node ~/.claude/hooks/are-check-update.js\" }\n        ]\n      }\n    ]\n  }\n}\n```\nSchema: `SettingsJson.hooks[event]` contains `HookEvent[]` where each `HookEvent` has `hooks: SessionHook[]` array with `{ type: 'command', command: string }` entries.\n\n**Gemini CLI format (flat structure):**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      { \"name\": \"are-check-update\", \"type\": \"command\", \"command\": \"node ~/.gemini/hooks/are-check-update.js\" }\n    ]\n  }\n}\n```\nSchema: `GeminiSettingsJson.hooks[event]` contains flat `GeminiHook[]` array with `{ name: string, type: 'command', command: string }` entries.\n\n**OpenCode (plugin system):** Copies `ARE_PLUGINS` files (`opencode-are-check-update.js` → `are-check-update.js`) to `${basePath}/plugins/`, sets `hookRegistered = true` without settings.json modification.\n\n## Behavioral Contracts\n\n**Hook definitions (operations.ts):**\n- `ARE_HOOKS: HookDefinition[]` currently empty (commented: \"causing issues\")\n- Intended entries: `{ event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }`\n\n**Plugin definitions (operations.ts):**\n- `ARE_PLUGINS: PluginDefinition[]` with one active entry: `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- Commented: `{ srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }`\n\n**Permission patterns (operations.ts):**\n```javascript\nARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)'\n]\n```\n\n**Bundled hook paths:** `__dirname/../../hooks/dist/${hookName}` where `__dirname` from `fileURLToPath(import.meta.url)`, built via `scripts/build-hooks.js` during `npm run build:hooks`.\n\n**Environment variable precedence:**\n- Claude: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- OpenCode: `OPENCODE_CONFIG_DIR` overrides `XDG_CONFIG_HOME/opencode` overrides `~/.config/opencode`\n- Gemini: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n**ANSI escape sequences (prompts.ts):**\n- `\\x1b[${n}A` — cursor up n lines\n- `\\x1b[1B` — cursor down 1 line\n- `\\x1b[2K` — clear entire line\n\n**Keypress matching (prompts.ts):** `key.name === 'up' | 'down' | 'return'`, interrupt via `key.ctrl && key.name === 'c'`.\n\n## File System Conventions\n\n**Template path extraction:** Remove runtime prefix via `template.path.split('/').slice(1).join('/')`, e.g., `.claude/commands/are/generate.md` → `commands/are/generate.md`.\n\n**Installation paths:**\n- Commands: `${basePath}/${relativePath}`\n- Hooks: `${basePath}/hooks/${filename}` (Claude/Gemini)\n- Plugins: `${basePath}/plugins/${destFilename}` (OpenCode)\n- Settings: `${basePath}/settings.json`\n- Version: `${basePath}/ARE-VERSION`\n\n**Config folder:** `.agents-reverse-engineer` deleted only for local uninstalls via `deleteConfigFolder(location === 'local', dryRun)`.\n\n## Error Handling\n\nFile write failures append to `errors[]` with format `\"Failed to write ${fullPath}: ${err}\"`. JSON parse failures in hook registration silently reset settings to `{}`. Missing bundled hooks throw Error in `readBundledHook()`. Directory cleanup ignores errors via empty catch blocks. `rmSync()` uses `{ recursive: true, force: true }` for graceful missing path handling. `InstallerResult.success = errors.length === 0`.\n\n## Dependencies\n\n**Core modules:** `node:os` (homedir), `node:path` (join/dirname), `node:fs` (writeFileSync/readFileSync/unlinkSync/mkdirSync/readdirSync/rmdirSync/rmSync/existsSync), `node:fs/promises` (stat), `node:url` (fileURLToPath).\n\n**External libraries:** `picocolors` (ANSI colors: green/red/yellow/cyan/dim/bold), `readline` (keypress events/interface creation).\n\n**Internal imports:**\n- `../version.js` → getVersion (banner.ts)\n- `../integration/templates.js` → getClaudeTemplates/getOpenCodeTemplates/getGeminiTemplates (operations.ts, uninstall.ts)\n- `./paths.js` → getRuntimePaths/resolveInstallPath/getAllRuntimes (operations.ts, uninstall.ts, index.ts)\n- `./types.js` → Runtime/Location/InstallerArgs/InstallerResult/RuntimePaths (all files)\n- `./banner.js` → display functions (index.ts)\n- `./prompts.js` → selectRuntime/selectLocation/confirmAction/isInteractive (index.ts)\n### src/integration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific integration file generation for AI coding assistants (Claude Code, OpenCode, Gemini CLI, Aider) via environment detection, template materialization with frontmatter variants, and bundled hook deployment.**\n\n## Contents\n\n**[detect.ts](./detect.ts)** — `detectEnvironments(projectRoot)` scans for `.claude/`, `CLAUDE.md`, `.opencode/`, `.aider.conf.yml`, `.aider/`, `.gemini/` artifacts via `existsSync()` and returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment(projectRoot, type)` predicate tests for specific platform presence.\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles(projectRoot, options)` orchestrates template generation with skip-if-exists behavior (unless `force=true`) and optional `dryRun` preview. Dispatches to platform-specific template getters via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` after `ensureDir()` parent directory creation, copies bundled hooks from `hooks/dist/` for Claude via `readBundledHook('are-session-end.js')`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays.\n\n**[templates.ts](./templates.ts)** — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generate command file arrays via `buildTemplate(platform, commandName, command)` with platform-specific paths (.claude/skills/are-{command}/SKILL.md, .opencode/commands/are-{command}.md, .gemini/commands/are-{command}.toml). `COMMANDS` constant defines seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`. `PLATFORM_CONFIGS` maps frontmatter schemas (Claude `name:` field, OpenCode `agent: build` directive, Gemini `description`/`prompt` TOML structure). `buildFrontmatter()` handles Markdown variants, `buildGeminiToml()` formats triple-quoted prompts. Placeholder substitution replaces `COMMAND_PREFIX` (`/are-`) and `VERSION_FILE_PATH` (.claude/ARE-VERSION, .opencode/ARE-VERSION, .gemini/ARE-VERSION).\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Architecture\n\n### Environment Detection\n\n`detectEnvironments()` applies existence checks via `path.join(projectRoot, relativePath)` → `existsSync()` for platform-specific markers:\n- Claude: `.claude/` directory OR `CLAUDE.md` file → `{ type: 'claude', configDir: '.claude', detected: true }`\n- OpenCode: `.opencode/` directory → `{ type: 'opencode', configDir: '.opencode', detected: true }`\n- Aider: `.aider.conf.yml` file OR `.aider/` directory → `{ type: 'aider', configDir: '.aider', detected: true }`\n- Gemini: `.gemini/` directory → `{ type: 'gemini', configDir: '.gemini', detected: true }`\n\nNo recursive parent directory scanning—only direct children of `projectRoot` tested.\n\n### Template Generation Flow\n\n1. `generateIntegrationFiles()` receives `projectRoot` and optional `GenerateOptions` (`dryRun`, `force`, `environment`)\n2. If `options.environment` specified, constructs single-element array via `configDirMap: Record<EnvironmentType, string>` lookup (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`)\n3. Otherwise calls `detectEnvironments(projectRoot)` to get auto-detected platforms\n4. For each environment, calls `getTemplatesForEnvironment(env.type)` to retrieve `IntegrationTemplate[]` from `templates.ts`\n5. Iterates templates, constructs `fullPath` via `path.join(projectRoot, template.path)`, writes file via `writeFileSync(fullPath, template.content, 'utf-8')` if not exists or `force=true`\n6. Special case for `claude` environment: after template processing, reads bundled hook via `getBundledHookPath('are-session-end.js')` → `readFileSync(hookPath, 'utf-8')`, writes to `.claude/hooks/are-session-end.js`\n7. Tracks created/skipped paths in `IntegrationResult` per environment\n\n### Platform Configuration Schema\n\n`PLATFORM_CONFIGS` defines variants for frontmatter generation and path construction:\n- **Claude**: Nested directory structure (.claude/skills/are-{command}/SKILL.md), frontmatter with `name: /are-{command}`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: Flat file structure (.opencode/commands/are-{command}.md), frontmatter with `agent: build`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: TOML format (.gemini/commands/are-{command}.toml), `description = \"...\"` and `prompt = \"\"\"...\"\"\"` fields, `versionFilePath: '.gemini/ARE-VERSION'`\n- **Aider**: Returns empty array (no templates defined)\n\n`buildFrontmatter(platform, commandName, description)` emits Markdown frontmatter with conditional `name:` field (Claude only) and `agent: build` directive (OpenCode only). `buildGeminiToml(commandName, command)` formats TOML structure per Gemini CLI spec at https://geminicli.com/docs/cli/custom-commands/.\n\n### Command Template Structure\n\n`COMMANDS` constant defines command metadata with `description`, `argumentHint`, `content` fields for seven commands:\n\n- **`generate`**: Three-phase pipeline documentation (Discovery → File Analysis → Directory/Root Documents). Embeds background execution pattern with `run_in_background: true`, polling `.agents-reverse-engineer/progress.log` via Read tool with `offset` parameter, TaskOutput monitoring with `block: false`, completion summarization covering file counts, failures, inconsistency warnings. Flags: `--dry-run`, `--concurrency N`, `--fail-fast`, `--debug`, `--trace`.\n- **`update`**: Incremental change detection workflow with `--uncommitted` flag, hash-based comparison, orphan cleanup, affected directory regeneration. Monitoring pattern identical to `generate`.\n- **`discover`**: Enforces strict rule constraints: \"Run ONLY this exact command: `npx agents-reverse-engineer@latest discover $ARGUMENTS`\" with explicit prohibition against adding unlisted flags. Polls progress.log after 10-second delay.\n- **`clean`**: Mirrors `discover` strict rules pattern, deletes .sum/AGENTS.md/GENERATION-PLAN.md/CLAUDE.md artifacts with `--dry-run` preview.\n- **`specify`**: AGENTS.md collection → AI synthesis → specs/SPEC.md generation. Auto-runs `generate` if no AGENTS.md files exist. Flags: `--output <path>`, `--multi-file`, `--force`.\n- **`rebuild`**: Spec partitioning into ordered rebuild units, sequential inter-group/concurrent intra-group processing, context accumulation, ===FILE:=== delimited output parsing, exit code semantics (0=success, 1=partial failure, 2=total failure). Flags: `--output <path>`, `--force`, `--concurrency N`, `--fail-fast`.\n- **`help`**: Command reference with usage tables, configuration YAML example, generated file schemas (.sum frontmatter: `file_type`, `generated_at`, `content_hash`, `purpose`, `public_interface`, `dependencies`, `patterns`, `related_files`), workflow recipes, repository link https://github.com/GeoloeG-IsT/agents-reverse-engineer.\n\n`buildTemplate(platform, commandName, command)` materializes templates via placeholder substitution: `COMMAND_PREFIX` → `/are-`, `VERSION_FILE_PATH` → platform-specific version cache path from `PLATFORM_CONFIGS[platform].versionFilePath`.\n\n### Background Execution Pattern\n\n`generate`, `update`, `discover`, `specify`, `rebuild` templates embed consistent monitoring workflow:\n1. Display version via Read tool on `VERSION_FILE_PATH`\n2. Execute `npx agents-reverse-engineer@latest {command} $ARGUMENTS` with `run_in_background: true`\n3. Wait 10-15 seconds via `sleep N` in Bash\n4. Poll `.agents-reverse-engineer/progress.log` using Read tool with `offset` parameter for last ~20 lines\n5. Check TaskOutput with `block: false` for completion\n6. Repeat polling until background task finishes\n7. Read full background task output and summarize command-specific metrics\n\nPattern includes explicit instruction: \"Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing).\"\n\n## Bundled Hook Deployment\n\n`generateIntegrationFiles()` includes special-case logic for Claude environment: after template generation loop completes, calls `readBundledHook('are-session-end.js')` which resolves path via `getBundledHookPath(hookName)` using `fileURLToPath(import.meta.url)` to navigate from `dist/integration/` up two levels to project root, then into `hooks/dist/`. Throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync(hookPath)` returns false. Writes hook content to `.claude/hooks/are-session-end.js` via `writeFileSync(fullHookPath, hookContent, 'utf-8')`. Hook files populated in `hooks/dist/` by `scripts/build-hooks.js` during `npm run build:hooks` step.\n\n## Integration with Project\n\nCalled by `src/installer/operations.ts` installer CLI commands to create IDE-specific command/hook files. Consumed by `src/cli/init.ts` initialization workflows requiring platform-specific configuration. Environment detection (`detectEnvironments()`) supports auto-discovery mode when `options.environment` undefined, enabling multi-platform batch generation (`npx agents-reverse-engineer --runtime all`). Single-environment targeting via `options.environment` parameter supports focused installs (`npx agents-reverse-engineer --runtime claude -g`).\n\n## Import Map\n\n- **Local**: `./detect.js` (`detectEnvironments`, `hasEnvironment`), `./templates.js` (`getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`), `./types.js` (`EnvironmentType`, `DetectedEnvironment`, `IntegrationTemplate`, `IntegrationResult`)\n- **External**: `path` (Node.js), `fs` (`existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`), `url` (`fileURLToPath`), `../installer/paths.js` (referenced in integration context)\n### src/orchestration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker-pool concurrency control with iterator-based task distribution, progress telemetry via ETA calculation, serialized plan/log/trace writers for concurrent-safe output, and high-level command workflows integrating three-phase AI-driven documentation pipelines.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export aggregating `runPool`, `createTraceWriter`, `cleanupOldTraces`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, plus types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`)\n\n**[pool.ts](./pool.ts)** — `runPool<T>(tasks, options, onComplete?)` iterator-based worker pool sharing single `tasks.entries()` iterator across N workers, returns `Promise<TaskResult<T>[]>` preserving task index, enforces `options.concurrency` cap via `Math.min(options.concurrency, tasks.length)`, supports fail-fast abort via shared `aborted` flag, emits trace events (`worker:start/end`, `task:pickup/done`) with `activeTasks` counter\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates `executeGenerate(plan)` via three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis) with pre/post validation phases (pre-phase-1 cache for stale-doc detection, post-phase-1 code-vs-doc/code-vs-code checks, post-phase-2 phantom path resolution), `executeUpdate(filesToAnalyze)` runs Phase 1 only for incremental workflows, helpers `stripPreamble()` remove LLM conversational prefix via `\\n---\\n` or bold purpose detection, `extractPurpose()` scans lines skipping `PREAMBLE_PREFIXES` (`['now i', 'perfect', 'based on', ...]`)\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams build-log output with format `[X/Y] ANALYZING/DONE/FAIL path` for files, `[dir X/Y] ANALYZING/DONE dirPath/AGENTS.md` for directories, `[root] DONE docPath` for roots, calculates ETA via moving average of last 10 completion times in `completionTimes[]`/`dirCompletionTimes[]` arrays, `printSummary(summary)` outputs end-of-run aggregates; `ProgressLog` mirrors console output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with lazy file handle creation, `stripAnsi(str)` removes color codes via regex `/\\x1b\\[[0-9;]*m/g`\n\n**[trace.ts](./trace.ts)** — `ITraceWriter` interface with `emit(event)`, `finalize()`, `filePath`, `createTraceWriter(projectRoot, enabled)` factory returns `TraceWriter` writing NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` or `NullTraceWriter` no-op stub, `TraceWriter` auto-populates `TraceEventBase` fields (`seq`, `ts`, `pid`, `elapsedMs` via `process.hrtime.bigint()`), serializes via promise-chain pattern, `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces keeping most recent, event types: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` updates `GENERATION-PLAN.md` checkboxes via `markDone(itemPath)` replacing `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\``, maintains `content: string` in-memory, serializes writes via `writeQueue: Promise<void>` chain pattern, `initialize()` creates parent directory and writes initial markdown, `flush()` drains pending writes before command completion\n\n**[types.ts](./types.ts)** — `FileTaskResult` (path, success, tokensIn/Out, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed/Failed/Skipped, totalCalls, token totals, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc/CodeVsCode, phantomPaths?, inconsistencyReport?), `ProgressEvent` discriminated by type (`start|done|error|dir-done|root-done`), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?, progressLog?)\n\n## Architecture Patterns\n\n### Iterator-Based Worker Pool\n\n`pool.ts` shares single `tasks.entries()` iterator across N workers via JavaScript iterator protocol guaranteeing atomic `.next()` calls. Workers consume `[index, task]` pairs in `for...of` loop, execute task factory, immediately pull next without batch idle periods. Effective concurrency capped via `Math.min(options.concurrency, tasks.length)`. Fail-fast mode sets shared `aborted` flag on error, workers check via `if (aborted) break` before pulling next. Results array sparse-populated via `results[index] = result` to preserve task position.\n\n### Promise-Chain Serialization\n\n`PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` chain writes via `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` pattern. Prevents NDJSON corruption and markdown TOCTOU errors from concurrent pool workers. Lazy file handle creation (first write opens file, subsequent writes reuse handle). Write errors silently swallowed (non-critical telemetry loss acceptable).\n\n### Three-Phase Execution Pipeline\n\n`runner.ts` executes:\n1. **Pre-Phase-1**: Throttled read (concurrency=20) of existing `.sum` files into `oldSumCache` for stale-doc detection\n2. **Phase 1**: Concurrent file analysis via `runPool()` calling `aiService.call()` with `buildFilePrompt()`, writes `.sum` with YAML frontmatter (generatedAt, contentHash, purpose), caches source in `sourceContentCache`\n3. **Post-Phase-1**: Quality validation via `checkCodeVsDoc()` twice (against `oldSumCache` for stale, against fresh `.sum` for omissions), `checkCodeVsCode()` for duplicate exports\n4. **Phase 2**: Post-order directory aggregation sorted by depth descending via `sort((a,b) => b-a)`, waits for all child `.sum` files via implicit dependency, calls `aiService.call()` with `buildDirectoryPrompt()`, writes `AGENTS.md` via `writeAgentsMd()` merging `AGENTS.local.md`\n5. **Post-Phase-2**: Phantom path validation via `checkPhantomPaths()` extracting path-like strings with three regex patterns (markdown links, backtick paths, prose-embedded), resolves via `existsSync()` with `.ts`/`.js` fallback\n6. **Phase 3**: Sequential root synthesis (concurrency=1) via `runPool()` calling `aiService.call()` with `buildRootPrompt()` injecting all `AGENTS.md` via `collectAgentsDocs()`, strips conversational preamble via markdown start detection (`indexOf('# ')`), writes to `rootTask.outputPath`\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes[]` and `dirCompletionTimes[]` with max size `windowSize=10`. Records task duration on completion, computes moving average via `reduce((a,b)=>a+b, 0)/length`, multiplies by remaining tasks. Formats via `formatETA()` as `~12s remaining` or `~2m 30s remaining`. Returns empty string if fewer than 2 completions (insufficient sample).\n\n### Trace Event Emission\n\nTracer threaded via `CommandRunOptions.tracer` → pool options → AIService. `TraceWriter` auto-populates base fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (process.pid), `elapsedMs` (high-resolution fractional via `process.hrtime.bigint()` delta). Events: `phase:start` (taskCount, concurrency, phase), `phase:end` (durationMs, tasksCompleted, tasksFailed, phase), `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks), `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode).\n\n## Incremental Update Strategy\n\n`runner.executeUpdate()` runs Phase 1 only for `filesToAnalyze: FileChange[]`. Passes `existingSum: existingSumContent?.summary` to `buildFilePrompt()` for incremental mode. Caches source in `updateSourceCache`, runs quality validation (code-vs-doc new-doc check, code-vs-code duplicate detection). Returns `RunSummary` with `filesSkipped: 0`. Caller (`src/update/orchestrator.ts`) handles Phase 2 `AGENTS.md` regeneration for `affectedDirs`.\n\n## Quality Validation Phases\n\n**Pre-Phase-1 Cache**: Reads existing `.sum` files into `oldSumCache: Map<string, SumFileContent>` for stale documentation detection in post-phase-1 code-vs-doc checks.\n\n**Post-Phase-1 Code-vs-Doc**: Calls `checkCodeVsDoc()` twice per file — once against `oldSumCache` (detect stale exports), once against freshly written `.sum` (detect omissions). Extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies substring presence in summary.\n\n**Post-Phase-1 Code-vs-Code**: Groups files by directory via `path.dirname()`, calls `checkCodeVsCode()` aggregating exports per directory into `Map<symbol, string[]>`, detects duplicates.\n\n**Post-Phase-2 Phantom Paths**: Reads each `AGENTS.md`, calls `checkPhantomPaths()` extracting path-like strings via three regex patterns (markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback via `existsSync()`.\n\n## Behavioral Contracts\n\n### PlanTracker Checkbox Update Pattern\n```typescript\nmarkDone(itemPath: string) {\n  // Replaces:  - [ ] `${itemPath}`\n  // With:      - [x] `${itemPath}`\n}\n```\n\n### ProgressReporter Output Formats\n```\nFile progress:\n  [X/Y] ANALYZING path\n  [X/Y] DONE path Xs in/out tok model ~Ns remaining\n  [X/Y] FAIL path error\n\nDirectory progress:\n  [dir X/Y] ANALYZING dirPath/AGENTS.md\n  [dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA\n\nRoot progress:\n  [root] DONE docPath\n\nSummary format:\n  === Run Summary ===\n    ARE version:     <version>\n    Files processed: <count>\n    Files failed:    <count>\n    Files skipped:   <count>\n    Total calls:     <count>\n    Tokens:          <totalIn> in / <totalOut> out\n    Cache:           <cacheRead> read / <cacheCreated> created\n    Files read:      <total> (<unique> unique)\n    Total time:      <elapsed>s\n    Errors:          <count>\n    Retries:         <count>\n```\n\n### Preamble Stripping Patterns\n```typescript\nstripPreamble(responseText: string): string {\n  // Pattern 1: Content after \\n---\\n separator within first 500 chars\n  // Pattern 2: Content starting with bold purpose **[A-Z] if preceding text <300 chars and lacks ##\n}\n\nextractPurpose(responseText: string): string {\n  // Skip lines: empty, #headers, ---separators\n  // Skip prefixes (case-insensitive): 'now i', 'perfect', 'based on', 'let me', \n  //   'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'\n  // Strip bold wrapper **, truncate to 120 chars with ... suffix\n}\n```\n\n### TraceEvent Base Field Population\n```typescript\nemit(event: TraceEventPayload): void {\n  // Auto-populate:\n  //   seq: monotonic counter\n  //   ts: new Date().toISOString()\n  //   pid: process.pid\n  //   elapsedMs: Number(process.hrtime.bigint() - startHr) / 1e6\n}\n```\n\n### Pool Worker Task Distribution\n```typescript\nrunPool<T>(tasks, options, onComplete?) {\n  const iterator = tasks.entries(); // Shared across workers\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) => \n    worker(iterator, workerId)\n  );\n  // Each worker pulls [index, task] via for...of over shared iterator\n  // Iterator protocol guarantees atomic .next() calls → no duplicate pickups\n}\n```\n\n## Reproduction-Critical Constants\n\nFull prompt template texts referenced in `runner.ts`:\n- [runner.ts.annex.md](./runner.ts.annex.md)\n### src/output/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal logging interface providing picocolors-based formatted output for CLI progress reporting, file discovery summaries, warnings, and errors with runtime color toggle support.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (6 methods: `info`, `file`, `excluded`, `summary`, `warn`, `error`), `createLogger(options: LoggerOptions)` factory with conditional color mode (`picocolors` vs identity passthrough), `createSilentLogger()` no-op factory for testing, and `LoggerOptions` configuration interface (`colors: boolean`). Implements format rules: file discovery prefixes (`  +` green for included, `  -` dim for excluded), bold summary counts with dim excluded counts, red `Error:` and yellow `Warning:` prefixes.\n\n## Color Mode Abstraction\n\n`ColorFunctions` internal interface wraps `picocolors` methods (`green`, `dim`, `red`, `bold`, `yellow`). Conditional logic `options.colors ? pc : noColor` selects between `picocolors` and `noColor` identity wrapper. `noColor` constant implements all five color methods as `identity: (s: string): string => s` passthrough, enabling compile-time type safety for color-enabled/disabled code paths without runtime string inspection.\n\n## Output Format Specification\n\n- **File discovery**: `c.green('  +') + ' ' + path` (included), `c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`)` (excluded)\n- **Summary**: `c.bold(\\`\\nDiscovered ${included} files\\`) + c.dim(` (${excluded} excluded)`)`\n- **Warnings**: `c.yellow('Warning: ') + message` via `console.warn()`\n- **Errors**: `c.red('Error: ') + message` via `console.error()`\n\n## Integration Points\n\nConsumed by CLI commands (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) for terminal progress reporting. Used by `src/discovery/run.ts` for file enumeration logging (`file()`, `excluded()`, `summary()` calls) and `src/orchestration/progress.ts` for phase execution updates (`info()`, `warn()` calls during pool orchestration).\n### src/quality/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation suite detecting three classes of documentation drift: code-vs-doc (exported symbols missing from `.sum` files), code-vs-code (duplicate exports within directory scope), and phantom-paths (unresolvable file references in `AGENTS.md`). Aggregates findings into `InconsistencyReport` objects with severity stratification and plain-text CLI formatting.\n\n## Contents\n\n### Barrel Exports\n\n**[index.ts](./index.ts)** — Re-exports `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` from types.ts; `extractExports()`, `checkCodeVsDoc()` from inconsistency/code-vs-doc.js; `checkCodeVsCode()` from inconsistency/code-vs-code.js; `buildInconsistencyReport()`, `formatReportForCli()` from inconsistency/reporter.js; `checkPhantomPaths()` from phantom-paths/index.js; `validateFindability()`, `FindabilityResult` from density/validator.js.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency` (code-vs-doc | code-vs-code | phantom-path) with shared `severity: 'info' | 'warning' | 'error'` field. `CodeDocInconsistency` contains `filePath`, `sumPath`, `details.missingFromDoc[]`. `CodeCodeInconsistency` contains `files[]`, `pattern: 'duplicate-export'`. `PhantomPathInconsistency` contains `agentsMdPath`, `details.referencedPath`, `details.resolvedTo`, `details.context`. `InconsistencyReport` aggregates `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, `summary` counts by type/severity.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — Implements code-vs-doc detection via regex export extraction and substring matching, code-vs-code duplicate symbol detection across per-directory file groups, and report aggregation with plain-text formatting. Exports `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`.\n\n**[phantom-paths/](./phantom-paths/)** — Scans `AGENTS.md` via three regex patterns (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location filesystem resolution with `.ts`/`.js` fallback, reports unresolved references as `PhantomPathInconsistency` with 120-char contextLine. Exports `checkPhantomPaths()`.\n\n**[density/](./density/)** — Stub module returning empty `FindabilityResult[]` array, disabled after removal of `SumFileContent.metadata.publicInterface` schema field. Exports `validateFindability()` signature for future symbol extraction support.\n\n## Validation Pipeline\n\n**Code-vs-Doc:** `extractExports()` parses source via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` verifies extracted symbols appear in `SumFileContent.summary` via substring search, returns `CodeDocInconsistency` with `missingFromDoc[]` on detection gaps.\n\n**Code-vs-Code:** `checkCodeVsCode()` aggregates exports into `Map<string, string[]>` across scoped file group, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Requires caller-enforced per-directory scoping to prevent cross-module false positives.\n\n**Phantom Paths:** `checkPhantomPaths()` extracts paths from `AGENTS.md` via three regex patterns, attempts `existsSync()` resolution at agentsMdDir-relative, projectRoot-relative, and `.ts` fallback locations, filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), reports unresolved references with severity='warning'.\n\n**Report Synthesis:** `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with metadata (timestamp, projectRoot, filesChecked, durationMs) and summary counts (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info). `formatReportForCli()` renders plain-text output with `[ERROR]`/`[WARN]`/`[INFO]` severity tags.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` Phase 1 post-processing: constructs `InconsistencyReport` via `buildInconsistencyReport()` after `.sum` generation completes, logs summary counts to `.agents-reverse-engineer/progress.log` via `src/orchestration/progress.ts` streaming reporter, includes report metadata in telemetry run logs via `src/ai/telemetry/run-log.ts`.\n\n**Upstream Inputs:**\n- Source files for `extractExports()` regex parsing\n- `.sum` files via `SumFileContent` schema from `src/generation/writers/sum.ts`\n- `AGENTS.md` files for `checkPhantomPaths()` scanning\n- Export maps from `src/imports/extractor.ts` for `checkCodeVsCode()` aggregation\n\n**Downstream Consumers:**\n- `src/orchestration/progress.ts` logs summary counts via `formatReportForCli()`\n- `src/ai/telemetry/run-log.ts` persists report metadata to NDJSON run logs\n- CLI output displays validation results after Phase 1 completion\n\n## Behavioral Contracts\n\n**Export extraction regex (inconsistency/code-vs-doc.ts):**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path extraction patterns (phantom-paths/validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (phantom-paths/validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n\n**Severity levels:** `'info' | 'warning' | 'error'` mapped to CLI tags `[INFO]`, `[WARN]`, `[ERROR]`.\n\n**Inconsistency type discriminants:** `code-vs-doc`, `code-vs-code`, `phantom-path` enable exhaustive pattern matching via `type` field.\n\n## Detection Limitations\n\n**Regex-based export extraction:** Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n**Substring matching:** `sumText.includes(e)` yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**Code-vs-code name-only comparison:** Cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions without AST context.\n\n**Phantom path resolution:** Four-location strategy (`agentsMdDir`, `projectRoot`, .ts fallback) does not resolve symlinks, aliased imports, or paths requiring module resolution algorithms.\n### src/quality/density/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub findability validation module, currently disabled after removal of structured `publicInterface` metadata from `.sum` file schema. Preserved for future re-implementation via post-processing symbol extraction.\n\n## Contents\n\n### [validator.ts](./validator.ts)\nExports `validateFindability()` stub returning empty `FindabilityResult[]` array and `FindabilityResult` interface defining validation outcome structure with `filePath`, `symbolsTested`, `symbolsFound`, `symbolsMissing`, `score` fields.\n\n## Architecture\n\n**Validation Approach (Disabled):** Originally performed string-based substring matching to verify exported symbols from `.sum` files appeared in parent `AGENTS.md` content without LLM calls. Logic removed when `SumFileContent.metadata.publicInterface` field was deleted from schema.\n\n**Current State:** `validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>)` unconditionally returns `[]` with parameters prefixed by underscores indicating unused status. Function signature retained for future structured extraction support via post-processing passes.\n\n## Integration Points\n\nConsumed by `src/quality/index.ts` which aggregates quality validators (`validateCodeDocConsistency`, `validateCodeCodeConsistency`, `validatePhantomPaths`, `validateFindability`). Produces no findings in current implementation but return type `FindabilityResult[]` preserved in reporting pipeline.\n\n**Type Dependencies:**\n- `SumFileContent` imported from `../../generation/writers/sum.js` (verified path)\n- `FindabilityResult` consumed by quality reporting aggregator\n\n## Behavioral Contracts\n\n**Return Value:** Always `[]` (empty array) indicating zero validation findings.\n\n**Score Calculation (Historical):** Ratio of `symbolsFound.length / symbolsTested.length` yielding `0.0` (no symbols) to `1.0` (all symbols present) when validation was active.\n### src/quality/inconsistency/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\n**Detects three classes of code-documentation drift: code-vs-doc (exports missing from .sum files), code-vs-code (duplicate symbol exports within directory scope), and phantom-paths (broken file references in AGENTS.md), aggregating findings into structured InconsistencyReport objects with plain-text CLI formatting.**\n\n## Contents\n\n### Detection Modules\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts identifiers from TypeScript/JavaScript source via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, `checkCodeVsDoc()` compares extracted symbols against SumFileContent.summary substring presence, returns CodeDocInconsistency with `missingFromDoc[]` when exports absent from documentation.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` aggregates exports via `Map<string, string[]>` across scoped file group, returns CodeCodeInconsistency array flagging symbols exported from multiple files with `pattern: 'duplicate-export'` sentinel, relies on caller to enforce per-directory scoping.\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates Inconsistency discriminated union into InconsistencyReport with summary counts across type/severity dimensions, `formatReportForCli()` renders plain-text output without picocolors dependency for testability.\n\n## File Relationships\n\ncode-vs-code.ts imports `extractExports()` from code-vs-doc.ts for shared regex-based export extraction. Both detection modules return type-specific inconsistency objects (CodeDocInconsistency, CodeCodeInconsistency) consumed by reporter.ts via Inconsistency discriminated union. Reporter aggregates findings from both modules into unified InconsistencyReport structure with metadata (timestamp, projectRoot, filesChecked, durationMs) and typed summary counts.\n\n## Behavioral Contracts\n\n### Export Extraction Regex\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nCaptures identifiers from `export function`, `export const`, `export default class`, `export type`, `export interface`, `export enum`. Misses destructured exports (`export const { foo, bar } = obj`), namespace exports (`export * as ns`), re-exports (`export { foo } from './other'`), dynamic exports (`module.exports`), multi-line declarations.\n\n### Inconsistency Type Discriminants\n- `code-vs-doc`: requires `filePath`, `sumPath`, `details.missingFromDoc[]`\n- `code-vs-code`: requires `files[]`, `pattern: 'duplicate-export'`\n- `phantom-path`: requires `agentsMdPath`, `details.referencedPath`\n\n### Severity Levels\nMapped to CLI tags: `'error'` → `'[ERROR]'`, `'warning'` → `'[WARN]'`, `'info'` → `'[INFO]'`\n\n## Detection Limitations\n\n**code-vs-doc**: Substring matching (`sumText.includes(e)`) yields false negatives when identifier appears in prose unrelated to API surface. No AST analysis or semantic validation.\n\n**code-vs-code**: Name-only comparison without AST analysis cannot distinguish intentional duplication (facade pattern, barrel exports, interface/implementation pairs) from unintended collisions. Requires caller-enforced per-directory scoping to prevent false positives across module boundaries.\n\n**Shared**: Regex-based extraction misses complex export patterns (destructured, namespace, dynamic). Both modules operate as pure heuristics without AI service calls.\n### src/quality/phantom-paths/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable file path references in generated `AGENTS.md` documentation through regex extraction, multi-strategy filesystem resolution with TypeScript/JavaScript extension fallback, and `PhantomPathInconsistency` issue reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Re-exports `checkPhantomPaths` from `validator.js` as barrel export for phantom path detection subsystem within parent `src/quality/` pipeline.\n\n**[validator.ts](./validator.ts)** — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` scanning AGENTS.md text via `PATH_PATTERNS` regex array (markdown links, backtick-quoted paths, prose-embedded src/ references), attempts four-location resolution (agentsMdDir-relative, projectRoot-relative, .ts fallback for both), filters via `SKIP_PATTERNS` (node_modules, .git, URLs, template syntax, globs), extracts 120-char contextLine from content on resolution failure, deduplicates via Set, returns `PhantomPathInconsistency[]` with severity='warning', type='phantom-path', referencedPath, resolvedTo, contextLine details.\n\n## Path Extraction Strategy\n\n`PATH_PATTERNS` contains three RegExp patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link syntax\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures code-formatted paths starting with src/, ./, ../\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures src/ paths following trigger keywords\n\n`SKIP_PATTERNS` excludes seven non-file reference types: node_modules, .git, HTTP(S) URLs, template placeholders (`{{`, `${`), glob wildcards, brace expansion syntax.\n\n## Resolution Protocol\n\nFor each extracted path, `checkPhantomPaths` attempts `existsSync()` validation at four filesystem locations in sequence:\n1. `path.resolve(agentsMdDir, rawPath)` — relative to AGENTS.md parent directory\n2. `path.resolve(projectRoot, rawPath)` — relative to project root (handles absolute-style src/ paths)\n3. agentsMdDir-relative path with .js → .ts extension substitution\n4. projectRoot-relative path with .js → .ts extension substitution\n\nStops at first successful resolution; reports unresolved path as `PhantomPathInconsistency` with contextLine extracted via `lines.find((l) => l.includes(rawPath))`, trimmed and sliced to 120-char maximum.\n\n## Behavioral Contracts\n\n**Path extraction patterns (validator.ts `PATH_PATTERNS`):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip filters (validator.ts `SKIP_PATTERNS`):**\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n### src/rebuild/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# rebuild\n\nImplements AI-driven project reconstruction from specification documents via ordered phase execution with checkpoint-based resumability, concurrent file generation within dependency groups, and progressive context accumulation from previously built modules.\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — Executes three-stage rebuild workflow: spec partitioning into `RebuildUnit[]` via `partitionSpec()`, sequential order group processing with concurrent AI calls per group via `runPool()`, and progressive context accumulation from generated file exports with LRU truncation at 100k chars.\n\n**[checkpoint.ts](./checkpoint.ts)** — Provides session continuity via `CheckpointManager` class with SHA-256 spec drift detection, per-module status tracking (`pending`/`done`/`failed`), and promise-chain write serialization to `.rebuild-checkpoint` JSON file.\n\n### Specification Processing\n\n**[spec-reader.ts](./spec-reader.ts)** — Reads markdown files from `specs/` directory via `readSpecFiles()`, partitions content into `RebuildUnit[]` via Build Plan phase extraction (`### Phase N:`) or top-level heading fallback, injects targeted context (Architecture, filtered API subsections, Data Structures, Behavioral Contracts) based on Defines/Consumes keyword matching.\n\n**[output-parser.ts](./output-parser.ts)** — Extracts file paths and contents from AI responses using delimiter-based parsing (`===FILE:===` / `===END_FILE===`) with markdown fenced block fallback (```` ```language:path ````), returning `Map<string, string>` for filesystem writes.\n\n### Prompt Engineering\n\n**[prompts.ts](./prompts.ts)** — Defines `REBUILD_SYSTEM_PROMPT` enforcing delimiter format, exact spec compliance (no synonym substitution), and production code constraints (no tests/stubs/placeholders), plus `buildRebuildPrompt()` constructing per-unit prompts with full spec, phase-specific content, and accumulated built context.\n\n### Type System\n\n**[types.ts](./types.ts)** — Exports `RebuildCheckpointSchema` Zod validator with `specHashes`/`modules` fields, `RebuildUnit` interface with `name`/`specContent`/`order`, `RebuildPlan` with `units[]`/`outputDir`, and `RebuildResult` with token counts and `filesWritten[]`.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting all public types, schemas (`RebuildCheckpointSchema`), functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`), classes (`CheckpointManager`), and constants (`REBUILD_SYSTEM_PROMPT`).\n\n## Architecture\n\n### Sequential Order Groups with Concurrent Execution\n\n`executeRebuild()` groups `RebuildUnit[]` by `order` field into `Map<number, RebuildUnit[]>`, processes order values sequentially (ascending), executes units within each group concurrently via `runPool()` with configurable concurrency. Accumulates exported symbols from completed groups into `builtContext` string for injection into subsequent groups as \"Already Built\" context, enabling correct import resolution across phases.\n\n### Checkpoint-Based Resume Workflow\n\n`CheckpointManager.load()` compares SHA-256 hashes of current spec files against `checkpoint.specHashes`, returns `isResume: false` if hash count differs or any individual hash mismatches (drift detected). Filters `pendingUnits[]` via `checkpoint.isDone(unitName)` predicate, increments `modulesSkipped` counter for already-complete work. Workers call `checkpoint.markDone(unitName, filesWritten)` or `checkpoint.markFailed(unitName, errorMsg)` on completion, serializing writes via promise chain to prevent corruption.\n\n### Context Accumulation with LRU Truncation\n\nAfter each order group completes, `orchestrator.ts` reads all `filesWrittenInGroup` via `readFile()`, filters out non-source files (`.md`/`.json`/`.yml`), appends to `builtContext` with `// === ${filePath} ===` delimiters. When `builtContext.length > BUILT_CONTEXT_LIMIT` (100,000 chars), splits on delimiter pattern, preserves recent files in full, truncates older files to first `TRUNCATED_HEAD_LINES` (20 lines) with `// ... (truncated)` marker.\n\n### Targeted API Injection (Change 2 Format)\n\n`extractFromBuildPlan()` detects `/^\\*\\*Defines:\\*\\*|^Defines:/m` pattern in phase content, extracts keywords from Defines/Consumes lists and file path references, calls `findRelevantSubsections()` with fuzzy matching (substring, word overlap) to filter API/Data/Behavioral subsections. Injects matched subsections under `## Interfaces for This Phase`, `## Data Structures for This Phase`, `## Behavioral Contracts for This Phase` headings. Falls back to full `## Public API Surface` inclusion for legacy specs without Defines/Consumes (graceful degradation).\n\n## Behavioral Contracts\n\n**Delimiter format (primary output parsing):** `/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g`\n\n**Fenced block format (fallback):** `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g`\n\n**Build Plan phase extraction:** `/^### Phase (\\d+):\\s*(.+)$/gm`\n\n**Change 2 format detection:** `/^\\*\\*Defines:\\*\\*|^Defines:/m`\n\n**Section heading pattern:** `/^## (?:\\d+\\.\\s*)?${sectionName}\\s*$/m`\n\n**Subsection heading pattern:** `/^### (.+)$/gm`\n\n**File path extraction:** `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`\n\n**Context delimiter format:** `// === ${filePath} ===` (used for splitting truncated context)\n\n## Integration Points\n\nInvoked by `src/cli/rebuild.ts` command handler. Consumes `../ai/index.js` `AIService` for subprocess spawning, `../orchestration/index.js` `runPool`/`ProgressReporter`/`ITraceWriter` for concurrency control, `../change-detection/index.js` `computeContentHashFromString` for drift detection. Writes checkpoint to `<outputDir>/.rebuild-checkpoint` JSON, generated source files to `<outputDir>/<relativePath>` as specified in parsed AI responses.\n### src/specify/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify\n\n**Project specification synthesis from AGENTS.md documentation corpus: prompt engineering, heading-based file splitting, and overwrite-protected filesystem operations for single-file or multi-file spec generation.**\n\n## Contents\n\n### Core Modules\n\n**[index.ts](./index.ts)** — Barrel export for specification synthesis API: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions`\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs, annexFiles?)` constructs `SpecPrompt` pair with `SPEC_SYSTEM_PROMPT` enforcing 11-section structure (Project Overview, Architecture, Public API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts [Runtime Behavior + Implementation Contracts], Test Contracts, Build Plan, Prompt Templates, IDE Integration) and verbatim annex reproduction mandates\n\n**[writer.ts](./writer.ts)** — `writeSpec(content, options)` writes markdown to `specs/SPEC.md` (single-file) or `specs/<slug>.md` (multi-file via `splitByHeadings`), throws `SpecExistsError` on conflicts unless `force=true`, returns written paths array\n\n## Architecture\n\n### Prompt Construction Strategy\n\n`buildSpecPrompt` assembles user prompts by concatenating:\n1. Header directive: `\"Generate a comprehensive project specification...\"`\n2. Documentation sections: `### ${doc.relativePath}` with embedded content from `AgentsDocs` array\n3. Optional annex block: `## Annex Files` with verbatim source code for reproduction-critical modules (prompt templates, IDE configs)\n4. Requirement checklist: 11-item numbered list matching `SPEC_SYSTEM_PROMPT` section order\n5. Verbatim mandate: `\"Sections 10 and 11 MUST reproduce annex content verbatim\"`\n6. Output constraint: `\"Output ONLY the markdown content. No preamble.\"`\n\nSystem prompt enforces:\n- **Audience**: AI agents requiring instruction-oriented language (not human documentation)\n- **Anti-patterns**: Prohibits folder-mirroring, file path prescription, directory-derived headings\n- **Module boundary focus**: Describe interfaces/exports rather than filesystem layout\n- **Type precision**: Full signatures with parameters, return types, generics for all public APIs\n- **Dependency versioning**: Exact version numbers for external dependencies\n- **Build Plan**: Phased implementation with explicit \"Defines:\"/\"Consumes:\" interface contract cross-references\n- **Behavioral contracts**: Exact error types/codes, verbatim regex patterns in backticks, format strings with structure, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures)\n- **Reproduction-critical sections**: Sections 10-11 reproduce annex content without summarization\n\n### Multi-File Splitting\n\n`splitByHeadings` partitions markdown on `/^(?=# )/m` regex (positive lookahead for top-level headings):\n- Extracts heading text via `/^# (.+)/` match, transforms via `slugify` (lowercase + whitespace→hyphens + strip non-alphanumeric + collapse consecutive hyphens + trim)\n- Pre-heading content assigned to `00-preamble.md`\n- Each section object contains `{ filename, content }` with trailing newline\n\nConflict detection: Before writing files, iterates sections to check existence via `fileExists` (wraps `fs.access(..., F_OK)`). Aggregates conflicts and throws `SpecExistsError` if non-empty and `force=false`. Ensures atomic write (all-or-nothing).\n\n## Integration Points\n\n### Consumed By\n\n`src/cli/specify.ts` command invokes `writeSpec(aiOutput, { outputPath: config.output.specPath, force: cliFlags.force, multiFile: cliFlags.multiFile })` after `AIService.call(buildSpecPrompt(docs, annexFiles))` completes.\n\n### Dependencies\n\n**[../generation/collector.js](../generation/collector.ts)** — `collectAgentsDocs()` provides recursive `AGENTS.md` traversal results via `AgentsDocs` type\n\n**node:fs/promises** — `mkdir`, `writeFile`, `access` for filesystem operations\n\n**node:path** — `dirname`, `join` for path resolution\n\n## Behavioral Contracts\n\n### Runtime Behavior\n\n- `writeSpec` throws `SpecExistsError` when target paths exist and `force=false`\n- Single-file mode creates parent directory recursively via `mkdir(..., { recursive: true })`\n- Multi-file mode writes all sections or none (atomic operation via upfront conflict check)\n\n### Implementation Contracts\n\n**Heading regex**: `/^(?=# )/m` (matches lines starting with `# ` via positive lookahead)\n\n**Heading extraction**: `/^# (.+)/` (captures heading text after `# ` prefix)\n\n**Slugification transforms**:\n- Whitespace→hyphens: `/\\s+/g` → `'-'`\n- Strip non-alphanumeric: `/[^a-z0-9-]/g` → `''`\n- Collapse hyphens: `/-+/g` → `'-'`\n- Trim hyphens: `/^-|-$/g` → `''`\n\n**SpecExistsError message format**: `\"Specification file(s) already exist:\\n\" + paths.map(p => \"  - \" + p).join(\"\\n\") + \"\\n\\nUse --force to overwrite.\"`\n\n**Preamble filename**: `00-preamble.md` (for content before first `# ` heading)\n### src/types/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types/\n\n**Core TypeScript interface definitions for file discovery results, exclusion metadata, and discovery statistics shared across discovery, change-detection, generation, and quality validation modules.**\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `ExcludedFile`, `DiscoveryStats` interfaces consumed by orchestrators and reporters.\n\n## Exported Interfaces\n\n**ExcludedFile** — Represents a filtered-out file with exclusion metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n**DiscoveryResult** — Encapsulates file discovery output:\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with reasons (used for statistics/reporting)\n\n**DiscoveryStats** — Aggregates discovery metrics:\n- `totalFiles: number` — Sum of included and excluded files\n- `includedFiles: number` — Count of files in `DiscoveryResult.files`\n- `excludedFiles: number` — Count of files in `DiscoveryResult.excluded`\n- `exclusionReasons: Record<string, number>` — Histogram of `ExcludedFile.reason` strings to occurrence counts\n\n## Usage Patterns\n\n**Producers:**\n- `discoverFiles()` in `src/discovery/run.ts` returns `DiscoveryResult`\n- Filter chain modules in `src/discovery/filters/` (gitignore.ts, binary.ts, vendor.ts, custom.ts) produce `ExcludedFile` instances aggregated into `DiscoveryResult.excluded`\n\n**Consumers:**\n- `generateDocumentation()` in `src/generation/orchestrator.ts` consumes `DiscoveryResult.files` for Phase 1 pool execution\n- `updateDocumentation()` in `src/update/orchestrator.ts` merges `DiscoveryResult` with `FileChange[]` from change detector\n- Progress reporters and telemetry loggers in `src/output/logger.ts` compute `DiscoveryStats` via reduction over `excluded[]` array to populate `exclusionReasons` histogram, written to `GENERATION-PLAN.md` and `.agents-reverse-engineer/progress.log`\n\n## File Relationships\n\n`DiscoveryResult` flows from discovery filters → orchestrator → worker pool → generation writers. `DiscoveryStats` computed post-discovery via reduction over `excluded[]` array for reporting and plan documentation.\n### src/update/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nIncremental documentation update workflow coordinating SHA-256 hash-based change detection, orphaned artifact cleanup, and selective `.sum` + `AGENTS.md` regeneration for modified files and affected directories.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export exposing UpdateOrchestrator, createUpdateOrchestrator, cleanupOrphans, cleanupEmptyDirectoryDocs, getAffectedDirectories, UpdatePlan, UpdateOptions, UpdateResult, UpdateProgress, CleanupResult types.\n\n**[orchestrator.ts](./orchestrator.ts)** — UpdateOrchestrator compares YAML frontmatter `content_hash` fields from existing `.sum` files against SHA-256 hashes of current file content via computeContentHash(), generates UpdatePlan segregating files into `filesToAnalyze` (added/modified), `filesToSkip` (unchanged), and `cleanup` (orphaned), computes `affectedDirs` requiring `AGENTS.md` regeneration sorted deepest-first. Factory createUpdateOrchestrator() constructs instances with optional ITraceWriter injection.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — cleanupOrphans() deletes `.sum` and `.annex.md` files for deleted/renamed source files (targeting `oldPath` for renames), invokes cleanupEmptyDirectoryDocs() removing `AGENTS.md` from directories with no remaining source files (excludes hidden files, `GENERATED_FILES` set, `.sum`/`.annex.md` from emptiness check). getAffectedDirectories() walks parent directories of non-deleted FileChange entries via path.dirname() to project root.\n\n**[types.ts](./types.ts)** — Defines CleanupResult (`deletedSumFiles[]`, `deletedAgentsMd[]`), UpdateOptions (`includeUncommitted`, `dryRun` flags), UpdateResult (execution summary with `analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`), UpdateProgress (event callbacks: `onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Workflow\n\n1. **Prerequisites** — UpdateOrchestrator.checkPrerequisites() validates git repository via isGitRepo(), throws Error if not a git repo\n2. **Plan Creation** — preparePlan() calls discoverFiles(), iterates results:\n   - Constructs `.sum` path via getSumPath()\n   - Calls readSumFile() extracting `contentHash` from frontmatter\n   - Missing `.sum`: adds `{ path, status: 'added' }` to `filesToAnalyze`\n   - Existing `.sum`: calls computeContentHash(), compares hashes\n   - Hash mismatch: adds `{ path, status: 'modified' }` to `filesToAnalyze`\n   - Hash match: adds path to `filesToSkip`\n3. **Orphan Cleanup** — cleanupOrphans() deletes `.sum`/`.annex.md` for deleted/renamed files, cleanupEmptyDirectoryDocs() removes `AGENTS.md` from directories with no source files\n4. **Affected Directories** — getAffectedDirectories() computes parent paths of changed files, sorts by depth descending (deepest-first processing)\n5. **Regeneration** — External caller (src/cli/update.ts) invokes Phase 1 pool execution for `filesToAnalyze`, Phase 2 sequential directory aggregation for `affectedDirs` (skips Phase 3 root synthesis)\n\n## Integration\n\n**Upstream callers**: src/cli/update.ts constructs UpdateOrchestrator via createUpdateOrchestrator(), calls preparePlan() returning UpdatePlan, passes `filesToAnalyze` to src/generation/executor.ts Phase 1 file analysis pool, passes `affectedDirs` to Phase 2 directory aggregation.\n\n**Dependencies**: src/change-detection/detector.ts (computeContentHash, isGitRepo, getCurrentCommit), src/generation/writers/sum.ts (readSumFile, getSumPath), src/discovery/run.ts (discoverFiles), src/orchestration/trace.ts (ITraceWriter).\n\n## Trace Events\n\nUpdateOrchestrator emits via optional ITraceWriter:\n\n- `phase:start` at plan creation with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` after plan construction with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` after completion with `durationMs` computed via process.hrtime.bigint() delta\n\n## Frontmatter Mode\n\nNo-op methods (recordFileAnalyzed, removeFileState, recordRun, getLastRun) retain signatures for API compatibility with hypothetical database-backed change detection but perform no operations. Content hash storage delegated to `.sum` file YAML frontmatter written by writeSumFile(). Run history tracking unavailable (getLastRun() returns undefined, recordRun() returns 0).\n\n## Dry-Run Support\n\nWhen `UpdateOptions.dryRun === true`, orphan-cleaner skips unlink() calls but executes stat() checks, returning would-be deletions in CleanupResult. Used for preview mode in src/cli/update.ts.\n\n## Annex Files (3 reproduction-critical source files)\n\n### src/generation/prompts/templates.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: templates.ts\n\nReproduction-critical source content from `templates.ts`.\nReferenced by `templates.ts.sum`.\n\n```\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`{{LANG}}\n{{CONTENT}}\n\\`\\`\\`\n{{PROJECT_PLAN_SECTION}}\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n### src/generation/prompts/types.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: types.ts\n\nReproduction-critical source content from `types.ts`.\nReferenced by `types.ts.sum`.\n\n```\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n### src/orchestration/runner.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: runner.ts\n\nReproduction-critical source content from `runner.ts`.\nReferenced by `runner.ts.sum`.\n\n```\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n\n## Output Requirements\n\nThe specification MUST include these sections in order:\n1. Project Overview (purpose, value, tech stack)\n2. Architecture (module boundaries, data flow, design decisions)\n3. Public API Surface (all exported interfaces, full type signatures)\n4. Data Structures & State (types, schemas, config objects)\n5. Configuration (options, types, defaults, validation)\n6. Dependencies (each with version and rationale)\n7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)\n8. Test Contracts (per-module test scenarios and edge cases)\n9. Build Plan (phased implementation order with dependencies, each phase listing \"Defines:\" and \"Consumes:\" with exact names from API Surface)\n10. Prompt Templates & System Instructions (FULL verbatim text from annex content)\n11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim from annex content)\n\nSections 10 and 11 MUST reproduce annex content verbatim.\nDo NOT summarize prompt templates or IDE templates into prose descriptions.\n\nOutput ONLY the markdown content. No preamble.",
      "systemPrompt": "You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence with explicit interface contracts per phase:\n   - Each phase MUST include a \"Defines:\" list naming the exact types, interfaces, classes, and functions this phase must export (use the exact names from section 3 Public API Surface)\n   - Each phase MUST include a \"Consumes:\" list naming the exact types and functions from earlier phases that this phase imports\n   - Include dependency ordering and implementation tasks as before\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Build Plan phases MUST cross-reference the Public API Surface: every type/function in the API Surface section must appear in exactly one phase's \"Defines:\" list\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.",
      "response": "Now I have all the necessary information to generate a comprehensive specification. Let me compile it:\n\n# agents-reverse-engineer Specification\n\n## 1. Project Overview\n\n### Purpose and Value Proposition\n\nagents-reverse-engineer (ARE) is a CLI tool that automates brownfield documentation generation for AI coding assistants. It implements the Recursive Language Model (RLM) algorithm to produce structured documentation that enables AI agents to understand and navigate codebases effectively.\n\n**Problem Solved**: AI coding assistants need comprehensive codebase context to provide accurate assistance. Manual documentation is time-consuming and quickly becomes stale. ARE automates this by analyzing source files via AI CLI subprocesses, generating file summaries (`.sum` files), synthesizing directory overviews (`AGENTS.md`), and producing platform-specific root documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`).\n\n**Core Capabilities**:\n- Parallel file analysis with configurable concurrency pools\n- Incremental updates via SHA-256 content hash comparison\n- Multi-platform AI backend support (Claude Code, Gemini CLI, OpenCode)\n- Gitignore-aware file discovery with binary detection and vendor directory exclusion\n- Quality validation detecting code-documentation inconsistencies and phantom path references\n- Session lifecycle hooks for automatic documentation refresh\n- NDJSON telemetry logging with token cost tracking\n\n### Technology Stack\n\n| Component | Version | Purpose |\n|-----------|---------|---------|\n| Node.js | ≥18.0.0 | Runtime (ES modules) |\n| TypeScript | 5.7.3 | Language (ES2022 target, NodeNext resolution, strict mode) |\n| fast-glob | ^3.3.3 | File discovery with glob patterns |\n| ignore | ^7.0.3 | Gitignore parsing |\n| isbinaryfile | ^5.0.4 | Binary file detection |\n| simple-git | ^3.27.0 | Change detection via git diff |\n| yaml | ^2.7.0 | Config parsing |\n| zod | ^3.24.1 | Schema validation |\n| ora | ^8.1.1 | Spinner UI |\n| picocolors | ^1.1.1 | Terminal colors |\n\n**Version**: 0.7.1  \n**License**: MIT (GeoloeG-IsT, 2026)\n\n---\n\n## 2. Architecture\n\n### Three-Phase Generation Pipeline\n\nThe pipeline executes in strict dependency order, with each phase completing before the next begins.\n\n**Phase 1: Concurrent File Analysis**\n\nAn iterator-based worker pool shares a single task iterator across N workers. Each worker:\n1. Pulls tasks atomically via iterator `.next()` protocol\n2. Invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses\n3. Writes `.sum` files with YAML frontmatter containing SHA-256 content hashes\n\nResource limits applied to each subprocess:\n- `NODE_OPTIONS='--max-old-space-size=512'` (512MB heap limit)\n- `UV_THREADPOOL_SIZE='4'` (4 libuv threads)\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` (no background tasks)\n- `--disallowedTools Task` (no subagent spawning)\n\n**Phase 2: Post-Order Directory Aggregation**\n\nDirectories are sorted by depth descending (deepest first) via `path.relative().split(path.sep).length`. The `isDirectoryComplete()` predicate polls for child `.sum` file existence before processing a directory. Each directory prompt aggregates:\n- Child `.sum` content via `readSumFile()`\n- Subdirectory `AGENTS.md` files\n- Import maps via `extractDirectoryImports()`\n- Manifest detection (9 types)\n\nUser-authored `AGENTS.md` files are renamed to `AGENTS.local.md` and prepended above generated content.\n\n**Phase 3: Root Document Synthesis**\n\nSequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal and enforce synthesis-only constraints (no invention of features not in source documents).\n\n### Module Boundaries\n\n| Module | Responsibility | Key Exports |\n|--------|----------------|-------------|\n| AI Service | Backend-agnostic AI CLI orchestration with subprocess pooling, retry, telemetry | `AIService`, `AIBackend`, `AIResponse`, `withRetry`, `runSubprocess` |\n| Change Detection | Git diff parsing and SHA-256 content hashing | `getChangedFiles`, `computeContentHash`, `isGitRepo` |\n| Config | YAML loading with Zod validation | `loadConfig`, `ConfigSchema`, `Config` |\n| Discovery | Gitignore-aware file walking with filter chain | `discoverFiles`, `FilterResult`, `FileFilter` |\n| Generation | Three-phase orchestration and prompt construction | `GenerationOrchestrator`, `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt` |\n| Orchestration | Worker pool, progress reporting, trace emission | `runPool`, `CommandRunner`, `ProgressReporter`, `TraceWriter` |\n| Quality | Code-vs-doc, code-vs-code, phantom-path validation | `checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`, `InconsistencyReport` |\n| Update | Incremental update workflow | `UpdateOrchestrator`, `cleanupOrphans` |\n| Specify | Project specification synthesis | `buildSpecPrompt`, `writeSpec` |\n| Rebuild | AI-driven project reconstruction | `executeRebuild`, `CheckpointManager`, `partitionSpec` |\n| Installer | IDE command/hook deployment | `runInstaller`, `installFiles` |\n\n### Data Flow Pattern\n\n```\ndiscoverFiles() → GenerationOrchestrator.createPlan() → buildExecutionPlan()\n    ↓\nrunPool(fileTasks) → AIService.call() → writeSumFile()\n    ↓\nrunPool(directoryTasks) → buildDirectoryPrompt() → writeAgentsMd()\n    ↓\nrunPool(rootTasks) → buildRootPrompt() → writeFile()\n```\n\n### Key Design Decisions\n\n1. **Iterator-based pool**: Shares single `tasks.entries()` iterator across workers via atomic `.next()` calls, preventing over-allocation and ensuring even work distribution.\n\n2. **Promise-chain serialization**: `PlanTracker`, `ProgressLog`, and `TraceWriter` use `writeQueue = writeQueue.then(...)` pattern to prevent file corruption from concurrent writes.\n\n3. **Memory management**: `PreparedFile.content` is cleared after prompt construction to free heap on large codebases.\n\n4. **Resource-adaptive concurrency**: Formula `clamp(cores * 5, 2, min(20, memCap))` where `memCap = floor(totalMemGB * 0.5 / 0.512)` prevents RAM exhaustion.\n\n---\n\n## 3. Public API Surface\n\n### AI Service Module\n\n```typescript\n// AIService class\nclass AIService {\n  constructor(backend: AIBackend, options: { timeoutMs: number; maxRetries: number; model: string; telemetry: { keepRuns: number } })\n  call(options: AICallOptions): Promise<AIResponse>\n  setTracer(tracer: ITraceWriter): void\n  setDebug(enabled: boolean): void\n  setSubprocessLogDir(dir: string): void\n  addFilesReadToLastEntry(files: FileRead[]): void\n  getSummary(): { totalCalls: number; totalInputTokens: number; totalOutputTokens: number; totalCacheReadTokens: number; totalCacheCreationTokens: number; errorCount: number; totalFilesRead: number; uniqueFilesRead: number }\n  finalize(projectRoot: string): Promise<void>\n}\n\n// Backend interface\ninterface AIBackend {\n  readonly name: string\n  readonly cliCommand: string\n  isAvailable(): Promise<boolean>\n  buildArgs(options: AICallOptions): string[]\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse\n  getInstallInstructions(): string\n}\n\n// Backend registry\nfunction createBackendRegistry(): BackendRegistry\nfunction resolveBackend(registry: BackendRegistry, backendName: string): AIBackend\nfunction detectBackend(registry: BackendRegistry): AIBackend | undefined\nfunction getInstallInstructions(registry: BackendRegistry): string\n\n// Retry utility\nfunction withRetry<T>(operation: () => Promise<T>, options: RetryOptions): Promise<T>\nconst DEFAULT_RETRY_OPTIONS: RetryOptions\n\n// Subprocess utility\nfunction runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>\nfunction isCommandOnPath(command: string): Promise<boolean>\n\n// Error class\nclass AIServiceError extends Error {\n  readonly code: AIServiceErrorCode\n  constructor(code: AIServiceErrorCode, message: string)\n}\n```\n\n### Change Detection Module\n\n```typescript\nfunction isGitRepo(projectRoot: string): Promise<boolean>\nfunction getCurrentCommit(projectRoot: string): Promise<string>\nfunction getChangedFiles(projectRoot: string, baseCommit: string, options?: ChangeDetectionOptions): Promise<ChangeDetectionResult>\nfunction computeContentHash(filePath: string): Promise<string>\nfunction computeContentHashFromString(content: string): string\n```\n\n### Config Module\n\n```typescript\nfunction loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>\nfunction configExists(root: string): Promise<boolean>\nfunction writeDefaultConfig(root: string): Promise<void>\nfunction getDefaultConcurrency(): number\n\nconst ConfigSchema: ZodSchema<Config>\nconst CONFIG_DIR: string  // '.agents-reverse-engineer'\nconst CONFIG_FILE: string // 'config.yaml'\nconst DEFAULT_VENDOR_DIRS: readonly string[]\nconst DEFAULT_EXCLUDE_PATTERNS: readonly string[]\nconst DEFAULT_BINARY_EXTENSIONS: readonly string[]\nconst DEFAULT_MAX_FILE_SIZE: number\nconst DEFAULT_CONFIG: Config\n```\n\n### Discovery Module\n\n```typescript\nfunction discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>\nfunction walkDirectory(options: WalkerOptions): Promise<string[]>\nfunction createGitignoreFilter(root: string): Promise<FileFilter>\nfunction createVendorFilter(vendorDirs: string[]): FileFilter\nfunction createBinaryFilter(options?: BinaryFilterOptions): FileFilter\nfunction createCustomFilter(patterns: string[], root: string): FileFilter\nfunction applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter }): Promise<FilterResult>\n\ninterface FileFilter {\n  name: string\n  shouldExclude(absolutePath: string): boolean | Promise<boolean>\n}\n```\n\n### Generation Module\n\n```typescript\n// Orchestrator\nclass GenerationOrchestrator {\n  createPlan(files: string[], projectRoot: string): Promise<GenerationPlan>\n}\nfunction createOrchestrator(config: Config, options?: { tracer?: ITraceWriter; debug?: boolean }): GenerationOrchestrator\n\n// Executor\nfunction buildExecutionPlan(plan: GenerationPlan): ExecutionPlan\nfunction formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string\nfunction isDirectoryComplete(dirPath: string, expectedSums: string[]): Promise<boolean>\nfunction getReadyDirectories(plan: ExecutionPlan): Promise<string[]>\nfunction getDirectoryDepth(dirPath: string): number\n\n// Prompts\nfunction buildFilePrompt(context: PromptContext, debug?: boolean): { system: string; user: string }\nfunction buildDirectoryPrompt(dirPath: string, projectRoot: string, debug?: boolean, knownDirs?: Set<string>, projectStructure?: string): Promise<{ system: string; user: string }>\nfunction buildRootPrompt(projectRoot: string, debug?: boolean): Promise<{ system: string; user: string }>\nfunction detectLanguage(filePath: string): string\n\n// Writers\nfunction writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>\nfunction readSumFile(sumPath: string): Promise<SumFileContent | null>\nfunction getSumPath(sourcePath: string): string\nfunction sumFileExists(sourcePath: string): Promise<boolean>\nfunction writeAnnexFile(sourcePath: string, sourceContent: string): Promise<string>\nfunction getAnnexPath(sourcePath: string): string\nfunction writeAgentsMd(dirPath: string, projectRoot: string, content: string): Promise<void>\nfunction isGeneratedAgentsMd(filePath: string): Promise<boolean>\nconst GENERATED_MARKER: string\n\n// Collector\nfunction collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>\nfunction collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>\ntype AgentsDocs = Array<{ relativePath: string; content: string }>\n```\n\n### Orchestration Module\n\n```typescript\n// Pool\nfunction runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<Array<TaskResult<T>>>\n\n// Runner\nclass CommandRunner {\n  constructor(aiService: AIService, options: CommandRunOptions)\n  executeGenerate(plan: ExecutionPlan): Promise<RunSummary>\n  executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>\n}\n\n// Progress\nclass ProgressReporter {\n  constructor(totalFiles: number, totalDirs: number, progressLog?: ProgressLog)\n  onFileStart(path: string): void\n  onFileDone(path: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onFileError(path: string, error: string): void\n  onDirectoryStart(path: string): void\n  onDirectoryDone(path: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onRootDone(path: string): void\n  printSummary(summary: RunSummary): void\n}\n\nclass ProgressLog {\n  static create(projectRoot: string): Promise<ProgressLog>\n  write(message: string): void\n  finalize(): Promise<void>\n}\n\n// Trace\ninterface ITraceWriter {\n  emit(event: TraceEventPayload): void\n  finalize(): Promise<void>\n  readonly filePath: string\n}\nfunction createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter\nfunction cleanupOldTraces(projectRoot: string, keepCount?: number): Promise<number>\n\n// Plan Tracker\nclass PlanTracker {\n  constructor(projectRoot: string, initialContent: string)\n  initialize(): Promise<void>\n  markDone(itemPath: string): void\n  flush(): Promise<void>\n}\n```\n\n### Quality Module\n\n```typescript\nfunction extractExports(sourceContent: string): string[]\nfunction checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null\nfunction checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]\nfunction checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]\nfunction buildInconsistencyReport(issues: Inconsistency[], metadata: { projectRoot: string; filesChecked: number; durationMs: number }): InconsistencyReport\nfunction formatReportForCli(report: InconsistencyReport): string\nfunction validateFindability(agentsMdContent: string, sumFiles: Map<string, SumFileContent>): FindabilityResult[]\n```\n\n### Update Module\n\n```typescript\nclass UpdateOrchestrator {\n  checkPrerequisites(): Promise<void>\n  preparePlan(options: UpdateOptions): Promise<UpdatePlan>\n}\nfunction createUpdateOrchestrator(projectRoot: string, config: Config, options?: { tracer?: ITraceWriter }): UpdateOrchestrator\nfunction cleanupOrphans(orphanedFiles: FileChange[], projectRoot: string, dryRun: boolean): Promise<CleanupResult>\nfunction cleanupEmptyDirectoryDocs(projectRoot: string, dryRun: boolean): Promise<string[]>\nfunction getAffectedDirectories(changes: FileChange[], projectRoot: string): string[]\n```\n\n### Specify Module\n\n```typescript\nfunction buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt\nfunction writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>\nclass SpecExistsError extends Error {\n  readonly paths: string[]\n}\nconst SPEC_SYSTEM_PROMPT: string\n```\n\n### Rebuild Module\n\n```typescript\nfunction readSpecFiles(specsDir: string): Promise<Array<{ relativePath: string; content: string }>>\nfunction partitionSpec(specFiles: Array<{ relativePath: string; content: string }>): RebuildUnit[]\nfunction parseModuleOutput(aiOutput: string): Map<string, string>\nfunction buildRebuildPrompt(unit: RebuildUnit, fullSpec: string, builtContext?: string): { system: string; user: string }\nfunction executeRebuild(plan: RebuildPlan, aiService: AIService, options: RebuildExecuteOptions): Promise<RebuildResult[]>\n\nclass CheckpointManager {\n  static load(outputDir: string, specFiles: Array<{ relativePath: string; content: string }>): Promise<{ checkpoint: CheckpointManager; isResume: boolean }>\n  isDone(unitName: string): boolean\n  markDone(unitName: string, filesWritten: string[]): Promise<void>\n  markFailed(unitName: string, error: string): Promise<void>\n}\n\nconst REBUILD_SYSTEM_PROMPT: string\nconst RebuildCheckpointSchema: ZodSchema<RebuildCheckpoint>\n```\n\n### Installer Module\n\n```typescript\nfunction runInstaller(args: string[]): Promise<void>\nfunction parseInstallerArgs(args: string[]): InstallerArgs\nfunction installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]\nfunction uninstallFiles(runtime: Runtime, location: Location, options: UninstallOptions): InstallerResult[]\nfunction registerHooks(basePath: string, runtime: Runtime, dryRun: boolean): boolean\nfunction registerPermissions(settingsPath: string, dryRun: boolean): boolean\nfunction getPackageVersion(): string\nfunction writeVersionFile(basePath: string, dryRun: boolean): void\n```\n\n### Integration Module\n\n```typescript\nfunction detectEnvironments(projectRoot: string): DetectedEnvironment[]\nfunction hasEnvironment(projectRoot: string, type: EnvironmentType): boolean\nfunction generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>\nfunction getClaudeTemplates(): IntegrationTemplate[]\nfunction getOpenCodeTemplates(): IntegrationTemplate[]\nfunction getGeminiTemplates(): IntegrationTemplate[]\n```\n\n### Imports Module\n\n```typescript\nfunction extractImports(content: string): ImportEntry[]\nfunction extractDirectoryImports(dirPath: string, files: string[]): Promise<FileImports[]>\nfunction formatImportMap(fileImports: FileImports[]): string\n```\n\n---\n\n## 4. Data Structures & State\n\n### Core Types\n\n```typescript\n// AI Types\ninterface AICallOptions {\n  prompt: string\n  systemPrompt?: string\n  model?: string\n  timeoutMs?: number\n  maxTurns?: number\n  taskLabel?: string\n}\n\ninterface AIResponse {\n  text: string\n  model: string\n  inputTokens: number\n  outputTokens: number\n  cacheReadTokens: number\n  cacheCreationTokens: number\n  durationMs: number\n  exitCode: number\n  raw: unknown\n}\n\ninterface SubprocessResult {\n  stdout: string\n  stderr: string\n  exitCode: number\n  signal: string | null\n  durationMs: number\n  timedOut: boolean\n  childPid?: number\n}\n\ninterface RetryOptions {\n  maxRetries: number\n  baseDelayMs: number\n  maxDelayMs: number\n  multiplier: number\n  isRetryable: (error: unknown) => boolean\n  onRetry?: (attempt: number, error: unknown) => void\n}\n\ntype AIServiceErrorCode = 'CLI_NOT_FOUND' | 'TIMEOUT' | 'PARSE_ERROR' | 'SUBPROCESS_ERROR' | 'RATE_LIMIT'\n\n// Telemetry Types\ninterface TelemetryEntry {\n  timestamp: string\n  prompt: string\n  systemPrompt?: string\n  response: string\n  model: string\n  inputTokens: number\n  outputTokens: number\n  cacheReadTokens: number\n  cacheCreationTokens: number\n  latencyMs: number\n  exitCode: number\n  error?: string\n  retryCount: number\n  thinking: string\n  filesRead: FileRead[]\n}\n\ninterface RunLog {\n  runId: string\n  startTime: string\n  endTime: string\n  entries: TelemetryEntry[]\n  summary: {\n    totalCalls: number\n    totalInputTokens: number\n    totalOutputTokens: number\n    totalDurationMs: number\n    errorCount: number\n    totalCacheReadTokens: number\n    totalCacheCreationTokens: number\n    totalFilesRead: number\n    uniqueFilesRead: number\n  }\n}\n\ninterface FileRead {\n  path: string\n  sizeBytes: number\n}\n\n// Change Detection Types\ntype ChangeType = 'added' | 'modified' | 'deleted' | 'renamed'\n\ninterface FileChange {\n  status: ChangeType\n  path: string\n  oldPath?: string  // Only for 'renamed' status\n}\n\ninterface ChangeDetectionResult {\n  currentCommit: string\n  baseCommit: string\n  changes: FileChange[]\n  includesUncommitted: boolean\n}\n\ninterface ChangeDetectionOptions {\n  includeUncommitted?: boolean\n}\n\n// Discovery Types\ninterface FilterResult {\n  included: string[]\n  excluded: ExcludedFile[]\n}\n\ninterface ExcludedFile {\n  path: string\n  reason: string\n  filter: string\n}\n\ninterface DiscoveryResult {\n  files: string[]\n  excluded: ExcludedFile[]\n}\n\ninterface DiscoveryStats {\n  totalFiles: number\n  includedFiles: number\n  excludedFiles: number\n  exclusionReasons: Record<string, number>\n}\n\ninterface WalkerOptions {\n  cwd: string\n  followSymlinks?: boolean\n  dot?: boolean\n}\n\n// Generation Types\ninterface GenerationPlan {\n  files: PreparedFile[]\n  tasks: AnalysisTask[]\n  complexity: ComplexityMetrics\n  projectStructure: string\n  projectRoot: string\n}\n\ninterface PreparedFile {\n  path: string\n  absolutePath: string\n  content: string\n}\n\ninterface AnalysisTask {\n  id: string\n  path: string\n  absolutePath: string\n  userPrompt: string\n  systemPrompt: string\n  dependencies: string[]\n}\n\ninterface ExecutionTask {\n  id: string\n  path: string\n  absolutePath: string\n  outputPath: string\n  userPrompt: string\n  systemPrompt: string\n  dependencies: string[]\n  metadata: Record<string, unknown>\n}\n\ninterface ExecutionPlan {\n  projectRoot: string\n  projectStructure: string\n  fileTasks: ExecutionTask[]\n  directoryTasks: ExecutionTask[]\n  rootTasks: ExecutionTask[]\n}\n\ninterface ComplexityMetrics {\n  fileCount: number\n  directoryDepth: number\n  files: string[]\n  directories: string[]\n}\n\ninterface AnalysisResult {\n  summary: string\n  metadata: SummaryMetadata\n}\n\ninterface SummaryMetadata {\n  purpose: string\n  criticalTodos?: string[]\n  relatedFiles?: string[]\n}\n\ninterface SumFileContent {\n  summary: string\n  metadata: SummaryMetadata\n  generatedAt: string\n  contentHash: string\n}\n\ninterface PromptContext {\n  filePath: string\n  content: string\n  contextFiles?: Array<{ path: string; content: string }>\n  projectPlan?: string\n  existingSum?: string\n}\n\n// Orchestration Types\ninterface PoolOptions {\n  concurrency: number\n  failFast?: boolean\n  tracer?: ITraceWriter\n  phaseLabel?: string\n  taskLabels?: string[]\n}\n\ninterface TaskResult<T> {\n  index: number\n  success: boolean\n  value?: T\n  error?: Error\n}\n\ninterface FileTaskResult {\n  path: string\n  success: boolean\n  tokensIn: number\n  tokensOut: number\n  cacheReadTokens: number\n  cacheCreationTokens: number\n  durationMs: number\n  model: string\n  error?: string\n}\n\ninterface RunSummary {\n  version: string\n  filesProcessed: number\n  filesFailed: number\n  filesSkipped: number\n  totalCalls: number\n  totalInputTokens: number\n  totalOutputTokens: number\n  totalCacheReadTokens: number\n  totalCacheCreationTokens: number\n  totalDurationMs: number\n  errorCount: number\n  retryCount: number\n  totalFilesRead: number\n  uniqueFilesRead: number\n  inconsistenciesCodeVsDoc?: number\n  inconsistenciesCodeVsCode?: number\n  phantomPaths?: number\n  inconsistencyReport?: InconsistencyReport\n}\n\ninterface CommandRunOptions {\n  concurrency: number\n  failFast?: boolean\n  debug?: boolean\n  dryRun?: boolean\n  tracer?: ITraceWriter\n  progressLog?: ProgressLog\n}\n\ninterface ProgressEvent {\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done'\n  filePath: string\n  index: number\n  total: number\n  durationMs?: number\n  tokensIn?: number\n  tokensOut?: number\n  model?: string\n  error?: string\n}\n\n// Quality Types\ntype InconsistencySeverity = 'info' | 'warning' | 'error'\n\ninterface CodeDocInconsistency {\n  type: 'code-vs-doc'\n  severity: InconsistencySeverity\n  filePath: string\n  sumPath: string\n  description: string\n  details: {\n    missingFromDoc: string[]\n    missingFromCode: string[]\n    purposeMismatch?: string\n  }\n}\n\ninterface CodeCodeInconsistency {\n  type: 'code-vs-code'\n  severity: InconsistencySeverity\n  files: string[]\n  description: string\n  pattern: string\n}\n\ninterface PhantomPathInconsistency {\n  type: 'phantom-path'\n  severity: InconsistencySeverity\n  agentsMdPath: string\n  description: string\n  details: {\n    referencedPath: string\n    resolvedTo: string\n    context: string\n  }\n}\n\ntype Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency\n\ninterface InconsistencyReport {\n  metadata: {\n    timestamp: string\n    projectRoot: string\n    filesChecked: number\n    durationMs: number\n  }\n  issues: Inconsistency[]\n  summary: {\n    total: number\n    codeVsDoc: number\n    codeVsCode: number\n    phantomPaths: number\n    errors: number\n    warnings: number\n    info: number\n  }\n}\n\ninterface FindabilityResult {\n  filePath: string\n  symbolsTested: number\n  symbolsFound: number\n  symbolsMissing: string[]\n  score: number\n}\n\n// Update Types\ninterface UpdatePlan {\n  filesToAnalyze: FileChange[]\n  filesToSkip: string[]\n  cleanup: FileChange[]\n  affectedDirs: string[]\n}\n\ninterface UpdateOptions {\n  includeUncommitted?: boolean\n  dryRun?: boolean\n}\n\ninterface UpdateResult {\n  analyzedFiles: number\n  skippedFiles: number\n  cleanup: CleanupResult\n  regeneratedDirs: string[]\n  baseCommit: string\n  currentCommit: string\n}\n\ninterface CleanupResult {\n  deletedSumFiles: string[]\n  deletedAgentsMd: string[]\n}\n\n// Rebuild Types\ninterface RebuildUnit {\n  name: string\n  specContent: string\n  order: number\n}\n\ninterface RebuildPlan {\n  specFiles: Array<{ relativePath: string; content: string }>\n  units: RebuildUnit[]\n  outputDir: string\n}\n\ninterface RebuildResult {\n  unitName: string\n  success: boolean\n  filesWritten: string[]\n  tokensIn: number\n  tokensOut: number\n  cacheReadTokens: number\n  cacheCreationTokens: number\n  durationMs: number\n  model: string\n  error?: string\n}\n\ninterface RebuildCheckpoint {\n  version: string\n  createdAt: string\n  updatedAt: string\n  outputDir: string\n  specHashes: Record<string, string>\n  modules: Record<string, {\n    status: 'pending' | 'done' | 'failed'\n    completedAt?: string\n    error?: string\n    filesWritten?: string[]\n  }>\n}\n\n// Installer Types\ntype Runtime = 'claude' | 'opencode' | 'gemini' | 'all'\ntype Location = 'global' | 'local'\n\ninterface InstallerArgs {\n  runtime?: Runtime\n  global: boolean\n  local: boolean\n  uninstall: boolean\n  force: boolean\n  help: boolean\n  quiet: boolean\n}\n\ninterface InstallerResult {\n  success: boolean\n  runtime: Exclude<Runtime, 'all'>\n  location: Location\n  filesCreated: string[]\n  filesSkipped: string[]\n  errors: string[]\n  hookRegistered?: boolean\n  versionWritten?: boolean\n}\n\ninterface RuntimePaths {\n  global: string\n  local: string\n  settingsFile: string\n}\n\ninterface InstallOptions {\n  force: boolean\n  dryRun: boolean\n}\n\n// Integration Types\ntype EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini'\n\ninterface DetectedEnvironment {\n  type: EnvironmentType\n  configDir: string\n  detected: boolean\n}\n\ninterface IntegrationTemplate {\n  filename: string\n  path: string\n  content: string\n}\n\ninterface IntegrationResult {\n  environment: EnvironmentType\n  filesCreated: string[]\n  filesSkipped: string[]\n}\n\n// Import Types\ninterface ImportEntry {\n  specifier: string\n  symbols: string[]\n  typeOnly: boolean\n}\n\ninterface FileImports {\n  fileName: string\n  externalImports: ImportEntry[]\n  internalImports: ImportEntry[]\n}\n\n// Specify Types\ninterface SpecPrompt {\n  system: string\n  user: string\n}\n\ninterface WriteSpecOptions {\n  outputPath?: string\n  force?: boolean\n  multiFile?: boolean\n}\n\n// Trace Types\ntype TraceEventPayload = \n  | { type: 'phase:start'; phase: string; taskCount: number; concurrency: number }\n  | { type: 'phase:end'; phase: string; durationMs: number; tasksCompleted: number; tasksFailed: number }\n  | { type: 'worker:start'; workerId: number; phase: string }\n  | { type: 'worker:end'; workerId: number; phase: string; tasksExecuted: number }\n  | { type: 'task:pickup'; workerId: number; taskIndex: number; taskLabel: string; activeTasks: number }\n  | { type: 'task:done'; workerId: number; taskIndex: number; taskLabel: string; durationMs: number; success: boolean; error?: string; activeTasks: number }\n  | { type: 'task:start'; taskLabel: string; phase: string }\n  | { type: 'subprocess:spawn'; childPid: number; command: string; taskLabel: string }\n  | { type: 'subprocess:exit'; childPid: number; command: string; taskLabel: string; exitCode: number; signal: string | null; durationMs: number; timedOut: boolean }\n  | { type: 'retry'; attempt: number; taskLabel: string; errorCode: string }\n  | { type: 'discovery:start'; targetPath: string }\n  | { type: 'discovery:end'; filesIncluded: number; filesExcluded: number; durationMs: number }\n  | { type: 'filter:applied'; filterName: string; filesMatched: number; filesRejected: number }\n  | { type: 'plan:created'; planType: 'generate' | 'update'; fileCount: number; taskCount: number }\n  | { type: 'config:loaded'; configPath: string; model: string; concurrency: number }\n```\n\n### Serialization Formats\n\n**YAML Frontmatter (.sum files)**:\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: One-line purpose statement\ncritical_todos: [Security issue, Performance bottleneck]\nrelated_files: [../config/schema.ts, ./index.ts]\n---\n\nMarkdown summary content...\n```\n\n**NDJSON (trace files)**: One JSON object per line with fields `seq`, `ts`, `pid`, `elapsedMs`, `type`, plus event-specific fields.\n\n**JSON (checkpoint files)**: See `RebuildCheckpoint` schema.\n\n**JSON (run logs)**: See `RunLog` interface.\n\n---\n\n## 5. Configuration\n\n### Config File Location\n\n`.agents-reverse-engineer/config.yaml`\n\n### Config Schema\n\n```typescript\nconst ConfigSchema = z.object({\n  exclude: z.object({\n    patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS]),\n    vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS]),\n    binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS]),\n  }).default({}),\n  \n  options: z.object({\n    followSymlinks: z.boolean().default(false),\n    maxFileSize: z.number().positive().default(1048576),  // 1MB\n  }).default({}),\n  \n  output: z.object({\n    colors: z.boolean().default(true),\n  }).default({}),\n  \n  ai: z.object({\n    backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto'),\n    model: z.string().default('sonnet'),\n    timeoutMs: z.number().positive().default(300000),  // 5 minutes\n    maxRetries: z.number().min(0).default(3),\n    concurrency: z.number().min(1).max(20).default(getDefaultConcurrency),\n    telemetry: z.object({\n      keepRuns: z.number().min(0).default(50),\n    }).default({}),\n  }).default({}),\n}).default({})\n```\n\n### Environment Variable Overrides\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `CLAUDE_CONFIG_DIR` | Override `~/.claude` path | `~/.claude` |\n| `OPENCODE_CONFIG_DIR` | Override `~/.config/opencode` path | `~/.config/opencode` |\n| `GEMINI_CONFIG_DIR` | Override `~/.gemini` path | `~/.gemini` |\n| `ARE_DISABLE_HOOK` | Disable session-end auto-update (set to `1`) | unset |\n\n### Default Constants\n\n**DEFAULT_VENDOR_DIRS** (18 entries):\n```typescript\n['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', \n 'venv', '.venv', 'target', '.cargo', '.gradle', '.agents-reverse-engineer', \n '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n**DEFAULT_EXCLUDE_PATTERNS** (26 entries):\n```typescript\n['AGENTS.md', 'CLAUDE.md', 'OPENCODE.md', 'GEMINI.md', '**/AGENTS.md', \n '**/CLAUDE.md', '**/OPENCODE.md', '**/GEMINI.md', '*.lock', 'package-lock.json', \n 'yarn.lock', 'pnpm-lock.yaml', 'bun.lock', 'bun.lockb', 'Gemfile.lock', \n 'Cargo.lock', 'poetry.lock', 'composer.lock', 'go.sum', '.gitignore', \n '.gitattributes', '.gitkeep', '.env', '**/.env', '**/.env.*', '*.log', \n '*.sum', '**/*.sum', '**/SKILL.md']\n```\n\n**DEFAULT_BINARY_EXTENSIONS** (26 entries in defaults.ts, 96 in binary filter):\n```typescript\n['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp', '.zip', '.tar', \n '.gz', '.rar', '.7z', '.exe', '.dll', '.so', '.dylib', '.mp3', '.mp4', \n '.wav', '.pdf', '.woff', '.woff2', '.ttf', '.eot', '.class', '.pyc']\n```\n\n---\n\n## 6. Dependencies\n\n| Package | Version | Rationale |\n|---------|---------|-----------|\n| `fast-glob` | ^3.3.3 | High-performance file discovery with glob pattern support, symlink handling, and directory filtering |\n| `ignore` | ^7.0.3 | Gitignore-compatible pattern matching for file exclusion rules |\n| `isbinaryfile` | ^5.0.4 | Binary file content detection for files without recognizable extensions |\n| `simple-git` | ^3.27.0 | Git operations for change detection (`git diff`, `git status`, commit retrieval) |\n| `yaml` | ^2.7.0 | YAML parsing and serialization for config files |\n| `zod` | ^3.24.1 | Schema validation with TypeScript type inference for configuration and checkpoint files |\n| `ora` | ^8.1.1 | Terminal spinner UI for long-running operations |\n| `picocolors` | ^1.1.1 | Lightweight terminal color formatting (ANSI escape codes) |\n| `@types/node` | ^22.10.7 | TypeScript type definitions for Node.js APIs |\n| `tsx` | ^4.19.2 | TypeScript execution for development mode (`npm run dev`) |\n| `typescript` | ^5.7.3 | TypeScript compiler targeting ES2022 with strict mode |\n\n---\n\n## 7. Behavioral Contracts\n\n### 7.1 Runtime Behavior\n\n#### Error Handling\n\n**AIServiceError Codes**:\n| Code | When Thrown |\n|------|-------------|\n| `CLI_NOT_FOUND` | Backend CLI not found on PATH during resolution |\n| `TIMEOUT` | Subprocess exceeded `timeoutMs` |\n| `PARSE_ERROR` | Failed to parse backend JSON output |\n| `SUBPROCESS_ERROR` | Subprocess exited with non-zero code |\n| `RATE_LIMIT` | Rate limit detected in stderr |\n\n**Exit Codes (CLI commands)**:\n- `0`: Success (all tasks completed or no tasks to process)\n- `1`: Partial failure (some tasks succeeded, some failed) OR file conflict OR first-run detection\n- `2`: Total failure (no tasks succeeded) OR AI CLI not found\n\n#### Retry Logic\n\n**Exponential Backoff Formula**:\n```typescript\ndelay = min(baseDelayMs * multiplier^attempt, maxDelayMs) + Math.random() * 500\n```\n\n**Default Retry Options**:\n```typescript\n{ maxRetries: 3, baseDelayMs: 1000, maxDelayMs: 8000, multiplier: 2 }\n```\n\n**Retryable Conditions**: Only rate limit errors are retryable. Timeouts are NOT retried (rationale: spawning another heavyweight subprocess on a struggling system makes things worse).\n\n#### Subprocess Lifecycle\n\n1. Spawn with `execFile()`, 10MB `maxBuffer`, SIGTERM `killSignal`\n2. Track in `activeSubprocesses` Map with PID → `{ command, spawnedAt }`\n3. Invoke `onSpawn()` callback for trace emission\n4. Write prompt to stdin, close with `.end()`\n5. Set unref'd SIGKILL timer at `timeoutMs + 5000ms`\n6. On callback: clear timer, attempt process group kill via `kill(-pid, 'SIGKILL')`\n7. Remove from `activeSubprocesses`, resolve with `SubprocessResult`\n\n#### Concurrency Model\n\n**Worker Pool**: Shares single `tasks.entries()` iterator across N workers. Workers consume tasks via `for...of` loop calling iterator `.next()` atomically. Fail-fast mode sets shared `aborted` flag, checked before each task pickup.\n\n**Resource-Adaptive Concurrency**:\n```typescript\nclamp(\n  os.availableParallelism() * 5,\n  2,\n  min(20, floor(os.totalmem() * 0.5 / 0.512))\n)\n```\n\nConstants: `CONCURRENCY_MULTIPLIER = 5`, `MIN_CONCURRENCY = 2`, `MAX_CONCURRENCY = 20`, `SUBPROCESS_HEAP_GB = 0.512`, `MEMORY_FRACTION = 0.5`\n\n#### Lifecycle Hooks\n\n**SessionStart Hook**: Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, writes to `~/.claude/cache/are-update-check.json`.\n\n**SessionEnd Hook**: Checks `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached process if changes detected.\n\n**Disable Mechanisms**:\n- Environment: `ARE_DISABLE_HOOK=1`\n- Config: `hook_enabled: false` substring in `.agents-reverse-engineer.yaml`\n\n### 7.2 Implementation Contracts\n\n#### Regex Patterns\n\n**Export Extraction** (code-vs-doc.ts):\n```\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Import Extraction** (extractor.ts):\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\n**Phantom Path Extraction** (validator.ts `PATH_PATTERNS`):\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g                          // Markdown links\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g              // Backtick paths\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi     // Prose-embedded paths\n```\n\n**Phantom Path Skip Patterns** (validator.ts `SKIP_PATTERNS`):\n```javascript\n/node_modules/\n/\\.git\\//\n/^https?:/\n/\\{\\{/\n/\\$\\{/\n/\\*/\n/\\{[^}]*,[^}]*\\}/\n```\n\n**Sum File Frontmatter Extraction**:\n```\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**Sum File Field Extraction**:\n```\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n```\n\n**YAML Array Inline Format**:\n```\n/<key>:\\s*\\[([^\\]]*)\\]/\n```\n\n**YAML Array Multi-line Format**:\n```\n/<key>:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m\n```\n\n**Build Plan Phase Extraction** (spec-reader.ts):\n```\n/^### Phase (\\d+):\\s*(.+)$/gm\n```\n\n**Rebuild File Delimiter** (output-parser.ts):\n```\n/===FILE:\\s*(.+?)===\\n([\\s\\S]*?)===END_FILE===/g\n```\n\n**Rebuild Fenced Block Fallback**:\n```\n/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g\n```\n\n**Spec Heading Split** (writer.ts):\n```\n/^(?=# )/m\n```\n\n**Slugification Transforms** (writer.ts):\n```typescript\n.toLowerCase()\n.replace(/\\s+/g, '-')\n.replace(/[^a-z0-9-]/g, '')\n.replace(/-+/g, '-')\n.replace(/^-|-$/g, '')\n```\n\n**PlanTracker Checkbox Update**:\n```\n/- \\[ \\] \\`${itemPath}\\`/ → /- \\[x\\] \\`${itemPath}\\`/\n```\n\n**Preamble Separator**:\n```\n\\n---\\n\n```\n\n**Preamble Bold Detection**:\n```\n/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/\n```\n\n**ANSI Strip Pattern** (progress.ts):\n```\n/\\x1b\\[[0-9;]*m/g\n```\n\n#### Format Strings and Templates\n\n**Sum File Path**: `${sourcePath}.sum`\n\n**Annex File Path**: `${sourcePath}.annex.md`\n\n**Trace File Path**: `.agents-reverse-engineer/traces/trace-${safeTimestamp}.ndjson`\n- Timestamp format: `2026-02-09T12:34:56.789Z` → `2026-02-09T12-34-56-789Z`\n\n**Run Log Path**: `.agents-reverse-engineer/logs/run-${safeTimestamp}.json`\n\n**Progress Log Path**: `.agents-reverse-engineer/progress.log`\n\n**Config Path**: `.agents-reverse-engineer/config.yaml`\n\n**Checkpoint Path**: `${outputDir}/.rebuild-checkpoint`\n\n**Progress Output Formats**:\n```\n[X/Y] ANALYZING path\n[X/Y] DONE path Xs in/out tok model ~Ns remaining\n[X/Y] FAIL path error\n[dir X/Y] ANALYZING dirPath/AGENTS.md\n[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA\n[root] DONE docPath\n```\n\n**Update Check Cache Schema**:\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": string,\n  \"latest\": string,\n  \"checked\": number  // Unix timestamp\n}\n```\n\n#### Magic Constants and Sentinel Values\n\n| Constant | Value | Meaning |\n|----------|-------|---------|\n| `GENERATED_MARKER` | `'<!-- Generated by agents-reverse-engineer -->'` | Marker for tool-generated AGENTS.md |\n| `CONFIG_DIR` | `'.agents-reverse-engineer'` | Configuration directory name |\n| `CONFIG_FILE` | `'config.yaml'` | Configuration file name |\n| `TRACES_DIR` | `'.agents-reverse-engineer/traces'` | Trace output directory |\n| `DEFAULT_MAX_FILE_SIZE` | `1048576` (1MB) | Binary detection threshold |\n| `SUBPROCESS_HEAP_GB` | `0.512` | Subprocess heap limit in GB |\n| `CONCURRENCY_MULTIPLIER` | `5` | CPU cores multiplier for concurrency |\n| `MIN_CONCURRENCY` | `2` | Minimum worker pool size |\n| `MAX_CONCURRENCY` | `20` | Maximum worker pool size |\n| `MEMORY_FRACTION` | `0.5` | System memory fraction for subprocesses |\n| `BUILT_CONTEXT_LIMIT` | `100000` | LRU truncation limit for rebuild context |\n| `TRUNCATED_HEAD_LINES` | `20` | Lines to keep when truncating context |\n| `FILTER_CONCURRENCY` | `30` | Concurrency for filter application |\n| `TRACE_KEEP_COUNT` | `500` | Default trace file retention count |\n| `RUN_LOG_KEEP_COUNT` | `50` | Default run log retention count |\n\n#### Rate Limit Detection Patterns\n\n```javascript\n[\"rate limit\", \"429\", \"too many requests\", \"overloaded\"]\n```\n\nChecked via lowercase `stderr.includes()`.\n\n#### Claude Backend Arguments\n\n```javascript\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n// Appends conditionally:\n['--model', options.model]        // if options.model present\n['--system-prompt', options.systemPrompt]  // if options.systemPrompt present\n['--max-turns', String(options.maxTurns)]  // if options.maxTurns defined\n```\n\n#### Git Diff Format\n\n```\nA       → { status: 'added', path }\nM       → { status: 'modified', path }\nD       → { status: 'deleted', path }\nR<pct>  → { status: 'renamed', path: newPath, oldPath }\n```\n\n#### Manifest Detection Array\n\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml',\n 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n#### Language Detection Map\n\n```javascript\n.ts → typescript, .tsx → typescript, .js → javascript, .jsx → javascript,\n.py → python, .go → go, .rs → rust, .java → java, .kt → kotlin,\n.rb → ruby, .php → php, .c → c, .cpp → cpp, .cs → csharp,\n.swift → swift, .scala → scala, .sh → bash, .md → markdown,\n.yaml → yaml, .yml → yaml, .json → json, .toml → toml\nDefault: text\n```\n\n#### Skip Directories (Collector)\n\n```javascript\n['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', \n 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle']\n```\n\n#### Binary Extensions (Full Set - 96 entries)\n\n```javascript\n['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp', '.svg', '.tiff', '.tif', \n '.psd', '.raw', '.heif', '.heic', '.zip', '.tar', '.gz', '.rar', '.7z', '.bz2', \n '.xz', '.tgz', '.exe', '.dll', '.so', '.dylib', '.bin', '.msi', '.app', '.dmg', \n '.mp3', '.mp4', '.wav', '.avi', '.mov', '.mkv', '.flac', '.ogg', '.webm', '.m4a', \n '.aac', '.wma', '.wmv', '.flv', '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', \n '.pptx', '.odt', '.ods', '.odp', '.woff', '.woff2', '.ttf', '.eot', '.otf', \n '.class', '.pyc', '.pyo', '.o', '.obj', '.a', '.lib', '.wasm', '.db', '.sqlite', \n '.sqlite3', '.mdb', '.ico', '.icns', '.cur', '.deb', '.rpm', '.jar', '.war', '.ear']\n```\n\n---\n\n## 8. Test Contracts\n\n### AI Service Module\n\n**Test Scenarios**:\n- Backend registry correctly orders backends by priority (Claude → Gemini → OpenCode)\n- `resolveBackend('auto')` returns first available backend\n- `resolveBackend('auto')` throws `AIServiceError` with `CLI_NOT_FOUND` when no backends available\n- `withRetry()` retries on rate limit errors up to `maxRetries`\n- `withRetry()` does NOT retry on timeout errors\n- Exponential backoff formula produces correct delays: 1000, 2000, 4000, 8000ms (capped)\n- `runSubprocess()` enforces timeout with SIGTERM, escalates to SIGKILL after 5s grace\n- `runSubprocess()` captures stdout/stderr on success and failure\n- `AIService.call()` emits `subprocess:spawn` and `subprocess:exit` trace events\n- `TelemetryLogger` correctly aggregates token counts across entries\n- `cleanupOldLogs()` keeps exactly `keepCount` most recent files\n\n**Edge Cases**:\n- Empty stdout from subprocess\n- Subprocess exits before stdin write completes\n- SIGKILL required after SIGTERM timeout\n- Concurrent calls with shared tracer\n\n### Change Detection Module\n\n**Test Scenarios**:\n- `isGitRepo()` returns false for non-git directories\n- `getChangedFiles()` parses A/M/D status codes correctly\n- `getChangedFiles()` parses R100 rename format with oldPath\n- `includeUncommitted: true` merges staged and modified files\n- Deduplication prevents duplicate FileChange entries\n- `computeContentHash()` produces consistent SHA-256 hex output\n\n**Edge Cases**:\n- Empty git diff output (no changes)\n- Git rename with 50% similarity threshold\n- File path containing spaces or special characters\n\n### Config Module\n\n**Test Scenarios**:\n- `loadConfig()` returns defaults for empty object `{}`\n- `loadConfig()` merges partial config with defaults\n- `loadConfig()` throws `ConfigError` for invalid YAML\n- `loadConfig()` throws `ConfigError` for schema violations\n- `getDefaultConcurrency()` respects memory cap\n- `writeDefaultConfig()` creates valid YAML with all defaults\n\n**Edge Cases**:\n- Config file with ENOENT returns defaults without error\n- Very low memory system (< 1GB)\n- Config with unknown extra fields (should be ignored)\n\n### Discovery Module\n\n**Test Scenarios**:\n- `discoverFiles()` excludes vendor directories\n- `discoverFiles()` excludes binary files by extension\n- `discoverFiles()` excludes files matching gitignore patterns\n- `applyFilters()` short-circuits on first exclusion\n- `createBinaryFilter()` uses isBinaryFile for unknown extensions\n- Filter chain respects priority order\n\n**Edge Cases**:\n- File exceeds maxFileSize\n- Symlink handling when `followSymlinks: false`\n- Dotfiles included when `dot: true`\n- Empty gitignore file\n\n### Generation Module\n\n**Test Scenarios**:\n- `createPlan()` groups files by directory correctly\n- `buildExecutionPlan()` sorts directories by depth descending\n- `getDirectoryDepth()` returns correct values (`.` → 0, `src` → 1, `src/cli` → 2)\n- `isDirectoryComplete()` returns false when child .sum missing\n- `writeSumFile()` creates valid YAML frontmatter\n- `readSumFile()` parses both inline and multi-line array formats\n- `writeAgentsMd()` preserves AGENTS.local.md content\n- `collectAgentsDocs()` skips SKIP_DIRS directories\n\n**Edge Cases**:\n- Empty directory (no files)\n- Directory with only subdirectories\n- Circular symlinks\n- AGENTS.md already exists without GENERATED_MARKER\n\n### Orchestration Module\n\n**Test Scenarios**:\n- `runPool()` distributes tasks across workers evenly\n- `runPool()` respects fail-fast flag\n- `runPool()` returns results in original task order\n- `ProgressReporter` calculates ETA from moving average\n- `TraceWriter` serializes events in emission order despite concurrency\n- `PlanTracker` updates checkboxes correctly\n\n**Edge Cases**:\n- Zero tasks in pool\n- Single task with concurrency > 1\n- All tasks fail simultaneously\n- Promise chain handles write errors gracefully\n\n### Quality Module\n\n**Test Scenarios**:\n- `extractExports()` captures function, class, const, type, interface, enum\n- `extractExports()` captures default exports\n- `checkCodeVsDoc()` detects missing exports in summary\n- `checkCodeVsCode()` detects duplicate exports in same directory\n- `checkPhantomPaths()` resolves relative paths from AGENTS.md location\n- `checkPhantomPaths()` resolves src/ paths from project root\n- `checkPhantomPaths()` tries .ts fallback for .js references\n\n**Edge Cases**:\n- Destructured exports (currently missed by regex)\n- Namespace exports (currently missed)\n- Path with template placeholder `{{foo}}`\n- URL-like string in markdown\n\n### Update Module\n\n**Test Scenarios**:\n- `preparePlan()` identifies modified files by hash mismatch\n- `preparePlan()` identifies added files (no existing .sum)\n- `cleanupOrphans()` deletes .sum for deleted files\n- `cleanupOrphans()` targets oldPath for renamed files\n- `getAffectedDirectories()` walks to project root\n- `getAffectedDirectories()` sorts deepest-first\n\n**Edge Cases**:\n- File modified but hash unchanged (no regeneration)\n- Directory becomes empty after file deletion\n- Rename to different directory\n\n---\n\n## 9. Build Plan\n\n### Phase 1: Core Types and Utilities\n**Defines**: `AICallOptions`, `AIResponse`, `SubprocessResult`, `RetryOptions`, `AIServiceErrorCode`, `AIServiceError`, `TelemetryEntry`, `RunLog`, `FileRead`, `ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions`, `FilterResult`, `ExcludedFile`, `DiscoveryResult`, `DiscoveryStats`, `WalkerOptions`, `FileFilter`, `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`, `SummaryMetadata`, `SumFileContent`, `ImportEntry`, `FileImports`\n\n**Consumes**: None (foundation phase)\n\n**Tasks**:\n1. Define all interface and type definitions\n2. Implement `AIServiceError` class\n3. Define Zod schemas where applicable\n\n---\n\n### Phase 2: Configuration\n**Defines**: `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`, `ConfigSchema`, `loadConfig`, `configExists`, `writeDefaultConfig`, `getDefaultConcurrency`, `DEFAULT_VENDOR_DIRS`, `DEFAULT_EXCLUDE_PATTERNS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_CONFIG`, `CONFIG_DIR`, `CONFIG_FILE`\n\n**Consumes**: Phase 1 types (for config validation)\n\n**Tasks**:\n1. Implement Zod schemas for all config sections\n2. Implement `getDefaultConcurrency()` with memory-adaptive formula\n3. Implement YAML loader with error handling\n4. Implement config writer with YAML special character escaping\n\n---\n\n### Phase 3: Change Detection\n**Defines**: `isGitRepo`, `getCurrentCommit`, `getChangedFiles`, `computeContentHash`, `computeContentHashFromString`\n\n**Consumes**: `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions` from Phase 1\n\n**Tasks**:\n1. Implement git repository detection\n2. Implement `git diff --name-status -M` parser\n3. Implement SHA-256 content hashing\n4. Implement uncommitted change merging\n\n---\n\n### Phase 4: Discovery Filters\n**Defines**: `FileFilter` interface implementations, `createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter`, `applyFilters`, `BINARY_EXTENSIONS`\n\n**Consumes**: `FileFilter`, `FilterResult`, `ExcludedFile` from Phase 1; config defaults from Phase 2\n\n**Tasks**:\n1. Implement gitignore filter with `ignore` library\n2. Implement vendor filter with Set + path matching\n3. Implement binary filter with extension set + isBinaryFile\n4. Implement custom pattern filter\n5. Implement filter chain applicator with bounded concurrency\n\n---\n\n### Phase 5: File Discovery\n**Defines**: `discoverFiles`, `walkDirectory`, `DiscoveryConfig`, `DiscoverFilesOptions`\n\n**Consumes**: Filter functions from Phase 4; `WalkerOptions`, `FilterResult` from Phase 1\n\n**Tasks**:\n1. Implement fast-glob wrapper\n2. Implement filter chain composition\n3. Implement discovery result assembly\n\n---\n\n### Phase 6: Import Extraction\n**Defines**: `extractImports`, `extractDirectoryImports`, `formatImportMap`, `IMPORT_REGEX`\n\n**Consumes**: `ImportEntry`, `FileImports` from Phase 1\n\n**Tasks**:\n1. Implement regex-based import parser\n2. Implement directory-level import aggregation (first 100 lines)\n3. Implement import map formatter\n\n---\n\n### Phase 7: Sum File I/O\n**Defines**: `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `writeAnnexFile`, `getAnnexPath`, `formatSumFile`, `parseSumFile`, `GENERATED_MARKER`\n\n**Consumes**: `SumFileContent`, `SummaryMetadata` from Phase 1\n\n**Tasks**:\n1. Implement YAML frontmatter serialization\n2. Implement YAML array formatting (inline vs multi-line)\n3. Implement frontmatter parser with regex extraction\n4. Implement annex file generator\n\n---\n\n### Phase 8: AGENTS.md Writer\n**Defines**: `writeAgentsMd`, `isGeneratedAgentsMd`\n\n**Consumes**: `GENERATED_MARKER` from Phase 7\n\n**Tasks**:\n1. Implement AGENTS.local.md preservation logic\n2. Implement generated marker detection\n3. Implement content assembly with user content block\n\n---\n\n### Phase 9: Telemetry\n**Defines**: `TelemetryLogger`, `writeRunLog`, `cleanupOldLogs`\n\n**Consumes**: `TelemetryEntry`, `RunLog`, `FileRead` from Phase 1\n\n**Tasks**:\n1. Implement in-memory entry accumulation\n2. Implement summary computation\n3. Implement NDJSON file writer\n4. Implement retention enforcement\n\n---\n\n### Phase 10: Tracing\n**Defines**: `ITraceWriter`, `TraceWriter`, `NullTraceWriter`, `createTraceWriter`, `cleanupOldTraces`, `TraceEvent`, `TraceEventPayload`\n\n**Consumes**: None (standalone)\n\n**Tasks**:\n1. Define trace event type hierarchy\n2. Implement promise-chain file writer\n3. Implement null writer for disabled tracing\n4. Implement trace cleanup\n\n---\n\n### Phase 11: Subprocess Runner\n**Defines**: `runSubprocess`, `isCommandOnPath`, `getActiveSubprocessCount`, `getActiveSubprocesses`\n\n**Consumes**: `SubprocessResult` from Phase 1; `ITraceWriter` from Phase 10\n\n**Tasks**:\n1. Implement execFile wrapper with resource limits\n2. Implement timeout enforcement with SIGTERM/SIGKILL\n3. Implement process group killing\n4. Implement active subprocess tracking\n\n---\n\n### Phase 12: Retry Logic\n**Defines**: `withRetry`, `DEFAULT_RETRY_OPTIONS`\n\n**Consumes**: `RetryOptions` from Phase 1\n\n**Tasks**:\n1. Implement exponential backoff formula\n2. Implement retry loop with predicate\n3. Implement jitter addition\n\n---\n\n### Phase 13: Backend Adapters\n**Defines**: `AIBackend` implementations (`ClaudeBackend`, `GeminiBackend`, `OpenCodeBackend`), `BackendRegistry`, `createBackendRegistry`, `resolveBackend`, `detectBackend`, `getInstallInstructions`\n\n**Consumes**: `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` from Phase 1; `isCommandOnPath` from Phase 11\n\n**Tasks**:\n1. Implement Claude backend with Zod JSON parsing\n2. Implement stub backends for Gemini/OpenCode\n3. Implement backend registry with priority ordering\n4. Implement auto-detection\n\n---\n\n### Phase 14: AI Service\n**Defines**: `AIService`\n\n**Consumes**: `BackendRegistry` from Phase 13; `withRetry` from Phase 12; `runSubprocess` from Phase 11; `TelemetryLogger` from Phase 9; `ITraceWriter` from Phase 10; all AI types from Phase 1\n\n**Tasks**:\n1. Implement rate limit detection\n2. Implement retry integration (not for timeouts)\n3. Implement trace event emission\n4. Implement telemetry aggregation\n\n---\n\n### Phase 15: Worker Pool\n**Defines**: `runPool`, `PoolOptions`, `TaskResult`\n\n**Consumes**: `ITraceWriter` from Phase 10\n\n**Tasks**:\n1. Implement iterator-based task distribution\n2. Implement fail-fast flag handling\n3. Implement trace emission for workers/tasks\n\n---\n\n### Phase 16: Progress Reporting\n**Defines**: `ProgressReporter`, `ProgressLog`, `ProgressEvent`\n\n**Consumes**: `RunSummary`, `ProgressEvent` from Phase 1; ANSI strip pattern\n\n**Tasks**:\n1. Implement ETA calculation with moving average\n2. Implement console output formatting\n3. Implement progress.log file mirroring\n4. Implement summary printer\n\n---\n\n### Phase 17: Plan Tracking\n**Defines**: `PlanTracker`\n\n**Consumes**: None (standalone)\n\n**Tasks**:\n1. Implement GENERATION-PLAN.md writer\n2. Implement checkbox update via regex\n3. Implement promise-chain serialization\n\n---\n\n### Phase 18: Quality Validators\n**Defines**: `extractExports`, `checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`, `buildInconsistencyReport`, `formatReportForCli`, `validateFindability`, `FindabilityResult`\n\n**Consumes**: All inconsistency types from Phase 1; `SumFileContent` from Phase 7\n\n**Tasks**:\n1. Implement export extraction regex\n2. Implement code-vs-doc substring matching\n3. Implement code-vs-code duplicate detection\n4. Implement phantom path resolution with fallbacks\n5. Implement report aggregation\n\n---\n\n### Phase 19: Prompt Construction\n**Defines**: `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage`, `PromptContext`, `SUMMARY_GUIDELINES`, `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`\n\n**Consumes**: `readSumFile`, `GENERATED_MARKER` from Phases 7-8; `extractDirectoryImports`, `formatImportMap` from Phase 6; `collectAgentsDocs` from Phase 20\n\n**Tasks**:\n1. Implement prompt templates with placeholder substitution\n2. Implement language detection map\n3. Implement manifest detection\n4. Implement prompt builders for all three phases\n\n---\n\n### Phase 20: Document Collector\n**Defines**: `collectAgentsDocs`, `collectAnnexFiles`, `AgentsDocs`, `SKIP_DIRS`\n\n**Consumes**: None (filesystem operations only)\n\n**Tasks**:\n1. Implement recursive AGENTS.md collection\n2. Implement recursive .annex.md collection\n3. Implement SKIP_DIRS filtering\n\n---\n\n### Phase 21: Generation Orchestrator\n**Defines**: `GenerationOrchestrator`, `createOrchestrator`, `GenerationPlan`, `PreparedFile`, `AnalysisTask`, `ComplexityMetrics`, `analyzeComplexity`\n\n**Consumes**: `discoverFiles` from Phase 5; `buildFilePrompt` from Phase 19; `Config` from Phase 2; `ITraceWriter` from Phase 10\n\n**Tasks**:\n1. Implement file preparation with content loading\n2. Implement complexity analysis\n3. Implement project structure tree generation\n4. Implement task creation\n\n---\n\n### Phase 22: Execution Plan Builder\n**Defines**: `buildExecutionPlan`, `ExecutionPlan`, `ExecutionTask`, `formatExecutionPlanAsMarkdown`, `isDirectoryComplete`, `getReadyDirectories`, `getDirectoryDepth`\n\n**Consumes**: `GenerationPlan` from Phase 21; `sumFileExists` from Phase 7\n\n**Tasks**:\n1. Implement depth calculation\n2. Implement directory task dependency construction\n3. Implement post-order sorting\n4. Implement markdown plan formatter\n\n---\n\n### Phase 23: Command Runner\n**Defines**: `CommandRunner`, `FileTaskResult`, `RunSummary`, `CommandRunOptions`\n\n**Consumes**: `AIService` from Phase 14; `ExecutionPlan` from Phase 22; `runPool` from Phase 15; `ProgressReporter`, `ProgressLog` from Phase 16; `PlanTracker` from Phase 17; quality validators from Phase 18; writers from Phases 7-8; prompt builders from Phase 19\n\n**Tasks**:\n1. Implement three-phase execution loop\n2. Implement quality validation integration\n3. Implement preamble stripping\n4. Implement purpose extraction\n\n---\n\n### Phase 24: Update Orchestrator\n**Defines**: `UpdateOrchestrator`, `createUpdateOrchestrator`, `UpdatePlan`, `UpdateOptions`, `UpdateResult`, `CleanupResult`, `cleanupOrphans`, `cleanupEmptyDirectoryDocs`, `getAffectedDirectories`\n\n**Consumes**: `discoverFiles` from Phase 5; `readSumFile` from Phase 7; `computeContentHash` from Phase 3; `FileChange` from Phase 1\n\n**Tasks**:\n1. Implement hash comparison workflow\n2. Implement orphan cleanup\n3. Implement affected directory calculation\n4. Implement empty directory cleanup\n\n---\n\n### Phase 25: Specification Synthesis\n**Defines**: `buildSpecPrompt`, `writeSpec`, `SpecExistsError`, `SpecPrompt`, `WriteSpecOptions`, `SPEC_SYSTEM_PROMPT`\n\n**Consumes**: `collectAgentsDocs`, `collectAnnexFiles` from Phase 20\n\n**Tasks**:\n1. Implement system prompt constant\n2. Implement prompt builder\n3. Implement heading-based file splitter\n4. Implement conflict detection\n\n---\n\n### Phase 26: Rebuild Module\n**Defines**: `readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`, `CheckpointManager`, `RebuildUnit`, `RebuildPlan`, `RebuildResult`, `RebuildCheckpoint`, `RebuildCheckpointSchema`, `REBUILD_SYSTEM_PROMPT`\n\n**Consumes**: `AIService` from Phase 14; `runPool` from Phase 15; `computeContentHashFromString` from Phase 3\n\n**Tasks**:\n1. Implement spec file reader\n2. Implement Build Plan phase parser\n3. Implement output delimiter parser\n4. Implement checkpoint manager with drift detection\n5. Implement context accumulation with LRU truncation\n\n---\n\n### Phase 27: Integration Templates\n**Defines**: `detectEnvironments`, `hasEnvironment`, `generateIntegrationFiles`, `getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates`, `EnvironmentType`, `DetectedEnvironment`, `IntegrationTemplate`, `IntegrationResult`, `COMMANDS`, `PLATFORM_CONFIGS`\n\n**Consumes**: None (standalone)\n\n**Tasks**:\n1. Implement environment detection\n2. Implement command template definitions\n3. Implement platform config objects\n4. Implement frontmatter builders\n5. Implement TOML builder for Gemini\n\n---\n\n### Phase 28: Installer\n**Defines**: `runInstaller`, `parseInstallerArgs`, `installFiles`, `uninstallFiles`, `registerHooks`, `registerPermissions`, `getPackageVersion`, `writeVersionFile`, `InstallerArgs`, `InstallerResult`, `RuntimePaths`, `InstallOptions`, `Runtime`, `Location`, `ARE_PERMISSIONS`, `ARE_HOOKS`, `ARE_PLUGINS`\n\n**Consumes**: `getClaudeTemplates`, `getOpenCodeTemplates`, `getGeminiTemplates` from Phase 27\n\n**Tasks**:\n1. Implement argument parser\n2. Implement interactive prompts with arrow-key navigation\n3. Implement file copy operations\n4. Implement hook registration (Claude nested, Gemini flat)\n5. Implement permission registration\n6. Implement uninstall with cleanup\n\n---\n\n### Phase 29: Session Hooks\n**Defines**: `are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`\n\n**Consumes**: None (standalone scripts)\n\n**Tasks**:\n1. Implement version check with npm registry query\n2. Implement git status check\n3. Implement detached process spawning\n4. Implement OpenCode plugin wrappers\n\n---\n\n### Phase 30: CLI Commands\n**Defines**: `initCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `specifyCommand`, `rebuildCommand`, `cleanCommand`, `parseArgs`\n\n**Consumes**: All previous phases\n\n**Tasks**:\n1. Implement argument parser with short/long flags\n2. Implement each command handler\n3. Implement backend resolution error handling\n4. Implement exit code conventions\n5. Implement dry-run behavior\n6. Implement installer detection and routing\n\n---\n\n## 10. Prompt Templates & System Instructions\n\n### FILE_SYSTEM_PROMPT\n\n```\nYou are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation\n```\n\n### FILE_USER_PROMPT\n\n```\nAnalyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n```{{LANG}}\n{{CONTENT}}\n```\n{{PROJECT_PLAN_SECTION}}\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.\n```\n\n### DIRECTORY_SYSTEM_PROMPT\n\n```\nYou are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context\n```\n\n### FILE_UPDATE_SYSTEM_PROMPT\n\n```\nYou are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation\n```\n\n### DIRECTORY_UPDATE_SYSTEM_PROMPT\n\n```\nYou are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context\n```\n\n### ROOT_SYSTEM_PROMPT\n\n```\nYou generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided\n```\n\n### SPEC_SYSTEM_PROMPT\n\n```\nYou produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence with explicit interface contracts per phase:\n   - Each phase MUST include a \"Defines:\" list naming the exact types, interfaces, classes, and functions this phase must export (use the exact names from section 3 Public API Surface)\n   - Each phase MUST include a \"Consumes:\" list naming the exact types and functions from earlier phases that this phase imports\n   - Include dependency ordering and implementation tasks as before\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Build Plan phases MUST cross-reference the Public API Surface: every type/function in the API Surface section must appear in exactly one phase's \"Defines:\" list\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.\n```\n\n### REBUILD_SYSTEM_PROMPT\n\n```\nYou reconstruct source code from a project specification.\n\nTASK:\nGenerate all source files for the described module/phase. The code must be complete, compilable, and production-ready.\n\nOUTPUT FORMAT:\nUse this exact delimiter format for EVERY file:\n\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n\nGenerate ONLY the file content between delimiters. No markdown fencing, no commentary, no explanations outside the file delimiters.\n\nQUALITY:\n- Code must compile. Use exact type names, function signatures, and constants from the spec.\n- Follow the architecture and patterns described in the specification.\n- Imports must reference real modules described in the spec.\n- Generate production code only (no tests, no stubs, no placeholders).\n- Do not invent features not in the spec.\n- Do not add comments explaining what the spec says — write the code the spec describes.\n\nSTRICT COMPLIANCE:\n- When the specification defines exact names for functions, methods, types, classes, or constants, you MUST use those exact names. Do not invent synonyms (e.g., if the spec says done(), do not write reportSuccess()).\n- Pay close attention to the \"Interfaces for This Phase\" section in the current phase — it contains the exact signatures you must implement.\n- When \"Already Built\" context shows an exported symbol, import and use it. Do not redefine it.\n\nCONTEXT AWARENESS:\nWhen \"Already Built\" context is provided, import from those modules and use their exported types/functions. Do not redefine types that already exist in built modules.\nWhen \"Already Built\" context provides a function or method signature, your code MUST call it using the exact name shown. Match the API precisely.\n```\n\n### SUMMARY_GUIDELINES\n\n```typescript\nconst SUMMARY_GUIDELINES = {\n  targetLength: { min: 300, max: 500 },\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n```\n\n---\n\n## 11. IDE Integration & Installer\n\n### Platform Configuration Objects\n\n```typescript\nconst PLATFORM_CONFIGS: Record<Platform, PlatformConfig> = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands/',\n    filenameSeparator: '-',\n    extraFrontmatter: 'agent: build',\n    usesName: false,\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands/',\n    filenameSeparator: '-',\n    usesName: false,\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n```\n\n### Command Definitions\n\n**COMMANDS** object contains seven command definitions: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help`.\n\nEach command has:\n- `description`: Short description string\n- `argumentHint`: CLI argument placeholders\n- `content`: Full command execution instructions\n\n### Permission Lists\n\n```typescript\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n### Hook Definitions\n\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Currently disabled - causing issues\n  // { event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' },\n  // { event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' },\n];\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // { srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }, // Disabled\n];\n```\n\n### Settings.json Schemas\n\n**Claude Code format (nested)**:\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          { \"type\": \"command\", \"command\": \"node ~/.claude/hooks/are-check-update.js\" }\n        ]\n      }\n    ]\n  },\n  \"permissions\": {\n    \"allow\": [\"Bash(npx agents-reverse-engineer@latest generate*)\"]\n  }\n}\n```\n\n**Gemini CLI format (flat)**:\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      { \"name\": \"are-check-update\", \"type\": \"command\", \"command\": \"node ~/.gemini/hooks/are-check-update.js\" }\n    ]\n  }\n}\n```\n\n### Session Hook Scripts\n\n**are-check-update.js**:\n```javascript\n#!/usr/bin/env node\n// Spawns detached process to check npm registry version\n// Writes result to ~/.claude/cache/are-update-check.json\n// Checks project/.claude/ARE-VERSION first, then global\n\nconst child = spawn(process.execPath, ['-e', `\n  // Read installed version from ARE-VERSION files\n  // Query npm view agents-reverse-engineer version\n  // Write { update_available, installed, latest, checked } to cache\n`], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n});\nchild.unref();\n```\n\n**are-session-end.js**:\n```javascript\n#!/usr/bin/env node\n// Checks ARE_DISABLE_HOOK env and config file\n// Runs git status --porcelain\n// Spawns detached: npx agents-reverse-engineer@latest update --quiet\n\nif (process.env.ARE_DISABLE_HOOK === '1') process.exit(0);\n// Check config for hook_enabled: false\n// Check git status\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n}).unref();\n```\n\n### Frontmatter Formats\n\n**Claude (Markdown with name)**:\n```yaml\n---\nname: are-generate\ndescription: Generate AI-friendly documentation for the entire codebase\n---\n```\n\n**OpenCode (Markdown with agent)**:\n```yaml\n---\ndescription: Generate AI-friendly documentation for the entire codebase\nagent: build\n---\n```\n\n**Gemini (TOML)**:\n```toml\ndescription = \"Generate AI-friendly documentation for the entire codebase\"\n# Arguments: [path] [--dry-run] [--concurrency N]\nprompt = \"\"\"\n<command content>\n\"\"\"\n```\n\n### Environment Path Overrides\n\n| Variable | Platform | Default |\n|----------|----------|---------|\n| `CLAUDE_CONFIG_DIR` | Claude | `~/.claude` |\n| `OPENCODE_CONFIG_DIR` | OpenCode | `~/.config/opencode` (or `XDG_CONFIG_HOME/opencode`) |\n| `GEMINI_CONFIG_DIR` | Gemini | `~/.gemini` |",
      "model": "claude-opus-4-5-20251101",
      "inputTokens": 9,
      "outputTokens": 26610,
      "cacheReadTokens": 718110,
      "cacheCreationTokens": 141139,
      "latencyMs": 479298,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 1,
    "totalInputTokens": 9,
    "totalOutputTokens": 26610,
    "totalCacheReadTokens": 718110,
    "totalCacheCreationTokens": 141139,
    "totalDurationMs": 479298,
    "errorCount": 0,
    "totalFilesRead": 0,
    "uniqueFilesRead": 0
  }
}