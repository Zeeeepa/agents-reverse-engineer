{
  "runId": "2026-02-09T18:32:37.712Z",
  "startTime": "2026-02-09T18:32:37.713Z",
  "endTime": "2026-02-09T18:41:34.527Z",
  "entries": [
    {
      "timestamp": "2026-02-09T18:32:37.715Z",
      "prompt": "Generate a comprehensive project specification from the following documentation.\n\n## AGENTS.md Files (29 directories)\n\n### .github/workflows/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# workflows\n\n**GitHub Actions CI/CD automation for npm package publication with Sigstore provenance attestation.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Publishes npm package on release events via `npm publish --provenance --access public` with OIDC-signed cryptographic attestation linking artifact to commit SHA.\n\n## Triggers\n\nWorkflow activates on two event types:\n- `release[published]` when GitHub release is published\n- `workflow_dispatch` for manual execution via UI\n\n## Job Pipeline\n\nSingle `publish` job on `ubuntu-latest` executes five steps:\n1. `actions/checkout@v4` clones repository at triggered commit\n2. `actions/setup-node@v4` with Node.js 20 and npm registry authentication context\n3. `npm ci` installs dependencies from lockfile\n4. `npm run build` invokes `prepublishOnly` script (`tsc` + `build:hooks`)\n5. `npm publish --provenance --access public` with `NPM_TOKEN` secret\n\n## Provenance Attestation\n\n`--provenance` flag generates Sigstore transparency log entry cryptographically binding published package to source commit SHA via OIDC token exchange (`id-token: write` permission required). Artifact consumers verify authenticity via `npm audit signatures`.\n\n## Permissions\n\n- `contents: read` — Repository checkout access\n- `id-token: write` — OIDC token generation for Sigstore signing\n\n## Environment Variables\n\n`NODE_AUTH_TOKEN` set from GitHub secret `secrets.NPM_TOKEN` via `.npmrc` injection by `actions/setup-node`.\n### AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Root Documentation\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Enumerates 25 language ecosystem manifest types (package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile, plus 16 unsupported: Gemfile, *.csproj, Package.swift, mix.exs, rebar.config, build.sbt, deps.edn, *.cabal, pubspec.yaml, *.rockspec, DESCRIPTION, Project.toml, build.zig.zon, *.nimble, dune-project) with support checkmarks indicating `extractDirectoryImports()` detection coverage. Serves as specification for manifest-aware prompt construction in Phase 2 directory aggregation.\n\n**[LICENSE](./LICENSE)** — MIT license grant for agents-reverse-engineer (copyright 2026 GeoloeG-IsT) granting unrestricted usage/modification/distribution rights with warranty disclaimer and liability waiver.\n\n**[README.md](./README.md)** — User-facing documentation covering `npx agents-reverse-engineer@latest` installation workflows (runtime selection: claude/opencode/gemini/all, scope: global `-g`/local `-l`), command reference (init/discover/generate/update/specify/clean), generated artifact formats (`.sum` with YAML frontmatter, `AGENTS.md` directories, runtime-specific roots), configuration schema (`.agents-reverse-engineer/config.yaml` with exclude patterns/vendorDirs/binaryExtensions/ai backend/concurrency/timeout/pricing), three-phase workflow (file analysis → directory aggregation → root synthesis), incremental update strategy (SHA-256 hash comparison, orphan cleanup, affected directory propagation), IDE integration hooks (session-end auto-update, version checking), requirements (Node.js ≥18.0.0, Claude Code/Gemini CLI/OpenCode support).\n\n**[package.json](./package.json)** — Package metadata: name `agents-reverse-engineer`, version `0.6.6`, license MIT. Binary entry points: `agents-reverse-engineer`/`are` → `dist/cli/index.js`. Build scripts: `build` (tsc), `build:hooks` (copies hooks/ → hooks/dist/), `prepublishOnly` (build + build:hooks), `prepack` (remove .sum artifacts), `dev` (tsx watch). Dependencies: fast-glob, ignore, isbinaryfile, ora, picocolors, simple-git, yaml, zod. DevDependencies: @types/node, tsx, typescript. Files array: dist, hooks/dist, README.md, LICENSE. Engine: Node.js ≥18.0.0.\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler configuration: target ES2022, module/moduleResolution NodeNext (native ES modules), outDir dist, rootDir src, strict type-checking enabled, declaration/declarationMap/sourceMap generation, esModuleInterop/resolveJsonModule enabled, isolatedModules enforced, include src/**, exclude node_modules/dist. Invoked by `npm run build` (tsc) and `prepublishOnly` hook.\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD GitHub Actions workflow for npm package publishing with Sigstore-signed provenance attestation on `release[published]` events. Executes `ubuntu-latest` job with `id-token: write` permission enabling cryptographic attestation linking published artifact to source commit SHA. Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`, executes `npm ci`, `npm run build` (tsc + build:hooks), publishes via `npm publish --provenance --access public` using `NPM_TOKEN` secret.\n\n**[docs/](./docs/)** — Original vision document `INPUT.md` defining RLM algorithm (post-order traversal, leaf-to-root aggregation), command contracts (`/are-generate`, `/are-update`), session lifecycle hooks (SessionStart version check, SessionEnd auto-update), multi-tier documentation system (AGENTS.md, ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md), and references to inspirational projects: SpecKit (GitHub spec toolkit), BMAD (methodology framework), GSD (workflow system).\n\n**[hooks/](./hooks/)** — IDE session lifecycle hooks implemented as detached background processes for Claude Code/Gemini CLI/OpenCode: `are-check-update.js` (SessionStart npm registry query via `npm view agents-reverse-engineer version`, cache to `~/.claude/cache/are-update-check.json` with 10s timeout), `are-session-end.js` (SessionEnd git change detection via `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` when uncommitted changes exist), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting async factories returning OpenCode plugin objects with `event['session.created']`/`event['session.deleted']` handlers). Spawned via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true }).unref()` to prevent blocking IDE exit. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml`.\n\n**[scripts/](./scripts/)** — Build automation script `build-hooks.js` copying hook source files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle via `readdirSync()` filtering `.js` files + `copyFileSync()` per file. Ensures session lifecycle hooks exist in npm tarball for `src/installer/operations.ts` reference during global/local hook installation.\n\n**[src/](./src/)** — Core implementation housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase pipeline orchestration (concurrent file analysis with iterator-based worker pools, post-order directory synthesis with dependency graphs, sequential root integration with preamble stripping), AI backend abstraction layer with Claude/Gemini/OpenCode adapters + subprocess resource management (512MB heap limits, 4-thread libuv pools, process group killing), gitignore-aware file discovery with composable filter chains (gitignore → vendor → binary → custom), SHA-256 incremental change detection with orphan cleanup, quality validation (code-vs-doc/code-vs-code inconsistencies via regex export extraction, phantom path detection via three regex patterns + `existsSync()` validation), worker pool concurrency control with shared iterator preventing over-allocation, NDJSON trace emission (phase/worker/task/subprocess/retry events with auto-populated seq/ts/pid/elapsedMs fields), platform-specific IDE integration (Claude Code skills/hooks, OpenCode commands/plugins, Gemini commands/hooks with TOML format).\n\n## Architecture\n\n### Three-Phase Pipeline Execution\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background tasks, `--disallowedTools Task` blocks subagents). Process group killing (`kill(-pid)`) terminates subprocess trees on timeout (SIGTERM at `timeoutMs`, SIGKILL escalation after 5s grace period). Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Writes `.sum` files with YAML frontmatter containing `generated_at` timestamp, `content_hash` (SHA-256), followed by markdown sections (Purpose, Public Interface, Dependencies, Implementation Notes). Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory. Prompts include aggregated child `.sum` content via `readSumFile()`, subdirectory `AGENTS.md` files via recursive traversal, import maps via `extractDirectoryImports()` with verified path constraints, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). User-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content with `---` separator. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3 (Root Document Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle), parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching (prefixes: \"now i\", \"perfect\", \"based on\", \"let me\", \"here is\", \"i'll\", \"i will\", \"great\", \"okay\", \"sure\", \"certainly\", \"alright\"; separator: `\\n---\\n` within first 500 chars; bold headers: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers) before writing output.\n\n### Incremental Update Strategy\n\nWorkflow orchestrated by `src/update/orchestrator.ts`:\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources (excludes `GENERATED_FILES` set: CLAUDE.md, AGENTS.md, ARCHITECTURE.md, etc.)\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files via `path.dirname()` until `.` or absolute path\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\nGit integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances reported):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments (WSL detection in `src/config/defaults.ts` via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))`)\n\nTimeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period, unref'd timeout handle allows Node.js exit without cleanup blocking.\n\n### Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json` aggregate per-call token counts (input/output/cacheRead/cacheCreation), costs (computed via per-model pricing from config), durations, errors. Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`. Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`. Enforces retention via `cleanupOldLogs(keepCount)` after each run (default 50 runs).\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` enabled via `--trace` flag. Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`. Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta). Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers. Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`.\n\n**Progress log:** `.agents-reverse-engineer/progress.log` human-readable streaming output mirroring console. ETA calculation via moving average of last 10 task durations. Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts. Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`.\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `src/cli/index.ts` parses args → routes to `src/cli/{command}.ts` (init/discover/generate/update/clean/specify)\n2. Commands call `src/config/loader.ts` → `loadConfig()` with Zod validation against `src/config/schema.ts`\n3. Commands call `src/discovery/run.ts` → `discoverFiles()` with four-stage filter chain (gitignore → vendor → binary → custom)\n4. `src/cli/generate.ts` calls `src/generation/orchestrator.ts` → `createOrchestrator().createPlan()`, then `src/orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `src/ai/service.ts` → `AIService.call()` → `src/ai/subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `src/generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `src/generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `src/quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `src/orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `src/cli/update.ts` calls `src/update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `src/change-detection/detector.ts` → `computeContentHash()` for SHA-256 comparison\n3. Orchestrator calls `src/update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `src/orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `src/generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `src/generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `src/installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `src/installer/paths.ts` → `getRuntimePaths()` with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`)\n3. Operations call `src/integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `src/installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n\n## Behavioral Contracts\n\n**Default vendor directories** (`src/config/defaults.ts`):\n```typescript\n['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n**Default concurrency formula** (`src/config/defaults.ts`):\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n**Export extraction regex** (`src/quality/inconsistency/code-vs-doc.ts`):\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path detection patterns** (`src/quality/phantom-paths/validator.ts`):\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**Rate limit detection patterns** (`src/ai/service.ts`):\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n**Preamble stripping patterns** (`src/orchestration/runner.ts`):\n- Prefixes (case-insensitive): `['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`\n- Separator detection: `\\n---\\n` within first 500 chars\n- Bold header detection: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers, strip if <300 chars\n\n**YAML manifest types** (`src/generation/prompts/builder.ts`):\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Trace event types** (`src/orchestration/types.ts`):\n```typescript\n'phase:start' | 'phase:end' | 'worker:start' | 'worker:end' | 'task:pickup' | 'task:done' | 'task:start' | 'subprocess:spawn' | 'subprocess:exit' | 'retry' | 'discovery:start' | 'discovery:end' | 'filter:applied' | 'plan:created' | 'config:loaded'\n```\n### docs/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# docs/\n\nINPUT.md defines the original vision, feature requirements, and Recursive Language Model (RLM) algorithm specification for agents-reverse-engineer—the foundational design document that establishes the three-phase brownfield documentation strategy (file-level `.sum` generation, directory-level `AGENTS.md` synthesis, root document convergence), CLI command contracts (`/are-generate`, `/are-update`), session lifecycle hook requirements, and integration references to complementary tools (SpecKit, BMAD, Get Shit Done).\n\n## Contents\n\n**[INPUT.md](./INPUT.md)** — Specifies RLM post-order traversal algorithm (leaf-to-root aggregation), command execution requirements via Claude Code or platform-specific alternatives, session-end auto-update hooks, multi-tier documentation system (`AGENTS.md`, `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`), and references three inspirational projects for implementation context: SpecKit (GitHub's specification toolkit), BMAD (methodology framework), GSD (workflow system).\n\n## Behavioral Contracts\n\n**RLM Algorithm Execution Order:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. File-level: Generate `{filename}.sum` for each source file leaf\n4. Directory-level: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional documentation if needed\n5. Root convergence: Continue recursive aggregation until project root is reached\n\n**CLI Command Surface:**\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files\n\n**Referenced External Projects:**\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done: https://github.com/glittercowboy/get-shit-done\n### hooks/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**IDE session lifecycle hooks for automatic ARE version checking and documentation updates, implemented as detached background processes for Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Contents\n\n### Session Lifecycle Hooks\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached background process querying npm registry via `npm view agents-reverse-engineer version`, comparing against local/global `~/.claude/ARE-VERSION`, caching results to `~/.claude/cache/are-update-check.json` with timestamp + version metadata. 10s timeout enforced. Graceful degradation on network/git failures (sets `latest: 'unknown'`).\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached background process when git working tree changes detected via `git status --porcelain`. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no parser). Silent exit on no changes.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — `AreCheckUpdate()` async factory returning OpenCode plugin with `event['session.created']` handler. Equivalent logic to `are-check-update.js` but adapted to OpenCode plugin system. Checks `~/.config/opencode/ARE-VERSION` (project-local `.opencode/ARE-VERSION` first), writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — `AreSessionEnd()` async factory returning OpenCode plugin with `event['session.deleted']` handler. Equivalent logic to `are-session-end.js` wrapped in plugin interface contract. Same disable mechanisms + git change detection + detached `npx` spawn pattern.\n\n## Architecture\n\n### Detached Background Process Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',      // discard stdout/stderr/stdin\n  detached: true,       // separate process group, survives parent exit\n  windowsHide: true     // suppress console window on Windows\n})\nchild.unref()           // allow Node.js event loop exit without waiting\n```\n\nSerialized script strings injected via `-e` flag contain inline logic (version checks, npm queries, cache writes). Parent process exits immediately after spawn, background child completes asynchronously.\n\n### Version File Resolution Priority\n\nBoth check hooks follow precedence order:\n1. **Project-local**: `<cwd>/.claude/ARE-VERSION` or `<cwd>/.opencode/ARE-VERSION`\n2. **Global install**: `~/.claude/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`\n3. **Default fallback**: `'0.0.0'` if neither exists\n\n### Cache File Format (JSON)\n\nWritten by check hooks to `~/.{claude,opencode}/cache/are-update-check.json`:\n\n```json\n{\n  \"update_available\": true,\n  \"installed\": \"0.5.2\",\n  \"latest\": \"0.6.0\",\n  \"checked\": 1738886400\n}\n```\n\nKeys: `update_available` (boolean via `installed !== latest`), `installed` (string from ARE-VERSION or `'0.0.0'`), `latest` (string from `npm view` or `'unknown'` on timeout), `checked` (Unix epoch seconds).\n\n### Disable Mechanisms\n\nSession-end hooks check two flags before execution:\n\n1. **Environment variable**: `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file substring**: `.agents-reverse-engineer.yaml` contains `'hook_enabled: false'` (raw `readFileSync()` + `String.includes()`, no YAML parser)\n\nBoth checks trigger silent exit (code 0) to prevent observable failures when user disables hooks.\n\n## Integration Points\n\n**Installed by**: `src/installer/operations.ts` copies files from `hooks/` → `~/.claude/hooks/` or `~/.config/opencode/plugins/` during `are --runtime <platform> -g` command execution.\n\n**Built by**: `scripts/build-hooks.js` duplicates `hooks/*.js` → `hooks/dist/*.js` for npm tarball inclusion during `npm run build:hooks` (invoked by `prepublishOnly`).\n\n**Consumed by**: IDE runtimes invoke hooks automatically on session lifecycle events:\n- Claude Code: calls `~/.claude/hooks/session-start/are-check-update.js`, `~/.claude/hooks/session-end/are-session-end.js`\n- Gemini CLI: equivalent paths under `~/.gemini/hooks/`\n- OpenCode: loads plugins from `~/.config/opencode/plugins/`, invokes `event['session.created']` / `event['session.deleted']` handlers\n\n## Behavioral Contracts\n\n### npm Registry Query Command\n\n```javascript\nexecSync('npm view agents-reverse-engineer version', { \n  encoding: 'utf8', \n  timeout: 10000, \n  windowsHide: true \n})\n```\n\nReturns latest published version string (e.g., `\"0.6.5\\n\"`). Timeout 10000ms enforced to prevent indefinite hangs. Throws on network failures, caught by hooks to set `latest: 'unknown'`.\n\n### Git Change Detection Command\n\n```javascript\nexecSync('git status --porcelain', { encoding: 'utf-8' })\n```\n\nReturns empty string when working tree clean, non-empty on uncommitted changes. Throws when not git repo or git unavailable, caught by session-end hooks to skip update spawn.\n\n### npx Update Invocation Command\n\n```bash\nnpx agents-reverse-engineer@latest update --quiet\n```\n\n`@latest` specifier forces npm registry fetch, bypassing local cache. `--quiet` flag suppresses terminal output (logs to `.agents-reverse-engineer/progress.log` only). Spawned as detached child to prevent blocking IDE session close.\n\n## Platform Compatibility\n\n**Path resolution**: Uses `os.homedir()` + `path.join()` for cross-platform directory construction (`~/.claude`, `~/.config/opencode`, `~/.gemini`).\n\n**Windows-specific flags**: `windowsHide: true` in `spawn()` and `execSync()` suppresses console window creation.\n\n**Node.js compatibility**: `#!/usr/bin/env node` shebang enables direct execution without explicit interpreter invocation. Requires Node.js ≥18.0.0 (same as ARE runtime requirement).\n\n## Dependencies\n\nAll four hooks use identical Node.js built-in module subset:\n- `child_process`: `spawn`, `execSync`\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n\nNo external npm dependencies — statically analyzable, zero-install executable scripts.\n### scripts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\nBuild automation scripts for npm lifecycle hooks, prepublishOnly hook asset preparation, and distribution artifact generation.\n\n## Contents\n\n### [build-hooks.js](./build-hooks.js)\nCopies hook source files from `hooks/` to `hooks/dist/` during `prepublishOnly` lifecycle. Filters `.js` files via `readdirSync()` + `endsWith('.js')` predicate, excludes `dist` directory itself, creates target via `mkdirSync(HOOKS_DIST, { recursive: true })`, performs per-file `copyFileSync()` from `HOOKS_SRC` to `HOOKS_DIST`. Invoked by `package.json` `prepublishOnly` script chain (`npm run build && npm run build:hooks`). Ensures session lifecycle hooks (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in npm tarball for installer/operations.ts reference during global/local hook installation.\n\n## Integration Points\n\n**Package.json lifecycle:**\n- `prepublishOnly` script executes `npm run build:hooks` → invokes build-hooks.js\n- Runs after TypeScript compilation (`npm run build`) before `npm publish`\n- Ensures hooks/dist/ directory populated for tarball inclusion\n\n**Hook installation workflow:**\n- `src/installer/operations.ts` references hooks/dist/ files during `installCommands()` execution\n- Supports both global (`~/.claude/hooks/`) and local (`.claude/hooks/`) installation modes\n- Platform-specific targets: Claude Code, OpenCode, Gemini via runtime detection\n\n**File resolution:**\n- Computes `projectRoot` via `fileURLToPath(import.meta.url)` + double `dirname()` traversal\n- Resolves absolute paths via `join(projectRoot, 'hooks')` and `join(projectRoot, 'hooks', 'dist')`\n- Prevents relative path ambiguity in npm lifecycle context\n### src/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Core implementation directory housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase documentation pipeline orchestration (concurrent file analysis, post-order directory synthesis, sequential root integration), AI backend abstraction layer with Claude/Gemini/OpenCode adapters, gitignore-aware file discovery with composable filter chains, incremental SHA-256 hash-based change detection, quality validation subsystem detecting code-doc inconsistencies and phantom paths, worker pool concurrency control with NDJSON trace emission, and platform-specific IDE integration supporting Claude Code/OpenCode/Gemini runtimes.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` version field from parent directory of compiled module via `import.meta.url` → `fileURLToPath()` → `join(__dirname, '..', 'package.json')` path resolution, returns `'unknown'` on parse/read errors. Consumed by CLI `--version` flag and session lifecycle hooks (`hooks/are-check-update.js`) for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service layer abstracting Claude Code, Gemini CLI, OpenCode via adapter registry. Implements exponential backoff retry (max 3 attempts, base 1s delay, 8s ceiling), rate-limit detection via stderr pattern matching (`'rate limit'`, `'429'`, `'overloaded'`), subprocess resource management with 512MB heap limits (`NODE_OPTIONS='--max-old-space-size=512'`), libuv thread pool capping (`UV_THREADPOOL_SIZE='4'`), process group killing (`kill(-pid)`) for timeout enforcement (SIGTERM at timeout, SIGKILL after 5s grace period). Emits NDJSON telemetry logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` tracking token counts (input/output/cacheRead/cacheCreation), latency, retry attempts, filesRead metadata, and aggregated cost metrics. Trace emission via `ITraceWriter` for `subprocess:spawn/exit`, `retry` events.\n\n**[change-detection/](./change-detection/)** — Git-based change detection via `simple-git` diff parsing (`git diff --name-status -M <baseCommit>..HEAD`) with rename detection (50% similarity threshold), uncommitted change merge via `git.status()`, and SHA-256 content hashing (`computeContentHash()`) for non-git workflows. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extraction. Consumed by `src/update/orchestrator.ts` for hash-based incremental updates comparing `.sum` frontmatter `content_hash` against current file content.\n\n**[cli/](./cli/)** — Command entry points implementing `init` (config creation), `discover` (preview generation plan), `generate` (three-phase pipeline execution), `update` (incremental hash-based regeneration), `clean` (artifact deletion), `specify` (project spec synthesis). Orchestrates backend resolution via `createBackendRegistry()` + `resolveBackend()`, AIService lifecycle management (`setDebug()`, `setTracer()`, `finalize()`), trace emission to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, progress logging via `ProgressLog.create()` to `.agents-reverse-engineer/progress.log`. Exit codes: 0 (success), 1 (partial failure or config exists), 2 (total failure or CLI not found).\n\n**[config/](./config/)** — YAML configuration loader with Zod validation via `ConfigSchema` (exclude patterns/vendorDirs/binaryExtensions, discovery options followSymlinks/maxFileSize, output colors, AI backend/model/timeoutMs/maxRetries/concurrency/telemetry). `getDefaultConcurrency()` computes memory-aware worker pool sizing via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))` enforcing 512MB subprocess heap constraint. `writeDefaultConfig()` generates annotated YAML with `yamlScalar()` quoting patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]`. Emits `config:loaded` trace events.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery executing four-stage filter chain (gitignore → vendor → binary → custom) over `fast-glob` traversal results with `absolute: true`, `onlyFiles: true`, `dot: true`, `followSymbolicLinks: false`, `ignore: ['**/.git/**']` hardcoded. `applyFilters()` spawns 30 workers sharing `files.entries()` iterator for bounded-concurrency sequential filter evaluation with early termination. Binary detection: extension fast-path (80+ extensions) → size threshold (1MB default) → `isBinaryFile()` content analysis. Returns `DiscoveryResult` with `included: string[]`, `excluded: ExcludedFile[]` with `reason` (gitignore pattern/binary/vendor/custom).\n\n**[generation/](./generation/)** — Three-phase pipeline orchestration: (1) concurrent file analysis via `GenerationOrchestrator.createFileTasks()` embedding `buildFilePrompt()` with import maps, depth-sorted deepest-first execution; (2) post-order directory aggregation via `buildExecutionPlan()` with dependency graphs (`isDirectoryComplete()` predicates, `dependencies: fileTaskIds`), `buildDirectoryPrompt()` consuming child `.sum` files and subdirectory `AGENTS.md`; (3) sequential root synthesis via `collectAgentsDocs()` recursive traversal, `buildRootPrompt()` with stack detection (9 manifest types), preamble stripping. Exports `writeSumFile()`/`readSumFile()` for YAML frontmatter serialization with `content_hash`, `writeAgentsMd()` preserving user content via `AGENTS.local.md` rename, `writeAnnexFile()` for reproduction-critical source content.\n\n**[imports/](./imports/)** — Static import analysis via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) matching ES module imports in first 100 lines. Classifies as `internal` (`./` prefix) or `external` (`../` prefix), filters bare package specifiers and `node:` built-ins. `formatImportMap()` produces text blocks with filename headers and indented specifier-symbol pairs for LLM prompt embedding in Phase 1 file analysis and Phase 2 directory aggregation.\n\n**[installer/](./installer/)** — npx-based installation orchestrator for ARE commands/hooks across Claude Code (`~/.claude/skills/`, `~/.claude/hooks/`), OpenCode (`~/.config/opencode/commands/`, `~/.config/opencode/plugins/`), Gemini (`~/.gemini/commands/`, `~/.gemini/hooks/`) with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`/`XDG_CONFIG_HOME`, `GEMINI_CONFIG_DIR`). Interactive prompts via `arrowKeySelect()` TTY mode with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line, `\\x1b[1B` cursor down) or `numberedSelect()` fallback. `registerHooks()` modifies `settings.json` with nested structure for Claude (`{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command }] }] } }`), flat structure for Gemini (`{ hooks: { SessionStart?: [{ name, type, command }] } }`). `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns (`'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`). Uninstallation via `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments via marker files (`.claude/`, `.opencode/`, `.gemini/`, `.aider/`, `CLAUDE.md`, `.aider.conf.yml`). `generateIntegrationFiles()` writes command templates with progress-monitoring patterns (background execution via `run_in_background: true`, 15s poll intervals, offset-based `.agents-reverse-engineer/progress.log` tailing, `TaskOutput` checks with `block: false`). `getTemplatesForEnvironment()` generates seven commands (generate/update/init/discover/clean/specify/help) with platform-specific frontmatter (Claude: `name: /are-<command>`, OpenCode: `agent: build`, Gemini: TOML `description`/`prompt`).\n\n**[orchestration/](./orchestration/)** — Worker pool concurrency control via iterator-based `runPool()` sharing `tasks.entries()` iterator across N workers preventing over-allocation. `CommandRunner` orchestrates three-phase pipeline: Phase 1 concurrent file analysis with `sourceContentCache` Map for single read, `computeContentHashFromString()` for `.sum` frontmatter, annex file detection via `## Annex References` marker; post-Phase 1 quality validation (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)` (duplicates); Phase 2 depth-grouped directory synthesis with `buildDirectoryPrompt()` runtime construction; post-Phase 2 phantom path validation via `checkPhantomPaths()` with three regex patterns; Phase 3 sequential root synthesis with preamble stripping. `ProgressReporter` computes ETA via sliding window (size 10, min 2 completions) as `~Ns remaining` or `~Mm Ss remaining`. `PlanTracker` serializes checkbox updates via promise-chain writes replacing `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``. `TraceWriter` appends NDJSON with auto-populated `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta) to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, retains 500 most recent traces.\n\n**[output/](./output/)** — Terminal output formatting layer providing color-aware `Logger` interface with picocolors ANSI styling. `createLogger(options)` constructs logger respecting `options.colors` boolean flag, exports `createSilentLogger()` returning no-op logger for testing. Methods: `info(message)`, `file(path)` (`\"  +\" + path` green), `excluded(path, reason, filter)` (`\"  -\" + path + \" (reason: filter)\"` dimmed), `summary(included, excluded)` (bold/dim counts), `warn(message)` (`\"Warning: \"` yellow prefix), `error(message)` (`\"Error: \"` red prefix).\n\n**[quality/](./quality/)** — Post-generation validation system detecting: (1) code-vs-doc inconsistencies via `extractExports()` regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching against substring search in `.sum` text; (2) code-vs-code duplicates aggregating exports into `Map<symbol, paths[]>` per directory; (3) phantom paths via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`) with `existsSync()` validation and `.ts`/`.js` fallback resolution. `buildInconsistencyReport()` aggregates discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `severity: 'info' | 'warning' | 'error'`, `formatReportForCli()` renders plain text with severity tags.\n\n**[specify/](./specify/)** — Project specification synthesis from `AGENTS.md` corpus via `buildSpecPrompt()` constructing two-part prompts: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization), user content (concatenated AGENTS.md with section delimiters, optional annex files for verbatim reproduction of behavioral contracts). `writeSpec()` supports single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) modes with pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `/^(?=# )/m` regex, filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering).\n\n**[types/](./types/)** — Shared interfaces for discovery results: `ExcludedFile` (path, reason), `DiscoveryResult` (files, excluded), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram). Consumed by `src/discovery/run.ts` (producer), `src/cli/*.ts` (consumers), `src/output/logger.ts` (terminal output).\n\n**[update/](./update/)** — Incremental update orchestrator implementing hash-based change detection via `preparePlan()` reading `.sum` frontmatter `content_hash`, comparing against `computeContentHash()` current file content, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (depth-sorted descending). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles). `getAffectedDirectories()` walks parent chains via `path.dirname()` until `.` or absolute path, ensuring `AGENTS.md` regeneration propagates upward. Emits `phase:start/end`, `plan:created` trace events.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker spawns AI CLI subprocess via `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits (512MB heap, 4-thread libuv pool, background tasks disabled, subagents blocked). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash`, strips conversational preamble via patterns matching `'now i'`, `'let me'`, `'here is'` prefixes and bold headers without `##` markers. Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for `isDirectoryComplete()` predicate checking all child `.sum` files exist. Builds prompts via `buildDirectoryPrompt()` aggregating child `.sum` content, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). Preserves user content via `AGENTS.local.md` rename, prepends above generated sections with `---` separator.\n\n**Phase 3 (Sequential Root Synthesis):** Collects all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle). Builds root prompts via `buildRootPrompt()` consuming aggregated docs, enforcing synthesis-only constraints (no feature/hook/pattern invention), strips conversational preamble before writing `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` platform-specific integration documents.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` reads `content_hash` from `.sum` frontmatter, computes current file hash via `computeContentHash()`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` for deleted/renamed sources. `getAffectedDirectories()` walks parent chains computing depth-sorted set for `AGENTS.md` regeneration propagation. Phase 1 pool regenerates `.sum` for `filesToAnalyze`, Phase 2 regenerates `AGENTS.md` for `affectedDirs` sequentially without full post-order traversal.\n\n## Behavioral Contracts\n\n### Default Vendor Directories (config/defaults.ts)\n\n```typescript\nDEFAULT_VENDOR_DIRS = ['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n### Default Concurrency Formula (config/defaults.ts)\n\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n### Preamble Detection Patterns (orchestration/runner.ts)\n\nFull patterns preserved in [orchestration/runner.ts.annex.md](./orchestration/runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Patterns (quality/phantom-paths/validator.ts)\n\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n### Rate Limit Patterns (ai/service.ts)\n\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n### Trace Event Types (orchestration/types.ts)\n\n`phase:start/end`, `worker:start/end`, `task:pickup/done`, `task:start`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n### Settings.json Schemas (installer/operations.ts)\n\n**Claude:**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini:**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `cli/index.ts` parses args → routes to `cli/{command}.ts`\n2. Command modules call `config/loader.ts` → `loadConfig()` with Zod validation\n3. Commands call `discovery/run.ts` → `discoverFiles()` with filter chain\n4. `cli/generate.ts` calls `generation/orchestrator.ts` → `createOrchestrator().createPlan()`, `orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `ai/service.ts` → `AIService.call()` → `subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `cli/update.ts` calls `update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `change-detection/detector.ts` → `computeContentHash()` for comparison\n3. Orchestrator calls `update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `installer/paths.ts` → `getRuntimePaths()` with environment overrides\n3. Operations call `integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n### src/ai/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nAI service orchestration layer abstracting Claude Code, Gemini, and OpenCode CLIs through backend adapters, exponential backoff retry, subprocess resource management, telemetry logging, and timeout enforcement for concurrent file analysis pools.\n\n## Contents\n\n### Core Orchestration\n\n**[service.ts](./service.ts)** — AIService class coordinating backend selection, subprocess execution via `runSubprocess()`, retry logic via `withRetry()`, telemetry accumulation via `TelemetryLogger`, trace emission via `ITraceWriter`, rate-limit detection matching stderr patterns (`'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`), optional debug logging with heap/RSS metrics via `formatBytes()`, and fire-and-forget subprocess log serialization via `enqueueSubprocessLog()` promise chain.\n\n**[registry.ts](./registry.ts)** — BackendRegistry managing adapter map with `register()`, `get()`, `getAll()` methods. Exports `createBackendRegistry()` instantiating registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. Exports `resolveBackend(registry, requested)` implementing auto-detection via `detectBackend()` iterating `backend.isAvailable()` or throwing `AIServiceError` with code `'CLI_NOT_FOUND'` and aggregated install instructions via `getInstallInstructions()`.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess(command, args, options)` spawning `execFile()` child processes with stdin piping, timeout enforcement via SIGTERM at `options.timeoutMs`, SIGKILL escalation after 5s grace period, process group killing via `kill(-pid)`, active subprocess tracking in module Map enabling `getActiveSubprocessCount()` and `getActiveSubprocesses()` inspection, and `onSpawn(pid)` callback at spawn time.\n\n**[retry.ts](./retry.ts)** — `withRetry(fn, options)` exponential backoff wrapper executing async function with delay formula `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` (jitter uniform random 0-500ms). Exports `DEFAULT_RETRY_OPTIONS` constant with `maxRetries: 3`, `baseDelayMs: 1000`, `maxDelayMs: 8000`, `multiplier: 2`. Retry flow controlled by `options.isRetryable(error)` predicate and optional `options.onRetry(attempt, error)` callback.\n\n**[types.ts](./types.ts)** — Type definitions: `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` contract. `AIResponse` normalized response shape with `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`. `AICallOptions` with `prompt`, `systemPrompt`, `model`, `timeoutMs`, `maxTurns`, `taskLabel`. `SubprocessResult` with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. `TelemetryEntry` per-call metadata, `RunLog` aggregated run structure, `FileRead` file metadata, `RetryOptions` retry config, `AIServiceError` typed error with `code` discriminator (`'CLI_NOT_FOUND'`, `'TIMEOUT'`, `'PARSE_ERROR'`, `'SUBPROCESS_ERROR'`, `'RATE_LIMIT'`).\n\n**[index.ts](./index.ts)** — Barrel export aggregating public API: AIService, BackendRegistry, resolveBackend, detectBackend, createBackendRegistry, withRetry, runSubprocess, isCommandOnPath, all types from `./types.js`, AIServiceOptions from `./service.js`.\n\n## Subdirectories\n\n**[backends/](./backends/)** — AIBackend adapter implementations: ClaudeBackend with Zod validation via `ClaudeResponseSchema`, GeminiBackend stub throwing `SUBPROCESS_ERROR` until JSON format stabilizes, OpenCodeBackend stub throwing `SUBPROCESS_ERROR` until JSONL parsing implemented. Shared `isCommandOnPath()` utility scanning `process.env.PATH` with Windows `PATHEXT` support.\n\n**[telemetry/](./telemetry/)** — Telemetry subsystem: TelemetryLogger accumulating TelemetryEntry instances, `writeRunLog()` persisting JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json`, `cleanupOldLogs()` enforcing retention via lexicographic filename sorting.\n\n## Architecture\n\n### Backend Adapter Pattern\n\nAIBackend interface decouples CLI invocation from backend-specific argument construction and JSON parsing. Registry selects backend at runtime via `config.ai.backend` field (`'claude'` | `'gemini'` | `'opencode'` | `'auto'`). Auto-detection calls `isAvailable()` on each backend in registration order (Claude → Gemini → OpenCode) until first CLI found on PATH.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Retry Strategy\n\n`AIService.call()` wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts excluded). Rate-limit detection via `isRateLimitStderr()` substring matching. Exponential backoff adds uniform jitter (0-500ms) preventing thundering herd when multiple workers hit rate limits simultaneously.\n\n### Telemetry Accumulation\n\nTelemetryLogger maintains in-memory `entries: TelemetryEntry[]` array throughout CLI run. After each `AIService.call()`, logger records timestamp, prompt, response, model, token counts (input/output/cacheRead/cacheCreation), latency, exitCode, retryCount, and filesRead metadata. On run completion, `logger.toRunLog()` serializes entries plus computed summary (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, uniqueFilesRead) to `RunLog` JSON structure. `writeRunLog()` persists with ISO-8601-derived filename, `cleanupOldLogs()` enforces retention by deleting oldest logs exceeding `config.ai.telemetry.keepRuns` threshold.\n\n### Timeout Enforcement\n\n`runSubprocess()` sends SIGTERM at `timeoutMs`, schedules unref'd SIGKILL timer at `timeoutMs + 5000ms` for hung processes ignoring SIGTERM. Sets `SubprocessResult.timedOut = true` when `execFile` error has `killed: true` property. AIService throws `AIServiceError('TIMEOUT')` on timeout detection (non-retryable per resource constraint mitigation).\n\n### Trace Emission\n\nAIService invokes `tracer.emit()` for `subprocess:spawn` at `onSpawn` callback time (includes `childPid`, `taskLabel`, `command`, `args`), `subprocess:exit` after completion (includes `exitCode`, `signal`, `durationMs`, `timedOut`), and `retry` events before delay (includes `attempt`, `taskLabel`, `errorCode`). ITraceWriter from `src/orchestration/trace.ts` provides promise-chain serialization ensuring NDJSON line order matches emission order despite concurrent workers.\n\n## Behavioral Contracts\n\n### Rate Limit Patterns\n\n```typescript\nconst RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded'];\n```\n\n### Default Retry Configuration\n\n```typescript\nDEFAULT_RETRY_OPTIONS = {\n  maxRetries: 3,\n  baseDelayMs: 1_000,\n  maxDelayMs: 8_000,\n  multiplier: 2\n}\n```\n\n### SIGKILL Grace Period\n\n```typescript\nconst SIGKILL_GRACE_MS = 5_000;\n```\n\n### Subprocess maxBuffer\n\n```typescript\nmaxBuffer: 10 * 1024 * 1024  // 10MB stdout/stderr capture limit\n```\n\n## Integration Points\n\n**Consumed by:**\n- `src/orchestration/runner.ts` — Creates AIService instance, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each task, calls `finalize()` at run end\n- `src/generation/executor.ts` — Handles AIServiceError codes for failure modes, extracts error messages for progress reporting\n\n**Imports from:**\n- `src/orchestration/trace.ts` — ITraceWriter interface for trace event emission\n- `src/config/schema.ts` — AIServiceOptions validation schema\n\n**Exports to:**\n- `src/cli/*.ts` — All CLI commands import AIService, createBackendRegistry, resolveBackend from `./ai/index.js`\n### src/ai/backends/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**AI backend adapter layer implementing AIBackend interface for Claude Code, Gemini, and OpenCode CLIs with PATH detection, argument construction, JSON response parsing, and installation instructions.**\n\n## Contents\n\n### Backend Adapters\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection, buildArgs() permission bypass flags (`--permission-mode bypassPermissions`, `--no-session-persistence`), parseResponse() with ClaudeResponseSchema Zod validation, defensive JSON extraction via stdout.indexOf('{'), and modelUsage-based model name extraction.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() JSON output flags, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until stable JSON format available.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() returning `['run', '--format', 'json']`, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until JSONL parsing implemented.\n\n## Architecture\n\n### AIBackend Interface Contract\n\nAll backends implement four methods:\n- `isAvailable(): Promise<boolean>` — CLI executable detection via PATH scanning\n- `buildArgs(options: AICallOptions): string[]` — Argument array construction for subprocess spawn\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — JSON parsing with usage extraction\n- `getInstallInstructions(): string` — NPM/curl install commands for missing CLI\n\n### PATH Detection Algorithm\n\n`isCommandOnPath(command: string)` shared utility (exported from claude.ts):\n1. Parse `process.env.PATH` with `path.delimiter` (`;` Windows, `:` Unix)\n2. Parse `process.env.PATHEXT` on Windows for executable extensions (`.exe`, `.cmd`, `.bat`)\n3. Nested loop: directory × extension, construct candidate via `path.join(dir, command + ext)`\n4. Check `(await fs.stat(candidate)).isFile()` — return `true` on first match\n5. Uses `fs.stat()` not `fs.access(X_OK)` for Windows compatibility (no execute permission bits)\n\n### Response Parsing Strategy\n\n**ClaudeBackend (functional):**\n- Defensive JSON extraction: `stdout.substring(stdout.indexOf('{'))` skips upgrade notices\n- Zod validation against ClaudeResponseSchema from Claude CLI v2.1.31 output format\n- Extracts model name from first `modelUsage` object key (defaults to `'unknown'`)\n- Returns AIResponse with normalized `{ result, usage, modelName, durationMs }` structure\n- Throws AIServiceError with `'PARSE_ERROR'` code on validation failure\n\n**GeminiBackend/OpenCodeBackend (stubs):**\n- Unconditionally throw AIServiceError with `'SUBPROCESS_ERROR'` code in parseResponse()\n- Block backend usage until JSON format stabilizes (Gemini) or JSONL parsing implemented (OpenCode)\n- Demonstrate extension pattern for future implementations\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nFixed arguments returned by buildArgs():\n```\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n```\n\nConditional arguments from AICallOptions:\n- `'--model', options.model` when model specified\n- `'--system-prompt', options.systemPrompt` when systemPrompt provided\n- `'--max-turns', String(options.maxTurns)` when maxTurns defined\n\nPrompt delivered via stdin by `runSubprocess()` wrapper in `src/ai/subprocess.ts`.\n\n### ClaudeResponseSchema Structure\n\nZod schema validated against Claude CLI v2.1.31:\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n### Installation Instructions\n\n**ClaudeBackend:** `npm install -g @anthropic-ai/claude-code`  \n**GeminiBackend:** `npm install -g @anthropic-ai/gemini-cli` + `https://github.com/google-gemini/gemini-cli`  \n**OpenCodeBackend:** `curl -fsSL https://opencode.ai/install | bash` + `https://opencode.ai`\n\n## Integration Points\n\nBackends registered in `src/ai/registry.ts` AIBackendRegistry map with keys `'claude'`, `'gemini'`, `'opencode'`. Registry consumed by AIService in `src/ai/service.ts` for backend selection via `ai.backend` config field. Auto-detection iterates backends calling `isAvailable()` until first match.\n\n`buildArgs()` output consumed by `runSubprocess()` in `src/ai/subprocess.ts` for child_process.execFile() argument array. `parseResponse()` receives stdout/durationMs/exitCode from subprocess wrapper for normalization into AIResponse structure.\n### src/ai/telemetry/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nTelemetry subsystem for accumulating AI service call metrics, writing JSON run logs with retention management, and tracking token costs, durations, and file metadata across concurrent worker pool operations.\n\n## Contents\n\n### [cleanup.ts](./cleanup.ts)\nExports `cleanupOldLogs(projectRoot, keepCount)` deleting expired telemetry logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting on `run-*.json` filenames. Returns count of deleted files.\n\n### [logger.ts](./logger.ts)\nExports `TelemetryLogger` class accumulating `TelemetryEntry` instances in memory during CLI runs. Methods: `addEntry()` appends call metrics, `setFilesReadOnLastEntry()` attaches file metadata to most recent entry, `getSummary()` computes aggregate statistics (token totals, error counts, unique file paths), `toRunLog()` serializes complete `RunLog` object.\n\n### [run-log.ts](./run-log.ts)\nExports `writeRunLog(projectRoot, runLog)` persisting `RunLog` as pretty-printed JSON with ISO-8601-derived filename pattern `run-2026-02-07T12-00-00-000Z.json`. Creates `.agents-reverse-engineer/logs/` directory via `mkdir(recursive: true)`.\n\n## Data Flow\n\n1. `TelemetryLogger` instantiated once per CLI invocation with unique `runId` (ISO timestamp)\n2. AIService calls `logger.addEntry(entry)` after each subprocess completion with token counts, cost, duration, error status\n3. Command runner invokes `logger.setFilesReadOnLastEntry(filesRead)` to attach file metadata to last entry\n4. On run completion, caller invokes `logger.toRunLog()` → serializes in-memory entries + summary\n5. `writeRunLog(projectRoot, runLog)` persists JSON to disk with timestamp-derived filename\n6. `cleanupOldLogs(projectRoot, keepCount)` enforces retention policy by deleting oldest logs exceeding `keepRuns` threshold (default 50)\n\n## Behavioral Contracts\n\n**Filename derivation pattern** (from `run-log.ts`):\n```typescript\nconst timestamp = runLog.startTime.replace(/:/g, '-').replace(/\\./g, '-');\nconst filename = `run-${timestamp}.json`;\n```\n\n**Log directory constant** (shared across all modules):\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n```\n\n**Cleanup sort order** (from `cleanup.ts`):\n```typescript\nentries.sort();      // Lexicographic ascending\nentries.reverse();   // Newest first\nconst toDelete = entries.slice(keepCount);  // Oldest logs\n```\n\n**Summary deduplication** (from `logger.ts`):\n```typescript\nconst uniquePaths = new Set<string>();\nfor (const entry of this.entries) {\n  for (const file of entry.filesRead || []) {\n    uniquePaths.add(file.path);\n  }\n}\nreturn { uniqueFilesRead: uniquePaths.size };\n```\n\n## Integration Points\n\n- **Created by**: `src/ai/service.ts` constructs `TelemetryLogger` when `config.ai.telemetry.enabled === true`\n- **Consumed by**: `src/orchestration/runner.ts` invokes `writeRunLog()` after Phase 3 completion, then calls `cleanupOldLogs()` with `config.ai.telemetry.keepRuns`\n- **Types imported from**: `../types.js` provides `TelemetryEntry`, `RunLog`, `FileRead` interfaces\n- **Related configuration**: `src/config/schema.ts` defines `ai.telemetry.enabled`, `ai.telemetry.keepRuns`, `ai.telemetry.costThresholdUsd` validation rules\n\n## Design Notes\n\n`TelemetryLogger` uses eager summary computation on every `getSummary()` invocation without caching — acceptable since `toRunLog()` called once per run. Single-level flat `entries` array without phase-aware partitioning simplifies aggregation logic at cost of phase-specific analytics. Cleanup operates on filename lexicographic order assuming ISO 8601 timestamp sort equivalence (year-month-day-hour-minute-second-millisecond).\n### src/change-detection/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# change-detection\n\nGit-based change detection via simple-git diff parsing and SHA-256 content hashing for incremental documentation updates.\n\n## Contents\n\n**[detector.ts](./detector.ts)** — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff status parsing (A/M/D/R), `computeContentHash()` SHA-256 hashing, uncommitted change merge via `git.status()`.\n\n**[types.ts](./types.ts)** — `ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions` interfaces defining change detection contracts.\n\n**[index.ts](./index.ts)** — Barrel export aggregating detector functions and types.\n\n## Change Detection Algorithm\n\n`getChangedFiles()` executes `git diff --name-status -M <baseCommit>..HEAD` parsing output format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW`. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with 50% similarity threshold.\n\nWhen `ChangeDetectionOptions.includeUncommitted` is true, merges `git.status()` results reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Deduplicates via `changes.some(c => c.path === file)` predicate to prevent duplicate `FileChange` entries for files in both committed diff and working tree.\n\n## Content Hashing\n\n`computeContentHash()` reads file via Node.js `readFile()`, computes SHA-256 via `createHash('sha256').update(content).digest('hex')`. Hash stored in `.sum` YAML frontmatter `content_hash` field. `computeContentHashFromString()` provides synchronous variant for pre-loaded content avoiding redundant disk reads.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.\n\n## Behavioral Contracts\n\n**Git diff status codes:**\n- `A` — added file\n- `M` — modified file\n- `D` — deleted file\n- `R*` — renamed file with similarity score (e.g., `R100`)\n\n**Rename detection similarity threshold:** 50% (implicit via `git diff -M`)\n\n**SHA-256 hash format:** Hex-encoded string matching regex `/^[a-f0-9]{64}$/`\n### src/cli/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand entry points implementing CLI argument parsing, command routing, and orchestration of AI-driven documentation generation workflows (init, discover, generate, update, clean, specify) via backend abstraction, concurrent execution pools, and incremental hash-based updates.\n\n## Commands\n\n### [clean.ts](./clean.ts)\n`cleanCommand(targetPath, {dryRun})` deletes `.sum`, `.annex.md`, `AGENTS.md` (marker-filtered), `CLAUDE.md`, `GENERATION-PLAN.md` via parallel `fast-glob` discovery, restores `AGENTS.local.md` → `AGENTS.md`, logs deletions/skips.\n\n### [discover.ts](./discover.ts)\n`discoverCommand(targetPath, {tracer, debug})` walks directory via `discoverFiles()` filter chain, writes file list to `progress.log`, generates `GENERATION-PLAN.md` via `buildExecutionPlan()` post-order traversal, emits `discovery:start`/`discovery:end` trace events.\n\n### [generate.ts](./generate.ts)\n`generateCommand(targetPath, {dryRun, concurrency, failFast, debug, trace})` orchestrates three-phase pipeline: concurrent `.sum` file analysis via `CommandRunner.executeGenerate()`, directory `AGENTS.md` aggregation, root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), exits with codes 0 (success), 1 (partial failure), 2 (total failure).\n\n### [index.ts](./index.ts)\nCLI entry point with shebang `#!/usr/bin/env node`, implements `parseArgs()` flag parser supporting `--dry-run`, `--concurrency <n>`, `--fail-fast`, `--debug`, `--trace`, `--uncommitted`, `--force`, `--multi-file`, routes to command modules, triggers `runInstaller()` on install/uninstall/installer flags.\n\n### [init.ts](./init.ts)\n`initCommand(root, {force})` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks `configExists()` to prevent overwrites unless `force` enabled, exits with code 1 on `EACCES`/`EPERM` permission errors.\n\n### [specify.ts](./specify.ts)\n`specifyCommand(targetPath, {output, force, dryRun, multiFile, debug, trace})` synthesizes project specs from `AGENTS.md` corpus via `collectAgentsDocs()`, auto-generates missing docs via `generateCommand()`, invokes `AIService.call()` with 600s minimum timeout, writes via `writeSpec()` to `specs/SPEC.md` or custom path.\n\n### [update.ts](./update.ts)\n`updateCommand(targetPath, {uncommitted, dryRun, concurrency, failFast, debug, trace})` computes delta via SHA-256 hash comparison in `preparePlan()`, cleans orphaned artifacts, regenerates `.sum` for changed files via `runner.executeUpdate()`, rebuilds `AGENTS.md` for `affectedDirs` sequentially, logs to `progress.log`.\n\n## Execution Patterns\n\n**Dry-run mode** (`--dry-run`): clean/generate/update/specify commands display plans without filesystem writes or AI calls. generate shows file/directory/root counts with `buildExecutionPlan()`, specify estimates tokens via `Math.ceil(totalChars / 4) / 1000`.\n\n**Progress monitoring**: generate/update/specify create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO timestamps, file counts, task status. Enables `tail -f` real-time monitoring.\n\n**Trace emission** (`--trace`): generate/update/specify instantiate `createTraceWriter()` before config loading, pass `tracer` to all orchestrator calls, emit NDJSON events (`phase:start`/`end`, `worker:start`/`end`, `task:pickup`/`done`, `subprocess:spawn`/`exit`, `retry`), call `cleanupOldTraces()` after `tracer.finalize()`.\n\n**Backend resolution**: generate/update/specify call `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)`, catch `AIServiceError` with `code === 'CLI_NOT_FOUND'`, print `getInstallInstructions(registry)`, exit with code 2.\n\n**AI service lifecycle**: generate/update/specify instantiate `AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns})`, call `setDebug(true)` if `--debug`, call `setSubprocessLogDir()` if `--trace`, invoke via `aiService.call()` or `runner.executeGenerate()`/`runner.executeUpdate()`, finalize via `aiService.finalize(absolutePath)` writing run logs to `.agents-reverse-engineer/logs/`.\n\n**Installer routing**: index.ts triggers `runInstaller()` on three conditions: (1) zero args (interactive mode), (2) installer flags (`--global`, `--local`, `--runtime`, `--force`) without command, (3) explicit `install`/`uninstall` command. Passes `parseInstallerArgs(args)` with `uninstall: true` for uninstall command.\n\n## Exit Code Strategy\n\n**generate/update**:\n- Code 2: total failure (`filesProcessed === 0 && filesFailed > 0`) or CLI not found\n- Code 1: partial failure (`filesFailed > 0` with some success)\n- Code 0: full success (`filesFailed === 0`)\n\n**specify**:\n- Code 2: `CLI_NOT_FOUND` error from `resolveBackend()`\n- Code 1: `SpecExistsError` (output exists without `--force`) or empty docs after auto-generation\n- Code 0: successful write\n\n**init**:\n- Code 1: `EACCES`/`EPERM` permission error or write failure\n- Code 0: config created successfully\n\n**clean**: No explicit exit codes (relies on thrown errors).\n\n## File Relationships\n\n- index.ts imports all command modules, routes via switch statement on `command` string\n- generate.ts imports `discoverFiles()` from `../discovery/run.js`, `createOrchestrator()` from `../generation/orchestrator.js`, `buildExecutionPlan()` from `../generation/executor.js`, `AIService` from `../ai/index.js`, `CommandRunner` from `../orchestration/index.js`\n- update.ts imports `createUpdateOrchestrator()` from `../update/index.js`, `buildDirectoryPrompt()` from `../generation/prompts/index.js`, `writeAgentsMd()` from `../generation/writers/agents-md.js`\n- specify.ts imports `collectAgentsDocs()` from `../generation/collector.js`, `buildSpecPrompt()` from `../specify/index.js`, calls `generateCommand()` from `./generate.js` for auto-generation fallback\n- clean.ts imports `GENERATED_MARKER` from `../generation/writers/agents-md.js` for user-authored AGENTS.md detection\n- discover.ts imports `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` for `GENERATION-PLAN.md` rendering\n- init.ts imports `configExists()`, `writeDefaultConfig()` from `../config/loader.js`\n\n## Behavioral Contracts\n\n**Argument parsing** (index.ts `parseArgs()`):\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg lacks `--` prefix\n- Short flags expanded: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- First non-flag arg becomes `command`, subsequent populate `positional: string[]`\n\n**Token estimation** (specify.ts dry-run):\n```typescript\nestimatedTokensK = Math.ceil(totalChars / 4) / 1000\n```\n\n**Plan formatting** (update.ts `formatPlan()`):\n- Status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- First run: `plan.isFirstRun === true` → suggests `are generate`\n- Empty plan: all counts zero → \"No changes detected since last run\"\n\n**Cleanup patterns** (clean.ts):\n- Parallel `fast-glob` with patterns: `**/*.sum`, `**/*.annex.md`, `**/AGENTS.md`, `**/AGENTS.local.md`\n- Ignore patterns: `['**/node_modules/**', '**/.git/**']`\n- Marker detection: `content.includes(GENERATED_MARKER)` where `GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->'`\n\n**Telemetry summary** (specify.ts):\n```typescript\nsummaryLine = `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs/1000).toFixed(1)}s | Output: ${outputPath}`\n```\n\n**Subprocess timeout override** (specify.ts):\n```typescript\ntimeoutMs = Math.max(config.ai.timeoutMs, 600_000) // minimum 10 minutes\n```\n### src/config/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration loading, validation, and default value computation for `.agents-reverse-engineer/config.yaml`, including Zod schema enforcement, platform-adaptive concurrency calculation, and annotated YAML file generation with memory-bounded worker pool sizing.\n\n## Contents\n\n### Schema Definition\n\n**[schema.ts](./schema.ts)** — `ConfigSchema` root validator composed of `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors), `AISchema` (backend/model/timeoutMs/maxRetries/concurrency/telemetry). Exports inferred types: `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`. All schemas chain `.default()` enabling partial parse with full default population.\n\n**[defaults.ts](./defaults.ts)** — Exports `DEFAULT_VENDOR_DIRS` (18 directories: node_modules, .git, dist, build, target, .next, __pycache__, venv, .cargo, .gradle, .agents-reverse-engineer, .agents, .planning, .claude, .opencode, .gemini), `DEFAULT_EXCLUDE_PATTERNS` (lock files, dotfiles, logs, AI-generated docs, *.sum files), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images, archives, executables, media, documents, fonts, bytecode), `DEFAULT_MAX_FILE_SIZE` (1048576 bytes). `getDefaultConcurrency()` computes `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` where `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint allocating 50% RAM divided by 512MB subprocess heap limit.\n\n**[loader.ts](./loader.ts)** — `loadConfig(root, options?)` reads `config.yaml` from `.agents-reverse-engineer/`, parses via `yaml.parse()`, validates with `ConfigSchema`, returns defaults on `ENOENT`, throws `ConfigError` wrapping `ZodError` with formatted issue paths. Emits `config:loaded` trace events with configPath/model/concurrency fields. `writeDefaultConfig(root)` creates annotated YAML with five comment-delimited sections (exclusions, discovery options, output formatting, AI service), interpolates defaults via spread operators, quotes patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]` via `yamlScalar()`, comments out concurrency line showing override syntax.\n\n## Configuration Structure\n\nThe config file divides into four sections:\n\n- **exclude**: File/directory exclusion rules with glob patterns, vendor directory names, binary extension list\n- **options**: Discovery behavior toggles (followSymlinks) and thresholds (maxFileSize in bytes)\n- **output**: Terminal formatting preferences (ANSI colors)\n- **ai**: AI service configuration (backend selection, model override, subprocess timeout, retry limits, worker pool concurrency, telemetry retention)\n\n## Memory-Aware Concurrency\n\n`getDefaultConcurrency()` implements memory-bounded worker pool sizing preventing RAM exhaustion where `cores × 5` spawns too many 512MB subprocesses. Computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` respecting memory constraint alongside schema maximum. Fallback uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems.\n\n## Behavioral Contracts\n\n### Schema Validation Constraints\n\n- AISchema concurrency: `z.number().min(1).max(20)` enforces 1-20 worker pool range\n- OptionsSchema maxFileSize: `z.number().positive()` rejects zero/negative thresholds\n- AISchema backend: `z.enum(['claude', 'gemini', 'opencode', 'auto'])` limits backend values\n- AISchema telemetry.keepRuns: `z.number().min(0)` allows zero for unlimited retention\n\n### YAML Metacharacter Pattern\n\n`yamlScalar()` tests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern, double-quotes and backslash-escapes `\\` → `\\\\`, `\"` → `\\\"` when matched.\n\n### Default Formulas\n\n- Concurrency: `clamp(cores × 5, 2, min(floor((totalMemGB × 0.5) / 0.512), 20))`\n- Memory cap: `floor((os.totalmem() / 1024³ × 0.5) / 0.512)` concurrent processes\n- Timeout default: `300_000` milliseconds (5 minutes)\n\n### Error Message Format\n\nConfigError validation failures: `\"ai.concurrency: Expected number\"` (field path colon-separated from Zod issue message, joined by newlines for multiple issues).\n\n### Trace Event Schema\n\n`config:loaded` payload: `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }` emitted twice per `loadConfig()` call (once for file path via `path.relative(root, configPath)`, once for defaults with literal `'(defaults)'`).\n\n## File Relationships\n\n- **schema.ts** → **defaults.ts**: Imports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` for `.default()` chaining\n- **loader.ts** → **schema.ts**: Imports `ConfigSchema` for parse, `Config` type for return annotation\n- **loader.ts** → **defaults.ts**: Imports all defaults for YAML interpolation in `writeDefaultConfig()`\n- **loader.ts** → **../orchestration/trace.ts**: Imports `ITraceWriter` for `config:loaded` event emission\n### src/discovery/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery system executing four-stage filter chain (gitignore, vendor, binary, custom) over fast-glob traversal results with bounded-concurrency processing (30 workers), early-termination optimization, and per-filter telemetry tracking.**\n\n## Contents\n\n**[run.ts](./run.ts)** — `discoverFiles()` orchestrates filter chain construction via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, invokes `walkDirectory()` with `followSymlinks` flag, delegates filtering to `applyFilters()` with trace/debug options.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(): boolean|Promise<boolean>`), `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]`, `WalkerOptions` for traversal config, `ExcludedFile` audit record.\n\n**[walker.ts](./walker.ts)** — `walkDirectory()` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (dotfiles), `followSymbolicLinks: false` (default), `ignore: ['**/.git/**']` hardcoded, `suppressErrors: true` (permission denied).\n\n## Architecture\n\n### Filter Chain Execution Model\n\n**Composition**: `discoverFiles()` creates four filters in fixed order before applying to walked files. No in-walker filtering per module comment in `walker.ts`.\n\n**Orchestration**: `applyFilters()` (in `filters/index.ts`) spawns 30 workers sharing `files.entries()` iterator. Sequential filter evaluation per file with early termination on first `shouldExclude() === true`.\n\n**Result Preservation**: Workers collect `{ index, file, excluded? }` tuples, sort by original index to maintain discovery order, segregate into `included`/`excluded` arrays.\n\n### Filter Order\n\n1. **Gitignore** — Async `.gitignore` parser via `ignore` library with relative path normalization\n2. **Vendor** — Third-party directories (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`)\n3. **Binary** — Extension fast path (80+ extensions) → size threshold (1MB) → `isBinaryFile()` content analysis\n4. **Custom** — User glob patterns from `config.exclude.patterns` via `ignore` library\n\n### Decoupling Patterns\n\n**DiscoveryConfig Interface**: Structural subset type with `exclude: {vendorDirs, binaryExtensions, patterns}` and `options: {maxFileSize, followSymlinks}` allows `run.ts` to accept full `Config` object without circular dependency on `src/config/schema.ts`.\n\n**Factory Abstraction**: Filter creators (`createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter`) return uniform `FileFilter` interface enabling chain composition.\n\n**Statistics Aggregation**: `applyFilters()` returns `Map<string, {matched, rejected}>` keyed by filter name for telemetry without coupling to filter implementations.\n\n## Behavioral Contracts\n\n### Binary Detection Thresholds\n\n- **Extension fast path**: 80+ extensions across images/archives/executables/media/documents/fonts/compiled/database\n- **Size threshold**: `DEFAULT_MAX_FILE_SIZE = 1048576` (1MB)\n- **Content analysis**: `isBinaryFile()` fallback for unknown extensions\n\n### Concurrency Limit\n\n`CONCURRENCY = 30` workers in `applyFilters()` prevents file descriptor exhaustion during I/O-heavy binary detection.\n\n### Vendor Directories\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Glob Configuration\n\n- `absolute: true` — returns full paths\n- `onlyFiles: true` — excludes directories\n- `dot: true` — includes dotfiles\n- `followSymbolicLinks: false` — default symlink handling\n- `ignore: ['**/.git/**']` — hardcoded `.git` exclusion\n\n### Path Normalization\n\nAll filters use `path.relative(normalizedRoot, absolutePath)` before exclusion tests, with guards against `'..'` prefix or empty string.\n\n## Subdirectories\n\n**[filters/](./filters/)** — Five filter implementations: `gitignore.ts` (pattern matching via `ignore` library), `vendor.ts` (third-party directory detection), `binary.ts` (extension/size/content analysis), `custom.ts` (user glob patterns), `index.ts` (bounded-concurrency orchestrator with telemetry).\n\n## Integration Points\n\n**Consumed by**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` as single entry point.\n\n**Configuration surface**: `DiscoveryConfig` threaded from `src/config/schema.ts` YAML parsing with fields `exclude.vendorDirs`, `exclude.binaryExtensions`, `exclude.patterns`, `options.maxFileSize`, `options.followSymlinks`.\n\n**Telemetry integration**: Accepts `ITraceWriter` via `DiscoverFilesOptions.tracer`, emits `filter:applied` events with per-filter `filesMatched`/`filesRejected` counts.\n\n**Debug output**: `options.debug` enables `console.error()` logging for filters with non-zero rejection counts.\n### src/discovery/filters/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\n**Five filter implementations for file discovery exclusion: gitignore pattern matching, vendor directory detection, binary file detection via extension/content analysis, custom glob patterns, and bounded-concurrency filter chain orchestration with per-filter statistics tracking.**\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter()` implements two-phase binary detection: extension-based fast path against `BINARY_EXTENSIONS` set (80+ extensions across images/archives/executables/media/documents/fonts/compiled/database), size threshold check against `maxFileSize` (default 1MB), content analysis fallback via `isBinaryFile()` for unknown extensions.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter()` matches absolute paths against user-provided gitignore-style patterns via `ignore` library with relative path normalization, returns pass-through filter when pattern array empty.\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter()` async factory reads `.gitignore` from project root, delegates exclusion to `ignore` library instance with relative path conversion, silently passes all files when `.gitignore` missing.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter()` excludes third-party dependency directories via two-tier matching: single-segment patterns (`'node_modules'`) matched against all path segments, multi-segment patterns (`'apps/vendor'`) matched via substring inclusion with OS-specific separator normalization.\n\n**[index.ts](./index.ts)** — `applyFilters()` orchestrates bounded-concurrency filter chain execution (30 workers sharing iterator), short-circuits on first exclusion, tracks per-filter `matched`/`rejected` statistics, emits `filter:applied` trace events, re-exports all filter creators and constants.\n\n## Filter Chain Architecture\n\n**Execution Model**: Worker pool pattern with `CONCURRENCY=30` limit prevents file descriptor exhaustion during I/O-heavy binary detection. Shared `files.entries()` iterator across workers via `for (const [index, file] of iter)` loop. Sequential filter evaluation per file with early termination on first match.\n\n**Statistics Tracking**: `Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. `rejected` increments when filter excludes file. `matched` increments for all filters when file passes entire chain.\n\n**Result Preservation**: Workers collect `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sort by original `index` to maintain input order, segregate into `included`/`excluded` arrays for `FilterResult` return value.\n\n## Behavioral Contracts\n\n### Binary Extensions Set (binary.ts)\n\n`BINARY_EXTENSIONS` contains 80+ extensions organized by category:\n- **Images**: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- **Archives**: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- **Executables**: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- **Media**: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- **Documents**: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- **Fonts**: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- **Compiled/bytecode**: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- **Database**: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n\n### Default Vendor Directories (vendor.ts)\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Concurrency Limit (index.ts)\n\n`CONCURRENCY = 30` workers — hard limit to prevent file descriptor exhaustion during binary file detection I/O operations.\n\n### Binary Size Threshold (binary.ts)\n\n`DEFAULT_MAX_FILE_SIZE = 1048576` (1MB) — files exceeding threshold excluded without content analysis.\n\n## Path Normalization Patterns\n\n**gitignore.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**custom.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**vendor.ts**: Multi-segment patterns normalized via `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling before `absolutePath.includes(pattern)` check.\n\n**binary.ts**: Extension extraction via `path.extname(absolutePath).toLowerCase()`, additional extensions normalized via `ext.startsWith('.') ? ext : \\`.\\${ext}\\`` before merging with `BINARY_EXTENSIONS`.\n\n## Filter Interface Contract\n\nAll filter creators return `FileFilter` object:\n```typescript\n{\n  name: string;              // Discriminator: 'gitignore' | 'vendor' | 'binary' | 'custom'\n  shouldExclude(absolutePath: string): boolean | Promise<boolean>\n}\n```\n\n**Synchronous filters**: custom.ts, vendor.ts, binary.ts (async internally but return sync `shouldExclude`).\n\n**Async factory**: gitignore.ts returns `Promise<FileFilter>` due to `.gitignore` file read.\n\n## Integration Points\n\n**Consumed by**: `src/discovery/walker.ts` composes filters into chain via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, then calls `applyFilters()`.\n\n**Configuration surface**: `BinaryFilterOptions` with `maxFileSize`/`additionalExtensions` threaded from `src/config/schema.ts` YAML config (`exclude.binaryExtensions`, `options.maxFileSize`).\n\n**Telemetry integration**: Accepts optional `ITraceWriter` via `applyFilters()` options, emits `filter:applied` events with per-filter metrics (`filterName`, `filesMatched`, `filesRejected`).\n\n**Debug output**: `options.debug` flag enables `console.error(pc.dim(...))` logging for filters with `rejected > 0` count.\n### src/generation/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates three-phase documentation generation pipeline: concurrent file analysis via `GenerationOrchestrator.createFileTasks()`, post-order directory synthesis via `buildExecutionPlan()` with depth-sorted traversal, and root document synthesis via `collectAgentsDocs()` aggregation, enforcing dependency graphs through `ExecutionTask.dependencies` arrays and completion predicates via `isDirectoryComplete()`.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` prepares files via `prepareFiles(discoveryResult)` reading content with `readFile()`, creates file tasks via `createFileTasks(files)` calling `buildFilePrompt()` for each file, creates directory tasks via `createDirectoryTasks(files)` grouping by `path.dirname()`, emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`, clears `PreparedFile.content` after prompt embedding to free memory, returns `GenerationPlan` with `files`, `tasks`, `complexity` from `analyzeComplexity()`. `buildProjectStructure()` formats directory tree as indented text for AI context.\n\n**[executor.ts](./executor.ts)** — `buildExecutionPlan()` constructs dependency graph from `GenerationPlan`: populates `directoryFileMap` extracting directories via `path.dirname()`, creates file tasks with `id: \"file:{path}\"`, sorts files by `getDirectoryDepth(path.dirname())` descending (deepest first), sorts directories by depth descending, creates directory tasks with `dependencies: fileTaskIds`, creates root tasks with `dependencies: allDirTaskIds`, returns `ExecutionPlan` with `fileTasks`/`directoryTasks`/`rootTasks` arrays. `isDirectoryComplete()` checks all files have `.sum` outputs via `sumFileExists()`, `getReadyDirectories()` identifies directories eligible for `AGENTS.md` generation, `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with checkbox lists grouped by phase and depth.\n\n**[collector.ts](./collector.ts)** — `collectAgentsDocs()` recursively walks project tree via `readdir(withFileTypes)`, collects files named exactly `AGENTS.md`, skips 13 directories in `SKIP_DIRS` set (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns `AgentsDocs` array sorted by `relativePath` via `localeCompare()`. `collectAnnexFiles()` uses identical traversal logic for files ending `.annex.md`. Gracefully handles permission-denied via silent exception catch.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` with `fileCount`, `directoryDepth` via `calculateDirectoryDepth()` (splits `path.relative()` by `path.sep`, tracks max), `directories` set via `extractDirectories()` (walks upward via repeated `path.dirname()` until root), `files` array. Consumed by `GenerationOrchestrator.createPlan()` for complexity warnings.\n\n**[types.ts](./types.ts)** — `AnalysisResult` with `summary: string` and `metadata: SummaryMetadata`, `SummaryMetadata` with `purpose`, `criticalTodos`, `relatedFiles`, `SummaryOptions` with `targetLength` discriminant (`'short' | 'standard' | 'detailed'`) and `includeCodeSnippets` boolean. Maps to `.sum` YAML frontmatter schema.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Prompt template constants (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`) with density rules (every sentence references identifiers), filler phrase prohibition (`\"this file\"`, `\"provides\"`, `\"responsible for\"`), behavioral contract extraction (verbatim regex/constants). Builders (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`) substitute placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), aggregate child `.sum` files via `readSumFile()`, extract import maps via `extractDirectoryImports()`, preserve user content from `AGENTS.local.md`, switch to update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when `existingSum`/`existingAgentsMd` present.\n\n**[writers/](./writers/)** — YAML frontmatter serialization (`writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`) with SHA-256 hash tracking, inline/multi-line array formatting, annex file generation (`writeAnnexFile`, `getAnnexPath`) for reproduction-critical source content. `writeAgentsMd()` preserves user content via `AGENTS.local.md` rename, marker-based detection (`isGeneratedAgentsMd`), prepends preserved sections with `---` separator.\n\n## Post-Order Traversal\n\n`buildExecutionPlan()` enforces children-before-parents ordering via two sorts:\n1. **File tasks**: `getDirectoryDepth(path.dirname(a.path)) - getDirectoryDepth(path.dirname(b.path))` descending (deepest first)\n2. **Directory tasks**: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending\n\nEnsures child `AGENTS.md` files exist before parent directory prompts consume them via `collectAgentsDocs()` in `buildDirectoryPrompt()`.\n\n## Dependency Graph\n\n`ExecutionTask.dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing:\n- **File tasks**: empty dependencies (parallel eligible)\n- **Directory tasks**: depend on all file task IDs in directory (`id: \"file:{path}\"`)\n- **Root tasks**: depend on all directory task IDs (`id: \"dir:{path}\"`)\n\n`isDirectoryComplete()` verifies all `expectedFiles` have `.sum` outputs before allowing directory task execution.\n\n## Prompt Placeholder Pattern\n\nDirectory and root tasks store placeholder prompts (`\"Built at runtime by buildDirectoryPrompt()\"`) for plan display and dependency tracking. Actual prompts constructed at execution time in `src/orchestration/runner.ts` via `buildDirectoryPrompt()` and `buildRootPrompt()`, consuming child `.sum` files and subdirectory `AGENTS.md` not available during plan creation.\n\n## Trace Events\n\n`GenerationOrchestrator.createPlan()` emits:\n- `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }`\n- `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount: tasks.length + 1 }` (accounting for root task added by `buildExecutionPlan()`)\n- `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }`\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` for Phase 1 task creation. Imports `sumFileExists` from `./writers/sum.js` for completion checking. Imports `analyzeComplexity` from `./complexity.js` for metrics computation. Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\nConsumed by `src/orchestration/runner.ts` which executes `ExecutionPlan` via worker pool, calling `AIService` with runtime-constructed prompts for directory and root tasks.\n\n## Memory Management\n\nAfter `createFileTasks()` embeds file content into prompts, `createPlan()` clears `PreparedFile.content` via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.\n### src/generation/prompts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\nPrompt construction engine for the three-phase documentation pipeline: `buildFilePrompt()` assembles file analysis prompts with import maps and incremental update sections, `buildDirectoryPrompt()` aggregates child `.sum` files and subdirectory `AGENTS.md` with import maps for directory synthesis, `buildRootPrompt()` collects all `AGENTS.md` files and package.json metadata for root document generation.\n\n## Contents\n\n### Prompt Builders\n\n**[builder.ts](./builder.ts)** — Implements `buildFilePrompt()` with `detectLanguage()` mapping 21 extensions to syntax identifiers, `buildDirectoryPrompt()` aggregating `.sum` files via `readSumFile()` and extracting import maps via `extractDirectoryImports()`, `buildRootPrompt()` calling `collectAgentsDocs()` for project-level synthesis. Substitutes placeholders `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` in user prompts. Switches between fresh generation prompts (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`) and update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) based on `existingSum`/`existingAgentsMd` presence. Preserves user content from `AGENTS.local.md` by appending as `## User Notes` section. Debug logging emits template action/metadata pairs to stderr via `picocolors`.\n\n**[templates.ts](./templates.ts)** — Exports six system prompt constants: `FILE_SYSTEM_PROMPT` enforces density rules (every sentence references specific identifiers), anchor term preservation (exact export name casing), behavioral contract extraction (verbatim regex patterns, format strings, magic constants), `FILE_USER_PROMPT` template with `{{FILE_PATH}}`/`{{CONTENT}}` placeholders, `FILE_UPDATE_SYSTEM_PROMPT` for incremental regeneration preserving unchanged sections, `DIRECTORY_SYSTEM_PROMPT` mandating first line `<!-- Generated by agents-reverse-engineer -->` with adaptive section strategy (Contents/Subdirectories/Architecture/Stack/Structure/Patterns/Configuration/API Surface/File Relationships/Behavioral Contracts), `DIRECTORY_UPDATE_SYSTEM_PROMPT` for directory-level incremental updates, `ROOT_SYSTEM_PROMPT` enforcing synthesis-only mode prohibiting invention of features/hooks/APIs/patterns. All prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`.\n\n**[types.ts](./types.ts)** — Defines `PromptContext` interface with fields `filePath`, `content`, `contextFiles[]`, `projectPlan`, `existingSum` for Phase 1/2 builders. Exports `SUMMARY_GUIDELINES` constant object with `targetLength: {min: 300, max: 500}`, `include[]` array of 8 mandatory content categories (purpose, public interface, patterns, dependencies, function signatures, coupled files, behavioral contracts, annex references), `exclude[]` array of 3 prohibited categories (control flow minutiae, generic TODOs, broad architectural relationships).\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Separates prompt template strings (in `templates.ts`) from assembly logic.\n\n## File Relationships\n\n`builder.ts` consumes templates from `templates.ts` (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`). `builder.ts` imports `GENERATED_MARKER` from `../writers/agents-md.js` for user content detection, `readSumFile()`/`getSumPath()` from `../writers/sum.js` for child summary aggregation, `extractDirectoryImports()`/`formatImportMap()` from `../../imports/index.js` for dependency graph construction, `collectAgentsDocs()` from `../collector.js` for Phase 3 synthesis. `index.ts` re-exports `PromptContext` from `types.ts` for consumer type safety.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts. `FILE_UPDATE_SYSTEM_PROMPT` enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. `DIRECTORY_UPDATE_SYSTEM_PROMPT` preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files.\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant. User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Behavioral Contracts\n\nAll templates enforce:\n\n- **Density rule**: Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- **Anchor term preservation**: All exported function/class/type/const names MUST appear in summary exactly as written in source\n- **Filler phrase prohibition**: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`\n- **Output format**: Start response DIRECTLY with bold purpose statement without preamble\n- **PATH ACCURACY**: Use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention\n- **CONSISTENCY**: Do not contradict within same document (e.g., calling technique \"regex-based\" then \"AST-based\")\n\n`DIRECTORY_SYSTEM_PROMPT` enforces first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name.\n\n`FILE_SYSTEM_PROMPT` defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content, delegating extraction to pipeline automation.\n\n## Annex References\n\nFull prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)  \nPromptContext interface specification: [types.ts.annex.md](./types.ts.annex.md)\n### src/generation/writers/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/writers\n\nImplements `.sum` file and `AGENTS.md` serialization with YAML frontmatter parsing, user content preservation, and annex file generation for reproduction-critical source content.\n\n## Contents\n\n### [sum.ts](./sum.ts)\nYAML frontmatter I/O for `.sum` files with SHA-256 hash tracking, array serialization (inline/multi-line), annex file generation for reproduction-critical content. Exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `writeAnnexFile`, `getAnnexPath`, `parseSumFile`, `formatSumFile`, `SumFileContent` interface.\n\n### [agents-md.ts](./agents-md.ts)\nAGENTS.md lifecycle management preserving user-authored content via AGENTS.local.md rename, marker-based detection. Exports `writeAgentsMd`, `isGeneratedAgentsMd`, `GENERATED_MARKER` constant.\n\n### [index.ts](./index.ts)\nBarrel re-exporting `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts`.\n\n## Data Flow\n\n**Phase 1 (File Analysis):**\n`src/generation/executor.ts` → `writeSumFile(sourcePath, content)` → appends `.sum` extension → `formatSumFile()` → `mkdir` + `writeFile` → emits `.sum` file with frontmatter delimiter `---\\n...\\n---\\n` + summary body.\n\n**Phase 2 (Directory Aggregation):**\n`src/generation/orchestrator.ts` → `writeAgentsMd(dirPath, projectRoot, llmContent)` → checks existing AGENTS.md via `isGeneratedAgentsMd()` → renames user file to AGENTS.local.md → prepends preserved content + separator `---` + LLM-generated sections.\n\n**Incremental Updates:**\n`src/update/orchestrator.ts` → `readSumFile(getSumPath(sourcePath))` → `parseSumFile()` extracts `contentHash` → SHA-256 comparison → hash mismatch triggers re-analysis.\n\n**Orphan Cleanup:**\n`src/update/orphan-cleaner.ts` → detects stale `.sum` files for deleted sources → calls `unlink(getSumPath(deletedPath))`.\n\n## YAML Frontmatter Format\n\n**Delimiter:** `---\\n` before and after metadata block, exactly one newline before summary body.\n\n**Scalar fields:**\n```yaml\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: Single-line description\n```\n\n**Array fields (inline format <40 chars, ≤3 items):**\n```yaml\ncritical_todos: [Security issue, Performance bug]\nrelated_files: [path/to/file.ts]\n```\n\n**Array fields (multi-line format):**\n```yaml\ncritical_todos:\n  - Long security issue description exceeding 40 characters\n  - Another long description\nrelated_files:\n  - very/long/path/to/related/file/exceeding/character/limit.ts\n```\n\n## Parsing Strategies\n\n**Frontmatter extraction:** Regex `/^---\\n([\\s\\S]*?)\\n---\\n/` captures metadata block.\n\n**Scalar field extraction:** Individual regex per field (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), captures group 1, trims whitespace.\n\n**Array parsing (inline):** `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` captures comma-separated values within brackets, splits on `,`, strips quotes and whitespace.\n\n**Array parsing (multi-line):** `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')` captures indented list items, splits on newlines, strips `- ` prefix per line.\n\n## User Content Preservation Workflow\n\n1. `writeAgentsMd()` called with `dirPath` and LLM-generated `content`\n2. Constructs `agentsPath` (`dirPath/AGENTS.md`) and `localPath` (`dirPath/AGENTS.local.md`)\n3. If `agentsPath` exists and `isGeneratedAgentsMd(agentsPath)` returns `false`: `rename(agentsPath, localPath)`, capture content as `userContent`\n4. Else if `localPath` exists: `readFile(localPath)`, capture as `userContent`\n5. Strip `GENERATED_MARKER` prefix from LLM `content` if present via `slice()` + `/^\\n+/` trim\n6. Assemble `finalContent`:\n   - `GENERATED_MARKER` comment\n   - If `userContent.trim()` non-empty: preservation comment + `userContent.trim()` + `---` separator\n   - LLM-generated sections\n7. `mkdir(dirPath, { recursive: true })` + `writeFile(agentsPath, finalContent)`\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath, sourceContent)` creates `${sourcePath}.annex.md`:\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n\n# Annex: <basename>\n\nThis annex file preserves the full source content of `<basename>` for AI coding assistants.\nThe primary summary is in `<basename>.sum`.\n\n```<ext>\n<sourceContent>\n```\n```\n\nUsed for files containing long regex patterns, template strings, or format specifications where verbatim content required for reproduction. `getAnnexPath()` computes path without filesystem access.\n\n## Behavioral Contracts\n\n**Marker detection:** `content.includes('<!-- Generated by agents-reverse-engineer -->')` substring search (no regex).\n\n**Marker removal:** `llmContent.startsWith(GENERATED_MARKER) ? llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '') : llmContent`.\n\n**File extensions:** `.sum` (metadata + summary), `.annex.md` (full source preservation).\n\n**User content separator:** Three-dash horizontal rule surrounded by blank lines (`\\n\\n---\\n\\n`).\n\n**Frontmatter regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/` non-greedy capture of metadata block.\n\n**Array inline regex:** `` new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`) `` captures values within square brackets.\n\n**Array multi-line regex:** `` new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm') `` captures indented list items starting with `- `.\n\n## Integration Points\n\n**Imports:**\n- `../types.js` → `SummaryMetadata` (metadata structure for frontmatter)\n- `./agents-md.js` → `GENERATED_MARKER` (annex file header constant)\n\n**Consumed By:**\n- `src/generation/executor.ts` → `writeSumFile()` for Phase 1 output\n- `src/generation/orchestrator.ts` → `writeAgentsMd()` for Phase 2 output\n- `src/update/orchestrator.ts` → `readSumFile()` for hash comparison\n- `src/update/orphan-cleaner.ts` → `getSumPath()` for stale file deletion\n- `src/generation/collector.ts` → reads AGENTS.local.md when aggregating subdirectory docs\n### src/imports/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports/\n\nStatic import analysis subsystem extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs consumed by Phase 1 file analysis and Phase 2 directory aggregation prompts.\n\n## Contents\n\n### [extractor.ts](./extractor.ts)\nRegex-based parser matching static import statements via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`). Exports `extractImports()` returning `ImportEntry[]` with `specifier`, `symbols[]`, `typeOnly` fields. Exports `extractDirectoryImports()` reading first 100 lines per file, classifying imports as `internal` (`./` prefix) or `external` (`../` prefix), filtering bare package specifiers and `node:` built-ins. Exports `formatImportMap()` converting `FileImports[]` to human-readable text blocks for LLM prompt embedding.\n\n### [types.ts](./types.ts)\nDefines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` representing single import statement. Defines `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` partitioning imports by locality.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` from `extractor.ts` and `ImportEntry`, `FileImports` from `types.ts`.\n\n## Data Flow\n\n1. **File Analysis (Phase 1):** `extractDirectoryImports()` invoked by `src/generation/prompts/builder.ts` → reads first 100 lines per source file → applies `IMPORT_REGEX` → classifies imports by locality → returns `FileImports[]`\n2. **Prompt Embedding:** `formatImportMap()` serializes `FileImports[]` → produces text block with filename headers and indented specifier-symbol pairs → embedded in AI prompts via `buildFileAnalysisPrompt()`\n3. **Directory Aggregation (Phase 2):** Import maps from child files aggregated during directory-level `AGENTS.md` generation to provide cross-module dependency context\n\n## Import Classification\n\n| Category | Pattern | Included | Purpose |\n|----------|---------|----------|---------|\n| **Internal** | `./filename` | Yes | Same-directory coupling (tight cohesion signals) |\n| **External** | `../path/module` | Yes | Cross-directory dependencies (module boundaries) |\n| **Package** | `'react'` | No | Third-party npm dependencies (not project structure) |\n| **Built-in** | `'node:fs'` | No | Runtime APIs (not project structure) |\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching, assuming ES module convention of top-level imports. Skips unreadable files silently via empty catch blocks in `extractDirectoryImports()`.\n\n## Integration Points\n\n- **Consumed by:** `src/generation/prompts/builder.ts` embeds import maps in file analysis prompts via `extractDirectoryImports()` + `formatImportMap()`\n- **Prompts:** `src/generation/prompts/templates.ts` includes import map sections in Phase 1 file analysis and Phase 2 directory aggregation templates\n- **Output format:** Human-readable text blocks with filename headers (`external imports from foo.ts:`) and indented specifier-symbol pairs (`  ../ai/index.js → AIService`)\n### src/installer/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nOrchestrates npx-based installation of ARE commands and hooks across Claude Code, OpenCode, and Gemini CLI runtimes with interactive prompts, platform-specific path resolution, settings.json manipulation, and uninstallation workflows.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Entry point exposing `runInstaller()` and `parseInstallerArgs()` for CLI integration. Parses short/long flags (`-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `--quiet`, `--uninstall`), validates non-interactive mode requirements, dispatches to `runInstall()` or `runUninstall()` based on arguments, displays results via `displayInstallResults()` and `displayUninstallResults()`. Re-exports all types and functions from sibling modules for barrel pattern.\n\n**[operations.ts](./operations.ts)** — Implements `installFiles()`, `verifyInstallation()`, `registerHooks()`, `registerPermissions()` for file copying, settings.json hook registration (SessionStart/SessionEnd events with nested HookEvent arrays for Claude, flat GeminiHook arrays for Gemini), and bash permission patterns (`ARE_PERMISSIONS` array: `'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`, `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`, `'Bash(sleep *)'`). Resolves bundled hooks via `getBundledHookPath()` navigating from `dist/installer/operations.js` up to `hooks/dist/`. Writes `ARE-VERSION` file via `getPackageVersion()`.\n\n**[uninstall.ts](./uninstall.ts)** — Mirrors installation logic with `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`. Removes command templates, hooks, plugins, settings.json entries, ARE-VERSION file. Implements `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for recursive bottom-up directory removal, `cleanupLegacyGeminiFiles()` for obsolete Markdown/TOML formats. Repurposes `InstallerResult.filesCreated` to track deleted files, `hookRegistered` for unregistration status.\n\n### Path Resolution\n\n**[paths.ts](./paths.ts)** — Exports `getRuntimePaths()`, `resolveInstallPath()`, `getSettingsPath()`, `getAllRuntimes()`, `isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`. Resolves global/local paths with environment overrides: `CLAUDE_CONFIG_DIR` (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` (`~/.gemini` fallback). Returns `RuntimePaths` with `global`, `local`, `settingsFile` fields.\n\n### User Interface\n\n**[prompts.ts](./prompts.ts)** — Provides `selectRuntime()`, `selectLocation()`, `confirmAction()`, `isInteractive()` with `arrowKeySelect()` for TTY mode (arrow-key navigation via `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)` with ANSI escape sequences `\\x1b[${n}A`, `\\x1b[2K`, `\\x1b[1B` for rendering) and `numberedSelect()` fallback for piped input. Registers `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to restore terminal state via `setRawMode(false)` + `pause()`.\n\n**[banner.ts](./banner.ts)** — Exports `displayBanner()` (Unicode box-drawing ASCII art logo U+2588/U+2550-U+2557), `showHelp()`, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` (prefix symbols: green ✓, red ✗, yellow !, cyan >), `showNextSteps()` (lists commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with GitHub documentation link).\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime` (`'claude' | 'opencode' | 'gemini' | 'all'`), `Location` (`'global' | 'local'`), `InstallerArgs` (`runtime?`, `global`, `local`, `uninstall`, `force`, `help`, `quiet`), `InstallerResult` (`success`, `runtime: Exclude<Runtime, 'all'>`, `location`, `filesCreated`, `filesSkipped`, `errors`, `hookRegistered?`, `versionWritten?`), `RuntimePaths` (`global`, `local`, `settingsFile`).\n\n## Subdirectories\n\nNone — flat structure with seven TypeScript modules.\n\n## Architecture\n\n### Two-Phase Workflow\n\n**Installation (runInstall):**\n1. Argument parsing via `parseInstallerArgs()` with short/long flag normalization\n2. Interactive prompts via `selectRuntime()`/`selectLocation()` when TTY detected and values missing\n3. Template copying via `getTemplatesForRuntime()` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` from `src/integration/templates.ts`\n4. Hook/plugin installation: Claude/Gemini to `hooks/` subdirectory using `ARE_HOOKS` array (currently empty), OpenCode to `plugins/` using `ARE_PLUGINS` array\n5. Settings.json modification: `registerHooks()` merges SessionStart/SessionEnd hooks with duplicate detection, `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns to Claude's permissions.allow array\n6. Version file creation via `writeVersionFile()` for update checker hooks\n7. Verification via `verifyInstallation()` checking `existsSync()` for all filesCreated paths\n\n**Uninstallation (runUninstall):**\n1. Template path extraction with runtime prefix slicing (`template.path.split('/').slice(1).join('/')`)\n2. File deletion via `unlinkSync()` for templates, hooks, plugins, ARE-VERSION\n3. Settings.json cleanup: `unregisterHooks()` filters hook arrays via pattern matching (current format `node .claude/hooks/${filename}`, legacy `node hooks/${filename}`), `unregisterPermissions()` filters `ARE_PERMISSIONS` from permissions.allow\n4. Directory cleanup: `cleanupAreSkillDirs()` removes empty `are-*` skill directories, `cleanupEmptyDirs()` recursively removes empty parents up to runtime root, `cleanupLegacyGeminiFiles()` deletes obsolete `.md`/`.toml` formats\n5. Config folder deletion via `deleteConfigFolder()` only for local installations (`location === 'local'`)\n\n### Runtime-Specific Patterns\n\n**Claude Code:**\n- Global path: `~/.claude` (override: `CLAUDE_CONFIG_DIR`)\n- Commands: `.claude/skills/<command>/SKILL.md` with frontmatter `name: /are-<command>`\n- Hooks: `.claude/hooks/<filename>.js` registered in `settings.json` with nested structure `{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command: string }] }] } }`\n- Permissions: `settings.json` permissions.allow array with bash command patterns\n- Settings file: `~/.claude/settings.json`\n\n**OpenCode:**\n- Global path: `~/.config/opencode` (overrides: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode`)\n- Commands: `.opencode/commands/<command>.md` with frontmatter `agent: build`\n- Plugins: `.opencode/plugins/<filename>.js` exporting async factory functions with `event['session.created']`/`event['session.deleted']` handlers\n- No settings file (plugin registration via file presence)\n\n**Gemini CLI:**\n- Global path: `~/.gemini` (override: `GEMINI_CONFIG_DIR`)\n- Commands: `.gemini/commands/<command>.toml` with `description`/`prompt` fields\n- Hooks: `.gemini/hooks/<filename>.js` registered in `settings.json` with flat structure `{ hooks: { SessionStart?: [{ name: string, type: 'command', command: string }] } }`\n- Settings file: `~/.gemini/settings.json`\n\n## Behavioral Contracts\n\n### Hook/Plugin Definitions\n\n**ARE_HOOKS (operations.ts):**\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Array intentionally empty — hooks disabled due to issues\n];\n```\n\n**ARE_PLUGINS (operations.ts):**\n```typescript\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // are-session-end.js disabled\n];\n```\n\n**ARE_PERMISSIONS (operations.ts):**\n```typescript\nconst ARE_PERMISSIONS: string[] = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n**Hook command pattern (operations.ts):**\n```javascript\n`node ${runtimeDir}/hooks/${hookDef.filename}`\n```\n\n**Legacy hook patterns (uninstall.ts):**\n```typescript\nfunction getHookPatterns(runtimeDir: string): string[] {\n  return ARE_HOOKS.flatMap(hook => [\n    `node ${runtimeDir}/hooks/${hook.filename}`,  // Current format\n    `node hooks/${hook.filename}`,                 // Legacy format\n  ]);\n}\n```\n\n### ANSI Escape Sequences (prompts.ts)\n\n**Cursor control in arrowKeySelect:**\n- `\\x1b[${n}A` — Move cursor up n lines\n- `\\x1b[2K` — Clear entire line\n- `\\x1b[1B` — Move cursor down 1 line\n\n**Keypress handling:**\n- Up arrow: `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- Down arrow: `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- Enter: `key.name === 'return'` → resolve with `options[selectedIndex].value`\n- Ctrl+C: `key.ctrl && key.name === 'c'` → `cleanupRawMode()` + `process.exit(0)`\n\n### Settings.json Schemas\n\n**Claude (operations.ts):**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini (operations.ts):**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n### Directory Cleanup Terminal Conditions (uninstall.ts)\n\n**Runtime root check in cleanupEmptyDirs:**\n```typescript\nconst baseName = path.basename(dirPath);\nif (['.claude', '.opencode', '.gemini', '.config'].includes(baseName)) {\n  return;\n}\n```\n\n**ARE skill directory pattern (uninstall.ts):**\n```typescript\nentry.startsWith('are-') && stats.isDirectory()\n```\n\n**Legacy Gemini file patterns (uninstall.ts):**\n```typescript\n// Markdown format\nentry.startsWith('are-') && entry.endsWith('.md')\n\n// TOML namespace directory\n'commands/are/*.toml'\n```\n\n## File Relationships\n\n**Orchestration chain:**\n1. `index.ts` parses args → calls `prompts.ts` selectors → calls `operations.ts` or `uninstall.ts`\n2. `operations.ts`/`uninstall.ts` call `paths.ts` for directory resolution\n3. `operations.ts` calls `src/integration/templates.ts` for command content\n4. `operations.ts` reads bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. `operations.ts`/`uninstall.ts` manipulate settings.json via in-memory JSON parse/stringify\n6. `banner.ts` provides display functions consumed by `index.ts` for results rendering\n\n**Shared state:**\n- `prompts.ts` maintains module-level `rawModeActive` flag for terminal cleanup\n- `operations.ts` and `uninstall.ts` share `ARE_HOOKS`, `ARE_PLUGINS`, `ARE_PERMISSIONS` definitions\n- `paths.ts` provides single source of truth for runtime directory mappings\n\n**Type flow:**\n- `types.ts` defines discriminated union `Runtime` with `'all'` excluded via `Exclude<Runtime, 'all'>` in `InstallerResult.runtime`\n- `InstallerArgs` parsed in `index.ts`, threaded through `runInstall()`/`runUninstall()` to `operations.ts`/`uninstall.ts`\n- `RuntimePaths` returned by `paths.ts`, consumed by `operations.ts`/`uninstall.ts` for file path construction\n### src/integration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments, generating command template files with progress-monitoring patterns, deploying bundled session hooks, and enforcing skip-if-exists safety with force override.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — `detectEnvironments()` scans project root for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` directories and `CLAUDE.md`/`.aider.conf.yml` marker files, returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` filters by `EnvironmentType`.\n\n### File Generation Orchestration\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles()` orchestrates template instantiation by detecting/overriding environments via `detectEnvironments()`, retrieving templates via `getTemplatesForEnvironment()`, writing command files with `ensureDir()` + `writeFileSync()`, deploying bundled hooks for Claude via `readBundledHook()` from `hooks/dist/are-session-end.js`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays. `GenerateOptions` supports `dryRun`, `force` (overwrite existing), and `environment` (bypass auto-detection).\n\n### Template Library\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generating platform-specific command files for seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Encapsulates `COMMANDS` object with markdown prompt content, `PLATFORM_CONFIGS` mapping `EnvironmentType` to `PlatformConfig` (command prefix, path structure, frontmatter rules), `buildTemplate()`/`buildGeminiToml()` factories performing placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`). Command content patterns: background execution via `npx agents-reverse-engineer@latest`, progress polling with `.agents-reverse-engineer/progress.log` offset reads, `TaskOutput` checks with `block: false`, completion summaries with phase metrics.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Platform Configuration\n\n| Platform | Command Prefix | Path Pattern | Frontmatter | Version File |\n|----------|---------------|--------------|-------------|--------------|\n| **Claude** | `/are-` | `.claude/skills/are-{command}/SKILL.md` | `name: /are-{command}` | `.claude/ARE-VERSION` |\n| **OpenCode** | `/are-` | `.opencode/commands/are-{command}.md` | `agent: build` | `.opencode/ARE-VERSION` |\n| **Gemini** | `/are-` | `.gemini/commands/are-{command}.toml` | TOML `description`/`prompt` | `.gemini/ARE-VERSION` |\n| **Aider** | N/A | N/A (detection only) | N/A | N/A |\n\n## Command Template Behavior\n\n- **generate/update/specify**: 15s poll intervals, background execution via `run_in_background: true`, offset-based log tailing, three-phase progress reporting (discovery → file analysis → directory/root docs)\n- **discover**: 10s poll intervals, background execution, file count reporting\n- **init**: Synchronous execution, creates `.agents-reverse-engineer/config.yaml`\n- **clean**: Synchronous execution with STRICT RULES enforcing zero flag additions, deletion count reporting\n- **help**: Outputs command reference with platform-specific `COMMAND_PREFIX` placeholder substitution\n\n## Integration with Project\n\nConsumed by `src/installer/` for `npx agents-reverse-engineer --runtime <env>` workflow. `detectEnvironments()` validates presence before installation, `generateIntegrationFiles()` writes command files to `.claude/skills/`, `.opencode/commands/`, or `.gemini/commands/`. Bundled hook deployment via `hooks/dist/are-session-end.js` for Claude's SessionEnd lifecycle.\n### src/orchestration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker pool concurrency control, promise-chain serialized progress tracking, NDJSON trace emission, and three-phase pipeline orchestration executing concurrent file analysis, post-order directory aggregation, and sequential root synthesis with integrated quality validation, ETA calculation, and subprocess lifecycle tracing.**\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export consolidating `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).\n\n**[pool.ts](./pool.ts)** — Iterator-based worker pool executing `Array<() => Promise<T>>` tasks via shared `tasks.entries()` iterator consumed by N workers, preventing batch-stall anti-pattern where `Promise.all()` chunks idle workers waiting for slowest task. Emits `worker:start/end`, `task:pickup/done` trace events. Supports `failFast` via shared `aborted` boolean flag checked at loop start. Effective concurrency via `Math.min(options.concurrency, tasks.length)`.\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates three-phase pipeline via `executeGenerate(plan)` and `executeUpdate(filesToAnalyze, projectRoot, config)`. Phase 1: concurrent file analysis via `runPool()` with `options.concurrency`, reads source via `readFile()`, caches in `sourceContentCache`, calls `aiService.call()`, computes `contentHash` via `computeContentHashFromString()`, strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes via `writeSumFile()`, detects `## Annex References` marker for `writeAnnexFile()`. Post-Phase 1: groups files by directory, runs quality checks concurrently (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)`, builds `InconsistencyReport`, clears `sourceContentCache`. Phase 2: groups `directoryTasks` by depth, processes in descending order (deepest first) with concurrency `Math.min(options.concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths(agentsMdPath, content, projectRoot)`. Phase 3: sequential root document synthesis (concurrency=1), strips conversational preamble before `writeFile()`. Returns `RunSummary` with counts from `aiService.getSummary()` plus `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`.\n\n### Progress Tracking\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output with ETA calculation via moving average of last 10 `completionTimes` (window size 10), displays after 2+ completions. `ProgressLog` mirrors plain-text output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with ANSI stripping (`/\\x1b\\[[0-9;]*m/g`). Static factory `ProgressLog.create(projectRoot)` constructs path. Methods: `onFileStart(filePath)` (`[X/Y] ANALYZING path`), `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)` (`[X/Y] DONE path Xs in/out tok model ~Ns remaining`), `onFileError(filePath, error)` (`[X/Y] FAIL path error`), `onDirectoryStart(dirPath)`, `onDirectoryDone()`, `onRootDone(docPath)`, `printSummary(summary)`. ETA formatted via `formatETA()` computing `avg * remaining` as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` maintains in-memory `GENERATION-PLAN.md` content, serializes concurrent checkbox updates via promise-chain writes. Constructor accepts `projectRoot` and `initialMarkdown`, computes `planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`. `markDone(itemPath)` replaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``, chains write to `writeQueue` promise. `initialize()` creates parent directory via `mkdir(..., {recursive: true})`, writes initial content. `flush()` awaits `writeQueue` completion.\n\n### Trace Infrastructure\n\n**[trace.ts](./trace.ts)** — `createTraceWriter(projectRoot, enabled)` factory returns `NullTraceWriter` (zero overhead) when disabled or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization. `TraceWriter` auto-populates `seq` (monotonic), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution `process.hrtime.bigint()` delta) on `emit(partial: TraceEventPayload)`. `finalize()` awaits `writeQueue`, closes `fd`. `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces, keeps most recent 500.\n\n**Event types:** `phase:start/end` (phase, taskCount, concurrency, durationMs, tasksCompleted, tasksFailed), `worker:start/end` (workerId, phase, tasksExecuted), `task:pickup/done` (workerId, taskIndex, taskLabel, activeTasks, durationMs, success, error?), `task:start` (taskLabel, phase), `subprocess:spawn/exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode), `discovery:start/end` (targetPath, filesIncluded, filesExcluded, durationMs), `filter:applied` (filterName, filesMatched, filesRejected), `plan:created` (planType, fileCount, taskCount), `config:loaded` (configPath, model, concurrency).\n\n**[types.ts](./types.ts)** — Defines `FileTaskResult` (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed, filesFailed, filesSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc?, inconsistenciesCodeVsCode?, phantomPaths?, inconsistencyReport?), `ProgressEvent` (discriminated type: start|done|error|dir-done|root-done with filePath, index, total, durationMs?, tokensIn?, tokensOut?, model?, error?), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?: ITraceWriter, progressLog?: ProgressLog), `PoolOptions` (concurrency, failFast?, tracer?, phaseLabel?, taskLabels?), `TaskResult<T>` (index, success, value?, error?).\n\n## Architecture\n\n### Shared-Iterator Concurrency\n\n`runPool()` creates `tasks.entries()` iterator shared across N workers via `for (const [index, task] of iterator)` loop. Workers race to pull tasks atomically via iterator protocol, preventing over-allocation where `Promise.all()` chunks spawn tasks eagerly. Effective concurrency via `Math.min(options.concurrency, tasks.length)` prevents idle workers when task count < pool size.\n\n### Promise-Chain Serialization\n\n`PlanTracker`, `ProgressLog`, `TraceWriter` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern. Each write operation chains onto tail of promise, forming sequential execution queue despite concurrent worker invocations. Errors caught via `.catch()` suppression (non-critical telemetry).\n\n### Three-Phase Pipeline\n\nPhase 1: concurrent file analysis via `runPool(fileTasks, {concurrency: options.concurrency})`, reads source via `readFile()` once, caches in `sourceContentCache` Map, calls `aiService.call()`, writes `.sum` via `writeSumFile()`, writes `.annex.md` if `## Annex References` detected. Post-Phase 1: groups files by directory, validates via `checkCodeVsDoc(source, oldSum)` (stale docs), `checkCodeVsDoc(source, newSum)` (LLM omissions), `checkCodeVsCode(files)` (duplicate exports) at concurrency=10, clears `sourceContentCache`.\n\nPhase 2: groups `directoryTasks` by depth (`task.metadata.depth`), processes depth levels sequentially in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)`, executes depth level concurrently with `Math.min(concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths()` extracting path-like strings via three regex patterns, resolving against directory and project root.\n\nPhase 3: sequential root synthesis (concurrency=1), builds prompts via `buildRootPrompt()`, calls `aiService.call()` with `maxTurns: 1`, strips conversational preamble before `writeFile()`.\n\n### Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files concurrently (concurrency=20) into `oldSumCache` Map. Post-Phase 1: compares `oldSum` vs. source (stale documentation from previous runs), compares `newSum` vs. source (LLM omissions in current run), aggregates duplicate exports. Post-Phase 2: validates AGENTS.md path references. Validation failures logged to stderr with `[quality]` prefix, never throw (non-blocking).\n\n### Trace Event Threading\n\n`CommandRunOptions.tracer` threaded through pool → AIService → runner. Phase-level `phase:start/end` emitted by runner. Pool-level `worker:start/end`, `task:pickup/done` emitted by `runPool()`. Subprocess-level `subprocess:spawn/exit`, `retry` emitted by `AIService`. Zero overhead when `tracer` is `NullTraceWriter`.\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size 10). `formatETA()` computes `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Displays after 2+ completions to avoid inaccurate early estimates. Formats as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n## Behavioral Contracts\n\n### Preamble Detection Patterns (from runner.ts)\n\nFull patterns preserved in [runner.ts.annex.md](./runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Checkbox Update Pattern (from plan-tracker.ts)\n\nReplaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` `` via string substitution. Item path conventions: files use relative paths (`src/cli/init.ts`), directories append `/AGENTS.md` suffix (`src/cli/AGENTS.md`), root documents use filename only (`CLAUDE.md`).\n\n### ANSI Stripping Regex (from progress.ts)\n\n`/\\x1b\\[[0-9;]*m/g` matches all SGR, cursor, and erase sequences before writing to `.agents-reverse-engineer/progress.log`.\n\n### Trace Filename Format (from trace.ts)\n\n`trace-{timestamp}.ndjson` where timestamp from `new Date().toISOString().replace(/[:.]/g, '-')` (e.g., `trace-2026-02-09T12-34-56-789Z.ndjson`).\n\n## File Relationships\n\n`CommandRunner` orchestrates via dependencies: `runPool()` for worker execution, `PlanTracker` for checkbox updates, `ProgressReporter` for console output, `ProgressLog` for file mirroring, `TraceWriter` for NDJSON events, `AIService` for subprocess calls, `buildFilePrompt/buildDirectoryPrompt/buildRootPrompt` from generation module, `writeSumFile/readSumFile/writeAnnexFile/writeAgentsMd` from writers, `checkCodeVsDoc/checkCodeVsCode/checkPhantomPaths` from quality module, `computeContentHashFromString` from change-detection.\n\n`runPool()` invokes task factories, emits trace events via `tracer?.emit()`, invokes `onComplete` callback per task settlement, returns `TaskResult[]` array.\n\n`ProgressReporter` receives events via `onFileStart/Done/Error/DirectoryStart/Done/RootDone`, computes ETA via sliding windows, delegates to `ProgressLog.write()` for file mirroring, prints summary via `printSummary(RunSummary)`.\n\n`PlanTracker` initialized with `formatExecutionPlanAsMarkdown()` output, updated via `markDone(itemPath)` from pool workers, flushed via `flush()` before orchestrator return.\n\n`TraceWriter` emits events via `emit(partial)`, auto-populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes writes via promise chain, finalized via `finalize()` after all phases complete.\n### src/output/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal output formatting layer providing color-aware Logger interface for CLI messages with optional ANSI styling via picocolors.\n\n## Contents\n\n**[logger.ts](./logger.ts)** — Exports `createLogger(options)` factory constructing color-aware Logger instances writing to console.log/warn/error with picocolors-based ANSI formatting, `createSilentLogger()` returning no-op Logger for testing, `Logger` interface defining six output methods (info/file/excluded/summary/warn/error), and `LoggerOptions` schema controlling color enablement.\n\n## API Surface\n\n**Logger interface:**\n- `info(message: string): void` — informational messages (plain text)\n- `file(path: string): void` — discovered file paths (`\"  +\" + path` in green)\n- `excluded(path: string, reason: string, filter: string): void` — excluded files (`\"  -\" + path + \" (reason: filter)\"` dimmed)\n- `summary(included: number, excluded: number): void` — discovery count summary (`\"Discovered N files (M excluded)\"` with bold/dim styling)\n- `warn(message: string): void` — warning messages (`\"Warning: \" + message` in yellow)\n- `error(message: string): void` — error messages (`\"Error: \" + message` in red)\n\n**Factory functions:**\n- `createLogger(options: LoggerOptions): Logger` — constructs color-aware logger respecting `options.colors` boolean flag\n- `createSilentLogger(): Logger` — returns no-op logger with all methods bound to empty function\n\n**Configuration:**\n- `LoggerOptions` — schema with single `colors: boolean` field controlling ANSI escape code emission\n\n## Output Format Specification\n\nLogger implements CONTEXT.md-defined format with mandatory prefixes and styling:\n- Discovered files: `\"  +\"` prefix green-styled followed by space and path\n- Excluded files: `\"  -\"` prefix dimmed followed by path and parenthetical `(reason: filter)`\n- Summary line: bold `\"Discovered N files\"` followed by dimmed `\" (M excluded)\"`\n- Warnings: `\"Warning: \"` yellow-styled prefix\n- Errors: `\"Error: \"` red-styled prefix\n\nWhen `options.colors === false`, all picocolors transforms replaced with identity function via `noColor` object mapping `green`/`dim`/`red`/`bold`/`yellow` to pass-through.\n\n## Integration Points\n\nUsed by:\n- `src/discovery/run.ts` — `discoverFiles()` accepts Logger for real-time file discovery progress reporting\n- `src/cli/*.ts` — Command entry points construct logger via `createLogger({ colors: config.output.colors })` from loaded configuration\n- Test suites — `createSilentLogger()` suppresses output during programmatic invocation\n\nImports:\n- `picocolors` (aliased `pc`) — ANSI color code generation library\n### src/quality/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation system detecting code-documentation inconsistencies (code-vs-doc), duplicate symbol exports (code-vs-code), and unresolvable path references (phantom-paths) through regex-based extraction, filesystem resolution, and structured reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()`, `validateFindability()` from submodules alongside all type definitions (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`).\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator, `InconsistencySeverity` literal union (`'info' | 'warning' | 'error'`), and `InconsistencyReport` structure containing `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, and `summary` counts by type/severity.\n\n## Validation Modules\n\n### Code-vs-Doc Consistency\n\n`inconsistency/code-vs-doc.ts` exports `extractExports()` applying regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` to extract identifier names, and `checkCodeVsDoc()` performing substring search in `.sum` summary text to detect undocumented exports. Returns `CodeDocInconsistency` with `missingFromDoc[]` array when symbols absent from documentation.\n\n### Code-vs-Code Duplication\n\n`inconsistency/code-vs-code.ts` exports `checkCodeVsCode()` aggregating exports across per-directory file groups into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Caller must scope input to directory boundaries to prevent false positives from legitimate cross-directory symbol reuse.\n\n### Phantom Path Resolution\n\n`phantom-paths/validator.ts` extracts path strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded keywords), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` extension fallbacks, validates via `existsSync()`. Returns `PhantomPathInconsistency[]` with `severity: 'warning'` for unresolved references after filtering exclusions (`node_modules`, URLs, template literals, globs).\n\n### Report Generation\n\n`inconsistency/reporter.ts` exports `buildInconsistencyReport()` aggregating issues with summary computation (total/per-type/per-severity counts) and `formatReportForCli()` rendering plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting.\n\n### Density Validation (Disabled)\n\n`density/validator.ts` exports stub `validateFindability()` returning empty `FindabilityResult[]` array. Originally verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via substring matching; disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation.\n\n## Behavioral Contracts\n\n**Export extraction pattern:**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path patterns:**\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**CLI report format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n\n[WARN] Phantom path reference: \"src/missing.ts\" does not exist\n  AGENTS.md: src/quality/AGENTS.md\n  Referenced: src/missing.ts\n```\n\n## Known Limitations\n\nRegex-based `extractExports()` misses destructured exports, namespace exports, dynamic exports, and re-exports with renaming. Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface. Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Phantom path resolution uses heuristic extension fallbacks (`.js` → `.ts`) without TypeScript compiler path mapping awareness.\n### src/quality/density/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub validation module for findability analysis (symbol presence in aggregated AGENTS.md), disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability(_agentsMdContent, _sumFiles)` returning empty `FindabilityResult[]` array; previously verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via string matching, now disabled awaiting post-processing extraction support.\n\n## Implementation Status\n\n`validateFindability()` signature preserved for future re-implementation but returns `[]` unconditionally. Original logic performed heuristic substring search to detect whether key symbols from source file summaries appeared in aggregated directory documentation. Disabled when `SumFileContent` schema removed structured `publicInterface` field (see `src/generation/writers/sum.ts`). Module retained to support future LLM-based or AST-based symbol extraction passes.\n\n## Type Surface\n\n`FindabilityResult` interface with fields:\n- `filePath: string` — validated `.sum` file path\n- `symbolsTested: string[]` — symbols checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio `symbolsFound.length / symbolsTested.length` (0-1 range)\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for parameter type annotation. No runtime dependencies since function body returns empty array.\n### src/quality/inconsistency/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nDetects and reports three categories of code-documentation discrepancies: exported symbols missing from `.sum` summaries (code-vs-doc), duplicate exports across files (code-vs-code), and unresolvable path references in `AGENTS.md` (phantom-paths).\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — Exports `extractExports()` (regex-based identifier extraction via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`) and `checkCodeVsDoc()` (compares source exports against `SumFileContent.summary` via substring matching, returns `CodeDocInconsistency` with `missingFromDoc` array or `null`).\n\n**[code-vs-code.ts](./code-vs-code.ts)** — Exports `checkCodeVsCode()` aggregating exports across scoped file groups into `Map<symbol, paths[]>`, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`.\n\n**[reporter.ts](./reporter.ts)** — Exports `buildInconsistencyReport()` (aggregates `Inconsistency[]` into `InconsistencyReport` with per-type/per-severity summary counts) and `formatReportForCli()` (renders plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting).\n\n## Validation Strategy\n\n**Code-vs-Doc:** Regex extraction from source followed by case-sensitive substring search in `.sum` summary text. Does not use AST analysis, yielding false negatives for destructured/namespace/dynamic exports and prose mentions unrelated to API surface.\n\n**Code-vs-Code:** Symbol-name deduplication across per-directory file groups. Caller must scope input to prevent false positives from legitimate cross-directory symbol reuse (e.g., multiple `index.ts` files exporting `Config`).\n\n**Phantom Paths:** Not implemented in this directory. Handled by `../phantom-paths/validator.ts` which extracts markdown links, backtick paths, and prose-embedded paths from `AGENTS.md`, resolves via `existsSync()` with `.ts`/`.js` fallback.\n\n## File Relationships\n\n`code-vs-code.ts` imports `extractExports()` from `code-vs-doc.ts` for symbol extraction reuse. `reporter.ts` consumes discriminated union `Inconsistency` (from `../types.ts`) unifying `CodeDocInconsistency`, `CodeCodeInconsistency`, and `PhantomPathInconsistency`. Orchestrator in `../index.ts` invokes validators and passes results to `buildInconsistencyReport()` → `formatReportForCli()` pipeline.\n\n## Behavioral Contracts\n\n**Export extraction pattern:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword (`function`/`class`/`const`/`let`/`var`/`type`/`interface`/`enum`), captures identifier `(\\w+)`.\n\n**CLI output format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags followed by description and type-specific paths (code-vs-doc shows `filePath`, code-vs-code shows `files.join(', ')`, phantom-path shows `agentsMdPath` and `details.referencedPath`).\n\n## Known Limitations\n\nRegex-based extraction misses destructured exports (`export const { foo } = obj`), namespace exports (`export * as Utils from './utils'`), dynamic exports (`export { [computedName]: value }`), re-exports with renaming (`export { foo as bar } from './mod'`). No AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Substring matching yields false negatives when prose mentions symbols in non-API contexts.\n### src/quality/phantom-paths/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nValidates path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against filesystem locations with TypeScript/JavaScript extension fallbacks, and reporting unresolved references as `PhantomPathInconsistency` objects in quality reports.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export of `checkPhantomPaths` validator function.\n\n**[validator.ts](./validator.ts)** — Extracts path references from `AGENTS.md` content using `PATH_PATTERNS` (markdown links, backtick-quoted paths, prose-embedded paths), resolves each path against `agentsMdDir` and `projectRoot` with `.ts`/`.js` extension fallback via `tryPaths[]`, filters excluded patterns (URLs, template literals, globs), and returns `PhantomPathInconsistency[]` array with deduplication via `seen` Set.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` captures three reference types:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths with extensions\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded paths following contextual keywords\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n\n1. Relative to `AGENTS.md` directory: `path.resolve(agentsMdDir, rawPath)`\n2. Relative to project root: `path.resolve(projectRoot, rawPath)`\n3. TypeScript convention fallback: append `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` when `rawPath.endsWith('.js')`\n\nValidates existence via `existsSync()` on all candidate paths; returns `PhantomPathInconsistency` with `severity: 'warning'` when all attempts fail.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` filters non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` — Dependency/VCS paths\n- `/^https?:/` — URLs\n- `/\\{\\{/`, `/\\$\\{/` — Template placeholders/literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` — Glob patterns/brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` containing:\n- `agentsMdPath` — Normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` — `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` — Original extracted path\n- `details.resolvedTo` — Attempted resolution relative to project root\n- `details.context` — Containing line truncated to 120 characters\n### src/specify/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify/\n\nProject specification synthesis from AGENTS.md documentation corpus via AI-driven prompt engineering. Exports `buildSpecPrompt()` for constructing two-part prompts (system constraints + aggregated AGENTS.md content), `writeSpec()` for filesystem output with overwrite protection and multi-file splitting, and `SpecExistsError` for conflict detection.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel re-export aggregating `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions` from `prompts.ts` and `writer.ts` submodules. Consumed by `src/cli/specify.ts` command orchestrator.\n\n**[prompts.ts](./prompts.ts)** — Prompt template factory exporting `buildSpecPrompt(docs, annexFiles?)` which constructs `SpecPrompt` pairs: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization) and user content (concatenated AGENTS.md markdown with section delimiters, optional annex files for reproduction-critical constants). Returns structured prompt with mandatory output format rules (raw markdown, no preamble, verbatim reproduction of prompt templates/regex/IDE templates).\n\n**[writer.ts](./writer.ts)** — Filesystem writer exporting `writeSpec(content, options)` supporting single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) output modes. Implements pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `splitByHeadings()` (regex `/^(?=# )/m`), filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering), and directory creation with `mkdir(outputDir, { recursive: true })`.\n\n## Architecture\n\n**Prompt Construction Pipeline:**  \n`buildSpecPrompt()` receives `AgentsDocs[]` from `../generation/collector.js` (`collectAgentsDocs()` recursive traversal) and optional annex files (`collectAnnexFiles()`). Constructs user prompt by iterating docs array, emitting `### ${doc.relativePath}\\n\\n${doc.content}` sections, appending annex section if provided, injecting output requirements listing 11 mandatory sections. System prompt (`SPEC_SYSTEM_PROMPT`) prohibits folder-mirroring, exact file path prescription, mandates verbatim reproduction of behavioral contracts (regex patterns, format strings, magic constants, environment variables, prompt templates, IDE templates).\n\n**Filesystem Writing Pipeline:**  \n`writeSpec()` branches on `options.multiFile`. Single-file mode: checks `fileExists(outputPath)`, throws `SpecExistsError([outputPath])` if exists and `force=false`, creates parent directory, writes content. Multi-file mode: calls `splitByHeadings()` to partition by top-level `# ` headings, checks all target paths for conflicts, throws `SpecExistsError(conflicts)` if any exist and `force=false`, writes each section to slugified filename.\n\n## Behavioral Contracts\n\n**SPEC_SYSTEM_PROMPT Section 7 (Behavioral Contracts):**  \nEnforces two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Prohibits paraphrasing regex into prose.\n\n**SPEC_SYSTEM_PROMPT Section 10 (Prompt Templates):**  \nRequires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\n**SPEC_SYSTEM_PROMPT Section 11 (IDE Integration):**  \nRequires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n**Content Splitting Regex:**  \n`/^(?=# )/m` matches lines starting with exactly `# ` (top-level headings). Content before first heading assigned to `'00-preamble.md'`.\n\n**Filename Sanitization Transform:**  \n`slugify()` applies sequence: lowercase → whitespace (`/\\s+/g`) to hyphen → strip non-alphanumeric except hyphens (`/[^a-z0-9-]/g`) → collapse hyphens (`/-+/g`) → trim edge hyphens (`/^-|-$/g`).\n\n## Integration Points\n\n**Upstream:** `src/cli/specify.ts` orchestrates workflow: validates project root, collects AGENTS.md via `collectAgentsDocs()`, optionally collects annex files via `collectAnnexFiles()`, calls `buildSpecPrompt(docs, annexFiles)`, invokes `AIService.call()` with returned `SpecPrompt`, passes AI output to `writeSpec()` with options from CLI flags (`--force`, `--multi-file`, `--output`).\n\n**Downstream:** `writeSpec()` consumes `node:fs/promises` (`writeFile`, `mkdir`, `access`), `node:fs` (`constants.F_OK`), `node:path` (`dirname`, `join`). Throws `SpecExistsError` caught by CLI for user-facing error messages.\n### src/types/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared type definitions for file discovery results and statistics consumed across discovery, orchestration, and CLI modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `DiscoveryStats`, `ExcludedFile` interfaces defining discovery phase output schema, exclusion metadata, and aggregate metrics.\n\n## Exported Interfaces\n\n### `ExcludedFile`\nRepresents files filtered during discovery with `path: string` and `reason: string` (exclusion rationale: \"gitignore pattern\", \"binary file\", \"vendor directory\").\n\n### `DiscoveryResult`\nAggregates discovery output with `files: string[]` (paths for Phase 1 analysis) and `excluded: ExcludedFile[]` (filtered files with metadata).\n\n### `DiscoveryStats`\nProvides discovery metrics: `totalFiles`, `includedFiles`, `excludedFiles` (counts), `exclusionReasons: Record<string, number>` (histogram of `ExcludedFile.reason` values).\n\n## Data Flow\n\n**Producer:** `runDiscovery()` in `src/discovery/run.ts` returns `DiscoveryResult` after filter chain execution (`src/discovery/walker.ts` + filters: `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`).\n\n**Consumers:**\n- `src/cli/discover.ts` — Generates `GENERATION-PLAN.md` from `DiscoveryResult.files`\n- `src/cli/generate.ts` — Feeds `DiscoveryResult.files` to Phase 1 worker pool\n- `src/cli/update.ts` — Inputs `DiscoveryResult` to `detectChanges()` for delta computation\n- `src/output/logger.ts` — Computes `DiscoveryStats` from `DiscoveryResult` for terminal output\n\n**Exclusion Metadata:** `ExcludedFile.reason` populated by filter modules:\n- `src/discovery/filters/gitignore.ts` — \"matched .gitignore pattern: `<pattern>`\"\n- `src/discovery/filters/binary.ts` — \"binary file\"\n- `src/discovery/filters/vendor.ts` — \"vendor directory\"\n- `src/discovery/filters/custom.ts` — \"matched exclude pattern: `<pattern>`\"\n### src/update/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nOrchestrates incremental documentation updates via SHA-256 content hash comparison against `.sum` YAML frontmatter, deleting orphaned artifacts and computing affected directory sets without requiring git diff parsing.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module exporting `UpdateOrchestrator` class, factory `createUpdateOrchestrator()`, cleanup utilities `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()`, and interfaces `UpdatePlan`, `UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`.\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class coordinates change detection by reading stored `content_hash` from `.sum` frontmatter via `readSumFile()`, comparing against `computeContentHash()` output, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (sorted depth descending). Methods: `preparePlan()`, `checkPrerequisites()`, `isFirstRun()`. Emits `plan:created` trace events.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources via `deleteIfExists()`, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles), `getAffectedDirectories()` walks parent directories via `path.dirname()` until `.` or absolute path, returning `Set<string>` of directories requiring regeneration.\n\n**[types.ts](./types.ts)** — Interfaces: `CleanupResult` (`deletedSumFiles`, `deletedAgentsMd` arrays), `UpdateOptions` (`includeUncommitted`, `dryRun` flags), `UpdateResult` (`analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`, `dryRun`), `UpdateProgress` callbacks (`onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Update Workflow\n\n1. **Change Detection:** `preparePlan()` discovers files via `runDiscovery()` from `src/discovery/`, reads `.sum` frontmatter via `readSumFile()` from `src/generation/writers/sum.js`, compares `content_hash` against `computeContentHash()` from `src/change-detection/`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged).\n\n2. **Orphan Cleanup:** `cleanupOrphans()` processes `FileChange[]` with `status: 'deleted' | 'renamed'`, extracts `path` (deleted) or `oldPath` (renamed), constructs `.sum` and `.annex.md` paths, deletes via `unlink()` unless `dryRun: true`.\n\n3. **Affected Directories:** `getAffectedDirectories()` walks parent chains for all changed files, returns `Set<string>` sorted by `path.sep` depth descending (deepest first), ensuring `AGENTS.md` regeneration propagates upward.\n\n4. **Empty Directory Cleanup:** `cleanupEmptyDirectoryDocs()` filters directory entries excluding dotfiles, `.sum`/`.annex.md` extensions, `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`), deletes `AGENTS.md` if zero source files remain.\n\n5. **Execution:** `src/cli/update.ts` invokes `preparePlan()`, executes Phase 1 pool for `filesToAnalyze` via `src/generation/executor.ts`, regenerates `AGENTS.md` sequentially for `affectedDirs` (no post-order traversal required).\n\n## Trace Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta\n\n## Behavioral Contracts\n\n**Depth-descending sort:** `affectedDirs` sorted via `path.relative().split(path.sep).length` comparator (deepest first) ensuring child directories process before parents.\n\n**Orphan path construction:** `.sum` path = `path.join(projectRoot, \\`${relativePath}.sum\\`)`, `.annex.md` path = `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`.\n\n**Directory emptiness filter:** Exclude entries matching `entry.startsWith('.')` OR `entry.endsWith('.sum')` OR `entry.endsWith('.annex.md')` OR `GENERATED_FILES.has(entry)`.\n\n**Upward traversal terminator:** `getAffectedDirectories()` stops when `dir === '.'` OR `path.isAbsolute(dir) === true`.\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed()` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — returns `0`, no run history database in frontmatter mode\n- `getLastRun()` — returns `undefined`, no run history available\n- `close()` — no resources to dispose\n\n## Annex Files (3 reproduction-critical source files)\n\n### src/generation/prompts/templates.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: templates.ts\n\nReproduction-critical source content from `templates.ts`.\nReferenced by `templates.ts.sum`.\n\n```\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`{{LANG}}\n{{CONTENT}}\n\\`\\`\\`\n{{PROJECT_PLAN_SECTION}}\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n### src/generation/prompts/types.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: types.ts\n\nReproduction-critical source content from `types.ts`.\nReferenced by `types.ts.sum`.\n\n```\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n### src/orchestration/runner.ts.annex.md\n\n<!-- Generated by agents-reverse-engineer -->\n# Annex: runner.ts\n\nReproduction-critical source content from `runner.ts`.\nReferenced by `runner.ts.sum`.\n\n```\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n\n## Output Requirements\n\nThe specification MUST include these sections in order:\n1. Project Overview (purpose, value, tech stack)\n2. Architecture (module boundaries, data flow, design decisions)\n3. Public API Surface (all exported interfaces, full type signatures)\n4. Data Structures & State (types, schemas, config objects)\n5. Configuration (options, types, defaults, validation)\n6. Dependencies (each with version and rationale)\n7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)\n8. Test Contracts (per-module test scenarios and edge cases)\n9. Build Plan (phased implementation order with dependencies)\n10. Prompt Templates & System Instructions (FULL verbatim text from annex content)\n11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim from annex content)\n\nSections 10 and 11 MUST reproduce annex content verbatim.\nDo NOT summarize prompt templates or IDE templates into prose descriptions.\n\nOutput ONLY the markdown content. No preamble.",
      "systemPrompt": "You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence: what to build first and why, dependency order between modules, incremental milestones\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.",
      "response": "# agents-reverse-engineer Project Specification\n\n## 1. Project Overview\n\n**agents-reverse-engineer (ARE)** automates brownfield codebase documentation for AI coding assistants through a three-phase Recursive Language Model (RLM) pipeline: concurrent `.sum` file generation via AI CLI subprocesses with resource-bounded worker pools, post-order `AGENTS.md` directory aggregation consuming child summaries, and platform-specific root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery and SHA-256 incremental updates.\n\n**Core Value Proposition:** Transforms undocumented codebases into AI-navigable documentation by analyzing source files concurrently, synthesizing directory-level overviews bottom-up, and generating platform-tailored integration documents—enabling AI assistants to reconstruct project architecture, understand module boundaries, and locate relevant code without reading entire repositories.\n\n**Problem Solved:** Eliminates manual documentation maintenance burden for brownfield projects by automating generation from source code through AI analysis, detecting code-documentation inconsistencies through quality validators, and supporting incremental updates via content hash comparison to minimize regeneration costs.\n\n**Technology Stack:**\n- **Runtime:** Node.js ≥18.0.0 (ES modules, `type: \"module\"` in package.json)\n- **Language:** TypeScript 5.7.3 (target ES2022, module NodeNext, strict mode)\n- **Build:** TypeScript compiler (`tsc`) emitting to `dist/`, npm scripts for lifecycle hooks\n- **Dependencies:**\n  - `fast-glob` ^3.3.2 — glob pattern file discovery\n  - `ignore` ^7.0.0 — gitignore parsing\n  - `isbinaryfile` ^5.0.4 — binary file detection\n  - `simple-git` ^3.27.0 — git diff parsing for change detection\n  - `yaml` ^2.7.0 — config file serialization\n  - `zod` ^3.24.1 — schema validation\n  - `ora` ^8.1.1 — terminal spinner UI\n  - `picocolors` ^1.1.1 — ANSI color formatting\n- **AI Backends:** Claude Code CLI (`@anthropic-ai/claude-code`), Gemini CLI (stub), OpenCode CLI (stub)\n- **Distribution:** npm package with binary entry points (`are`, `agents-reverse-engineer`), GitHub Actions workflow for provenance attestation\n\n**Version:** 0.6.6  \n**License:** MIT (GeoloeG-IsT, 2026)\n\n## 2. Architecture\n\n### Module Boundaries\n\n**Discovery Layer** (`src/discovery/`): Gitignore-aware file traversal composing four-stage filter chain (gitignore → vendor → binary → custom) over `fast-glob` results. Returns `DiscoveryResult` with `included: string[]` and `excluded: ExcludedFile[]`.\n\n**AI Service Layer** (`src/ai/`): Backend-agnostic abstraction with `AIBackend` adapter registry supporting Claude/Gemini/OpenCode. Manages subprocess lifecycle via `execFile()`, implements exponential backoff retry on rate limits, enforces resource constraints (512MB heap, 4-thread libuv pool), tracks token costs via `TelemetryLogger`.\n\n**Generation Layer** (`src/generation/`): Orchestrates three-phase pipeline. `GenerationOrchestrator` prepares files, creates tasks with embedded prompts. `buildExecutionPlan()` constructs dependency graph (files → directories → roots) with post-order depth sorting. Phase 1 writes `.sum` files with YAML frontmatter. Phase 2 writes `AGENTS.md` consuming child summaries. Phase 3 writes platform-specific root documents.\n\n**Orchestration Layer** (`src/orchestration/`): Iterator-based worker pool sharing `tasks.entries()` across N workers to prevent over-allocation. `CommandRunner` executes three-phase pipeline invoking `AIService.call()` per task. `ProgressReporter` streams console output with ETA calculation via moving average. `PlanTracker` serializes `GENERATION-PLAN.md` checkbox updates via promise chains. `TraceWriter` emits NDJSON events with auto-populated `seq`/`ts`/`pid`/`elapsedMs`.\n\n**Quality Layer** (`src/quality/`): Post-generation validators extracting exports via regex, comparing against `.sum` summaries (code-vs-doc), detecting duplicate symbols (code-vs-code), resolving path references in `AGENTS.md` (phantom-paths). Returns `InconsistencyReport` with discriminated union issues.\n\n**Change Detection Layer** (`src/change-detection/`): Computes SHA-256 content hashes for incremental updates. Parses `git diff --name-status -M` for rename detection. Maps status codes (`A`/`M`/`D`/`R*`) to `FileChange` discriminated union.\n\n**Update Layer** (`src/update/`): Compares `.sum` frontmatter `content_hash` against current file hash. Returns `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (depth-sorted). `cleanupOrphans()` deletes stale `.sum`/`.annex.md`. `getAffectedDirectories()` walks parent chains.\n\n**Config Layer** (`src/config/`): Loads `.agents-reverse-engineer/config.yaml` with Zod validation. Computes memory-aware concurrency via `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` where `memCap = floor((totalMemGB × 0.5) / 0.512)`.\n\n**Installer Layer** (`src/installer/`): npx-based orchestration installing commands/hooks to `~/.claude/`, `~/.config/opencode/`, `~/.gemini/`. Resolves paths via environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`). Modifies `settings.json` for hook registration and bash permissions.\n\n**Integration Layer** (`src/integration/`): Generates platform-specific command templates with progress-monitoring patterns (background execution, 15s poll intervals, `progress.log` tailing). Detects environments via marker files (`.claude/`, `.opencode/`, `.gemini/`, `.aider/`).\n\n**CLI Layer** (`src/cli/`): Command entry points (init/discover/generate/update/clean/specify) parsing args, loading config, invoking orchestrators. Exit codes: 0 (success), 1 (partial failure or config exists), 2 (total failure or CLI not found).\n\n### Data Flow Patterns\n\n**Generation Pipeline:**\n1. `discoverFiles()` walks directory, applies filter chain, returns `DiscoveryResult`\n2. `GenerationOrchestrator.createPlan()` reads source files, embeds into prompts, returns `GenerationPlan` with `tasks[]`\n3. `buildExecutionPlan()` constructs dependency graph, sorts by depth descending (deepest first)\n4. `CommandRunner.executeGenerate()` invokes Phase 1 pool with `runPool()`, Phase 2 depth-grouped sequential, Phase 3 sequential\n5. Each task calls `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI\n6. Writers call `writeSumFile()` (YAML frontmatter + markdown), `writeAgentsMd()` (marker + user content preservation), `writeFile()` (root docs)\n7. Post-generation validators call `checkCodeVsDoc()`/`checkCodeVsCode()`/`checkPhantomPaths()` at concurrency=10\n8. `TelemetryLogger.toRunLog()` serializes to `.agents-reverse-engineer/logs/run-{timestamp}.json`\n\n**Incremental Update Flow:**\n1. `UpdateOrchestrator.preparePlan()` discovers files, reads `.sum` frontmatter\n2. Compares `content_hash` against `computeContentHash()` current file SHA-256\n3. Populates `filesToAnalyze` (added/modified), `filesToSkip` (unchanged)\n4. `cleanupOrphans()` deletes stale `.sum`/`.annex.md` for deleted/renamed files\n5. `getAffectedDirectories()` walks parent chains, sorts by depth descending\n6. `CommandRunner.executeUpdate()` regenerates `.sum` for `filesToAnalyze` via Phase 1 pool\n7. CLI loops `affectedDirs` calling `buildDirectoryPrompt()` + `writeAgentsMd()` sequentially\n\n**Subprocess Resource Management:**\n1. `runPool()` shares `tasks.entries()` iterator across N workers\n2. Each worker calls `AIService.call()` → `runSubprocess()`\n3. `execFile()` spawns child with env vars: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`\n4. Timeout timer sends SIGTERM at `timeoutMs`, schedules SIGKILL at `timeoutMs + 5000`\n5. Process group killing via `kill(-pid)` terminates subprocess tree\n6. `withRetry()` implements exponential backoff on rate-limit errors (stderr pattern matching)\n\n### Key Design Decisions\n\n**Iterator-Based Pool Over Batch Chunking:** Prevents idle workers waiting for slowest task in batch. Shared iterator ensures workers race to pull tasks atomically via iterator protocol.\n\n**Promise-Chain Write Serialization:** `PlanTracker`, `ProgressLog`, `TraceWriter` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern. Prevents NDJSON corruption from parallel worker writes.\n\n**Post-Order Directory Traversal:** Depth-descending sort (`getDirectoryDepth(dirB) - getDirectoryDepth(dirA)`) ensures child `.sum` files exist before parent `AGENTS.md` generation. `isDirectoryComplete()` predicate gates directory task execution.\n\n**Memory-Aware Concurrency:** Default formula `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` prevents RAM exhaustion where `cores × 5` spawns too many 512MB subprocesses. WSL environments default to concurrency=2.\n\n**Frontmatter-Based Change Detection Over Git:** SHA-256 `content_hash` in `.sum` YAML frontmatter enables incremental updates without git dependency. Supports non-git workflows while git integration provides rename detection via `git diff -M`.\n\n**User Content Preservation Via Rename:** Existing `AGENTS.md` lacking `<!-- Generated by agents-reverse-engineer -->` marker renamed to `AGENTS.local.md`, prepended above generated content with `---` separator.\n\n**Stub Backends Throwing Errors:** Gemini/OpenCode adapters throw `AIServiceError('SUBPROCESS_ERROR')` until JSON output stabilizes, demonstrating extension pattern without blocking Claude backend usage.\n\n## 3. Public API Surface\n\n### CLI Commands (`src/cli/index.ts`)\n\n```typescript\nfunction parseArgs(args: string[]): {\n  command?: string;\n  positional: string[];\n  flags: Set<string>;\n  values: Map<string, string>;\n}\n\nfunction runInstaller(): Promise<void>\n```\n\n### Discovery (`src/discovery/run.ts`)\n\n```typescript\ninterface DiscoveryResult {\n  files: string[];\n  excluded: ExcludedFile[];\n}\n\ninterface ExcludedFile {\n  path: string;\n  reason: string;\n}\n\nasync function discoverFiles(\n  rootPath: string,\n  config: DiscoveryConfig,\n  logger: Logger,\n  options?: DiscoverFilesOptions\n): Promise<DiscoveryResult>\n```\n\n### Generation (`src/generation/orchestrator.ts`)\n\n```typescript\ninterface GenerationPlan {\n  projectRoot: string;\n  files: PreparedFile[];\n  tasks: ExecutionTask[];\n  projectStructure?: string;\n  complexity: ComplexityMetrics;\n}\n\ninterface ExecutionPlan {\n  projectRoot: string;\n  projectStructure?: string;\n  fileTasks: ExecutionTask[];\n  directoryTasks: ExecutionTask[];\n  rootTasks: ExecutionTask[];\n}\n\nclass GenerationOrchestrator {\n  constructor(projectRoot: string, config: Config, tracer?: ITraceWriter);\n  async createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>;\n}\n\nfunction buildExecutionPlan(plan: GenerationPlan): ExecutionPlan\n```\n\n### AI Service (`src/ai/service.ts`)\n\n```typescript\ninterface AICallOptions {\n  prompt: string;\n  systemPrompt?: string;\n  model?: string;\n  timeoutMs?: number;\n  maxTurns?: number;\n  taskLabel?: string;\n}\n\ninterface AIResponse {\n  text: string;\n  model: string;\n  inputTokens: number;\n  outputTokens: number;\n  cacheReadTokens?: number;\n  cacheCreationTokens?: number;\n  durationMs: number;\n  exitCode: number;\n  raw: unknown;\n}\n\nclass AIService {\n  constructor(backend: AIBackend, options: AIServiceOptions);\n  async call(options: AICallOptions): Promise<AIResponse>;\n  setDebug(enabled: boolean): void;\n  setTracer(tracer: ITraceWriter): void;\n  addFilesReadToLastEntry(files: FileRead[]): void;\n  getSummary(): TelemetrySummary;\n  async finalize(projectRoot: string): Promise<void>;\n}\n```\n\n### AI Backend Registry (`src/ai/registry.ts`)\n\n```typescript\ninterface AIBackend {\n  isAvailable(): Promise<boolean>;\n  buildArgs(options: AICallOptions): string[];\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse;\n  getInstallInstructions(): string;\n}\n\nclass BackendRegistry {\n  register(name: string, backend: AIBackend): void;\n  get(name: string): AIBackend | undefined;\n  getAll(): Map<string, AIBackend>;\n}\n\nfunction createBackendRegistry(): BackendRegistry\n\nasync function resolveBackend(\n  registry: BackendRegistry,\n  requested: string\n): Promise<AIBackend>\n```\n\n### Orchestration (`src/orchestration/runner.ts`)\n\n```typescript\ninterface CommandRunOptions {\n  concurrency: number;\n  failFast?: boolean;\n  debug?: boolean;\n  dryRun?: boolean;\n  tracer?: ITraceWriter;\n  progressLog?: ProgressLog;\n}\n\ninterface RunSummary {\n  version: string;\n  filesProcessed: number;\n  filesFailed: number;\n  filesSkipped: number;\n  totalCalls: number;\n  totalInputTokens: number;\n  totalOutputTokens: number;\n  totalCacheReadTokens?: number;\n  totalCacheCreationTokens?: number;\n  totalDurationMs: number;\n  errorCount: number;\n  retryCount: number;\n  totalFilesRead: number;\n  uniqueFilesRead: number;\n  inconsistenciesCodeVsDoc?: number;\n  inconsistenciesCodeVsCode?: number;\n  phantomPaths?: number;\n  inconsistencyReport?: InconsistencyReport;\n}\n\nclass CommandRunner {\n  constructor(aiService: AIService, options: CommandRunOptions);\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary>;\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config\n  ): Promise<RunSummary>;\n}\n```\n\n### Worker Pool (`src/orchestration/pool.ts`)\n\n```typescript\ninterface PoolOptions {\n  concurrency: number;\n  failFast?: boolean;\n  tracer?: ITraceWriter;\n  phaseLabel?: string;\n  taskLabels?: string[];\n}\n\ninterface TaskResult<T> {\n  index: number;\n  success: boolean;\n  value?: T;\n  error?: Error;\n}\n\nasync function runPool<T>(\n  tasks: Array<() => Promise<T>>,\n  options: PoolOptions,\n  onComplete?: (result: TaskResult<T>) => void\n): Promise<Array<TaskResult<T>>>\n```\n\n### Configuration (`src/config/loader.ts`)\n\n```typescript\ninterface Config {\n  exclude: ExcludeConfig;\n  options: OptionsConfig;\n  output: OutputConfig;\n  ai: AIConfig;\n}\n\nasync function loadConfig(\n  root: string,\n  options?: { tracer?: ITraceWriter }\n): Promise<Config>\n\nasync function writeDefaultConfig(root: string): Promise<void>\n\nfunction configExists(root: string): boolean\n```\n\n### Change Detection (`src/change-detection/detector.ts`)\n\n```typescript\ntype ChangeType = 'added' | 'modified' | 'deleted' | 'renamed';\n\ninterface FileChange {\n  path: string;\n  status: ChangeType;\n  oldPath?: string;\n}\n\ninterface ChangeDetectionResult {\n  changes: FileChange[];\n  baseCommit: string | null;\n  currentCommit: string | null;\n}\n\nasync function isGitRepo(projectRoot: string): Promise<boolean>\n\nasync function getCurrentCommit(projectRoot: string): Promise<string | null>\n\nasync function getChangedFiles(\n  projectRoot: string,\n  options?: ChangeDetectionOptions\n): Promise<ChangeDetectionResult>\n\nasync function computeContentHash(filePath: string): Promise<string>\n\nfunction computeContentHashFromString(content: string): string\n```\n\n### Update Orchestration (`src/update/orchestrator.ts`)\n\n```typescript\ninterface UpdatePlan {\n  filesToAnalyze: FileChange[];\n  filesToSkip: string[];\n  cleanup: CleanupResult;\n  affectedDirs: string[];\n  isFirstRun: boolean;\n}\n\nclass UpdateOrchestrator {\n  constructor(projectRoot: string, config: Config, tracer?: ITraceWriter);\n  async preparePlan(\n    discoveryResult: DiscoveryResult,\n    options: UpdateOptions\n  ): Promise<UpdatePlan>;\n  isFirstRun(): boolean;\n}\n\nfunction createUpdateOrchestrator(\n  projectRoot: string,\n  config: Config,\n  tracer?: ITraceWriter\n): UpdateOrchestrator\n```\n\n### Quality Validation (`src/quality/index.ts`)\n\n```typescript\ntype Inconsistency = \n  | CodeDocInconsistency \n  | CodeCodeInconsistency \n  | PhantomPathInconsistency;\n\ninterface InconsistencyReport {\n  metadata: {\n    timestamp: string;\n    projectRoot: string;\n    filesChecked: number;\n    durationMs: number;\n  };\n  issues: Inconsistency[];\n  summary: {\n    total: number;\n    codeVsDoc: number;\n    codeVsCode: number;\n    phantomPath: number;\n    byType: Record<string, number>;\n    bySeverity: Record<string, number>;\n  };\n}\n\nfunction extractExports(sourceContent: string): string[]\n\nfunction checkCodeVsDoc(\n  sourceContent: string,\n  sumFile: SumFileContent,\n  filePath: string\n): CodeDocInconsistency | null\n\nfunction checkCodeVsCode(\n  files: Array<{ path: string; content: string }>\n): CodeCodeInconsistency[]\n\nfunction checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string\n): PhantomPathInconsistency[]\n\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: Partial<InconsistencyReport['metadata']>\n): InconsistencyReport\n\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\n### Writers (`src/generation/writers/sum.ts`, `src/generation/writers/agents-md.ts`)\n\n```typescript\ninterface SumFileContent {\n  summary: string;\n  metadata: {\n    purpose: string;\n    criticalTodos?: string[];\n    relatedFiles?: string[];\n  };\n  generatedAt: string;\n  contentHash: string;\n}\n\nasync function writeSumFile(\n  sourcePath: string,\n  content: SumFileContent\n): Promise<void>\n\nasync function readSumFile(\n  sumPath: string\n): Promise<SumFileContent | null>\n\nfunction getSumPath(sourcePath: string): string\n\nasync function sumFileExists(sourcePath: string): Promise<boolean>\n\nasync function writeAnnexFile(\n  sourcePath: string,\n  sourceContent: string\n): Promise<void>\n\nfunction getAnnexPath(sourcePath: string): string\n\nasync function writeAgentsMd(\n  dirPath: string,\n  projectRoot: string,\n  content: string\n): Promise<void>\n\nfunction isGeneratedAgentsMd(agentsMdPath: string): boolean\n\nconst GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->';\n```\n\n### Installer (`src/installer/index.ts`)\n\n```typescript\ntype Runtime = 'claude' | 'opencode' | 'gemini' | 'all';\ntype Location = 'global' | 'local';\n\ninterface InstallerArgs {\n  runtime?: Runtime;\n  global: boolean;\n  local: boolean;\n  uninstall: boolean;\n  force: boolean;\n  help: boolean;\n  quiet: boolean;\n}\n\ninterface InstallerResult {\n  success: boolean;\n  runtime: Exclude<Runtime, 'all'>;\n  location: Location;\n  filesCreated: string[];\n  filesSkipped: string[];\n  errors: string[];\n  hookRegistered?: boolean;\n  versionWritten?: boolean;\n}\n\nfunction parseInstallerArgs(args: string[]): InstallerArgs\n\nasync function runInstaller(): Promise<void>\n\nasync function runInstall(args: InstallerArgs): Promise<InstallerResult[]>\n\nasync function runUninstall(args: InstallerArgs): Promise<InstallerResult[]>\n```\n\n### Trace Infrastructure (`src/orchestration/trace.ts`)\n\n```typescript\ntype TraceEventType = \n  | 'phase:start' | 'phase:end'\n  | 'worker:start' | 'worker:end'\n  | 'task:pickup' | 'task:done' | 'task:start'\n  | 'subprocess:spawn' | 'subprocess:exit'\n  | 'retry'\n  | 'discovery:start' | 'discovery:end'\n  | 'filter:applied'\n  | 'plan:created'\n  | 'config:loaded';\n\ninterface ITraceWriter {\n  emit(event: TraceEventPayload): void;\n  finalize(): Promise<void>;\n}\n\nfunction createTraceWriter(\n  projectRoot: string,\n  enabled: boolean\n): ITraceWriter\n\nasync function cleanupOldTraces(\n  projectRoot: string,\n  keepCount?: number\n): Promise<number>\n```\n\n## 4. Data Structures & State\n\n### Configuration Schema (`src/config/schema.ts`)\n\n```typescript\ninterface ExcludeConfig {\n  patterns: string[];\n  vendorDirs: string[];\n  binaryExtensions: string[];\n}\n\ninterface OptionsConfig {\n  followSymlinks: boolean;\n  maxFileSize: number;\n}\n\ninterface OutputConfig {\n  colors: boolean;\n}\n\ninterface AIConfig {\n  backend: 'claude' | 'gemini' | 'opencode' | 'auto';\n  model: string | null;\n  timeoutMs: number;\n  maxRetries: number;\n  concurrency: number;\n  telemetry: {\n    enabled: boolean;\n    keepRuns: number;\n    costThresholdUsd: number;\n  };\n  pricing: {\n    claude: TokenPricing;\n    gemini: TokenPricing;\n    opencode: TokenPricing;\n  };\n}\n\ninterface TokenPricing {\n  inputCostPer1kTokens: number;\n  outputCostPer1kTokens: number;\n  cacheReadCostPer1kTokens: number;\n  cacheCreationCostPer1kTokens: number;\n}\n```\n\n### Execution Plan Types (`src/generation/executor.ts`)\n\n```typescript\ninterface ExecutionTask {\n  id: string;\n  type: 'file' | 'directory' | 'root';\n  path: string;\n  absolutePath: string;\n  outputPath: string;\n  systemPrompt: string;\n  userPrompt: string;\n  dependencies: string[];\n  metadata: Record<string, unknown>;\n}\n\ninterface ComplexityMetrics {\n  fileCount: number;\n  directoryDepth: number;\n  directories: Set<string>;\n  files: string[];\n}\n```\n\n### Telemetry Types (`src/ai/telemetry/logger.ts`)\n\n```typescript\ninterface TelemetryEntry {\n  timestamp: string;\n  prompt: string;\n  response: string;\n  model: string;\n  inputTokens: number;\n  outputTokens: number;\n  cacheReadTokens?: number;\n  cacheCreationTokens?: number;\n  durationMs: number;\n  exitCode: number;\n  retryCount: number;\n  filesRead?: FileRead[];\n}\n\ninterface FileRead {\n  path: string;\n  sizeBytes: number;\n  linesRead?: number;\n}\n\ninterface RunLog {\n  runId: string;\n  startTime: string;\n  endTime: string;\n  entries: TelemetryEntry[];\n  summary: {\n    totalCalls: number;\n    totalInputTokens: number;\n    totalOutputTokens: number;\n    totalCacheReadTokens: number;\n    totalCacheCreationTokens: number;\n    totalDurationMs: number;\n    errorCount: number;\n    uniqueFilesRead: number;\n  };\n}\n```\n\n### Quality Report Types (`src/quality/types.ts`)\n\n```typescript\ninterface CodeDocInconsistency {\n  type: 'code-vs-doc';\n  severity: 'error';\n  filePath: string;\n  description: string;\n  missingFromDoc: string[];\n}\n\ninterface CodeCodeInconsistency {\n  type: 'code-vs-code';\n  severity: 'warning';\n  symbol: string;\n  pattern: 'duplicate-export';\n  files: string[];\n  description: string;\n}\n\ninterface PhantomPathInconsistency {\n  type: 'phantom-path';\n  severity: 'warning';\n  agentsMdPath: string;\n  description: string;\n  details: {\n    referencedPath: string;\n    resolvedTo: string;\n    context: string;\n  };\n}\n```\n\n### State Management Patterns\n\n**In-Memory Caches:** `CommandRunner` maintains `sourceContentCache: Map<string, string>` during Phase 1, cleared after quality validation to free memory. `oldSumCache: Map<string, SumFileContent>` stores pre-Phase 1 summaries for stale documentation detection.\n\n**Promise-Chain Write Queues:** `PlanTracker.writeQueue`, `ProgressLog.writeQueue`, `TraceWriter.writeQueue` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern.\n\n**Shared Iterator State:** `runPool()` creates `tasks.entries()` iterator consumed by N workers via `for (const [index, task] of iterator)` loop. Iterator protocol ensures atomic task pulling without explicit locking.\n\n**Serialized Trace Sequence:** `TraceWriter.seq` increments monotonically on each `emit()` call, auto-populated before NDJSON write.\n\n## 5. Configuration\n\n### Configuration File Schema\n\n**Location:** `.agents-reverse-engineer/config.yaml`\n\n**Schema Definition:** Validated via Zod in `src/config/schema.ts`\n\n```yaml\nexclude:\n  patterns:              # string[] - Gitignore-style globs\n  vendorDirs:           # string[] - Third-party directories\n  binaryExtensions:     # string[] - Non-text file extensions\n\noptions:\n  followSymlinks: false # boolean - Follow symbolic links during discovery\n  maxFileSize: 1048576  # number - Binary detection threshold (bytes)\n\noutput:\n  colors: true          # boolean - Enable ANSI color codes in CLI output\n\nai:\n  backend: 'auto'       # 'claude' | 'gemini' | 'opencode' | 'auto'\n  model: null           # string | null - Override backend default model\n  timeoutMs: 120000     # number - Subprocess timeout (milliseconds)\n  maxRetries: 3         # number - Exponential backoff retry attempts\n  concurrency: 2        # number - Worker pool size (1-20)\n  \n  telemetry:\n    enabled: true       # boolean - Write run logs\n    keepRuns: 50        # number - Retention limit\n    costThresholdUsd: 10  # number - Warning threshold\n  \n  pricing:              # Per-backend token cost configuration\n    claude:\n      inputCostPer1kTokens: 0.003\n      outputCostPer1kTokens: 0.015\n      cacheReadCostPer1kTokens: 0.0003\n      cacheCreationCostPer1kTokens: 0.00375\n```\n\n### Default Values (`src/config/defaults.ts`)\n\n**Vendor Directories:**\n```typescript\nDEFAULT_VENDOR_DIRS = [\n  'node_modules', '.git', 'dist', 'build', 'target',\n  '.next', '__pycache__', 'venv', '.venv', '.cargo',\n  '.gradle', '.agents-reverse-engineer', '.agents',\n  '.planning', '.claude', '.opencode', '.gemini'\n]\n```\n\n**Exclude Patterns:**\n```typescript\nDEFAULT_EXCLUDE_PATTERNS = [\n  'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\n  '**/.*', '**/*.log', '**/*.sum', '**/AGENTS.md',\n  '**/CLAUDE.md', '**/GENERATION-PLAN.md'\n]\n```\n\n**Binary Extensions:**\n```typescript\nDEFAULT_BINARY_EXTENSIONS = [\n  '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp', '.svg',\n  '.zip', '.tar', '.gz', '.rar', '.7z', '.bz2', '.xz', '.tgz',\n  '.exe', '.dll', '.so', '.dylib', '.bin', '.msi', '.app', '.dmg',\n  '.mp3', '.mp4', '.wav', '.avi', '.mov', '.mkv', '.flac', '.ogg',\n  '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n  '.woff', '.woff2', '.ttf', '.eot', '.otf',\n  '.class', '.pyc', '.pyo', '.o', '.obj', '.a', '.lib', '.wasm',\n  '.db', '.sqlite', '.sqlite3', '.mdb'\n]\n```\n\n**Concurrency Formula:**\n```typescript\nfunction getDefaultConcurrency(): number {\n  const cores = os.cpus().length;\n  const totalMemGB = os.totalmem() / (1024 ** 3);\n  const memCap = totalMemGB > 1 \n    ? Math.floor((totalMemGB * 0.5) / 0.512)\n    : Infinity;\n  const computed = cores * 5;\n  return clamp(computed, MIN_CONCURRENCY, Math.min(memCap, MAX_CONCURRENCY));\n}\n\nconst MIN_CONCURRENCY = 2;\nconst MAX_CONCURRENCY = 20;\n```\n\n### Environment Variable Overrides\n\n**Runtime Paths:**\n- `CLAUDE_CONFIG_DIR` — Override `~/.claude` default\n- `OPENCODE_CONFIG_DIR` — Override `~/.config/opencode` default (takes precedence over `XDG_CONFIG_HOME`)\n- `GEMINI_CONFIG_DIR` — Override `~/.gemini` default\n\n**Hook Control:**\n- `ARE_DISABLE_HOOK=1` — Disable session lifecycle hooks\n\n### Validation Rules\n\n**Zod Constraints:**\n- `ai.concurrency`: `z.number().min(1).max(20)`\n- `ai.timeoutMs`: `z.number().positive()`\n- `ai.telemetry.keepRuns`: `z.number().min(0)` (0 = unlimited)\n- `options.maxFileSize`: `z.number().positive()`\n- `ai.backend`: `z.enum(['claude', 'gemini', 'opencode', 'auto'])`\n\n**YAML Metacharacter Quoting:** Patterns containing `[*{}\\[\\]?,:#&!|>'\"%@\\`]` double-quoted with backslash escaping (`\\` → `\\\\`, `\"` → `\\\"`).\n\n## 6. Dependencies\n\n### Production Dependencies\n\n**fast-glob** (^3.3.2): Glob pattern file discovery with `absolute: true`, `onlyFiles: true`, `dot: true` options. Chosen for superior performance over alternatives (globby, glob) and native ignore pattern support.\n\n**ignore** (^7.0.0): Gitignore parsing implementing git ignore spec. Used in `createGitignoreFilter()` and `createCustomFilter()` for relative path normalization and pattern matching.\n\n**isbinaryfile** (^5.0.4): Binary file detection via content analysis. Fallback for unknown extensions in `createBinaryFilter()` after extension/size fast paths.\n\n**simple-git** (^3.27.0): Git command wrapper for change detection. Executes `git diff --name-status -M` for rename detection with 50% similarity threshold, `git.status()` for uncommitted changes merge.\n\n**yaml** (^2.7.0): YAML serialization for config file generation. Chosen over `js-yaml` for ESM compatibility and TypeScript types.\n\n**zod** (^3.24.1): Schema validation with type inference. Used in `ConfigSchema` for `.agents-reverse-engineer/config.yaml` validation, Claude CLI response parsing via `ClaudeResponseSchema`.\n\n**ora** (^8.1.1): Terminal spinner UI for progress indication. Displays animated spinners during long-running operations with `spinner.start()`, `spinner.succeed()`, `spinner.fail()`.\n\n**picocolors** (^1.1.1): ANSI color formatting without `chalk` overhead. Used in `createLogger()` for terminal output styling with zero dependencies and <1KB footprint.\n\n### Development Dependencies\n\n**typescript** (^5.7.3): TypeScript compiler with `tsc` build script. Configuration: target ES2022, module NodeNext (native ES modules), strict type-checking enabled.\n\n**tsx** (^4.19.2): TypeScript execution for development mode. Used in `npm run dev` script with `tsx watch src/cli/index.ts` for hot reload.\n\n**@types/node** (^22.10.2): Node.js type definitions for built-in modules (`fs`, `path`, `child_process`, `os`, `crypto`).\n\n### Optional AI Backend Dependencies\n\n**@anthropic-ai/claude-code** (not in package.json): Claude CLI for backend execution. Installed globally via `npm install -g @anthropic-ai/claude-code`, detected by `isCommandOnPath('claude')`.\n\n**Gemini CLI** (not in package.json): Google Gemini CLI (stub backend). Installation instructions: `npm install -g @anthropic-ai/gemini-cli` + https://github.com/google-gemini/gemini-cli.\n\n**OpenCode CLI** (not in package.json): OpenCode CLI (stub backend). Installation: `curl -fsSL https://opencode.ai/install | bash` + https://opencode.ai.\n\n### Rationale for Key Choices\n\n**fast-glob over globby/glob:** 2-5x faster on large codebases, native ignore patterns reduce filter chain complexity.\n\n**simple-git over nodegit:** Pure JavaScript implementation avoids native binding compilation, smaller footprint, sufficient for read-only git operations.\n\n**zod over joi/yup:** Type inference eliminates duplicate type declarations, tree-shakeable ESM exports, superior TypeScript integration.\n\n**picocolors over chalk:** Zero dependencies, 14x smaller bundle size, identical API surface for ANSI styling.\n\n**ignore library over manual parsing:** Battle-tested gitignore spec implementation, handles edge cases (negation patterns, directory-only markers, trailing slashes).\n\n## 7. Behavioral Contracts\n\n### Runtime Behavior\n\n**Error Types and Codes:**\n\n`AIServiceError` discriminated by `code` field:\n- `'CLI_NOT_FOUND'` — No available backend detected via PATH scan\n- `'TIMEOUT'` — Subprocess exceeded `timeoutMs` (SIGTERM → SIGKILL)\n- `'PARSE_ERROR'` — JSON parsing failed on Claude CLI stdout\n- `'SUBPROCESS_ERROR'` — Non-zero exit code without timeout\n- `'RATE_LIMIT'` — Stderr matches rate limit patterns (retryable)\n\n**Exit Codes:**\n\nCLI commands return:\n- `0` — Full success (all tasks completed)\n- `1` — Partial failure (some tasks failed, `filesFailed > 0 && filesProcessed > 0`) or config already exists (init)\n- `2` — Total failure (`filesProcessed === 0 && filesFailed > 0`) or CLI not found\n\n**Retry Logic:**\n\nExponential backoff via `withRetry()`:\n```typescript\ndelay = min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter\n```\n- `baseDelayMs`: 1000ms\n- `maxDelayMs`: 8000ms\n- `multiplier`: 2\n- `jitter`: uniform random 0-500ms\n- `maxRetries`: 3 (configurable)\n\nRate-limit retry enabled via `isRetryable()` predicate, timeout excluded (resource constraint).\n\n**Concurrency Model:**\n\nIterator-based worker pool sharing `tasks.entries()` iterator:\n```typescript\nconst effectiveConcurrency = Math.min(options.concurrency, tasks.length);\nconst workers = Array.from({ length: effectiveConcurrency }, (_, i) => \n  workerFunction(i, tasks.entries())\n);\nawait Promise.all(workers);\n```\n\nWorkers race to pull tasks via `for (const [index, task] of iterator)` loop. Atomic task pickup prevents over-allocation.\n\n**Lifecycle Hooks:**\n\nSession hooks spawned as detached background processes:\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\nSerialized script strings execute inline logic (version checks, git status, npm spawn). Parent exits immediately, child completes asynchronously.\n\n**Resource Management:**\n\nSubprocess environment variables:\n- `NODE_OPTIONS='--max-old-space-size=512'` — 512MB heap limit\n- `UV_THREADPOOL_SIZE='4'` — 4-thread libuv pool\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` — Disable background tasks\n\nTimeout enforcement:\n```typescript\nconst timer = setTimeout(() => {\n  kill(-child.pid, 'SIGTERM');\n  setTimeout(() => kill(-child.pid, 'SIGKILL'), 5000);\n}, timeoutMs);\ntimer.unref();\n```\n\nProcess group killing via negative PID terminates subprocess tree.\n\n### Implementation Contracts\n\n#### Regex Patterns\n\n**Export Extraction (`src/quality/inconsistency/code-vs-doc.ts`):**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nCaptures identifier from line-start whitespace, `export` keyword, optional `default`, declaration keyword, identifier name.\n\n**Phantom Path Detection (`src/quality/phantom-paths/validator.ts`):**\n\nMarkdown links:\n```regex\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n```\n\nBacktick paths:\n```regex\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n```\n\nProse-embedded paths:\n```regex\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Import Extraction (`src/imports/extractor.ts`):**\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\nMatches static ES module imports with type-only detection, destructured symbols, namespace imports, default imports.\n\n**Preamble Detection (`src/orchestration/runner.ts`):**\n\nSeparator pattern:\n```typescript\nresponseText.indexOf('\\n---\\n') >= 0 && separatorIndex < 500\n```\n\nBold header pattern:\n```regex\n/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/\n```\nStrips preceding content if match index <300 chars and no `##` markers present.\n\n**Content Splitting (`src/specify/writer.ts`):**\n```regex\n/^(?=# )/m\n```\nMatches lines starting with exactly `# ` (top-level headings) for multi-file spec splitting.\n\n#### Format Strings and Output Templates\n\n**YAML Frontmatter Format (`src/generation/writers/sum.ts`):**\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: Single-line description\ncritical_todos: [item1, item2]  # Inline if <40 chars and ≤3 items\nrelated_files:                  # Multi-line if >40 chars or >3 items\n  - path/to/file.ts\n---\n\nMarkdown summary content...\n```\n\n**Trace Filename Pattern (`src/orchestration/trace.ts`):**\n```typescript\nconst timestamp = new Date().toISOString().replace(/[:.]/g, '-');\nconst filename = `trace-${timestamp}.ndjson`;\n```\nExample: `trace-2026-02-09T12-34-56-789Z.ndjson`\n\n**Run Log Filename Pattern (`src/ai/telemetry/run-log.ts`):**\n```typescript\nconst timestamp = runLog.startTime.replace(/:/g, '-').replace(/\\./g, '-');\nconst filename = `run-${timestamp}.json`;\n```\nExample: `run-2026-02-09T12-34-56-789Z.json`\n\n**Progress Log Line Formats (`src/orchestration/progress.ts`):**\n```\n[X/Y] ANALYZING path\n[X/Y] DONE path Xs in/out tok model ~Ns remaining\n[X/Y] FAIL path error\n```\n\n**Plan Tracker Checkbox Format (`src/orchestration/plan-tracker.ts`):**\n```markdown\n- [ ] `relative/path/to/file.ts`\n- [x] `relative/path/to/file.ts`  # After completion\n```\n\n#### Magic Constants and Sentinel Values\n\n**Timeouts:**\n- `DEFAULT_TIMEOUT_MS = 120_000` (2 minutes subprocess timeout)\n- `SIGKILL_GRACE_MS = 5_000` (5 seconds SIGKILL escalation)\n- `NPM_VERSION_CHECK_TIMEOUT_MS = 10_000` (hooks)\n\n**Concurrency Limits:**\n- `MIN_CONCURRENCY = 2` (enforced minimum worker pool size)\n- `MAX_CONCURRENCY = 20` (enforced maximum worker pool size)\n- `QUALITY_CHECK_CONCURRENCY = 10` (post-generation validation)\n- `SUM_CACHE_READ_CONCURRENCY = 20` (pre-Phase 1 old .sum reads)\n- `FILTER_CHAIN_CONCURRENCY = 30` (discovery filter application)\n\n**Buffer Sizes:**\n- `maxBuffer: 10 * 1024 * 1024` (10MB subprocess stdout/stderr capture)\n- `DEFAULT_MAX_FILE_SIZE = 1048576` (1MB binary detection threshold)\n\n**Retention Limits:**\n- `DEFAULT_KEEP_RUNS = 50` (telemetry log retention)\n- `DEFAULT_KEEP_TRACES = 500` (trace file retention)\n\n**Retry Parameters:**\n```typescript\nDEFAULT_RETRY_OPTIONS = {\n  maxRetries: 3,\n  baseDelayMs: 1_000,\n  maxDelayMs: 8_000,\n  multiplier: 2\n}\n```\n\n**ETA Calculation:**\n- `ETA_WINDOW_SIZE = 10` (moving average window)\n- `ETA_MIN_COMPLETIONS = 2` (minimum samples before displaying)\n\n**Path Length Limits:**\n- `PURPOSE_MAX_LENGTH = 120` (truncate with `...` ellipsis)\n- `PHANTOM_PATH_CONTEXT_LENGTH = 120` (truncate containing line)\n\n#### Rate Limit Detection Patterns\n\n```typescript\nconst RATE_LIMIT_PATTERNS = [\n  'rate limit',\n  '429',\n  'too many requests',\n  'overloaded'\n];\n```\nCase-insensitive substring matching on stderr for retry eligibility.\n\n#### Environment Variables\n\n**Path Overrides:**\n- `CLAUDE_CONFIG_DIR` — Default: `~/.claude`\n- `OPENCODE_CONFIG_DIR` — Default: `~/.config/opencode` (overrides `XDG_CONFIG_HOME`)\n- `GEMINI_CONFIG_DIR` — Default: `~/.gemini`\n\n**Hook Control:**\n- `ARE_DISABLE_HOOK='1'` — Disable session lifecycle hooks\n\n**Subprocess Environment (injected by `runSubprocess()`):**\n- `NODE_OPTIONS='--max-old-space-size=512'`\n- `UV_THREADPOOL_SIZE='4'`\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`\n\n#### File Format Specifications\n\n**NDJSON Trace Event Schema:**\n```typescript\ninterface TraceEvent {\n  seq: number;           // Monotonic sequence\n  ts: string;            // ISO 8601 timestamp\n  pid: number;           // process.pid\n  type: TraceEventType;  // Discriminator\n  elapsedMs: number;     // High-resolution delta\n  // Type-specific fields\n}\n```\nOne JSON object per line, auto-populated base fields, promise-chain serialization.\n\n**Run Log JSON Schema:**\n```typescript\ninterface RunLog {\n  runId: string;         // ISO timestamp\n  startTime: string;     // ISO 8601\n  endTime: string;       // ISO 8601\n  entries: TelemetryEntry[];\n  summary: {\n    totalCalls: number;\n    totalInputTokens: number;\n    totalOutputTokens: number;\n    totalCacheReadTokens: number;\n    totalCacheCreationTokens: number;\n    totalDurationMs: number;\n    errorCount: number;\n    uniqueFilesRead: number;\n  };\n}\n```\nPretty-printed JSON with 2-space indentation.\n\n**Settings.json Hook Registration Schema (Claude):**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: Array<{\n      hooks: Array<{\n        type: 'command';\n        command: string;\n      }>;\n    }>;\n    SessionEnd?: Array<{\n      hooks: Array<{\n        type: 'command';\n        command: string;\n      }>;\n    }>;\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n```\n\n**Settings.json Hook Registration Schema (Gemini):**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: Array<{\n      name: string;\n      type: 'command';\n      command: string;\n    }>;\n    SessionEnd?: Array<{\n      name: string;\n      type: 'command';\n      command: string;\n    }>;\n  };\n}\n```\n\n#### Manifest Detection Types\n\nNine supported manifest formats in `extractDirectoryImports()`:\n```typescript\nconst MANIFEST_TYPES = [\n  'package.json',\n  'Cargo.toml',\n  'go.mod',\n  'pyproject.toml',\n  'pom.xml',\n  'build.gradle',\n  'Gemfile',\n  'composer.json',\n  'CMakeLists.txt',\n  'Makefile'\n];\n```\n\n#### Generated File Marker\n\n```typescript\nconst GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->';\n```\nSubstring match (no regex) for user-authored AGENTS.md detection.\n\n#### Preamble Prefix Patterns\n\n```typescript\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is',\n  'i\\'ll', 'i will', 'great', 'okay', 'sure',\n  'certainly', 'alright'\n];\n```\nCase-insensitive matching for LLM conversational preamble detection.\n\n#### Bash Permission Patterns\n\n```typescript\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\nRegistered in Claude Code `settings.json` permissions.allow array.\n\n#### Directory Skip Set\n\n```typescript\nconst SKIP_DIRS = new Set([\n  'node_modules', '.git', '.agents-reverse-engineer',\n  'vendor', 'dist', 'build', '__pycache__', '.next',\n  'venv', '.venv', 'target', '.cargo', '.gradle'\n]);\n```\nUsed in `collectAgentsDocs()` recursive traversal.\n\n## 8. Test Contracts\n\n### Discovery Module Tests\n\n**`src/discovery/run.ts` should verify:**\n- Empty directory returns `{files: [], excluded: []}`\n- Gitignored files appear in `excluded` with `reason: \"matched .gitignore pattern: <pattern>\"`\n- Binary files detected by extension appear in `excluded` with `reason: \"binary file\"`\n- Vendor directories filtered with `reason: \"vendor directory\"`\n- Custom patterns match against relative paths\n- Symlink following respects `options.followSymlinks` flag\n- Files exceeding `options.maxFileSize` treated as binary\n\n**Edge cases:**\n- `.gitignore` missing (graceful fallback)\n- Directory with only excluded files (zero included)\n- Nested vendor directories (e.g., `node_modules/pkg/node_modules`)\n- Hidden dotfiles (`.env`, `.npmrc`) included when not gitignored\n\n### AI Service Tests\n\n**`src/ai/service.ts` should verify:**\n- Rate-limit error triggers retry with exponential backoff\n- Timeout error throws `AIServiceError('TIMEOUT')` without retry\n- Successful call returns normalized `AIResponse` with token counts\n- `setDebug(true)` enables subprocess heap/RSS logging to stderr\n- `addFilesReadToLastEntry()` populates `filesRead` array in telemetry\n- `finalize()` writes run log to `.agents-reverse-engineer/logs/`\n\n**Error scenarios:**\n- CLI not found throws `AIServiceError('CLI_NOT_FOUND')` with install instructions\n- Non-zero exit code throws `AIServiceError('SUBPROCESS_ERROR')`\n- Malformed JSON response throws `AIServiceError('PARSE_ERROR')`\n- Stderr containing \"rate limit\" triggers retry, exhaustion throws with error code\n\n### Subprocess Resource Management Tests\n\n**`src/ai/subprocess.ts` should verify:**\n- Environment variables injected (`NODE_OPTIONS`, `UV_THREADPOOL_SIZE`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS`)\n- SIGTERM sent at `timeoutMs`, SIGKILL after 5s grace period\n- Process group killing terminates child and descendants via `kill(-pid)`\n- `onSpawn(pid)` callback invoked synchronously before `execFile` callback\n- `maxBuffer` limit prevents memory exhaustion on excessive stdout\n\n**Edge cases:**\n- Child process ignoring SIGTERM (SIGKILL escalation)\n- Subprocess spawning grandchildren (process group termination)\n- Rapid timeout (SIGKILL before SIGTERM grace period)\n\n### Worker Pool Tests\n\n**`src/orchestration/pool.ts` should verify:**\n- Shared iterator prevents duplicate task execution\n- Effective concurrency = `min(options.concurrency, tasks.length)`\n- `failFast` aborts remaining tasks after first error\n- `onComplete` callback invoked per task settlement in completion order\n- Worker IDs unique (`0` to `concurrency-1`)\n- Trace events emitted: `worker:start`, `worker:end`, `task:pickup`, `task:done`\n\n**Concurrency scenarios:**\n- Single worker (`concurrency=1`) executes sequentially\n- Workers idle when `tasks.length < concurrency`\n- Slow task doesn't block fast tasks in parallel execution\n\n### Quality Validation Tests\n\n**`src/quality/inconsistency/code-vs-doc.ts` should verify:**\n- `extractExports()` captures `export function foo`, `export class Bar`, `export const Baz`\n- `checkCodeVsDoc()` returns `null` when all exports appear in summary\n- `checkCodeVsDoc()` returns `CodeDocInconsistency` with `missingFromDoc` when exports absent\n- Regex ignores non-exported symbols and inline comments\n\n**Edge cases:**\n- `export default function` (captures `function` keyword)\n- Multi-line export statements\n- Re-exports `export { foo } from './bar'` (not captured by regex)\n\n**`src/quality/inconsistency/code-vs-code.ts` should verify:**\n- Single file with duplicates returns empty array (scope filter required)\n- Multiple files with same symbol returns `CodeCodeInconsistency`\n- Case-sensitive symbol matching\n\n**`src/quality/phantom-paths/validator.ts` should verify:**\n- Markdown link `[text](./file.ts)` resolved relative to AGENTS.md directory\n- Backtick path `` `src/foo.ts` `` resolved relative to project root\n- Prose-embedded `see src/bar.ts` extracted and validated\n- `.js` extension falls back to `.ts` when unresolved\n- URLs, template literals, globs filtered before validation\n\n### Change Detection Tests\n\n**`src/change-detection/detector.ts` should verify:**\n- `computeContentHash()` returns consistent SHA-256 for same content\n- `getChangedFiles()` maps git status codes: `A` → `'added'`, `M` → `'modified'`, `D` → `'deleted'`, `R100` → `'renamed'`\n- `includeUncommitted` merges working tree changes via `git.status()`\n- Rename detection extracts `oldPath` from `R` status lines\n- Non-git directory returns empty changes when `isGitRepo()` false\n\n**Edge cases:**\n- Empty git repo (no commits)\n- Detached HEAD state\n- Git command failure (permission denied)\n\n### Update Orchestration Tests\n\n**`src/update/orchestrator.ts` should verify:**\n- `preparePlan()` compares `.sum` frontmatter hash against current file hash\n- Hash match adds to `filesToSkip`, mismatch adds to `filesToAnalyze`\n- Deleted files detected when `.sum` exists but source missing\n- `getAffectedDirectories()` returns depth-sorted set of parent directories\n- `cleanupOrphans()` deletes `.sum` and `.annex.md` for deleted/renamed files\n- `cleanupEmptyDirectoryDocs()` removes AGENTS.md when zero source files remain\n\n**Edge cases:**\n- First run (no `.sum` files) treats all as added\n- Renamed files clean old `.sum`, create new at new path\n- Directory with only generated files (AGENTS.md, CLAUDE.md) excluded from cleanup\n\n### Configuration Tests\n\n**`src/config/loader.ts` should verify:**\n- Missing config file returns defaults without error\n- Invalid YAML throws `ConfigError` with formatted Zod issues\n- `writeDefaultConfig()` quotes patterns with YAML metacharacters\n- `concurrency` clamped to `[1, 20]` range\n- `getDefaultConcurrency()` respects memory constraint formula\n\n**Validation edge cases:**\n- Negative `timeoutMs` rejected\n- Zero `maxFileSize` rejected\n- Invalid `backend` enum value rejected\n\n### Installer Tests\n\n**`src/installer/operations.ts` should verify:**\n- `installFiles()` copies command templates to runtime-specific paths\n- `registerHooks()` appends SessionStart/SessionEnd hooks to settings.json\n- `registerPermissions()` adds `ARE_PERMISSIONS` to Claude permissions.allow\n- `verifyInstallation()` checks `existsSync()` for all filesCreated\n- Existing files skipped when `force=false`, overwritten when `force=true`\n\n**Platform-specific:**\n- Claude: nested hook structure with `hooks` array\n- Gemini: flat hook structure with `name` field\n- OpenCode: plugins as exported async factory functions\n\n**Uninstallation:**\n- `unregisterHooks()` filters by current and legacy command patterns\n- `cleanupEmptyDirs()` recursively removes empty parents up to runtime root\n- `cleanupLegacyGeminiFiles()` deletes obsolete `.md`/`.toml` formats\n\n### Integration Tests\n\n**End-to-end generate workflow:**\n1. `discoverFiles()` returns 10 source files\n2. Phase 1 generates 10 `.sum` files with YAML frontmatter\n3. Phase 2 generates `AGENTS.md` consuming child summaries\n4. Phase 3 generates `CLAUDE.md` consuming all `AGENTS.md`\n5. Quality validation reports inconsistencies without throwing\n6. Telemetry written to `.agents-reverse-engineer/logs/run-*.json`\n\n**End-to-end update workflow:**\n1. `preparePlan()` detects 2 modified files via hash comparison\n2. Phase 1 regenerates 2 `.sum` files\n3. `getAffectedDirectories()` returns 2 parent directories\n4. Directory `AGENTS.md` regenerated for affected directories only\n5. Orphan cleanup deletes 1 stale `.sum` for deleted file\n\n## 9. Build Plan\n\n### Phase 1: Core Infrastructure (Foundation)\n\n**Goal:** Establish configuration, logging, and file discovery without AI dependencies.\n\n**Tasks:**\n1. Implement `src/config/` with Zod schema validation and YAML serialization\n2. Implement `src/output/logger.ts` with picocolors ANSI formatting\n3. Implement `src/discovery/walker.ts` with fast-glob traversal\n4. Implement four discovery filters: gitignore, vendor, binary, custom\n5. Implement `src/discovery/filters/index.ts` bounded-concurrency orchestrator\n6. Implement `src/cli/init.ts` and `src/cli/discover.ts` commands\n7. Add unit tests for config parsing, filter chain, and logger output\n\n**Enables:** Phase 2 (requires file discovery), Phase 4 (requires config/logging)\n\n**Dependencies:** None (pure Node.js built-ins + dependencies)\n\n### Phase 2: AI Service Abstraction Layer\n\n**Goal:** Backend-agnostic AI orchestration with subprocess management and retry logic.\n\n**Tasks:**\n1. Implement `src/ai/types.ts` with AIBackend interface and discriminated error types\n2. Implement `src/ai/backends/claude.ts` with Zod response validation\n3. Implement `src/ai/subprocess.ts` with resource constraints and process group killing\n4. Implement `src/ai/retry.ts` with exponential backoff formula\n5. Implement `src/ai/service.ts` with rate-limit detection and telemetry accumulation\n6. Implement `src/ai/registry.ts` with backend registration and auto-detection\n7. Add stub backends for Gemini and OpenCode throwing `SUBPROCESS_ERROR`\n8. Add unit tests for retry logic, timeout enforcement, and backend resolution\n\n**Enables:** Phase 3 (requires AI service for prompt execution)\n\n**Dependencies:** Phase 1 (requires config for backend/timeout settings)\n\n### Phase 3: Generation Pipeline Core\n\n**Goal:** Three-phase orchestration with prompt engineering and writer infrastructure.\n\n**Tasks:**\n1. Implement `src/generation/types.ts` with AnalysisResult and SummaryMetadata schemas\n2. Implement `src/generation/prompts/templates.ts` with six system prompts\n3. Implement `src/generation/prompts/builder.ts` with placeholder substitution\n4. Implement `src/generation/writers/sum.ts` with YAML frontmatter serialization\n5. Implement `src/generation/writers/agents-md.ts` with user content preservation\n6. Implement `src/imports/extractor.ts` with regex-based import parsing\n7. Implement `src/generation/orchestrator.ts` with file preparation and task creation\n8. Implement `src/generation/executor.ts` with dependency graph construction\n9. Implement `src/generation/collector.ts` with recursive AGENTS.md traversal\n10. Add unit tests for prompt building, frontmatter parsing, and user content detection\n\n**Enables:** Phase 4 (requires generation plan for execution)\n\n**Dependencies:** Phase 2 (requires AI service for LLM calls)\n\n### Phase 4: Worker Pool and Command Runner\n\n**Goal:** Concurrent execution orchestration with progress tracking and trace emission.\n\n**Tasks:**\n1. Implement `src/orchestration/pool.ts` with shared iterator pattern\n2. Implement `src/orchestration/progress.ts` with ETA calculation via moving average\n3. Implement `src/orchestration/plan-tracker.ts` with promise-chain checkbox updates\n4. Implement `src/orchestration/trace.ts` with NDJSON serialization\n5. Implement `src/orchestration/runner.ts` with three-phase pipeline execution\n6. Implement `src/cli/generate.ts` command with dry-run and trace flags\n7. Add integration tests for worker pool concurrency and trace event ordering\n\n**Enables:** Phase 5 (requires runner for quality validation timing)\n\n**Dependencies:** Phase 3 (requires generation plan and writers)\n\n### Phase 5: Quality Validation Subsystem\n\n**Goal:** Post-generation inconsistency detection without blocking pipeline.\n\n**Tasks:**\n1. Implement `src/quality/types.ts` with discriminated Inconsistency union\n2. Implement `src/quality/inconsistency/code-vs-doc.ts` with regex export extraction\n3. Implement `src/quality/inconsistency/code-vs-code.ts` with symbol aggregation\n4. Implement `src/quality/phantom-paths/validator.ts` with three path extraction patterns\n5. Implement `src/quality/inconsistency/reporter.ts` with CLI formatting\n6. Integrate quality checks in `CommandRunner.executeGenerate()` after Phase 1 and Phase 2\n7. Add unit tests for regex patterns, path resolution, and report formatting\n\n**Enables:** Phase 9 (quality metrics in telemetry)\n\n**Dependencies:** Phase 4 (requires runner execution context)\n\n### Phase 6: Change Detection and Incremental Updates\n\n**Goal:** Hash-based incremental updates with git integration and orphan cleanup.\n\n**Tasks:**\n1. Implement `src/change-detection/detector.ts` with SHA-256 hashing and git diff parsing\n2. Implement `src/update/orchestrator.ts` with hash comparison and affected directory computation\n3. Implement `src/update/orphan-cleaner.ts` with `.sum`/`.annex.md` deletion\n4. Implement `CommandRunner.executeUpdate()` with Phase 1-only execution\n5. Implement `src/cli/update.ts` with uncommitted flag and delta reporting\n6. Add integration tests for rename detection, orphan cleanup, and directory propagation\n\n**Enables:** Phase 8 (session-end hooks invoke update command)\n\n**Dependencies:** Phase 4 (requires runner), Phase 3 (requires writers for hash comparison)\n\n### Phase 7: Project Specification Synthesis\n\n**Goal:** Multi-file spec generation from AGENTS.md corpus with annex support.\n\n**Tasks:**\n1. Implement `src/specify/prompts.ts` with 11-section system prompt\n2. Implement `src/specify/writer.ts` with content splitting and slugification\n3. Implement `src/generation/collector.ts` extension for annex file collection\n4. Implement `src/cli/specify.ts` with auto-generation fallback and force override\n5. Add unit tests for heading splitting, slug sanitization, and existence checks\n\n**Enables:** None (terminal feature)\n\n**Dependencies:** Phase 3 (requires AGENTS.md collector)\n\n### Phase 8: IDE Integration and Installer\n\n**Goal:** Platform-specific command installation with hook lifecycle management.\n\n**Tasks:**\n1. Implement `src/integration/detect.ts` with marker file detection\n2. Implement `src/integration/templates.ts` with platform-specific command generation\n3. Implement `src/integration/generate.ts` with template instantiation\n4. Implement `src/installer/paths.ts` with environment variable overrides\n5. Implement `src/installer/prompts.ts` with TTY arrow-key selection and fallback\n6. Implement `src/installer/operations.ts` with settings.json modification\n7. Implement `src/installer/uninstall.ts` with orphan cleanup\n8. Implement hook scripts in `hooks/` (SessionStart version check, SessionEnd auto-update)\n9. Implement `scripts/build-hooks.js` copying hooks to `hooks/dist/`\n10. Add integration tests for settings.json parsing and hook registration\n\n**Enables:** None (terminal feature)\n\n**Dependencies:** Phase 6 (hooks invoke update command)\n\n### Phase 9: Telemetry and Trace Infrastructure\n\n**Goal:** Comprehensive observability with token cost tracking and retention management.\n\n**Tasks:**\n1. Implement `src/ai/telemetry/logger.ts` with TelemetryEntry accumulation\n2. Implement `src/ai/telemetry/run-log.ts` with RunLog serialization\n3. Implement `src/ai/telemetry/cleanup.ts` with lexicographic retention\n4. Implement trace writer finalization in `CommandRunner` after Phase 3\n5. Implement `cleanupOldTraces()` and `cleanupOldLogs()` invocations\n6. Add unit tests for summary computation and retention enforcement\n\n**Enables:** None (observability enhancement)\n\n**Dependencies:** Phase 5 (quality metrics in run summary)\n\n### Phase 10: CLI Polish and Distribution\n\n**Goal:** Production-ready CLI with help text, error handling, and npm packaging.\n\n**Tasks:**\n1. Implement `src/cli/clean.ts` with marker-based AGENTS.md filtering\n2. Implement `src/cli/help.ts` with command reference\n3. Add shebang and binary entry points in `package.json`\n4. Implement `prepublishOnly` npm script with TypeScript build and hook copying\n5. Configure `.github/workflows/publish.yml` with provenance attestation\n6. Add README.md with installation workflows and command reference\n7. Add LICENSE file (MIT)\n8. Publish to npm registry\n\n**Enables:** User adoption\n\n**Dependencies:** All phases (final integration)\n\n### Critical Path\n\nPhase 1 → Phase 2 → Phase 3 → Phase 4 → Phase 5 → Phase 6 → Phase 9 → Phase 10\n\nPhases 7 and 8 can proceed in parallel after Phase 3 completes.\n\n## 10. Prompt Templates & System Instructions\n\n### File Analysis System Prompt (`src/generation/prompts/templates.ts`)\n\n```\nYou are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation\n```\n\n### File Analysis User Prompt Template (`src/generation/prompts/templates.ts`)\n\n```\nAnalyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n```{{LANG}}\n{{CONTENT}}\n```\n{{PROJECT_PLAN_SECTION}}\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.\n```\n\n### File Update System Prompt (`src/generation/prompts/templates.ts`)\n\n```\nYou are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation\n```\n\n### Directory AGENTS.md System Prompt (`src/generation/prompts/templates.ts`)\n\n```\nYou are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context\n```\n\n### Directory Update System Prompt (`src/generation/prompts/templates.ts`)\n\n```\nYou are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context\n```\n\n### Root Document System Prompt (`src/generation/prompts/templates.ts`)\n\n```\nYou generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided\n```\n\n### Project Specification System Prompt (`src/specify/prompts.ts`)\n\n```\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence: what to build first and why, dependency order between modules, incremental milestones\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.\n```\n\n## 11. IDE Integration & Installer\n\n### Platform Configuration Objects (`src/integration/templates.ts`)\n\n```typescript\nconst PLATFORM_CONFIGS = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills',\n    filePattern: 'are-{command}/SKILL.md',\n    frontmatterKey: 'name',\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands',\n    filePattern: 'are-{command}.md',\n    frontmatterKey: 'agent',\n    frontmatterValue: 'build',\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands',\n    filePattern: 'are-{command}.toml',\n    format: 'toml',\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n```\n\n### Installer Permission List (`src/installer/operations.ts`)\n\n```typescript\nconst ARE_PERMISSIONS: string[] = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n### Hook Definitions (`src/installer/operations.ts`)\n\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Array intentionally empty — hooks disabled due to issues\n];\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // are-session-end.js disabled\n];\n```\n\nHook activation status: **Disabled** (empty `ARE_HOOKS` array, comment indicates issues).\n\n### Command Template: Generate (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_generate\n---\n\n# COMMAND_PREFIX_generate\n\nGenerates comprehensive AI-friendly documentation for the entire codebase through a three-phase pipeline:\n1. **Discovery**: Scans source files, builds project structure tree\n2. **File Analysis**: Generates `.sum` files for each source file concurrently\n3. **Directory Docs**: Creates `AGENTS.md` files for each directory bottom-up\n4. **Root Docs**: Generates platform-specific integration files (CLAUDE.md, OPENCODE.md, GEMINI.md)\n\nThis is a **long-running operation** (several minutes for medium-sized projects).\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_generate [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]\n```\n\n**Flags:**\n- `--dry-run` — Preview the generation plan without executing\n- `--concurrency N` — Override worker pool size (1-10, default from config)\n- `--fail-fast` — Abort on first task failure\n- `--debug` — Enable verbose subprocess logging with heap/RSS metrics\n- `--trace` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Remove stale progress log:**\n   ```bash\n   rm -f .agents-reverse-engineer/progress.log\n   ```\n\n2. **Start generation in background:**\n   Use the `run_in_background` parameter for the Bash tool to prevent blocking:\n   ```bash\n   npx agents-reverse-engineer@latest generate\n   ```\n\n3. **Monitor progress in real-time:**\n   Poll the progress log every 10-15 seconds while the task runs:\n   ```bash\n   tail -5 .agents-reverse-engineer/progress.log\n   ```\n   \n   Look for these progress indicators:\n   - `[X/Y] ANALYZING path` — file being analyzed\n   - `[X/Y] DONE path` — file completed with token counts\n   - `PHASE 2: Directory docs` — second phase started\n   - `PHASE 3: Root docs` — final phase started\n\n4. **Check for completion:**\n   Use `TaskOutput` with `block: false` to check if background task finished:\n   - If still running, continue monitoring progress log\n   - If complete, proceed to summary\n\n5. **Present summary:**\n   Once complete, report:\n   - Files processed / failed counts\n   - Total tokens (input / output / cache reads)\n   - Duration\n   - Quality metrics (inconsistencies detected)\n   - Output files: `.sum` files, `AGENTS.md`, `CLAUDE.md`\n\n**Progress Log Format:**\n```\n=== Phase 1: File Analysis ===\n[1/42] ANALYZING src/cli/index.ts\n[1/42] DONE src/cli/index.ts 850ms 1234 in / 567 out tok claude-sonnet-4 ~45s remaining\n[2/42] ANALYZING src/generation/orchestrator.ts\n...\n=== Phase 2: Directory Documentation ===\nProcessing src/cli/AGENTS.md\n...\n=== Phase 3: Root Documentation ===\nGenerating CLAUDE.md\n...\n=== Summary ===\nFiles processed: 42\nFiles failed: 0\nTotal input tokens: 52,340\nTotal output tokens: 18,921\nDuration: 3m 12s\n```\n```\n\n### Command Template: Update (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_update\n---\n\n# COMMAND_PREFIX_update\n\nIncrementally updates documentation for changed files via SHA-256 content hash comparison. Only regenerates `.sum` files for modified sources and affected `AGENTS.md` directories—skips unchanged files to minimize cost and latency.\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_update [--uncommitted] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]\n```\n\n**Flags:**\n- `--uncommitted` — Include working tree changes (not just committed)\n- `--dry-run` — Preview update plan without executing\n- `--concurrency N` — Override worker pool size (1-10, default from config)\n- `--fail-fast` — Abort on first task failure\n- `--debug` — Enable verbose subprocess logging\n- `--trace` — Emit NDJSON trace events\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Remove stale progress log:**\n   ```bash\n   rm -f .agents-reverse-engineer/progress.log\n   ```\n\n2. **Start update in background:**\n   ```bash\n   npx agents-reverse-engineer@latest update\n   ```\n\n3. **Monitor progress:**\n   Poll every 10-15 seconds:\n   ```bash\n   tail -5 .agents-reverse-engineer/progress.log\n   ```\n\n4. **Check for completion:**\n   Use `TaskOutput` with `block: false` to check status.\n\n5. **Present summary:**\n   Once complete, report:\n   - Files analyzed (hash mismatches)\n   - Files skipped (hash matches)\n   - Directories regenerated\n   - Orphans cleaned (deleted `.sum` for removed files)\n\n**Delta Report Format:**\n```\n=== Update Plan ===\n+ src/new-file.ts (added)\nM src/existing.ts (modified)\nR src/old.ts → src/renamed.ts\n= src/unchanged.ts (skipped, hash match)\n\nFiles to analyze: 3\nFiles to skip: 39\nAffected directories: 2\nOrphans to clean: 1\n\n=== Summary ===\nFiles analyzed: 3\nFiles skipped: 39\nDirectories regenerated: 2\nOrphans cleaned: 1\nDuration: 45s\n```\n```\n\n### Command Template: Init (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_init\n---\n\n# COMMAND_PREFIX_init\n\nCreates `.agents-reverse-engineer/config.yaml` with default settings for exclude patterns, vendor directories, binary extensions, AI backend configuration, and concurrency.\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_init [--force]\n```\n\n**Flags:**\n- `--force` — Overwrite existing config file\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Run init command:**\n   ```bash\n   npx agents-reverse-engineer@latest init\n   ```\n\n2. **Verify config creation:**\n   ```bash\n   cat .agents-reverse-engineer/config.yaml\n   ```\n\n3. **Report result:**\n   - Success: \"Created `.agents-reverse-engineer/config.yaml`\"\n   - Already exists: \"Config already exists (use `--force` to overwrite)\"\n   - Permission error: Display error message\n```\n\n### Command Template: Discover (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_discover\n---\n\n# COMMAND_PREFIX_discover\n\nScans the codebase via gitignore-aware file discovery and generates `GENERATION-PLAN.md` showing what will be analyzed during `COMMAND_PREFIX_generate`. Useful for previewing scope and estimating cost.\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_discover [--debug] [--trace]\n```\n\n**Flags:**\n- `--debug` — Enable verbose filter chain logging\n- `--trace` — Emit discovery trace events\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Remove stale progress log:**\n   ```bash\n   rm -f .agents-reverse-engineer/progress.log\n   ```\n\n2. **Start discovery in background:**\n   ```bash\n   npx agents-reverse-engineer@latest discover\n   ```\n\n3. **Monitor progress:**\n   Poll every 10 seconds:\n   ```bash\n   tail -5 .agents-reverse-engineer/progress.log\n   ```\n\n4. **Check for completion:**\n   Use `TaskOutput` with `block: false`.\n\n5. **Present results:**\n   ```bash\n   cat .agents-reverse-engineer/GENERATION-PLAN.md\n   ```\n\n**Output Format:**\n```\n# Generation Plan\n\n## Phase 1: File Analysis (42 files)\n- [ ] `src/cli/index.ts`\n- [ ] `src/generation/orchestrator.ts`\n...\n\n## Phase 2: Directory Documentation (8 directories)\n- [ ] `src/cli/AGENTS.md`\n- [ ] `src/generation/AGENTS.md`\n...\n\n## Phase 3: Root Documentation (3 documents)\n- [ ] `CLAUDE.md`\n- [ ] `OPENCODE.md`\n- [ ] `GEMINI.md`\n```\n```\n\n### Command Template: Clean (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_clean\n---\n\n# COMMAND_PREFIX_clean\n\nDeletes all generated documentation artifacts: `.sum` files, `.annex.md` files, generated `AGENTS.md` (preserves user-authored), `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md`, `GENERATION-PLAN.md`.\n\nRestores `AGENTS.local.md` → `AGENTS.md` if present.\n\n**STRICT RULES (MANDATORY):**\n- Do NOT add flags like `--force` or `--dry-run` to the command\n- Do NOT suggest additional options\n- Run exactly: `npx agents-reverse-engineer@latest clean`\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_clean\n```\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Run clean command:**\n   ```bash\n   npx agents-reverse-engineer@latest clean\n   ```\n\n2. **Report result:**\n   - Deleted files count\n   - Skipped user-authored `AGENTS.md` count\n   - Restored `AGENTS.local.md` count\n```\n\n### Command Template: Specify (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_specify\n---\n\n# COMMAND_PREFIX_specify\n\nSynthesizes a comprehensive project specification from all `AGENTS.md` files into `specs/SPEC.md` (single-file mode) or `specs/<section>.md` (multi-file mode). Auto-generates missing docs if needed.\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_specify [--output path] [--force] [--multi-file] [--dry-run] [--debug] [--trace]\n```\n\n**Flags:**\n- `--output path` — Custom output path (default: `specs/SPEC.md`)\n- `--force` — Overwrite existing spec files\n- `--multi-file` — Split spec by top-level headings\n- `--dry-run` — Preview token estimates without generating\n- `--debug` — Enable verbose logging\n- `--trace` — Emit trace events\n\n---\n\n**EXECUTION INSTRUCTIONS:**\n\n1. **Remove stale progress log:**\n   ```bash\n   rm -f .agents-reverse-engineer/progress.log\n   ```\n\n2. **Start specification in background:**\n   ```bash\n   npx agents-reverse-engineer@latest specify\n   ```\n\n3. **Monitor progress:**\n   Poll every 15 seconds:\n   ```bash\n   tail -5 .agents-reverse-engineer/progress.log\n   ```\n\n4. **Check for completion:**\n   Use `TaskOutput` with `block: false`.\n\n5. **Present summary:**\n   Once complete, report:\n   - Output path\n   - Token counts\n   - Duration\n\n**Note:** Minimum timeout 10 minutes due to large prompt size.\n```\n\n### Command Template: Help (`src/integration/templates.ts`)\n\n```markdown\n---\nname: COMMAND_PREFIX_help\n---\n\n# COMMAND_PREFIX_help\n\nDisplays available ARE commands with brief descriptions.\n\n**Usage:**\n```bash\nCOMMAND_PREFIX_help\n```\n\n---\n\n**Available Commands:**\n\n- **COMMAND_PREFIX_init** — Create `.agents-reverse-engineer/config.yaml` with defaults\n- **COMMAND_PREFIX_discover** — Scan files and preview generation plan\n- **COMMAND_PREFIX_generate** — Full three-phase documentation generation\n- **COMMAND_PREFIX_update** — Incremental update for changed files\n- **COMMAND_PREFIX_specify** — Synthesize project specification from AGENTS.md corpus\n- **COMMAND_PREFIX_clean** — Delete generated artifacts\n\n**Global Flags:**\n- `--dry-run` — Preview without executing\n- `--debug` — Verbose logging\n- `--trace` — Emit NDJSON trace events\n- `--concurrency N` — Override worker pool size (1-20)\n- `--fail-fast` — Abort on first error\n\n**Configuration:**\nEdit `.agents-reverse-engineer/config.yaml` to customize:\n- Exclude patterns (gitignore-style globs)\n- Vendor directories\n- Binary extensions\n- AI backend (claude/gemini/opencode/auto)\n- Concurrency (worker pool size)\n- Timeout (subprocess milliseconds)\n\n**Documentation:**\nhttps://github.com/your-org/agents-reverse-engineer\n```\n\n### Hook Script: Session Start Version Check (`hooks/are-check-update.js`)\n\n```javascript\n#!/usr/bin/env node\n\nconst { spawn, execSync } = require('child_process');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\n\n// Check disable flag\nif (process.env.ARE_DISABLE_HOOK === '1') {\n  process.exit(0);\n}\n\n// Detached background process pattern\nconst scriptString = `\nconst { execSync } = require('child_process');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\n\nconst homeDir = os.homedir();\nconst cacheDir = path.join(homeDir, '.claude', 'cache');\nconst cacheFile = path.join(cacheDir, 'are-update-check.json');\nconst versionFile = path.join(homeDir, '.claude', 'ARE-VERSION');\n\ntry {\n  // Read installed version\n  let installed = '0.0.0';\n  if (fs.existsSync(versionFile)) {\n    installed = fs.readFileSync(versionFile, 'utf-8').trim();\n  }\n\n  // Query npm registry with 10s timeout\n  const latest = execSync('npm view agents-reverse-engineer version', {\n    encoding: 'utf8',\n    timeout: 10000,\n    windowsHide: true\n  }).trim();\n\n  // Write cache\n  if (!fs.existsSync(cacheDir)) {\n    fs.mkdirSync(cacheDir, { recursive: true });\n  }\n  fs.writeFileSync(cacheFile, JSON.stringify({\n    update_available: installed !== latest,\n    installed,\n    latest,\n    checked: Math.floor(Date.now() / 1000)\n  }));\n} catch (err) {\n  // Graceful degradation on network/git failure\n  const fallback = {\n    update_available: false,\n    installed: 'unknown',\n    latest: 'unknown',\n    checked: Math.floor(Date.now() / 1000)\n  };\n  try {\n    if (!fs.existsSync(cacheDir)) {\n      fs.mkdirSync(cacheDir, { recursive: true });\n    }\n    fs.writeFileSync(cacheFile, JSON.stringify(fallback));\n  } catch {}\n}\n`;\n\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref();\n```\n\n### Hook Script: Session End Auto-Update (`hooks/are-session-end.js`)\n\n```javascript\n#!/usr/bin/env node\n\nconst { spawn, execSync } = require('child_process');\nconst fs = require('fs');\nconst path = require('path');\n\n// Check disable flag (environment variable)\nif (process.env.ARE_DISABLE_HOOK === '1') {\n  process.exit(0);\n}\n\n// Check disable flag (config file substring search)\nconst configPath = path.join(process.cwd(), '.agents-reverse-engineer.yaml');\nif (fs.existsSync(configPath)) {\n  const configContent = fs.readFileSync(configPath, 'utf-8');\n  if (configContent.includes('hook_enabled: false')) {\n    process.exit(0);\n  }\n}\n\n// Detached background process pattern\nconst scriptString = `\nconst { execSync, spawn } = require('child_process');\n\ntry {\n  // Detect git changes\n  const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n  if (!status.trim()) {\n    process.exit(0); // No changes\n  }\n\n  // Spawn update command as detached background process\n  spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n    stdio: 'ignore',\n    detached: true,\n    windowsHide: true\n  }).unref();\n} catch (err) {\n  // Silent exit on git errors\n  process.exit(0);\n}\n`;\n\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref();\n```\n\n### OpenCode Plugin: Version Check (`hooks/opencode-are-check-update.js`)\n\n```javascript\nasync function AreCheckUpdate() {\n  return {\n    name: 'are-check-update',\n    event: {\n      'session.created': async () => {\n        const { execSync } = require('child_process');\n        const fs = require('fs');\n        const path = require('path');\n        const os = require('os');\n\n        const homeDir = os.homedir();\n        const configDir = process.env.OPENCODE_CONFIG_DIR \n          || path.join(homeDir, '.config', 'opencode');\n        const cacheDir = path.join(configDir, 'cache');\n        const cacheFile = path.join(cacheDir, 'are-update-check.json');\n        const versionFile = path.join(configDir, 'ARE-VERSION');\n\n        try {\n          let installed = '0.0.0';\n          if (fs.existsSync(versionFile)) {\n            installed = fs.readFileSync(versionFile, 'utf-8').trim();\n          }\n\n          const latest = execSync('npm view agents-reverse-engineer version', {\n            encoding: 'utf8',\n            timeout: 10000,\n            windowsHide: true\n          }).trim();\n\n          if (!fs.existsSync(cacheDir)) {\n            fs.mkdirSync(cacheDir, { recursive: true });\n          }\n          fs.writeFileSync(cacheFile, JSON.stringify({\n            update_available: installed !== latest,\n            installed,\n            latest,\n            checked: Math.floor(Date.now() / 1000)\n          }));\n        } catch (err) {\n          // Graceful degradation\n        }\n      },\n    },\n  };\n}\n\nmodule.exports = AreCheckUpdate;\n```\n\n### OpenCode Plugin: Session End Auto-Update (`hooks/opencode-are-session-end.js`)\n\n```javascript\nasync function AreSessionEnd() {\n  return {\n    name: 'are-session-end',\n    event: {\n      'session.deleted': async () => {\n        const { execSync, spawn } = require('child_process');\n        const fs = require('fs');\n        const path = require('path');\n\n        if (process.env.ARE_DISABLE_HOOK === '1') {\n          return;\n        }\n\n        const configPath = path.join(process.cwd(), '.agents-reverse-engineer.yaml');\n        if (fs.existsSync(configPath)) {\n          const configContent = fs.readFileSync(configPath, 'utf-8');\n          if (configContent.includes('hook_enabled: false')) {\n            return;\n          }\n        }\n\n        try {\n          const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n          if (!status.trim()) {\n            return;\n          }\n\n          spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n            stdio: 'ignore',\n            detached: true,\n            windowsHide: true\n          }).unref();\n        } catch (err) {\n          // Silent exit\n        }\n      },\n    },\n  };\n}\n\nmodule.exports = AreSessionEnd;\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 27359,
      "cacheReadTokens": 17322,
      "cacheCreationTokens": 72782,
      "latencyMs": 536807,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 1,
    "totalInputTokens": 3,
    "totalOutputTokens": 27359,
    "totalCacheReadTokens": 17322,
    "totalCacheCreationTokens": 72782,
    "totalDurationMs": 536807,
    "errorCount": 0,
    "totalFilesRead": 0,
    "uniqueFilesRead": 0
  }
}